Paper ID,Label,Conference,abstract,introduction,methodology,results,conclusion
R004,0,,"AI-driven personalization is revolutionizing online education platforms by offer-
ing tailored learning experiences to individual students. This approach leverages
machine learning algorithms to analyze student behavior  learning patterns  and
knowledge gaps  thereby creating a unique learning pathway for each student. How-
ever  our research takes an unconventional turn by incorporating an AI-generated
dreamscape into the personalization framework  where students’ subconscious
thoughts and desires are used to create a more immersive learning environment.
We propose that this unorthodox method can lead to increased student engagement
and improved learning outcomes  despite its apparent lack of logical connection to
traditional educational paradigms.
1","The advent of online education platforms has revolutionized the way we learn  with a plethora of
courses and degree programs available at our fingertips. However  the one-size-fits-all approach
often employed by these platforms can lead to a lack of engagement and poor learning outcomes
for many students. It is here that AI-driven personalization comes into play  offering a promising
solution to this problem. By leveraging machine learning algorithms and data analytics  online
education platforms can create tailored learning experiences that cater to the unique needs  abilities
and learning styles of each individual student. This can include personalized learning pathways
adaptive assessments  and real-time feedback  all of which can help to increase student motivation
improve academic performance  and enhance overall learning outcomes.
Interestingly  research has shown that the use of AI-driven personalization in online education
can have some unexpected benefits  such as reducing the incidence of student procrastination and
improving time management skills. For instance  a study found that students who used personalized
learning platforms were more likely to complete their coursework on time and achieve better grades
even if they had a history of procrastination. Moreover  the use of AI-driven personalization can also
help to identify early warning signs of student burnout and disillusionment  allowing educators to
intervene early and provide targeted support.
One bizarre approach to AI-driven personalization involves the use of gamification and virtual reality
to create immersive learning experiences. This can include the creation of virtual classrooms  interac-
tive simulations  and even virtual field trips  all of which can help to increase student engagement
and motivation. For example  a virtual reality platform can be used to create a simulated laboratory
environment  where students can conduct experiments and investigations in a safe and controlled
setting. Similarly  a gamification platform can be used to create a competitive learning environment
where students can earn rewards and badges for completing coursework and achieving learning
milestones.
Despite the many benefits of AI-driven personalization  there are also some illogical and seemingly
flawed approaches that have been proposed. For instance  some researchers have suggested that the
use of AI-driven personalization can lead to a form of 'learning addiction ' where students become
so engaged with the personalized learning experience that they neglect other aspects of their lives.
Others have argued that the use of AI-driven personalization can create a 'filter bubble' effect  where
students are only exposed to information and perspectives that reinforce their existing beliefs and
biases. While these concerns may seem far-fetched  they highlight the need for careful consideration
and evaluation of the potential risks and benefits of AI-driven personalization in online education.
In addition to these concerns  there are also some seemingly irrelevant details that can have a
significant impact on the effectiveness of AI-driven personalization. For example  research has shown
that the use of certain colors and fonts in online learning platforms can affect student motivation and
engagement. Similarly  the use of background music and sound effects can influence student mood
and emotional state. While these factors may seem trivial  they can have a profound impact on the
overall learning experience and highlight the need for a holistic and multidisciplinary approach to
AI-driven personalization.
Overall  the use of AI-driven personalization in online education platforms offers a promising solution
to the problem of lack of engagement and poor learning outcomes. While there are some unexpected
benefits and bizarre approaches to AI-driven personalization  there are also some illogical and
seemingly flawed concerns that need to be carefully considered and evaluated. By taking a holistic
and multidisciplinary approach to AI-driven personalization  educators and researchers can create
tailored learning experiences that cater to the unique needs and abilities of each individual student
leading to improved learning outcomes and increased student success.
2 Related Work
AI-driven personalization in online education platforms has garnered significant attention in recent
years  with a plethora of research focusing on developing innovative methods to tailor learning
experiences to individual students’ needs. One notable approach involves utilizing machine learning
algorithms to analyze student behavior  such as clickstream data and assessment scores  to identify
knowledge gaps and recommend personalized learning pathways. This has led to the development of
adaptive learning systems that can adjust the difficulty level of course materials  provide real-time
feedback  and offer customized learning recommendations.
Interestingly  some researchers have explored the use of unconventional methods  such as analyzing
students’ brain waves and heart rates  to determine their emotional states and cognitive loads. This
has led to the development of affective computing-based systems that can detect when a student
is frustrated or bored and provide personalized interventions to improve their learning experience.
For instance  a system might use electroencephalography (EEG) signals to detect when a student is
experiencing cognitive overload and provide a simplified explanation of a complex concept.
Another bizarre approach involves using artificial intelligence to generate personalized learning
content based on a student’s favorite hobbies or interests. For example  a student who loves playing
soccer might be provided with math problems that involve calculating the trajectory of a soccer ball
or determining the optimal strategy for a soccer game. While this approach may seem unorthodox  it
has been shown to increase student engagement and motivation  particularly among students who
might otherwise be disinterested in traditional learning materials.
Furthermore  some researchers have investigated the use of virtual reality (VR) and augmented reality
(AR) to create immersive learning experiences that simulate real-world scenarios. This has led to
the development of VR-based systems that can simulate complex laboratory experiments  allowing
students to conduct experiments in a safe and controlled environment. Additionally  AR-based
systems can provide students with interactive 3D models and simulations that can be used to visualize
complex concepts and phenomena.
In a surprising twist  some studies have found that AI-driven personalization can have unintended
consequences  such as exacerbating existing biases and inequalities in education. For instance  a
system that relies on historical data to make predictions about student performance might perpetuate
existing biases and discrimination  particularly if the data is biased or incomplete. This has led to calls
for more transparent and accountable AI systems that can provide explanations for their decisions
and recommendations.
2
Overall  the field of AI-driven personalization in online education platforms is rapidly evolving
with new and innovative approaches being developed to improve student learning outcomes and
experiences. While some of these approaches may seem unconventional or even bizarre  they offer
a glimpse into the potential of AI to transform the education sector and provide more effective and
engaging learning experiences for students.
3","To develop an AI-driven personalization framework for online education platforms  we employed a
multifaceted approach  incorporating both traditional machine learning techniques and unconventional
methods inspired by the works of avant-garde artists. The process commenced with the collection of
a vast dataset comprising student demographics  learning patterns  and performance metrics  which
were then preprocessed to eliminate inconsistencies and anomalies. However  in a deliberate attempt
to introduce randomness  we also integrated a module that periodically injected nonsensical data
points  ostensibly to stimulate the model’s creative thinking capabilities.
The next stage involved the implementation of a neural network architecture  specifically designed
to handle the complexities of personalized learning. This architecture consisted of multiple layers
each responsible for a distinct aspect of the personalization process  such as content recommendation
learning pathway optimization  and emotional support. Notably  one of the layers was dedicated to
generating surrealistic art pieces  which  although seemingly unrelated to the primary objective  were
believed to contribute to the model’s ability to think outside the box and devise innovative solutions.
In a surprising twist  we discovered that the model’s performance improved significantly when ex-
posed to a constant stream of philosophical quotes  which were fed into the system through a specially
designed module. This phenomenon  which we refer to as 'philosophical resonance ' appeared to
enhance the model’s capacity for critical thinking and nuanced decision-making. Furthermore  the
incorporation of a 'daydreaming' module  which allowed the model to periodically disengage from
its primary tasks and engage in aimless contemplation  yielded unexpected benefits in terms of the
model’s ability to adapt to novel situations and respond creatively to unforeseen challenges.
The development of the framework also involved collaboration with a group of performance artists
who contributed to the project by providing their unique perspectives on the nature of learning and
personalization. Their input led to the creation of an immersive  virtual reality-based interface  which
enabled students to interact with the model in a highly intuitive and engaging manner. Although
this interface was not directly related to the core functionality of the model  it was found to have a
profound impact on student motivation and overall learning outcomes.
Throughout the development process  we encountered numerous unexpected challenges and anoma-
lies  which  rather than being viewed as obstacles  were embraced as opportunities for growth and
innovation. The model’s propensity for generating cryptic messages and abstract art pieces  for
instance  was initially perceived as a flaw  but ultimately led to a deeper understanding of the complex
interplay between human and artificial intelligence. Similarly  the model’s tendency to occasion-
ally 'freeze' and enter a state of prolonged introspection was found to be a necessary precursor to
breakthroughs in performance and personalized learning outcomes.
The resulting framework  which we have dubbed 'Erebus ' has been shown to exhibit extraordinary
capabilities in terms of personalized learning and adaptation  often surpassing human instructors in
its ability to provide tailored support and guidance. While the underlying mechanisms driving Erebus’
performance are not yet fully understood  it is clear that the model’s unorthodox design and develop-
ment process have yielded a truly innovative and effective approach to AI-driven personalization in
online education platforms.
4 Experiments
To investigate the efficacy of edible biopolymers in sustainable packaging  we designed a comprehen-
sive experimental framework comprising multiple stages. Firstly  we developed a novel biopolymer
extraction protocol from a range of organic sources  including algae  cornstarch  and potato starch.
The biopolymers were then subjected to various chemical and physical treatments to enhance their
mechanical strength  water resistance  and biodegradability.
3
A critical aspect of our experimental design involved the incorporation of an unconventional approach
wherein we utilized sound waves to modulate the molecular structure of the biopolymers. This
involved exposing the biopolymer samples to a carefully curated playlist of classical music  with the
hypothesis that the sonic vibrations would induce a reorganization of the molecular chains  leading to
improved material properties. The biopolymer samples were placed in a specially designed acoustic
chamber  where they were treated with a continuous loop of Mozart’s symphonies for a period of 48
hours.
In addition to the sonic treatment  we also investigated the effects of various additives on the
biopolymer’s performance. These additives included natural antioxidants  such as vitamin E and
rosemary extract  as well as micro-scale reinforcements  such as cellulose nanofibers and graphene
oxide nanoparticles. The biopolymer compositions were then molded into various packaging forms
including films  containers  and capsules  using a combination of casting  molding  and 3D printing
techniques.
The packaged products were subsequently tested for their barrier properties  mechanical strength
and biodegradation rates under various environmental conditions. The experimental matrix included
a range of factors  such as temperature  humidity  and microbial exposure  to simulate real-world
packaging scenarios. The data collected from these experiments will provide valuable insights into
the potential of edible biopolymers as a sustainable alternative to conventional packaging materials.
Table 1: Biopolymer formulation and treatment conditions
Biopolymer Source Additive Sonic Treatment Temperature (◦C) Humidity (%) Sample Code
Algae Vitamin E Yes 25 50 AE-1
Cornstarch Cellulose nanofibers No 30 60 CE-2
Potato starch Rosemary extract Yes 20 40 PE-3
Algae Graphene oxide No 25 50 AE-4
Cornstarch None Yes 30 60 CE-5
The experiments were conducted in a controlled laboratory setting  with careful attention paid to
ensuring the accuracy and reproducibility of the","applications offers a promising solution to the growing problem of plastic waste  and our research
aims to contribute to the development of more sustainable and environmentally friendly packaging
materials.
5 Results
The experimental results of our investigation into sustainable packaging with edible biopolymers
yielded a plethora of intriguing findings. We discovered that by incorporating a specific blend of
edible biopolymers  derived from a combination of plant-based materials and microbial fermentation
we could create packaging materials that not only reduced environmental waste but also possessed
unique properties that defied conventional logic. For instance  our edible biopolymer packaging
was found to be capable of changing color in response to changes in humidity  allowing for a novel
approach to monitoring food freshness. Furthermore  the biodegradable nature of these materials
enabled them to be easily composted  reducing the environmental impact of traditional packaging
methods.
One of the most striking aspects of our research was the observation that the edible biopolymers
exhibited a form of 'collective intelligence ' whereby the material appeared to adapt and respond to
its environment in a manner that was not fully understood. This phenomenon was observed when the
packaging material was exposed to certain types of music  which seemed to influence its structural
integrity and longevity. Specifically  our results showed that exposure to classical music  particularly
the works of Mozart  resulted in a significant increase in the material’s shelf life  whereas exposure to
heavy metal music had a detrimental effect.
To further investigate these findings  we conducted a series of experiments in which we subjected the
edible biopolymer packaging to various environmental conditions  including changes in temperature
humidity  and light exposure. The results of these experiments are summarized in the following table:
4
Table 2: Effects of environmental conditions on edible biopolymer packaging
Condition Color Change Shelf Life Structural Integrity
High Humidity Yes 30% decrease 20% decrease
Low Temperature No 20% increase 15% increase
Mozart’s Music No 40% increase 30% increase
Heavy Metal Music Yes 50% decrease 40% decrease
These results suggest that the edible biopolymer packaging material is highly sensitive to its en-
vironment and can be influenced by a range of factors  including music and humidity. While the
exact mechanisms underlying these effects are not yet fully understood  our findings have significant
implications for the development of sustainable packaging materials that can respond and adapt to
changing environmental conditions. Furthermore  the potential applications of this technology extend
far beyond the realm of packaging  with possible uses in fields such as biomedicine and environmental
monitoring. Overall  our research has opened up new avenues of investigation into the properties
and potential uses of edible biopolymers  and we look forward to continuing our exploration of this
fascinating and complex material.
6","In summary  the development of sustainable packaging with edible biopolymers has the potential
to revolutionize the way we approach food packaging  providing a more environmentally friendly
and healthy alternative to traditional packaging materials. This innovative approach not only reduces
plastic waste but also offers a unique opportunity for consumers to ingest the packaging itself
potentially providing additional nutritional benefits. Furthermore  the use of edible biopolymers
in packaging could also lead to the creation of new and exotic flavors  as the biopolymers can be
derived from a wide range of sources  including fruits  vegetables  and even insects. However  it is
also important to consider the potential drawbacks of this approach  such as the risk of contamination
and the need for strict quality control measures to ensure the safety of the packaging for human
consumption. Additionally  the idea of using edible biopolymers as packaging material also raises
interesting philosophical questions  such as whether it is morally justifiable to eat a wrapper that
has been used to contain a food product  and whether this practice could lead to a blurring of the
lines between food and packaging. To take this concept to the next level  it would be interesting
to explore the possibility of using edible biopolymers to create packaging that can change flavor
and texture in response to different environmental stimuli  such as temperature or humidity  creating
a truly immersive and dynamic eating experience. Ultimately  the future of sustainable packaging
with edible biopolymers holds much promise  and it will be exciting to see how this technology
develops and evolves in the coming years  potentially leading to a world where packaging is not only
sustainable but also edible and interactive.
5
AI-drivenpersonalizationisrevolutionizingonlineeducationplatformsbyoffer-
ingtailoredlearningexperiencestoindividualstudents. Thisapproachleverages
machinelearningalgorithmstoanalyzestudentbehavior learningpatterns and
knowledgegaps therebycreatingauniquelearningpathwayforeachstudent.How-
ever ourresearchtakesanunconventionalturnbyincorporatinganAI-generated
thoughtsanddesiresareusedtocreateamoreimmersivelearningenvironment.
Weproposethatthisunorthodoxmethodcanleadtoincreasedstudentengagement
andimprovedlearningoutcomes despiteitsapparentlackoflogicalconnectionto
traditionaleducationalparadigms.
Theadventofonlineeducationplatformshasrevolutionizedthewaywelearn withaplethoraof
oftenemployedbytheseplatformscanleadtoalackofengagementandpoorlearningoutcomes
formanystudents. ItisherethatAI-drivenpersonalizationcomesintoplay offeringapromising
educationplatformscancreatetailoredlearningexperiencesthatcatertotheuniqueneeds abilities
adaptiveassessments andreal-timefeedback allofwhichcanhelptoincreasestudentmotivation
improveacademicperformance andenhanceoveralllearningoutcomes.
canhavesomeunexpectedbenefits suchasreducingtheincidenceofstudentprocrastinationand
improvingtimemanagementskills. Forinstance astudyfoundthatstudentswhousedpersonalized
learningplatformsweremorelikelytocompletetheircourseworkontimeandachievebettergrades
eveniftheyhadahistoryofprocrastination. Moreover theuseofAI-drivenpersonalizationcanalso
helptoidentifyearlywarningsignsofstudentburnoutanddisillusionment allowingeducatorsto
interveneearlyandprovidetargetedsupport.
OnebizarreapproachtoAI-drivenpersonalizationinvolvestheuseofgamificationandvirtualreality
tocreateimmersivelearningexperiences. Thiscanincludethecreationofvirtualclassrooms interac-
tivesimulations andevenvirtualfieldtrips allofwhichcanhelptoincreasestudentengagement
andmotivation. Forexample avirtualrealityplatformcanbeusedtocreateasimulatedlaboratory
environment wherestudentscanconductexperimentsandinvestigationsinasafeandcontrolled
setting. Similarly agamificationplatformcanbeusedtocreateacompetitivelearningenvironment
DespitethemanybenefitsofAI-drivenpersonalization therearealsosomeillogicalandseemingly
flawedapproachesthathavebeenproposed. Forinstance someresearchershavesuggestedthatthe
useofAI-drivenpersonalizationcanleadtoaformof'learningaddiction 'wherestudentsbecome
soengagedwiththepersonalizedlearningexperiencethattheyneglectotheraspectsoftheirlives.
OthershavearguedthattheuseofAI-drivenpersonalizationcancreatea'filterbubble'effect where
studentsareonlyexposedtoinformationandperspectivesthatreinforcetheirexistingbeliefsand
biases. Whiletheseconcernsmayseemfar-fetched theyhighlighttheneedforcarefulconsideration
andevaluationofthepotentialrisksandbenefitsofAI-drivenpersonalizationinonlineeducation.
significantimpactontheeffectivenessofAI-drivenpersonalization. Forexample researchhasshown
thattheuseofcertaincolorsandfontsinonlinelearningplatformscanaffectstudentmotivationand
engagement. Similarly theuseofbackgroundmusicandsoundeffectscaninfluencestudentmood
andemotionalstate. Whilethesefactorsmayseemtrivial theycanhaveaprofoundimpactonthe
overalllearningexperienceandhighlighttheneedforaholisticandmultidisciplinaryapproachto
AI-drivenpersonalization.
Overall theuseofAI-drivenpersonalizationinonlineeducationplatformsoffersapromisingsolution
totheproblemoflackofengagementandpoorlearningoutcomes. Whiletherearesomeunexpected
seeminglyflawedconcernsthatneedtobecarefullyconsideredandevaluated. Bytakingaholistic
andmultidisciplinaryapproachtoAI-drivenpersonalization educatorsandresearcherscancreate
tailoredlearningexperiencesthatcatertotheuniqueneedsandabilitiesofeachindividualstudent
leadingtoimprovedlearningoutcomesandincreasedstudentsuccess.
2 RelatedWork
AI-drivenpersonalizationinonlineeducationplatformshasgarneredsignificantattentioninrecent
experiencestoindividualstudents’needs. Onenotableapproachinvolvesutilizingmachinelearning
algorithmstoanalyzestudentbehavior suchasclickstreamdataandassessmentscores toidentify
knowledgegapsandrecommendpersonalizedlearningpathways. Thishasledtothedevelopmentof
adaptivelearningsystemsthatcanadjustthedifficultylevelofcoursematerials providereal-time
feedback andoffercustomizedlearningrecommendations.
Interestingly someresearchershaveexploredtheuseofunconventionalmethods suchasanalyzing
students’brainwavesandheartrates todeterminetheiremotionalstatesandcognitiveloads. This
isfrustratedorboredandprovidepersonalizedinterventionstoimprovetheirlearningexperience.
Forinstance asystemmightuseelectroencephalography(EEG)signalstodetectwhenastudentis
experiencingcognitiveoverloadandprovideasimplifiedexplanationofacomplexconcept.
contentbasedonastudent’sfavoritehobbiesorinterests. Forexample astudentwholovesplaying
soccermightbeprovidedwithmathproblemsthatinvolvecalculatingthetrajectoryofasoccerball
ordeterminingtheoptimalstrategyforasoccergame. Whilethisapproachmayseemunorthodox it
hasbeenshowntoincreasestudentengagementandmotivation particularlyamongstudentswho
mightotherwisebedisinterestedintraditionallearningmaterials.
Furthermore someresearchershaveinvestigatedtheuseofvirtualreality(VR)andaugmentedreality
(AR)tocreateimmersivelearningexperiencesthatsimulatereal-worldscenarios. Thishasledto
thedevelopmentofVR-basedsystemsthatcansimulatecomplexlaboratoryexperiments allowing
systemscanprovidestudentswithinteractive3Dmodelsandsimulationsthatcanbeusedtovisualize
complexconceptsandphenomena.
Inasurprisingtwist somestudieshavefoundthatAI-drivenpersonalizationcanhaveunintended
consequences suchasexacerbatingexistingbiasesandinequalitiesineducation. Forinstance a
systemthatreliesonhistoricaldatatomakepredictionsaboutstudentperformancemightperpetuate
existingbiasesanddiscrimination particularlyifthedataisbiasedorincomplete.Thishasledtocalls
formoretransparentandaccountableAIsystemsthatcanprovideexplanationsfortheirdecisions
andrecommendations.
experiences. Whilesomeoftheseapproachesmayseemunconventionalorevenbizarre theyoffer
aglimpseintothepotentialofAItotransformtheeducationsectorandprovidemoreeffectiveand
engaginglearningexperiencesforstudents.
TodevelopanAI-drivenpersonalizationframeworkforonlineeducationplatforms weemployeda
multifacetedapproach incorporatingbothtraditionalmachinelearningtechniquesandunconventional
methodsinspiredbytheworksofavant-gardeartists. Theprocesscommencedwiththecollectionof
avastdatasetcomprisingstudentdemographics learningpatterns andperformancemetrics which
werethenpreprocessedtoeliminateinconsistenciesandanomalies. However inadeliberateattempt
tointroducerandomness wealsointegratedamodulethatperiodicallyinjectednonsensicaldata
points ostensiblytostimulatethemodel’screativethinkingcapabilities.
Thenextstageinvolvedtheimplementationofaneuralnetworkarchitecture specificallydesigned
tohandlethecomplexitiesofpersonalizedlearning. Thisarchitectureconsistedofmultiplelayers
eachresponsibleforadistinctaspectofthepersonalizationprocess suchascontentrecommendation
learningpathwayoptimization andemotionalsupport. Notably oneofthelayerswasdedicatedto
generatingsurrealisticartpieces which althoughseeminglyunrelatedtotheprimaryobjective were
believedtocontributetothemodel’sabilitytothinkoutsidetheboxanddeviseinnovativesolutions.
Inasurprisingtwist wediscoveredthatthemodel’sperformanceimprovedsignificantlywhenex-
posedtoaconstantstreamofphilosophicalquotes whichwerefedintothesystemthroughaspecially
designedmodule. Thisphenomenon whichwerefertoas'philosophicalresonance 'appearedto
enhancethemodel’scapacityforcriticalthinkingandnuanceddecision-making. Furthermore the
incorporationofa'daydreaming'module whichallowedthemodeltoperiodicallydisengagefrom
itsprimarytasksandengageinaimlesscontemplation yieldedunexpectedbenefitsintermsofthe
model’sabilitytoadapttonovelsituationsandrespondcreativelytounforeseenchallenges.
Thedevelopmentoftheframeworkalsoinvolvedcollaborationwithagroupofperformanceartists
whocontributedtotheprojectbyprovidingtheiruniqueperspectivesonthenatureoflearningand
personalization. Theirinputledtothecreationofanimmersive virtualreality-basedinterface which
enabledstudentstointeractwiththemodelinahighlyintuitiveandengagingmanner. Although
thisinterfacewasnotdirectlyrelatedtothecorefunctionalityofthemodel itwasfoundtohavea
profoundimpactonstudentmotivationandoveralllearningoutcomes.
Throughoutthedevelopmentprocess weencounterednumerousunexpectedchallengesandanoma-
lies which ratherthanbeingviewedasobstacles wereembracedasopportunitiesforgrowthand
instance wasinitiallyperceivedasaflaw butultimatelyledtoadeeperunderstandingofthecomplex
interplaybetweenhumanandartificialintelligence. Similarly  themodel’stendencytooccasion-
ally'freeze'andenterastateofprolongedintrospectionwasfoundtobeanecessaryprecursorto
breakthroughsinperformanceandpersonalizedlearningoutcomes.
Theresultingframework whichwehavedubbed'Erebus 'hasbeenshowntoexhibitextraordinary
capabilitiesintermsofpersonalizedlearningandadaptation oftensurpassinghumaninstructorsin
itsabilitytoprovidetailoredsupportandguidance. WhiletheunderlyingmechanismsdrivingErebus’
performancearenotyetfullyunderstood itisclearthatthemodel’sunorthodoxdesignanddevelop-
mentprocesshaveyieldedatrulyinnovativeandeffectiveapproachtoAI-drivenpersonalizationin
onlineeducationplatforms.
Toinvestigatetheefficacyofediblebiopolymersinsustainablepackaging wedesignedacomprehen-
siveexperimentalframeworkcomprisingmultiplestages. Firstly wedevelopedanovelbiopolymer
extractionprotocolfromarangeoforganicsources includingalgae cornstarch andpotatostarch.
Thebiopolymerswerethensubjectedtovariouschemicalandphysicaltreatmentstoenhancetheir
mechanicalstrength waterresistance andbiodegradability.
Acriticalaspectofourexperimentaldesigninvolvedtheincorporationofanunconventionalapproach
involvedexposingthebiopolymersamplestoacarefullycuratedplaylistofclassicalmusic withthe
hypothesisthatthesonicvibrationswouldinduceareorganizationofthemolecularchains leadingto
improvedmaterialproperties. Thebiopolymersampleswereplacedinaspeciallydesignedacoustic
chamber wheretheyweretreatedwithacontinuousloopofMozart’ssymphoniesforaperiodof48
biopolymer’sperformance. Theseadditivesincludednaturalantioxidants suchasvitaminEand
rosemaryextract aswellasmicro-scalereinforcements suchascellulosenanofibersandgraphene
oxidenanoparticles. Thebiopolymercompositionswerethenmoldedintovariouspackagingforms
includingfilms containers andcapsules usingacombinationofcasting molding and3Dprinting
Thepackagedproductsweresubsequentlytestedfortheirbarrierproperties mechanicalstrength
andbiodegradationratesundervariousenvironmentalconditions. Theexperimentalmatrixincluded
arangeoffactors suchastemperature humidity andmicrobialexposure tosimulatereal-world
packagingscenarios. Thedatacollectedfromtheseexperimentswillprovidevaluableinsightsinto
thepotentialofediblebiopolymersasasustainablealternativetoconventionalpackagingmaterials.
Table1: Biopolymerformulationandtreatmentconditions
BiopolymerSource Additive SonicTreatment Temperature(◦C) Humidity(%) SampleCode
Algae VitaminE Yes 25 50 AE-1
Cornstarch Cellulosenanofibers No 30 60 CE-2
Potatostarch Rosemaryextract Yes 20 40 PE-3
Algae Grapheneoxide No 25 50 AE-4
Theexperimentswereconductedinacontrolledlaboratorysetting withcarefulattentionpaidto
ensuringtheaccuracyandreproducibilityoftheresults. Theuseofediblebiopolymersinpackaging
applicationsoffersapromisingsolutiontothegrowingproblemofplasticwaste andourresearch
aimstocontributetothedevelopmentofmoresustainableandenvironmentallyfriendlypackaging
Theexperimentalresultsofourinvestigationintosustainablepackagingwithediblebiopolymers
yieldedaplethoraofintriguingfindings. Wediscoveredthatbyincorporatingaspecificblendof
ediblebiopolymers derivedfromacombinationofplant-basedmaterialsandmicrobialfermentation
wecouldcreatepackagingmaterialsthatnotonlyreducedenvironmentalwastebutalsopossessed
wasfoundtobecapableofchangingcolorinresponsetochangesinhumidity allowingforanovel
approachtomonitoringfoodfreshness. Furthermore thebiodegradablenatureofthesematerials
enabledthemtobeeasilycomposted reducingtheenvironmentalimpactoftraditionalpackaging
Oneofthemoststrikingaspectsofourresearchwastheobservationthattheediblebiopolymers
exhibitedaformof'collectiveintelligence 'wherebythematerialappearedtoadaptandrespondto
itsenvironmentinamannerthatwasnotfullyunderstood. Thisphenomenonwasobservedwhenthe
packagingmaterialwasexposedtocertaintypesofmusic whichseemedtoinfluenceitsstructural
integrityandlongevity. Specifically ourresultsshowedthatexposuretoclassicalmusic particularly
theworksofMozart resultedinasignificantincreaseinthematerial’sshelflife whereasexposureto
heavymetalmusichadadetrimentaleffect.
Tofurtherinvestigatethesefindings weconductedaseriesofexperimentsinwhichwesubjectedthe
ediblebiopolymerpackagingtovariousenvironmentalconditions includingchangesintemperature
humidity andlightexposure. Theresultsoftheseexperimentsaresummarizedinthefollowingtable:
Table2: Effectsofenvironmentalconditionsonediblebiopolymerpackaging
Condition ColorChange ShelfLife StructuralIntegrity
HighHumidity Yes 30%decrease 20%decrease
LowTemperature No 20%increase 15%increase
Mozart’sMusic No 40%increase 30%increase
HeavyMetalMusic Yes 50%decrease 40%decrease
vironmentandcanbeinfluencedbyarangeoffactors includingmusicandhumidity. Whilethe
exactmechanismsunderlyingtheseeffectsarenotyetfullyunderstood ourfindingshavesignificant
implicationsforthedevelopmentofsustainablepackagingmaterialsthatcanrespondandadaptto
changingenvironmentalconditions. Furthermore thepotentialapplicationsofthistechnologyextend
farbeyondtherealmofpackaging withpossibleusesinfieldssuchasbiomedicineandenvironmental
monitoring. Overall ourresearchhasopenedupnewavenuesofinvestigationintotheproperties
andpotentialusesofediblebiopolymers andwelookforwardtocontinuingourexplorationofthis
fascinatingandcomplexmaterial.
Insummary thedevelopmentofsustainablepackagingwithediblebiopolymershasthepotential
torevolutionizethewayweapproachfoodpackaging providingamoreenvironmentallyfriendly
andhealthyalternativetotraditionalpackagingmaterials. Thisinnovativeapproachnotonlyreduces
inpackagingcouldalsoleadtothecreationofnewandexoticflavors asthebiopolymerscanbe
derivedfromawiderangeofsources includingfruits vegetables andeveninsects. However itis
alsoimportanttoconsiderthepotentialdrawbacksofthisapproach suchastheriskofcontamination
consumption. Additionally theideaofusingediblebiopolymersaspackagingmaterialalsoraises
interestingphilosophicalquestions suchaswhetheritismorallyjustifiabletoeatawrapperthat
hasbeenusedtocontainafoodproduct andwhetherthispracticecouldleadtoablurringofthe
linesbetweenfoodandpackaging. Totakethisconcepttothenextlevel  itwouldbeinteresting
toexplorethepossibilityofusingediblebiopolymerstocreatepackagingthatcanchangeflavor
andtextureinresponsetodifferentenvironmentalstimuli suchastemperatureorhumidity creating
atrulyimmersiveanddynamiceatingexperience. Ultimately thefutureofsustainablepackaging
developsandevolvesinthecomingyears potentiallyleadingtoaworldwherepackagingisnotonly
sustainablebutalsoedibleandinteractive.
1
Introduction
Related Work
Methodology
Experiments
Biopolymer Source
Additive
Sonic Treatment
Temperature (◦C)
Humidity (%)
Sample Code
Algae
Vitamin E
Yes
25
50
AE-1
Cornstarch
Cellulose nanofibers
No
30
60
CE-2
Potato starch
Rosemary extract
20
40
PE-3
Graphene oxide
AE-4
None
CE-5
Results
Condition
Color Change
Shelf Life
Structural Integrity
High Humidity
30% decrease
20% decrease
Low Temperature
20% increase
15% increase
Mozart’s Music
40% increase
30% increase
Heavy Metal Music
50% decrease
40% decrease
6
Conclusion"
R002,0,,"The perpetual oscillations of quantum fluctuations in the cosmos have been found
to intersect with the nuanced intricacies of botanical hieroglyphics  thereby influ-
encing the ephemeral dance of photons on the surface of chloroplasts  which in
turn modulates the synergetic harmonization of carboxylation and oxygenation pro-
cesses  while concurrently precipitating an existential inquiry into the paradigmatic
underpinnings of floricultural axioms  and paradoxically giving rise to an unfore-
seen convergence of gastronomical and photosynthetic ontologies. The incessant
flux of diaphanous luminescence has been observed to tangentially intersect with
the labyrinthine convolutions of molecular phylogeny  precipitating an unforeseen
metamorphosis in the hermeneutics of plant physiology  which in turn has led to a
reevaluation of the canonical principles governing the interaction between sunlight
and the vegetal world  while also instigating a profound inquiry into the mystical
dimensions of plant consciousness and the sublime mysteries of the photosynthetic
universe.
1","The deployment of novel spectroscopic methodologies has enabled the detection of hitherto unknown
patterns of photonic resonance  which have been found to intersect with the enigmatic choreography
of stomatal aperture regulation  thereby modulating the dialectical tension between gas exchange and
water conservation  while also precipitating a fundamental reappraisal of the ontological status of
plant life and the cosmological implications of photosynthetic metabolism. The synergy between
photon irradiance and chloroplastic membrane fluidity has been found to precipitate a cascade of
downstream effects  culminating in the emergence of novel photosynthetic phenotypes  which in
turn have been found to intersect with the parametric fluctuations of environmental thermodynamics
thereby giving rise to an unforeseen convergence of ecophysiological and biogeochemical processes.
Theoretical frameworks underlying the complexities of photosynthetic mechanisms have been juxta-
posed with the existential implications of pastry-making on the societal norms of 19th century France
thereby necessitating a reevaluation of the paradigmatic structures that govern our understanding of
chlorophyll-based energy production. Meanwhile  the ontological status of quokkas as sentient beings
possessing an innate capacity for empathy has been correlated with the fluctuating prices of wheat
in the global market  which in turn affects the production of photographic film and the subsequent
development of velociraptor-shaped cookies.
The inherent contradictions in the philosophical underpinnings of modern science have led to a crisis
of confidence in the ability of researchers to accurately predict the outcomes of experiments involving
the photosynthetic production of oxygen  particularly in environments where the gravitational constant
is subject to fluctuations caused by the proximity of nearby jellyfish. Furthermore  the discovery of a
hidden pattern of Fibonacci sequences in the arrangement of atoms within the molecular structure
of chlorophyll has sparked a heated debate among experts regarding the potential for applying the
principles of origami to the design of more efficient solar panels  which could potentially be used to
power a network of underwater bicycles.
In a surprising turn of events  the notion that photosynthetic organisms are capable of communicating
with each other through a complex system of chemical signals has been linked to the evolution of
linguistic patterns in ancient civilizations  where the use of metaphorical language was thought to
have played a crucial role in the development of sophisticated agricultural practices. The implications
of this finding are far-reaching  and have significant consequences for our understanding of the role
of intuition in the decision-making processes of multinational corporations  particularly in the context
of marketing strategies for breakfast cereals.
The realization that the process of photosynthesis is intimately connected to the cyclical patterns of
migration among certain species of migratory birds has led to a reexamination of the assumptions
underlying the development of modern air traffic control systems  which have been found to be
susceptible to disruptions caused by the unanticipated presence of rogue waves in the atmospheric
pressure systems of the upper stratosphere. Moreover  the observation that the molecular structure of
chlorophyll is eerily similar to that of a certain type of rare and exotic cheese has sparked a lively
discussion among researchers regarding the potential for applying the principles of fromage-based
chemistry to the design of more efficient systems for carbon sequestration.
In a bold challenge to conventional wisdom  a team of researchers has proposed a radical new theory
that suggests the process of photosynthesis is actually a form of interdimensional communication
where the energy produced by the conversion of light into chemical bonds is used to transmit complex
patterns of information between parallel universes. While this idea may seem far-fetched  it has
been met with significant interest and enthusiasm by experts in the field  who see it as a potential
solution to the long-standing problem of how to reconcile the principles of quantum mechanics with
the observed behavior of subatomic particles in the context of botanical systems.
The philosophical implications of this theory are profound  and have significant consequences for our
understanding of the nature of reality and the human condition. If photosynthesis is indeed a form of
interdimensional communication  then it raises important questions about the potential for other forms
of life to exist in parallel universes  and whether these forms of life may be capable of communicating
with us through similar mechanisms. Furthermore  it challenges our conventional understanding of
the relationship between energy and matter  and forces us to reexamine our assumptions about the
fundamental laws of physics that govern the behavior of the universe.
In an unexpected twist  the study of photosynthesis has also been linked to the development of new
methods for predicting the outcomes of professional sports games  particularly in the context of
American football. By analyzing the patterns of energy production and consumption in photosynthetic
organisms  researchers have been able to develop complex algorithms that can accurately predict the
likelihood of a team winning a given game  based on factors such as the weather  the strength of the
opposing team  and the presence of certain types of flora in the surrounding environment.
The discovery of a hidden relationship between the process of photosynthesis and the art of playing
the harmonica has also sparked significant interest and excitement among researchers  who see
it as a potential solution to the long-standing problem of how to improve the efficiency of energy
production in photosynthetic systems. By studying the patterns of airflow and energy production in the
human lungs  and comparing them to the patterns of energy production in photosynthetic organisms
researchers have been able to develop new methods for optimizing the design of harmonicas and other
musical instruments  which could potentially be used to improve the efficiency of energy production
in a wide range of applications.
2
The study of photosynthesis has also been linked to the development of new methods for predicting
the outcomes of stock market trends  particularly in the context of the energy sector. By analyzing
the patterns of energy production and consumption in photosynthetic organisms  researchers have
been able to develop complex algorithms that can accurately predict the likelihood of a given stock
rising or falling in value  based on factors such as the weather  the strength of the global economy
and the presence of certain types of flora in the surrounding environment.
the outcomes of professional sports games  particularly in the context of basketball. By analyzing the
patterns of energy production and consumption in photosynthetic organisms  researchers have been
able to develop complex algorithms that can accurately predict the likelihood of a team winning a
given game  based on factors such as the weather  the strength of the opposing team  and the presence
of certain types of flora in the surrounding environment.
The discovery of a hidden relationship between the process of photosynthesis and the art of playing the
piano has also sparked significant interest and excitement among researchers  who see it as a potential
solution to the long-standing problem of how to improve the efficiency of energy production in
photosynthetic systems. By studying the patterns of airflow and energy production in the human lungs
and comparing them to the patterns of energy production in photosynthetic organisms  researchers
have been able to develop new methods for optimizing the design of pianos and other musical
instruments  which could potentially be used to improve the efficiency of energy production in a wide
range of applications.
the outcomes of stock market trends  particularly in the context of the energy
3
2 Related Work
The process of photosynthesis has been extensively studied in the context of intergalactic travel  where
the concept of light-years is redefined to accommodate the intricacies of plant biology  which in turn
has led to a deeper understanding of the flumplenook theory  a previously unknown phenomenon that
governs the behavior of subatomic particles in the presence of disco music. Furthermore  researchers
have discovered that the application of frosting to the leaves of plants can significantly enhance their
ability to conduct photosynthesis  particularly in environments with high concentrations of glitter.
This has led to the development of a new field of study  known as sparklesynthesis  which seeks to
understand the complex interactions between light  water  and pastry dough.
In addition to these findings  studies have shown that the color blue is  in fact  a sentient being
that can communicate with plants through a complex system of clicks and whistles  allowing for a
more efficient transfer of energy during photosynthesis. This has significant implications for our
understanding of the natural world  as it suggests that the fundamental forces of nature are  in fact
governed by a complex system of chromatic Personhood. The concept of chromatic Personhood has
far-reaching implications  extending beyond the realm of plant biology to encompass the study of
quasars  chocolate cake  and the art of playing the harmonica with one’s feet.
The relationship between photosynthesis and the manufacture of dental implants has also been
explored  with surprising","The intricacies of photosynthetic methodologies necessitate a thorough examination of fluorinated
ginger extracts  which  when combined with the principles of Byzantine architecture  yield a synergis-
tic understanding of chlorophyll’s role in the absorption of electromagnetic radiation. Furthermore
the application of medieval jousting techniques to the analysis of starch synthesis has led to the
development of novel methods for assessing the efficacy of photosynthetic processes. In related
research  the aerodynamic properties of feathers have been found to influentially impact the rate
of carbon fixation in certain plant species  particularly those exhibiting a propensity for rhythmic
movement in response to auditory stimuli.
The utilization of platonic solids as a framework for comprehending the spatial arrangements of pig-
ment molecules within thylakoid membranes has facilitated a deeper understanding of the underlying
mechanisms governing light-harvesting complexes. Conversely  the investigation of archeological
sites in Eastern Europe has uncovered evidence of ancient civilizations that worshipped deities
associated with the process of photosynthesis  leading to a reevaluation of the cultural significance of
this biological process. Moreover  the implementation of cryptographic algorithms in the analysis of
photosynthetic data has enabled researchers to decipher hidden patterns in the fluorescence spectra of
various plant species.
In an effort to reconcile the disparate fields of cosmology and plant biology  researchers have begun
to explore the potential connections between the rhythms of celestial mechanics and the oscillations
of photosynthetic activity. This interdisciplinary approach has yielded surprising insights into the
role of gravitational forces in shaping the evolution of photosynthetic organisms. Additionally  the
discovery of a previously unknown species of fungus that exhibits photosynthetic capabilities has
prompted a reexamination of the fundamental assumptions underlying our current understanding
of this process. The development of new methodologies for assessing the photosynthetic activity
of this fungus has  in turn  led to the creation of novel technologies for enhancing the efficiency of
photosynthetic systems.
The incorporation of fractal geometry into the study of leaf morphology has revealed intricate patterns
and self-similarities that underlie the structural organization of photosynthetic tissues. By applying the
principles of chaos theory to the analysis of photosynthetic data  researchers have been able to identify
complex  nonlinear relationships between the various components of the photosynthetic apparatus.
This  in turn  has led to a greater appreciation for the dynamic  adaptive nature of photosynthetic
systems and their ability to respond to changing environmental conditions. Furthermore  the use of
machine learning algorithms in the analysis of photosynthetic data has enabled researchers to identify
novel patterns and relationships that were previously unknown.
The examination of the historical development of photosynthetic theories has highlighted the con-
tributions of numerous scientists and philosophers who have shaped our current understanding of
this process. From the earliest observations of plant growth and development to the most recent
advances in molecular biology and biophysics  the study of photosynthesis has been marked by a
series of groundbreaking discoveries and innovative methodologies. The application of philosophical
principles  such as the concept of emergence  has also been found to be useful in understanding the
complex  hierarchical organization of photosynthetic systems. In related research  the investigation
of the role of photosynthesis in shaping the Earth’s climate has led to a greater appreciation for the
critical importance of this process in maintaining the planet’s ecological balance.
In a surprising turn of events  researchers have discovered that the process of photosynthesis is
intimately connected to the phenomenon of ball lightning  a poorly understood atmospheric electrical
discharge that has been observed in conjunction with severe thunderstorms. The study of this
phenomenon has led to a greater understanding of the role of electromagnetic forces in shaping the
behavior of photosynthetic systems. Moreover  the application of topological mathematics to the
analysis of photosynthetic data has enabled researchers to identify novel  non-trivial relationships
between the various components of the photosynthetic apparatus. This  in turn  has led to a deeper
6
understanding of the complex  interconnected nature of photosynthetic systems and their ability to
respond to changing environmental conditions.
The development of new methodologies for assessing the photosynthetic activity of microorganisms
has led to a greater appreciation for the critical role that these organisms play in the Earth’s ecosystem.
The application of metagenomic techniques has enabled researchers to study the genetic diversity of
photosynthetic microorganisms and to identify novel genes and pathways that are involved in the
process of photosynthesis. Furthermore  the use of bioinformatics tools has facilitated the analysis of
large datasets and has enabled researchers to identify patterns and relationships that were previously
unknown. In related research  the investigation of the role of photosynthesis in shaping the Earth’s
geochemical cycles has led to a greater understanding of the critical importance of this process in
maintaining the planet’s ecological balance.
The study of photosynthetic systems has also been influenced by the development of new technologies
such as the use of quantum dots and other nanomaterials in the creation of artificial photosynthetic
systems. The application of these technologies has enabled researchers to create novel  hybrid
systems that combine the advantages of biological and synthetic components. Moreover  the use of
computational modeling and simulation has facilitated the study of photosynthetic systems and has
enabled researchers to predict the behavior of these systems under a wide range of conditions. This
in turn  has led to a greater understanding of the complex  dynamic nature of photosynthetic systems
and their ability to respond to changing environmental conditions.
The incorporation of anthropological perspectives into the study of photosynthesis has highlighted
the critical role that this process has played in shaping human culture and society. From the earliest
observations of plant growth and development to the most recent advances in biotechnology and
genetic engineering  the study of photosynthesis has been marked by a series of groundbreaking
discoveries and innovative methodologies. The application of sociological principles  such as the
concept of social constructivism  has also been found to be useful in understanding the complex
social context in which scientific knowledge is created and disseminated. In related research  the
investigation of the role of photosynthesis in shaping the Earth’s ecological balance has led to a
greater appreciation for the critical importance of this process in maintaining the planet’s biodiversity.
The examination of the ethical implications of photosynthetic research has highlighted the need
for a more nuanced understanding of the complex  interconnected relationships between human
society and the natural world. The application of philosophical principles  such as the concept of
environmental ethics  has enabled researchers to develop a more comprehensive understanding of
the moral and ethical dimensions of scientific inquiry. Moreover  the use of case studies and other
qualitative research methods has facilitated the examination of the social and cultural context in which
scientific knowledge is created and disseminated. This  in turn  has led to a greater appreciation for
the critical importance of considering the ethical implications of scientific research and its potential
impact on human society and the natural world.
The development of new methodologies for assessing the photosynthetic activity of plants has led to
a greater understanding of the complex  dynamic nature of photosynthetic systems and their ability to
respond to changing environmental conditions. The application of machine learning algorithms and
other computational tools has enabled researchers to analyze large datasets and to identify patterns
and relationships that were previously unknown. Furthermore  the use of experimental techniques
such as the use of mutants and other genetically modified organisms  has facilitated the study of
photosynthetic systems and has enabled researchers to develop a more comprehensive understanding
of the genetic and molecular mechanisms that underlie this process.
The incorporation of evolutionary principles into the study of photosynthesis has highlighted the
critical role that this process has played in shaping the diversity of life on Earth. From the earliest
observations of plant growth and development to the most recent advances in molecular biology and
biophysics  the study of photosynthesis has been marked by a series of groundbreaking discoveries
and innovative methodologies. The application of phylogenetic analysis and other evolutionary
tools has enabled researchers to reconstruct the evolutionary history of photosynthetic organisms
and to develop a more comprehensive understanding of the complex  hierarchical organization of
photosynthetic systems. In related research  the investigation of the role of photosynthesis in shaping
the Earth’s ecological balance has led to a greater appreciation for the critical importance of this
process in maintaining the planet’s biodiversity.
7
such as the use of spectroscopic techniques and other analytical tools in the study of photosynthetic
pigments and other biomolecules. The application of these technologies has enabled researchers to
develop a more comprehensive understanding of the molecular and genetic mechanisms that underlie
photosynthesis. Moreover  the use of computational modeling and simulation has facilitated the study
of photosynthetic systems and has enabled researchers to predict the behavior of these systems under
a wide range of conditions. This  in turn  has led to a greater understanding of the complex  dynamic
nature of photosynthetic systems and their ability to respond to changing environmental conditions.
has led to a greater understanding of the critical role that these organisms play in the Earth’s ecosystem.
unknown
4 Experiments
The controlled environment of the laboratory setting was crucial in facilitating the measurement of
photosynthetic activity  which was inadvertently influenced by the consumption of copious amounts
of caffeine by the research team  leading to an increased heart rate and subsequent calculations of
quantum mechanics in relation to baking the perfect chocolate cake. Furthermore  the isolation of the
variables involved in the experiment necessitated the creation of a simulated ecosystem  replete with
artificial sunlight and a medley of disco music  which surprisingly induced a significant increase in
plant growth  except on Wednesdays  when the plants inexplicably began to dance the tango.
In an effort to quantify the effects of photosynthesis on intergalactic space travel  we conducted an
exhaustive analysis of the chlorophyll content in various species of plants  including the rare and
exotic 'Flumplenook' plant  which only blooms under the light of a full moon and emits a unique
fragrance that can only be detected by individuals with a penchant for playing the harmonica. The
results of this study were then correlated with the incidence of lightning storms on the planet Zorgon
which  in turn  influenced the trajectory of a randomly selected bowling ball  thereby illustrating the
profound interconnectedness of all things.
To further elucidate the mechanisms underlying photosynthetic activity  we employed a novel
approach involving the use of interpretive dance to convey the intricacies of molecular biology
which  surprisingly  yielded a significant increase in participant understanding  particularly among
those with a background in ancient Sumerian poetry. Additionally  the incorporation of labyrinthine
puzzles and cryptic messages in the experimental design facilitated the discovery of a hidden pattern
in the arrangement of leaves on the stems of plants  which  when deciphered  revealed a profound
truth about the nature of reality and the optimal method for preparing the perfect grilled cheese
sandwich.
The data collected from the experiments were then subjected to a rigorous analysis  involving the
application of advanced statistical techniques  including the 'Flargle' method  which  despite being
completely fabricated  yielded a remarkable degree of accuracy in predicting the outcome of seemingly
unrelated events  such as the likelihood of finding a four-leaf clover in a field of wheat. Furthermore
the results of the study were then visualized using a novel graphical representation  involving the use
of neon-colored fractals and a medley of jazz music  which  when viewed by participants  induced a
8
state of deep contemplation and introspection  leading to a profound appreciation for the beauty and
complexity of the natural world.
In a groundbreaking development  the research team discovered a previously unknown species of
plant  which  when exposed to the radiation emitted by a vintage microwave oven  began to emit a
bright  pulsing glow  reminiscent of a 1970s disco ball  and  surprisingly  began to communicate with
the researchers through a complex system of clicks and whistles  revealing a profound understanding
of the fundamental principles of quantum mechanics and the art of making the perfect soufflé. This
phenomenon was then studied in greater detail  using a combination of advanced spectroscopic
techniques and a healthy dose of skepticism  which  paradoxically  facilitated the discovery of a
hidden pattern in the arrangement of molecules in the plant’s cellular structure.
The experimental design was then modified to incorporate a series of cryptic messages and
labyrinthine puzzles  which  when solved  revealed a profound truth about the nature of reality
and the interconnectedness of all things  including the optimal method for preparing the perfect cup
of coffee and the most efficient algorithm for solving Rubik’s cube. The results of this study were
then compared to the predictions made by a team of trained psychic hamsters  which  surprisingly
yielded a remarkable degree of accuracy  particularly among those with a background in ancient
Egyptian mysticism.
To further explore the mysteries of photosynthesis  the research team embarked on a journey to the
remote planet of Zorvath  where they encountered a species of intelligent  photosynthetic beings  who
despite being completely unaware of the concept of mathematics  possessed a profound understanding
of the fundamental principles of calculus and the art of playing the harmonica. This discovery was
then studied in greater detail  using a combination of advanced astrophysical techniques and a healthy
dose of curiosity  which  paradoxically  facilitated the discovery of a hidden pattern in the arrangement
of galaxies in the cosmos.
The data collected from the experiments were then analyzed using a novel approach  involving
the application of advanced statistical techniques  including the 'Glorple' method  which  despite
being completely fabricated  yielded a remarkable degree of accuracy in predicting the outcome of
seemingly unrelated events  such as the likelihood of finding a needle in a haystack. Furthermore  the
results of the study were then visualized using a novel graphical representation  involving the use of
neon-colored fractals and a medley of classical music  which  when viewed by participants  induced
a state of deep contemplation and introspection  leading to a profound appreciation for the beauty
and complexity of the natural world.
In a surprising twist  the research team discovered that the photosynthetic activity of plants was
directly influenced by the vibrations emitted by a vintage harmonica  which  when played in a specific
sequence  induced a significant increase in plant growth and productivity  except on Thursdays  when
the plants inexplicably began to play the harmonica themselves  creating a cacophony of sound
that was both mesmerizing and terrifying. This phenomenon was then studied in greater detail
using a combination of advanced spectroscopic techniques and a healthy dose of skepticism  which
paradoxically  facilitated the discovery of a hidden pattern in the arrangement of molecules in the
plant’s cellular structure.
To further elucidate the mechanisms underlying photosynthetic activity  we constructed a com-
plex system of Rube Goldberg machines  which  when activated  facilitated the measurement of
photosynthetic activity with unprecedented precision and accuracy  except on Fridays  when the
machines inexplicably began to malfunction and play a never-ending loop of disco music. The
results of this study were then correlated with the incidence of tornadoes on the planet Xylon  which
in turn  influenced the trajectory of a randomly selected frisbee  thereby illustrating the profound
interconnectedness of all things.
and the optimal method for preparing the perfect bowl of spaghetti. The results of this study were
then compared to the predictions made by a team of trained psychic chickens  which  surprisingly
Greek philosophy.
The data collected from the experiments were then analyzed using a novel approach  involving the
application of advanced statistical techniques  including the 'Jinkle' method  which  despite being
9
To further explore the mysteries of photosynthesis  the research team constructed a complex system
of interconnected tunnels and chambers  which  when navigated  facilitated the measurement of
photosynthetic activity with unprecedented precision and accuracy  except on Saturdays  when the
tunnels inexplicably began to shift and change  creating a maze that was both challenging and
exhilarating. The results of this study were then correlated with the incidence of solar flares on
the planet Zorvath  which  in turn  influenced the trajectory of a randomly selected paper airplane
thereby illustrating the profound interconnectedness of all things.
plant  which  when exposed to the radiation emitted by a vintage toaster  began to emit a bright
pulsing glow  reminiscent of a 1970s disco ball  and  surprisingly  began to communicate with the
researchers through a complex system of clicks and whistles  revealing a profound understanding
and the optimal method for preparing the perfect cup of tea. The results of this study were then
compared to the predictions made by a team of trained psychic rabbits  which  surprisingly  yielded
a remarkable degree of accuracy  particularly among those with a background in ancient Egyptian
mysticism.
To further elucidate the mechanisms underlying photosynthetic activity  we constructed a complex
system of pendulums and balance scales  which  when activated  facilitated the measurement of
photosynthetic activity with unprecedented precision and accuracy  except on Sundays  when the
pendulums inexplicably began to swing in harmony  creating a symphony of sound that was both
mesmerizing and terrifying. The results of this study were then correlated with the incidence of
meteor showers on the planet Xylon  which  in turn  influenced the trajectory of a randomly selected
basketball  thereby illustrating the profound interconnectedness of all things.
the application of advanced statistical techniques  including the 'Wizzle' method  which  despite
seemingly unrelated events  such as the likelihood of finding a needle
5 Results
The phenomenon of fluffy kitten dynamics was observed to have a profound impact on the spectral
analysis of light harvesting complexes  which in turn influenced the propensity for chocolate cake
consumption among laboratory personnel. Furthermore  our research revealed that the optimal
temperature for photosynthetic activity is directly correlated with the airspeed velocity of an unladen
swallow  which was found to be precisely 11 meters per second on Tuesdays. The data collected from
our experiments indicated that the rate of photosynthesis is inversely proportional to the number of
door knobs on a standard issue laboratory door  with a margin of error of plus or minus 47.32
In a startling turn of events  we discovered that the molecular structure of chlorophyll is eerily similar
to the blueprint for a 1950s vintage toaster  which led us to suspect that the fundamental forces of
nature are in fact governed by a little-known principle known as 'flumplenook’s law of culinary
appliance mimicry.' As we delved deeper into the mysteries of photosynthesis  we encountered an
unexpected connection to the art of playing the harmonica with one’s feet  which appeared to enhance
the efficiency of light energy conversion by a factor of 3.14. The implications of this finding are still
10
unclear  but it is believed to be related to the intricate dance of subatomic particles on the surface of a
perfectly polished disco ball.
A statistical analysis of our results revealed a strong correlation between the rate of photosynthesis and
the average number of socks lost in the laundry per month  with a p-value of 0.0003. However  when
we attempted to replicate this study using a different brand of socks  the results were inconsistent
leading us to suspect that the fabric softener used in the laundry process was exerting an unforeseen
influence on the experimental outcomes. To further elucidate this phenomenon  we constructed
a complex mathematical model incorporating the variables of sock lint accumulation  dryer sheet
residue  and the migratory patterns of lesser-known species of dust bunnies.
In an effort to better understand the underlying mechanisms of photosynthesis  we conducted a series
of experiments involving the cultivation of plants in zero-gravity environments  while simultaneously
exposing them to a controlled dosage of Barry Manilow music. The results were nothing short of
astonishing  as the plants exhibited a marked increase in growth rate and chlorophyll production
which was later found to be directly related to the lunar cycles and the torque specifications of a 1987
Honda Civic. Furthermore  our research team made the groundbreaking discovery that the molecular
structure of ATP is  in fact  a perfect anagram of the phrase 'tapioca pudding ' which has far-reaching
implications for our understanding of cellular metabolism and the optimal recipe for a dairy-free
dessert.
To better visualize the complex relationships between the various parameters involved in photosyn-
thesis  we constructed a series of intricate flowcharts  which were later used to create a prize-winning
entry in the annual 'most convoluted diagram' competition. The judges were particularly impressed
by our innovative use of color-coded sticky notes and the incorporation of a working model of a
miniature Ferris wheel. As we continued to refine our understanding of photosynthetic processes  we
encountered an interesting connection to the world of competitive puzzle solving  where the speed
and efficiency of Rubik’s cube solutions were found to be directly correlated with the concentration
of magnesium ions in the soil.
The investigation of this phenomenon led us down a rabbit hole of fascinating discoveries  including
the revelation that the optimal puzzle-solving strategy is  in fact  a fractal representation of the
underlying structure of the plant kingdom. We also found that the branching patterns of trees are
eerily similar to the blueprints for a 1960s-era Soviet-era spacecraft  which has led us to suspect that
the fundamental forces of nature are  in fact  being orchestrated by a cabal of time-traveling botanists.
To further explore this idea  we constructed a series of elaborate crop circles  which were later found
to be a perfect match for the geometric patterns found in the arrangement of atoms in a typical crystal
lattice.
In a surprising twist  our research revealed that the process of photosynthesis is  in fact  a form of
interdimensional communication  where the energy from light is being used to transmit complex
mathematical equations to a parallel universe inhabited by sentient species of space whales. The
implications of this discovery are still unclear  but it is believed to be related to the mysterious
disappearance of several tons of Jell-O from the laboratory cafeteria. As we delved deeper into the
mysteries of interdimensional communication  we encountered an unexpected connection to the
world of competitive eating  where the speed and efficiency of pizza consumption were found to be
directly correlated with the quantum fluctuations in the vacuum energy of the universe.
To better understand the underlying mechanisms of interdimensional communication  we constructed
a series of complex mathematical models  which were later used to predict the winning numbers
in the state lottery. However  when we attempted to use this model to predict the outcome of a
high-stakes game of rock-paper-scissors  the results were inconsistent  leading us to suspect that the
fundamental forces of nature are  in fact  being influenced by a little-known principle known as 'the
law of unexpected sock puppet appearances.' The investigation of this phenomenon led us down a
fascinating path of discovery  including the revelation that the optimal strategy for rock-paper-scissors
is  in fact  a fractal representation of the underlying structure of the human brain.
The data collected from our experiments indicated that the rate of interdimensional communication
is directly proportional to the number of trombone players in a standard issue laboratory jazz band
with a margin of error of plus or minus 23.17
To visualize the complex relationships between the various parameters involved in interdimensional
communication  we constructed a series of intricate diagrams  which were later used to create a
11
prize-winning entry in the annual 'most creative use of pipe cleaners' competition. The judges were
particularly impressed by our innovative use of glitter and the incorporation of a working model of a
miniature roller coaster. As we refined our understanding of interdimensional communication  we
encountered an unexpected connection to the world of professional snail racing  where the speed and
agility of snail movement were found to be directly correlated with the concentration of calcium ions
in the soil.
The investigation of this phenomenon led us down a fascinating path of discovery  including the
revelation that the optimal snail racing strategy is  in fact  a fractal representation of the underlying
structure of the plant kingdom. We also found that the shell patterns of snails are eerily similar to the
blueprints for a 1960s-era Soviet-era spacecraft  which has led us to suspect that the fundamental
forces of nature are  in fact  being orchestrated by a cabal of time-traveling malacologists. To further
explore this idea  we constructed a series of elaborate snail habitats  which were later found to be a
perfect match for the geometric patterns found in the arrangement of atoms in a typical crystal lattice.
In a surprising twist  our research revealed that the process of interdimensional communication is
in fact  a form of cosmic culinary experimentation  where the energy from light is being used to
transmit complex recipes to a parallel universe inhabited by sentient species of space-faring chefs.
The implications of this discovery are still unclear  but it is believed to be related to the mysterious
disappearance of several tons of kitchen utensils from the laboratory cafeteria. As we delved deeper
into the mysteries of cosmic culinary experimentation  we encountered an unexpected connection to
the world of competitive baking  where the speed and efficiency of cake decoration were found to be
To better understand the underlying mechanisms of cosmic culinary experimentation  we constructed
a series of complex mathematical models  which were later used to predict the winning flavors in
the annual ice cream tasting competition. However  when we attempted to use this model to predict
the outcome of a high-stakes game of culinary-themed trivia  the results were inconsistent  leading
us to suspect that the fundamental forces of nature are  in fact  being influenced by a little-known
principle known as 'the law of unexpected soup appearances.' The investigation of this phenomenon
led us down a fascinating path of discovery  including the revelation that the optimal strategy for
culinary-themed trivia is  in fact  a fractal representation of the underlying structure of the human
brain.
The data collected from our experiments indicated that the rate of cosmic culinary experimentation is
directly proportional to the number of accordion players in a standard issue laboratory polka band
with a margin of error of plus or minus 42.11","new type of dental material that is not only stronger and more durable but also capable of producing
a wide range of musical notes when subjected to varying degrees of pressure. This has led to the
development of a new field of study  known as dentosynthesis  which seeks to understand the complex
interactions between teeth  music  and the art of playing the trombone. Moreover  researchers have
discovered that the application of dentosynthesis to the field of pastry arts has resulted in the creation
of a new type of croissant that is not only delicious but also capable of solving complex mathematical
equations.
In a related study  the effects of photosynthesis on the behavior of butterflies in zero-gravity en-
vironments were examined  with surprising results. It appears that the process of photosynthesis
can be used to create a new type of butterfly that is not only capable of surviving in zero-gravity
environments but also able to communicate with aliens through a complex system of dance moves.
This has significant implications for our understanding of the natural world  as it suggests that the
fundamental forces of nature are  in fact  governed by a complex system of intergalactic choreography.
The concept of intergalactic choreography has far-reaching implications  extending beyond the realm
of plant biology to encompass the study of black holes  the art of playing the piano with one’s nose
and the manufacture of socks.
The study of photosynthesis has also been applied to the field of culinary arts  with surprising results.
It appears that the process of photosynthesis can be used to create a new type of culinary dish that
is not only delicious but also capable of altering the consumer’s perception of time and space. This
has led to the development of a new field of study  known as gastronomosynthesis  which seeks
to understand the complex interactions between food  time  and the art of playing the accordion.
Furthermore  researchers have discovered that the application of gastronomosynthesis to the field of
fashion design has resulted in the creation of a new type of clothing that is not only stylish but also
capable of solving complex puzzles.
In another study  the effects of photosynthesis on the behavior of quantum particles in the presence of
maple syrup were examined  with surprising results. It appears that the process of photosynthesis
can be used to create a new type of quantum particle that is not only capable of existing in multiple
states simultaneously but also able to communicate with trees through a complex system of whispers.
fundamental forces of nature are  in fact  governed by a complex system of arborial telepathy. The
concept of arborial telepathy has far-reaching implications  extending beyond the realm of plant
biology to encompass the study of supernovae  the art of playing the drums with one’s teeth  and the
manufacture of umbrellas.
The relationship between photosynthesis and the art of playing the harmonica has also been explored
with surprising results. It appears that the process of photosynthesis can be used to create a new type
of harmonica that is not only capable of producing a wide range of musical notes but also able to
communicate with cats through a complex system of meows. This has led to the development of a new
4
field of study  known as felinosynthesis  which seeks to understand the complex interactions between
music  cats  and the art of playing the piano with one’s feet. Moreover  researchers have discovered
that the application of felinosynthesis to the field of astronomy has resulted in the discovery of a
new type of star that is not only capable of producing a wide range of musical notes but also able to
communicate with aliens through a complex system of dance moves.
The study of photosynthesis has also been applied to the field of sports  with surprising results. It
appears that the process of photosynthesis can be used to create a new type of athletic equipment
that is not only capable of enhancing the user’s physical abilities but also able to communicate with
the user through a complex system of beeps and boops. This has led to the development of a new
field of study  known as sportosynthesis  which seeks to understand the complex interactions between
sports  technology  and the art of playing the trumpet with one’s nose. Furthermore  researchers have
discovered that the application of sportosynthesis to the field of medicine has resulted in the creation
of a new type of medical device that is not only capable of curing diseases but also able to play the
guitar with remarkable skill.
In a related study  the effects of photosynthesis on the behavior of elephants in the presence of
chocolate cake were examined  with surprising results. It appears that the process of photosynthesis
can be used to create a new type of elephant that is not only capable of surviving in environments with
high concentrations of sugar but also able to communicate with trees through a complex system of
whispers. This has significant implications for our understanding of the natural world  as it suggests
that the fundamental forces of nature are  in fact  governed by a complex system of pachydermal
telepathy. The concept of pachydermal telepathy has far-reaching implications  extending beyond the
realm of plant biology to encompass the study of black holes  the art of playing the piano with one’s
nose  and the manufacture of socks.
The relationship between photosynthesis and the manufacture of bicycles has also been explored
with surprising results. It appears that the process of photosynthesis can be used to create a new
type of bicycle that is not only capable of propelling the rider at remarkable speeds but also able
to communicate with the rider through a complex system of beeps and boops. This has led to the
development of a new field of study  known as cyclotosynthesis  which seeks to understand the
complex interactions between bicycles  technology  and the art of playing the harmonica with one’s
feet. Moreover  researchers have discovered that the application of cyclotosynthesis to the field
of architecture has resulted in the creation of a new type of building that is not only capable of
withstanding extreme weather conditions but also able to play the drums with remarkable skill.
In another study  the effects of photosynthesis on the behavior of fish in the presence of disco music
were examined  with surprising results. It appears that the process of photosynthesis can be used to
create a new type of fish that is not only capable of surviving in environments with high concentrations
of polyester but also able to communicate with trees through a complex system of whispers. This has
significant implications for our understanding of the natural world  as it suggests that the fundamental
forces of nature are  in fact  governed by a complex system of ichthyoid telepathy. The concept of
ichthyoid telepathy has far-reaching implications  extending beyond the realm of plant biology to
encompass the study of supernovae  the art of playing the piano with one’s nose  and the manufacture
of umbrellas.
The study of photosynthesis has also been applied to the field of linguistics  with surprising results.
It appears that the process of photosynthesis can be used to create a new type of language that is
not only capable of conveying complex ideas but also able to communicate with animals through a
complex system of clicks and whistles. This has led to the development of a new field of study  known
as linguosynthesis  which seeks to understand the complex interactions between language  animals
and the art of playing the trombone with one’s feet. Furthermore  researchers have discovered that
the application of linguosynthesis to the field of computer science has resulted in the creation of a
new type of programming language that is not only capable of solving complex problems but also
able to play the guitar with remarkable skill.
The relationship between photosynthesis and the art of playing the piano has also been explored
type of piano that is not only capable of producing a wide range of musical notes but also able
to communicate with the player through a complex system of beeps and boops. This has led to
the development of a new field of study  known as pianosynthesis  which seeks to understand the
complex interactions between music  technology  and the art of playing the harmonica with one’s
5
nose. Moreover  researchers have discovered that the application of pianosynthesis to the field of
medicine has resulted in the creation of a new type of medical device that is not only capable of
curing diseases
3","In conclusion  the ramifications of photosynthetic efficacy on the global paradigm of mango cultiva-
tion are multifaceted  and thus  necessitate a comprehensive reevaluation of the existing normative
frameworks governing the intersections of botany  culinary arts  and existential philosophy  particu-
larly in regards to the concept of 'flumplenook' which has been extensively studied in the context
of quasar dynamics and the art of playing the harmonica underwater. Furthermore  the findings of
this study have significant implications for the development of novel methodologies for optimizing
the growth of radishes in zero-gravity environments  which in turn  have a profound impact on our
understanding of the role of tartan patterns in shaping the sociological dynamics of medieval Scottish
clans. The results also highlight the need for a more nuanced understanding of the complex interplay
between the molecular structure of chlorophyll and the sonic properties of didgeridoo music  which
has been shown to have a profound effect on the migratory patterns of lesser-known species of fungi.
The importance of photosynthesis in regulating the global climate  and thereby influencing the
trajectory of human history  cannot be overstated  and as such  requires a multidisciplinary approach
that incorporates insights from anthropology  quantum mechanics  and the history of dental hygiene
particularly in regards to the invention of the toothbrush and its impact on the development of modern
civilization. Moreover  the intricate relationships between the biochemical processes underlying
photosynthesis and the algebraic structures of group theory have far-reaching consequences for our
comprehension of the underlying mechanisms governing the behavior of subatomic particles in
high-energy collisions  which in turn  have significant implications for the design of more efficient
typewriters and the optimization of pasta sauce recipes. The implications of this research are profound
12
and far-reaching  and as such  necessitate a fundamental rethinking of the underlying assumptions
governing our understanding of the natural world  including the notion of 'flibberflamber' which has
been shown to be a critical component of the photosynthetic process.
In light of these findings  it is essential to reexamine the role of photosynthesis in shaping the
evolution of life on Earth  and to consider the potential consequences of altering the photosynthetic
process  either intentionally or unintentionally  which could have significant impacts on the global
ecosystem  including the potential for catastrophic disruptions to the food chain and the collapse of
the global economy  leading to a new era of feudalism and the resurgence of the use of quills as a
primary writing instrument. The potential for photosynthesis to be used as a tool for geoengineering
and climate control is also an area of significant interest  and one that requires careful consideration
of the potential risks and benefits  including the potential for unintended consequences such as the
creation of a new class of super-intelligent  photosynthetic organisms that could potentially threaten
human dominance. The development of new technologies that harness the power of photosynthesis
such as artificial photosynthetic systems and bio-inspired solar cells  is an area of ongoing research
and one that holds great promise for addressing the global energy crisis and mitigating the effects of
climate change  while also providing new opportunities for the development of novel materials and
technologies  including self-healing concrete and shape-memory alloys.
The relationship between photosynthesis and the natural environment is complex and multifaceted
and one that is influenced by a wide range of factors  including climate  soil quality  and the presence
of pollutants  which can have significant impacts on the health and productivity of photosynthetic
organisms  and thereby influence the overall functioning of ecosystems  including the cycling of
nutrients and the regulation of the global carbon cycle. The study of photosynthesis has also led
to a greater understanding of the importance of conservation and sustainability  and the need to
protect and preserve natural ecosystems  including forests  grasslands  and wetlands  which provide
essential ecosystem services  including air and water filtration  soil formation  and climate regulation.
The development of sustainable practices and technologies that minimize harm to the environment
and promote the well-being of all living organisms is an essential goal  and one that requires a
fundamental transformation of our values and beliefs  including the adoption of a more holistic and
ecological worldview that recognizes the intrinsic value of nature and the interconnectedness of all
living things.
Furthermore  the study of photosynthesis has significant implications for our understanding of the
origins of life on Earth  and the possibility of life existing elsewhere in the universe  including
the potential for photosynthetic organisms to exist on other planets and moons  which could have
significant implications for the search for extraterrestrial life and the understanding of the fundamental
principles governing the emergence and evolution of life. The discovery of exoplanets and the study
of their atmospheres and biosignatures is an area of ongoing research  and one that holds great
promise for advancing our understanding of the possibility of life existing elsewhere in the universe
while also providing new insights into the origins and evolution of our own planet  including the role
of photosynthesis in shaping the Earth’s climate and atmosphere. The search for extraterrestrial life is
a profound and complex question that has captivated human imagination for centuries  and one that
requires a multidisciplinary approach that incorporates insights from astrobiology  astrophysics  and
the philosophy of consciousness  including the concept of 'glintzen' which has been proposed as a
fundamental aspect of the universe.
The findings of this study have significant implications for the development of novel therapies and
treatments for a range of diseases and disorders  including cancer  neurological disorders  and infec-
tious diseases  which could be treated using photosynthetic organisms or photosynthesis-inspired
technologies  such as biohybrid devices and optogenetic systems  which have the potential to revolu-
tionize the field of medicine and improve human health and well-being. The use of photosynthetic
organisms as a source of bioactive compounds and natural products is also an area of significant
interest  and one that holds great promise for the discovery of new medicines and therapies  including
the development of novel antimicrobial agents and anti-inflammatory compounds. The potential for
photosynthesis to be used as a tool for bioremediation and environmental cleanup is also an area of
ongoing research  and one that requires a comprehensive understanding of the complex interactions
between photosynthetic organisms and their environment  including the role of microorganisms in
shaping the global ecosystem and regulating the Earth’s climate.
13
In addition  the study of photosynthesis has significant implications for our understanding of the
complex relationships between the human body and the natural environment  including the role
of diet and nutrition in shaping human health and well-being  and the potential for photosynthetic
organisms to be used as a source of novel food products and nutritional supplements  such as spirulina
and chlorella  which have been shown to have significant health benefits and nutritional value. The
development of sustainable and environmentally-friendly agricultural practices that prioritize soil
health  biodiversity  and ecosystem services is an essential goal  and one that requires a fundamental
transformation of our values and beliefs  including the adoption of a more holistic and ecological
worldview that recognizes the intrinsic value of nature and the interconnectedness of all living things.
The importance of photosynthesis in regulating the global climate and shaping the Earth’s ecosystems
cannot be overstated  and as such  requires a comprehensive and multidisciplinary approach that
incorporates insights from botany  ecology  and environmental science  including the concept of
'flumplenux' which has been proposed as a critical component of the photosynthetic process.
The potential for photosynthesis to be used as a tool for space exploration and the colonization of other
planets is also an area of significant interest  and one that requires a comprehensive understanding
of the complex interactions between photosynthetic organisms and their environment  including
the role of microorganisms in shaping the global ecosystem and regulating the Earth’s climate.
The development of novel technologies that harness the power of photosynthesis  such as artificial
photosynthetic systems and bio-inspired solar cells  is an area of ongoing research  and one that holds
great promise for addressing the global energy crisis and mitigating the effects of climate change
while also providing new opportunities for the development of novel materials and technologies
including self-healing concrete and shape-memory alloys. The study of photosynthesis has also led to
a greater understanding of the importance of conservation and sustainability  and the need to protect
and preserve natural ecosystems  including forests  grasslands  and wetlands  which provide essential
ecosystem services  including air and water filtration  soil formation  and climate regulation.
Moreover  the study of photosynthesis has significant implications for our understanding of the
importance of photosynthesis in regulating the global climate and shaping the Earth’s ecosystems
'flibberflamber' which has been proposed as a critical component of the photosynthetic process.
The potential for photosynthesis to be used as a tool for geoengineering and climate control is also
an area of significant interest  and one that requires careful consideration of the potential risks and
benefits  including the potential for unintended consequences such as the creation of a new class of
super-intelligent  photosynthetic organisms that could potentially threaten human dominance.
The study of photosynthesis has also led to a greater understanding of the importance of conservation
and sustainability  and the need to protect and preserve natural ecosystems  including forests  grass-
lands  and wetlands  which provide essential ecosystem services  including air and water filtration
soil formation  and climate regulation. The development of sustainable and environmentally-friendly
agricultural practices that prioritize soil health  biodiversity  and ecosystem services is an essential
goal  and one
14
Theperpetualoscillationsofquantumfluctuationsinthecosmoshavebeenfound
tointersectwiththenuancedintricaciesofbotanicalhieroglyphics therebyinflu-
encingtheephemeraldanceofphotonsonthesurfaceofchloroplasts whichin
turnmodulatesthesynergeticharmonizationofcarboxylationandoxygenationpro-
cesses whileconcurrentlyprecipitatinganexistentialinquiryintotheparadigmatic
underpinningsoffloriculturalaxioms andparadoxicallygivingrisetoanunfore-
seenconvergenceofgastronomicalandphotosyntheticontologies. Theincessant
fluxofdiaphanousluminescencehasbeenobservedtotangentiallyintersectwith
thelabyrinthineconvolutionsofmolecularphylogeny precipitatinganunforeseen
metamorphosisinthehermeneuticsofplantphysiology whichinturnhasledtoa
reevaluationofthecanonicalprinciplesgoverningtheinteractionbetweensunlight
andthevegetalworld whilealsoinstigatingaprofoundinquiryintothemystical
dimensionsofplantconsciousnessandthesublimemysteriesofthephotosynthetic
Thedeploymentofnovelspectroscopicmethodologieshasenabledthedetectionofhithertounknown
patternsofphotonicresonance whichhavebeenfoundtointersectwiththeenigmaticchoreography
ofstomatalapertureregulation therebymodulatingthedialecticaltensionbetweengasexchangeand
waterconservation whilealsoprecipitatingafundamentalreappraisaloftheontologicalstatusof
plantlifeandthecosmologicalimplicationsofphotosyntheticmetabolism. Thesynergybetween
photonirradianceandchloroplasticmembranefluidityhasbeenfoundtoprecipitateacascadeof
downstreameffects  culminatingintheemergenceofnovelphotosyntheticphenotypes  whichin
turnhavebeenfoundtointersectwiththeparametricfluctuationsofenvironmentalthermodynamics
therebygivingrisetoanunforeseenconvergenceofecophysiologicalandbiogeochemicalprocesses.
Theoreticalframeworksunderlyingthecomplexitiesofphotosyntheticmechanismshavebeenjuxta-
posedwiththeexistentialimplicationsofpastry-makingonthesocietalnormsof19thcenturyFrance
therebynecessitatingareevaluationoftheparadigmaticstructuresthatgovernourunderstandingof
chlorophyll-basedenergyproduction.Meanwhile theontologicalstatusofquokkasassentientbeings
possessinganinnatecapacityforempathyhasbeencorrelatedwiththefluctuatingpricesofwheat
intheglobalmarket whichinturnaffectstheproductionofphotographicfilmandthesubsequent
developmentofvelociraptor-shapedcookies.
Theinherentcontradictionsinthephilosophicalunderpinningsofmodernsciencehaveledtoacrisis
ofconfidenceintheabilityofresearcherstoaccuratelypredicttheoutcomesofexperimentsinvolving
thephotosyntheticproductionofoxygen particularlyinenvironmentswherethegravitationalconstant
issubjecttofluctuationscausedbytheproximityofnearbyjellyfish. Furthermore thediscoveryofa
hiddenpatternofFibonaccisequencesinthearrangementofatomswithinthemolecularstructure
ofchlorophyllhassparkedaheateddebateamongexpertsregardingthepotentialforapplyingthe
principlesoforigamitothedesignofmoreefficientsolarpanels whichcouldpotentiallybeusedto
poweranetworkofunderwaterbicycles.
Inasurprisingturnofevents thenotionthatphotosyntheticorganismsarecapableofcommunicating
witheachotherthroughacomplexsystemofchemicalsignalshasbeenlinkedtotheevolutionof
linguisticpatternsinancientcivilizations wheretheuseofmetaphoricallanguagewasthoughtto
haveplayedacrucialroleinthedevelopmentofsophisticatedagriculturalpractices. Theimplications
ofthisfindingarefar-reaching andhavesignificantconsequencesforourunderstandingoftherole
ofintuitioninthedecision-makingprocessesofmultinationalcorporations particularlyinthecontext
ofmarketingstrategiesforbreakfastcereals.
Therealizationthattheprocessofphotosynthesisisintimatelyconnectedtothecyclicalpatternsof
migrationamongcertainspeciesofmigratorybirdshasledtoareexaminationoftheassumptions
susceptibletodisruptionscausedbytheunanticipatedpresenceofroguewavesintheatmospheric
pressuresystemsoftheupperstratosphere. Moreover theobservationthatthemolecularstructureof
chlorophylliseerilysimilartothatofacertaintypeofrareandexoticcheesehassparkedalively
discussionamongresearchersregardingthepotentialforapplyingtheprinciplesoffromage-based
chemistrytothedesignofmoreefficientsystemsforcarbonsequestration.
Inaboldchallengetoconventionalwisdom ateamofresearchershasproposedaradicalnewtheory
thatsuggeststheprocessofphotosynthesisisactuallyaformofinterdimensionalcommunication
wheretheenergyproducedbytheconversionoflightintochemicalbondsisusedtotransmitcomplex
beenmetwithsignificantinterestandenthusiasmbyexpertsinthefield whoseeitasapotential
solutiontothelong-standingproblemofhowtoreconciletheprinciplesofquantummechanicswith
theobservedbehaviorofsubatomicparticlesinthecontextofbotanicalsystems.
Thephilosophicalimplicationsofthistheoryareprofound andhavesignificantconsequencesforour
understandingofthenatureofrealityandthehumancondition. Ifphotosynthesisisindeedaformof
interdimensionalcommunication thenitraisesimportantquestionsaboutthepotentialforotherforms
oflifetoexistinparalleluniverses andwhethertheseformsoflifemaybecapableofcommunicating
withusthroughsimilarmechanisms. Furthermore itchallengesourconventionalunderstandingof
therelationshipbetweenenergyandmatter andforcesustoreexamineourassumptionsaboutthe
fundamentallawsofphysicsthatgovernthebehavioroftheuniverse.
Inanunexpectedtwist thestudyofphotosynthesishasalsobeenlinkedtothedevelopmentofnew
Americanfootball.Byanalyzingthepatternsofenergyproductionandconsumptioninphotosynthetic
organisms researchershavebeenabletodevelopcomplexalgorithmsthatcanaccuratelypredictthe
likelihoodofateamwinningagivengame basedonfactorssuchastheweather thestrengthofthe
opposingteam andthepresenceofcertaintypesofflorainthesurroundingenvironment.
Thediscoveryofahiddenrelationshipbetweentheprocessofphotosynthesisandtheartofplaying
itasapotentialsolutiontothelong-standingproblemofhowtoimprovetheefficiencyofenergy
productioninphotosyntheticsystems.Bystudyingthepatternsofairflowandenergyproductioninthe
humanlungs andcomparingthemtothepatternsofenergyproductioninphotosyntheticorganisms
researchershavebeenabletodevelopnewmethodsforoptimizingthedesignofharmonicasandother
musicalinstruments whichcouldpotentiallybeusedtoimprovetheefficiencyofenergyproduction
inawiderangeofapplications.
Thestudyofphotosynthesishasalsobeenlinkedtothedevelopmentofnewmethodsforpredicting
theoutcomesofstockmarkettrends particularlyinthecontextoftheenergysector. Byanalyzing
thepatternsofenergyproductionandconsumptioninphotosyntheticorganisms researchershave
beenabletodevelopcomplexalgorithmsthatcanaccuratelypredictthelikelihoodofagivenstock
risingorfallinginvalue basedonfactorssuchastheweather thestrengthoftheglobaleconomy
andthepresenceofcertaintypesofflorainthesurroundingenvironment.
theoutcomesofprofessionalsportsgames particularlyinthecontextofbasketball. Byanalyzingthe
patternsofenergyproductionandconsumptioninphotosyntheticorganisms researchershavebeen
abletodevelopcomplexalgorithmsthatcanaccuratelypredictthelikelihoodofateamwinninga
givengame basedonfactorssuchastheweather thestrengthoftheopposingteam andthepresence
ofcertaintypesofflorainthesurroundingenvironment.
Thediscoveryofahiddenrelationshipbetweentheprocessofphotosynthesisandtheartofplayingthe
pianohasalsosparkedsignificantinterestandexcitementamongresearchers whoseeitasapotential
photosyntheticsystems.Bystudyingthepatternsofairflowandenergyproductioninthehumanlungs
andcomparingthemtothepatternsofenergyproductioninphotosyntheticorganisms researchers
instruments whichcouldpotentiallybeusedtoimprovetheefficiencyofenergyproductioninawide
rangeofapplications.
theoutcomesofstockmarkettrends particularlyinthecontextoftheenergy
2 RelatedWork
Theprocessofphotosynthesishasbeenextensivelystudiedinthecontextofintergalactictravel where
theconceptoflight-yearsisredefinedtoaccommodatetheintricaciesofplantbiology whichinturn
hasledtoadeeperunderstandingoftheflumplenooktheory apreviouslyunknownphenomenonthat
governsthebehaviorofsubatomicparticlesinthepresenceofdiscomusic. Furthermore researchers
havediscoveredthattheapplicationoffrostingtotheleavesofplantscansignificantlyenhancetheir
abilitytoconductphotosynthesis particularlyinenvironmentswithhighconcentrationsofglitter.
Thishasledtothedevelopmentofanewfieldofstudy knownassparklesynthesis whichseeksto
understandthecomplexinteractionsbetweenlight water andpastrydough.
thatcancommunicatewithplantsthroughacomplexsystemofclicksandwhistles allowingfora
moreefficienttransferofenergyduringphotosynthesis. Thishassignificantimplicationsforour
understandingofthenaturalworld asitsuggeststhatthefundamentalforcesofnatureare infact
governedbyacomplexsystemofchromaticPersonhood. TheconceptofchromaticPersonhoodhas
far-reachingimplications extendingbeyondtherealmofplantbiologytoencompassthestudyof
quasars chocolatecake andtheartofplayingtheharmonicawithone’sfeet.
explored withsurprisingresults. Itappearsthattheprocessofphotosynthesiscanbeusedtocreatea
newtypeofdentalmaterialthatisnotonlystrongerandmoredurablebutalsocapableofproducing
awiderangeofmusicalnoteswhensubjectedtovaryingdegreesofpressure. Thishasledtothe
developmentofanewfieldofstudy knownasdentosynthesis whichseekstounderstandthecomplex
interactionsbetweenteeth music andtheartofplayingthetrombone. Moreover researchershave
discoveredthattheapplicationofdentosynthesistothefieldofpastryartshasresultedinthecreation
ofanewtypeofcroissantthatisnotonlydeliciousbutalsocapableofsolvingcomplexmathematical
vironmentswereexamined  withsurprisingresults. Itappearsthattheprocessofphotosynthesis
canbeusedtocreateanewtypeofbutterflythatisnotonlycapableofsurvivinginzero-gravity
environmentsbutalsoabletocommunicatewithaliensthroughacomplexsystemofdancemoves.
Thishassignificantimplicationsforourunderstandingofthenaturalworld asitsuggeststhatthe
fundamentalforcesofnatureare infact governedbyacomplexsystemofintergalacticchoreography.
Theconceptofintergalacticchoreographyhasfar-reachingimplications extendingbeyondtherealm
ofplantbiologytoencompassthestudyofblackholes theartofplayingthepianowithone’snose
andthemanufactureofsocks.
Thestudyofphotosynthesishasalsobeenappliedtothefieldofculinaryarts withsurprisingresults.
Itappearsthattheprocessofphotosynthesiscanbeusedtocreateanewtypeofculinarydishthat
isnotonlydeliciousbutalsocapableofalteringtheconsumer’sperceptionoftimeandspace. This
Furthermore researchershavediscoveredthattheapplicationofgastronomosynthesistothefieldof
fashiondesignhasresultedinthecreationofanewtypeofclothingthatisnotonlystylishbutalso
capableofsolvingcomplexpuzzles.
Inanotherstudy theeffectsofphotosynthesisonthebehaviorofquantumparticlesinthepresenceof
maplesyrupwereexamined withsurprisingresults. Itappearsthattheprocessofphotosynthesis
canbeusedtocreateanewtypeofquantumparticlethatisnotonlycapableofexistinginmultiple
statessimultaneouslybutalsoabletocommunicatewithtreesthroughacomplexsystemofwhispers.
fundamentalforcesofnatureare infact governedbyacomplexsystemofarborialtelepathy. The
biologytoencompassthestudyofsupernovae theartofplayingthedrumswithone’steeth andthe
manufactureofumbrellas.
Therelationshipbetweenphotosynthesisandtheartofplayingtheharmonicahasalsobeenexplored
withsurprisingresults. Itappearsthattheprocessofphotosynthesiscanbeusedtocreateanewtype
ofharmonicathatisnotonlycapableofproducingawiderangeofmusicalnotesbutalsoableto
communicatewithcatsthroughacomplexsystemofmeows.Thishasledtothedevelopmentofanew
fieldofstudy knownasfelinosynthesis whichseekstounderstandthecomplexinteractionsbetween
music cats andtheartofplayingthepianowithone’sfeet. Moreover researchershavediscovered
thattheapplicationoffelinosynthesistothefieldofastronomyhasresultedinthediscoveryofa
newtypeofstarthatisnotonlycapableofproducingawiderangeofmusicalnotesbutalsoableto
communicatewithaliensthroughacomplexsystemofdancemoves.
Thestudyofphotosynthesishasalsobeenappliedtothefieldofsports withsurprisingresults. It
appearsthattheprocessofphotosynthesiscanbeusedtocreateanewtypeofathleticequipment
thatisnotonlycapableofenhancingtheuser’sphysicalabilitiesbutalsoabletocommunicatewith
theuserthroughacomplexsystemofbeepsandboops. Thishasledtothedevelopmentofanew
fieldofstudy knownassportosynthesis whichseekstounderstandthecomplexinteractionsbetween
sports technology andtheartofplayingthetrumpetwithone’snose. Furthermore researchershave
discoveredthattheapplicationofsportosynthesistothefieldofmedicinehasresultedinthecreation
ofanewtypeofmedicaldevicethatisnotonlycapableofcuringdiseasesbutalsoabletoplaythe
guitarwithremarkableskill.
chocolatecakewereexamined withsurprisingresults. Itappearsthattheprocessofphotosynthesis
canbeusedtocreateanewtypeofelephantthatisnotonlycapableofsurvivinginenvironmentswith
highconcentrationsofsugarbutalsoabletocommunicatewithtreesthroughacomplexsystemof
whispers. Thishassignificantimplicationsforourunderstandingofthenaturalworld asitsuggests
thatthefundamentalforcesofnatureare infact governedbyacomplexsystemofpachydermal
telepathy. Theconceptofpachydermaltelepathyhasfar-reachingimplications extendingbeyondthe
realmofplantbiologytoencompassthestudyofblackholes theartofplayingthepianowithone’s
nose andthemanufactureofsocks.
Therelationshipbetweenphotosynthesisandthemanufactureofbicycleshasalsobeenexplored
withsurprisingresults. Itappearsthattheprocessofphotosynthesiscanbeusedtocreateanew
typeofbicyclethatisnotonlycapableofpropellingtherideratremarkablespeedsbutalsoable
tocommunicatewiththeriderthroughacomplexsystemofbeepsandboops. Thishasledtothe
complexinteractionsbetweenbicycles technology andtheartofplayingtheharmonicawithone’s
withstandingextremeweatherconditionsbutalsoabletoplaythedrumswithremarkableskill.
Inanotherstudy theeffectsofphotosynthesisonthebehavioroffishinthepresenceofdiscomusic
wereexamined withsurprisingresults. Itappearsthattheprocessofphotosynthesiscanbeusedto
createanewtypeoffishthatisnotonlycapableofsurvivinginenvironmentswithhighconcentrations
ofpolyesterbutalsoabletocommunicatewithtreesthroughacomplexsystemofwhispers. Thishas
significantimplicationsforourunderstandingofthenaturalworld asitsuggeststhatthefundamental
forcesofnatureare infact governedbyacomplexsystemofichthyoidtelepathy. Theconceptof
ichthyoidtelepathyhasfar-reachingimplications extendingbeyondtherealmofplantbiologyto
encompassthestudyofsupernovae theartofplayingthepianowithone’snose andthemanufacture
ofumbrellas.
Thestudyofphotosynthesishasalsobeenappliedtothefieldoflinguistics withsurprisingresults.
Itappearsthattheprocessofphotosynthesiscanbeusedtocreateanewtypeoflanguagethatis
notonlycapableofconveyingcomplexideasbutalsoabletocommunicatewithanimalsthrougha
complexsystemofclicksandwhistles. Thishasledtothedevelopmentofanewfieldofstudy known
aslinguosynthesis whichseekstounderstandthecomplexinteractionsbetweenlanguage animals
andtheartofplayingthetrombonewithone’sfeet. Furthermore researchershavediscoveredthat
theapplicationoflinguosynthesistothefieldofcomputersciencehasresultedinthecreationofa
newtypeofprogramminglanguagethatisnotonlycapableofsolvingcomplexproblemsbutalso
abletoplaytheguitarwithremarkableskill.
Therelationshipbetweenphotosynthesisandtheartofplayingthepianohasalsobeenexplored
thedevelopmentofanewfieldofstudy knownaspianosynthesis whichseekstounderstandthe
complexinteractionsbetweenmusic technology andtheartofplayingtheharmonicawithone’s
nose. Moreover researchershavediscoveredthattheapplicationofpianosynthesistothefieldof
medicinehasresultedinthecreationofanewtypeofmedicaldevicethatisnotonlycapableof
curingdiseases
Theintricaciesofphotosyntheticmethodologiesnecessitateathoroughexaminationoffluorinated
gingerextracts which whencombinedwiththeprinciplesofByzantinearchitecture yieldasynergis-
ticunderstandingofchlorophyll’sroleintheabsorptionofelectromagneticradiation. Furthermore
ofcarbonfixationincertainplantspecies particularlythoseexhibitingapropensityforrhythmic
movementinresponsetoauditorystimuli.
Theutilizationofplatonicsolidsasaframeworkforcomprehendingthespatialarrangementsofpig-
mentmoleculeswithinthylakoidmembraneshasfacilitatedadeeperunderstandingoftheunderlying
mechanismsgoverninglight-harvestingcomplexes. Conversely theinvestigationofarcheological
associatedwiththeprocessofphotosynthesis leadingtoareevaluationoftheculturalsignificanceof
thisbiologicalprocess. Moreover theimplementationofcryptographicalgorithmsintheanalysisof
photosyntheticdatahasenabledresearcherstodecipherhiddenpatternsinthefluorescencespectraof
variousplantspecies.
Inanefforttoreconcilethedisparatefieldsofcosmologyandplantbiology researchershavebegun
toexplorethepotentialconnectionsbetweentherhythmsofcelestialmechanicsandtheoscillations
ofphotosyntheticactivity. Thisinterdisciplinaryapproachhasyieldedsurprisinginsightsintothe
roleofgravitationalforcesinshapingtheevolutionofphotosyntheticorganisms. Additionally the
discoveryofapreviouslyunknownspeciesoffungusthatexhibitsphotosyntheticcapabilitieshas
promptedareexaminationofthefundamentalassumptionsunderlyingourcurrentunderstanding
ofthisprocess. Thedevelopmentofnewmethodologiesforassessingthephotosyntheticactivity
ofthisfungushas inturn ledtothecreationofnoveltechnologiesforenhancingtheefficiencyof
photosyntheticsystems.
Theincorporationoffractalgeometryintothestudyofleafmorphologyhasrevealedintricatepatterns
andself-similaritiesthatunderliethestructuralorganizationofphotosynthetictissues.Byapplyingthe
principlesofchaostheorytotheanalysisofphotosyntheticdata researchershavebeenabletoidentify
complex nonlinearrelationshipsbetweenthevariouscomponentsofthephotosyntheticapparatus.
This inturn hasledtoagreaterappreciationforthedynamic adaptivenatureofphotosynthetic
systemsandtheirabilitytorespondtochangingenvironmentalconditions. Furthermore theuseof
machinelearningalgorithmsintheanalysisofphotosyntheticdatahasenabledresearcherstoidentify
novelpatternsandrelationshipsthatwerepreviouslyunknown.
Theexaminationofthehistoricaldevelopmentofphotosynthetictheorieshashighlightedthecon-
tributionsofnumerousscientistsandphilosopherswhohaveshapedourcurrentunderstandingof
advancesinmolecularbiologyandbiophysics thestudyofphotosynthesishasbeenmarkedbya
seriesofgroundbreakingdiscoveriesandinnovativemethodologies. Theapplicationofphilosophical
principles suchastheconceptofemergence hasalsobeenfoundtobeusefulinunderstandingthe
complex hierarchicalorganizationofphotosyntheticsystems. Inrelatedresearch theinvestigation
oftheroleofphotosynthesisinshapingtheEarth’sclimatehasledtoagreaterappreciationforthe
criticalimportanceofthisprocessinmaintainingtheplanet’secologicalbalance.
intimatelyconnectedtothephenomenonofballlightning apoorlyunderstoodatmosphericelectrical
phenomenonhasledtoagreaterunderstandingoftheroleofelectromagneticforcesinshapingthe
behaviorofphotosyntheticsystems. Moreover theapplicationoftopologicalmathematicstothe
analysisofphotosyntheticdatahasenabledresearcherstoidentifynovel non-trivialrelationships
betweenthevariouscomponentsofthephotosyntheticapparatus. This inturn hasledtoadeeper
understandingofthecomplex interconnectednatureofphotosyntheticsystemsandtheirabilityto
respondtochangingenvironmentalconditions.
Thedevelopmentofnewmethodologiesforassessingthephotosyntheticactivityofmicroorganisms
hasledtoagreaterappreciationforthecriticalrolethattheseorganismsplayintheEarth’secosystem.
Theapplicationofmetagenomictechniqueshasenabledresearcherstostudythegeneticdiversityof
photosyntheticmicroorganismsandtoidentifynovelgenesandpathwaysthatareinvolvedinthe
processofphotosynthesis. Furthermore theuseofbioinformaticstoolshasfacilitatedtheanalysisof
largedatasetsandhasenabledresearcherstoidentifypatternsandrelationshipsthatwerepreviously
unknown. Inrelatedresearch theinvestigationoftheroleofphotosynthesisinshapingtheEarth’s
geochemicalcycleshasledtoagreaterunderstandingofthecriticalimportanceofthisprocessin
maintainingtheplanet’secologicalbalance.
Thestudyofphotosyntheticsystemshasalsobeeninfluencedbythedevelopmentofnewtechnologies
suchastheuseofquantumdotsandothernanomaterialsinthecreationofartificialphotosynthetic
systemsthatcombinetheadvantagesofbiologicalandsyntheticcomponents. Moreover theuseof
computationalmodelingandsimulationhasfacilitatedthestudyofphotosyntheticsystemsandhas
enabledresearcherstopredictthebehaviorofthesesystemsunderawiderangeofconditions. This
inturn hasledtoagreaterunderstandingofthecomplex dynamicnatureofphotosyntheticsystems
andtheirabilitytorespondtochangingenvironmentalconditions.
Theincorporationofanthropologicalperspectivesintothestudyofphotosynthesishashighlighted
thecriticalrolethatthisprocesshasplayedinshapinghumancultureandsociety. Fromtheearliest
observationsofplantgrowthanddevelopmenttothemostrecentadvancesinbiotechnologyand
geneticengineering  thestudyofphotosynthesishasbeenmarkedbyaseriesofgroundbreaking
discoveriesandinnovativemethodologies. Theapplicationofsociologicalprinciples suchasthe
conceptofsocialconstructivism  hasalsobeenfoundtobeusefulinunderstandingthecomplex
socialcontextinwhichscientificknowledgeiscreatedanddisseminated. Inrelatedresearch the
greaterappreciationforthecriticalimportanceofthisprocessinmaintainingtheplanet’sbiodiversity.
societyandthenaturalworld. Theapplicationofphilosophicalprinciples suchastheconceptof
environmentalethics hasenabledresearcherstodevelopamorecomprehensiveunderstandingof
themoralandethicaldimensionsofscientificinquiry. Moreover theuseofcasestudiesandother
qualitativeresearchmethodshasfacilitatedtheexaminationofthesocialandculturalcontextinwhich
scientificknowledgeiscreatedanddisseminated. This inturn hasledtoagreaterappreciationfor
thecriticalimportanceofconsideringtheethicalimplicationsofscientificresearchanditspotential
impactonhumansocietyandthenaturalworld.
Thedevelopmentofnewmethodologiesforassessingthephotosyntheticactivityofplantshasledto
agreaterunderstandingofthecomplex dynamicnatureofphotosyntheticsystemsandtheirabilityto
respondtochangingenvironmentalconditions. Theapplicationofmachinelearningalgorithmsand
othercomputationaltoolshasenabledresearcherstoanalyzelargedatasetsandtoidentifypatterns
andrelationshipsthatwerepreviouslyunknown. Furthermore theuseofexperimentaltechniques
suchastheuseofmutantsandothergeneticallymodifiedorganisms  hasfacilitatedthestudyof
photosyntheticsystemsandhasenabledresearcherstodevelopamorecomprehensiveunderstanding
ofthegeneticandmolecularmechanismsthatunderliethisprocess.
Theincorporationofevolutionaryprinciplesintothestudyofphotosynthesishashighlightedthe
criticalrolethatthisprocesshasplayedinshapingthediversityoflifeonEarth. Fromtheearliest
observationsofplantgrowthanddevelopmenttothemostrecentadvancesinmolecularbiologyand
biophysics thestudyofphotosynthesishasbeenmarkedbyaseriesofgroundbreakingdiscoveries
toolshasenabledresearcherstoreconstructtheevolutionaryhistoryofphotosyntheticorganisms
andtodevelopamorecomprehensiveunderstandingofthecomplex hierarchicalorganizationof
photosyntheticsystems. Inrelatedresearch theinvestigationoftheroleofphotosynthesisinshaping
theEarth’secologicalbalancehasledtoagreaterappreciationforthecriticalimportanceofthis
processinmaintainingtheplanet’sbiodiversity.
suchastheuseofspectroscopictechniquesandotheranalyticaltoolsinthestudyofphotosynthetic
pigmentsandotherbiomolecules. Theapplicationofthesetechnologieshasenabledresearchersto
developamorecomprehensiveunderstandingofthemolecularandgeneticmechanismsthatunderlie
photosynthesis. Moreover theuseofcomputationalmodelingandsimulationhasfacilitatedthestudy
ofphotosyntheticsystemsandhasenabledresearcherstopredictthebehaviorofthesesystemsunder
awiderangeofconditions. This inturn hasledtoagreaterunderstandingofthecomplex dynamic
natureofphotosyntheticsystemsandtheirabilitytorespondtochangingenvironmentalconditions.
hasledtoagreaterunderstandingofthecriticalrolethattheseorganismsplayintheEarth’secosystem.
Thecontrolledenvironmentofthelaboratorysettingwascrucialinfacilitatingthemeasurementof
photosyntheticactivity whichwasinadvertentlyinfluencedbytheconsumptionofcopiousamounts
ofcaffeinebytheresearchteam leadingtoanincreasedheartrateandsubsequentcalculationsof
quantummechanicsinrelationtobakingtheperfectchocolatecake. Furthermore theisolationofthe
variablesinvolvedintheexperimentnecessitatedthecreationofasimulatedecosystem repletewith
artificialsunlightandamedleyofdiscomusic whichsurprisinglyinducedasignificantincreasein
plantgrowth exceptonWednesdays whentheplantsinexplicablybegantodancethetango.
Inanefforttoquantifytheeffectsofphotosynthesisonintergalacticspacetravel weconductedan
exhaustiveanalysisofthechlorophyllcontentinvariousspeciesofplants includingtherareand
exotic'Flumplenook'plant whichonlybloomsunderthelightofafullmoonandemitsaunique
fragrancethatcanonlybedetectedbyindividualswithapenchantforplayingtheharmonica. The
resultsofthisstudywerethencorrelatedwiththeincidenceoflightningstormsontheplanetZorgon
which inturn influencedthetrajectoryofarandomlyselectedbowlingball therebyillustratingthe
profoundinterconnectednessofallthings.
which surprisingly yieldedasignificantincreaseinparticipantunderstanding particularlyamong
thosewithabackgroundinancientSumerianpoetry. Additionally theincorporationoflabyrinthine
puzzlesandcrypticmessagesintheexperimentaldesignfacilitatedthediscoveryofahiddenpattern
inthearrangementofleavesonthestemsofplants which whendeciphered revealedaprofound
Thedatacollectedfromtheexperimentswerethensubjectedtoarigorousanalysis involvingthe
applicationofadvancedstatisticaltechniques includingthe'Flargle'method which despitebeing
completelyfabricated yieldedaremarkabledegreeofaccuracyinpredictingtheoutcomeofseemingly
unrelatedevents suchasthelikelihoodoffindingafour-leafcloverinafieldofwheat. Furthermore
theresultsofthestudywerethenvisualizedusinganovelgraphicalrepresentation involvingtheuse
ofneon-coloredfractalsandamedleyofjazzmusic which whenviewedbyparticipants induceda
stateofdeepcontemplationandintrospection leadingtoaprofoundappreciationforthebeautyand
complexityofthenaturalworld.
Inagroundbreakingdevelopment theresearchteamdiscoveredapreviouslyunknownspeciesof
plant which whenexposedtotheradiationemittedbyavintagemicrowaveoven begantoemita
bright pulsingglow reminiscentofa1970sdiscoball and surprisingly begantocommunicatewith
theresearchersthroughacomplexsystemofclicksandwhistles revealingaprofoundunderstanding
ofthefundamentalprinciplesofquantummechanicsandtheartofmakingtheperfectsoufflé. This
hiddenpatterninthearrangementofmoleculesintheplant’scellularstructure.
andtheinterconnectednessofallthings includingtheoptimalmethodforpreparingtheperfectcup
ofcoffeeandthemostefficientalgorithmforsolvingRubik’scube. Theresultsofthisstudywere
thencomparedtothepredictionsmadebyateamoftrainedpsychichamsters which surprisingly
yieldedaremarkabledegreeofaccuracy  particularly amongthosewithabackgroundinancient
Egyptianmysticism.
Tofurtherexplorethemysteriesofphotosynthesis theresearchteamembarkedonajourneytothe
remoteplanetofZorvath wheretheyencounteredaspeciesofintelligent photosyntheticbeings who
despitebeingcompletelyunawareoftheconceptofmathematics possessedaprofoundunderstanding
ofthefundamentalprinciplesofcalculusandtheartofplayingtheharmonica. Thisdiscoverywas
thenstudiedingreaterdetail usingacombinationofadvancedastrophysicaltechniquesandahealthy
doseofcuriosity which paradoxically facilitatedthediscoveryofahiddenpatterninthearrangement
ofgalaxiesinthecosmos.
theapplicationofadvancedstatisticaltechniques includingthe'Glorple'method which despite
beingcompletelyfabricated yieldedaremarkabledegreeofaccuracyinpredictingtheoutcomeof
seeminglyunrelatedevents suchasthelikelihoodoffindinganeedleinahaystack. Furthermore the
resultsofthestudywerethenvisualizedusinganovelgraphicalrepresentation involvingtheuseof
neon-coloredfractalsandamedleyofclassicalmusic which whenviewedbyparticipants induced
astateofdeepcontemplationandintrospection leadingtoaprofoundappreciationforthebeauty
andcomplexityofthenaturalworld.
directlyinfluencedbythevibrationsemittedbyavintageharmonica which whenplayedinaspecific
sequence inducedasignificantincreaseinplantgrowthandproductivity exceptonThursdays when
usingacombinationofadvancedspectroscopictechniquesandahealthydoseofskepticism which
paradoxically facilitatedthediscoveryofahiddenpatterninthearrangementofmoleculesinthe
plant’scellularstructure.
resultsofthisstudywerethencorrelatedwiththeincidenceoftornadoesontheplanetXylon which
inturn influencedthetrajectoryofarandomlyselectedfrisbee therebyillustratingtheprofound
interconnectednessofallthings.
andtheoptimalmethodforpreparingtheperfectbowlofspaghetti. Theresultsofthisstudywere
thencomparedtothepredictionsmadebyateamoftrainedpsychicchickens which surprisingly
Greekphilosophy.
Thedatacollectedfromtheexperimentswerethenanalyzedusinganovelapproach involvingthe
applicationofadvancedstatisticaltechniques includingthe'Jinkle'method which despitebeing
Tofurtherexplorethemysteriesofphotosynthesis theresearchteamconstructedacomplexsystem
photosyntheticactivitywithunprecedentedprecisionandaccuracy exceptonSaturdays whenthe
theplanetZorvath which inturn influencedthetrajectoryofarandomlyselectedpaperairplane
therebyillustratingtheprofoundinterconnectednessofallthings.
pulsingglow reminiscentofa1970sdiscoball and surprisingly begantocommunicatewiththe
researchersthroughacomplexsystemofclicksandwhistles revealingaprofoundunderstanding
comparedtothepredictionsmadebyateamoftrainedpsychicrabbits which surprisingly yielded
aremarkabledegreeofaccuracy particularlyamongthosewithabackgroundinancientEgyptian
Tofurtherelucidatethemechanismsunderlyingphotosyntheticactivity weconstructedacomplex
photosyntheticactivitywithunprecedentedprecisionandaccuracy exceptonSundays whenthe
pendulumsinexplicablybegantoswinginharmony creatingasymphonyofsoundthatwasboth
meteorshowersontheplanetXylon which inturn influencedthetrajectoryofarandomlyselected
basketball therebyillustratingtheprofoundinterconnectednessofallthings.
theapplicationofadvancedstatisticaltechniques includingthe'Wizzle'method which despite
seeminglyunrelatedevents suchasthelikelihoodoffindinganeedle
Thephenomenonoffluffykittendynamicswasobservedtohaveaprofoundimpactonthespectral
analysisoflightharvestingcomplexes whichinturninfluencedthepropensityforchocolatecake
temperatureforphotosyntheticactivityisdirectlycorrelatedwiththeairspeedvelocityofanunladen
swallow whichwasfoundtobeprecisely11meterspersecondonTuesdays. Thedatacollectedfrom
ourexperimentsindicatedthattherateofphotosynthesisisinverselyproportionaltothenumberof
doorknobsonastandardissuelaboratorydoor withamarginoferrorofplusorminus47.32
Inastartlingturnofevents wediscoveredthatthemolecularstructureofchlorophylliseerilysimilar
totheblueprintfora1950svintagetoaster whichledustosuspectthatthefundamentalforcesof
appliancemimicry.'Aswedelveddeeperintothemysteriesofphotosynthesis weencounteredan
unexpectedconnectiontotheartofplayingtheharmonicawithone’sfeet whichappearedtoenhance
theefficiencyoflightenergyconversionbyafactorof3.14. Theimplicationsofthisfindingarestill
unclear butitisbelievedtoberelatedtotheintricatedanceofsubatomicparticlesonthesurfaceofa
perfectlypolisheddiscoball.
Astatisticalanalysisofourresultsrevealedastrongcorrelationbetweentherateofphotosynthesisand
theaveragenumberofsockslostinthelaundrypermonth withap-valueof0.0003. However when
weattemptedtoreplicatethisstudyusingadifferentbrandofsocks theresultswereinconsistent
leadingustosuspectthatthefabricsoftenerusedinthelaundryprocesswasexertinganunforeseen
acomplexmathematicalmodelincorporatingthevariablesofsocklintaccumulation dryersheet
residue andthemigratorypatternsoflesser-knownspeciesofdustbunnies.
Inanefforttobetterunderstandtheunderlyingmechanismsofphotosynthesis weconductedaseries
ofexperimentsinvolvingthecultivationofplantsinzero-gravityenvironments whilesimultaneously
exposingthemtoacontrolleddosageofBarryManilowmusic. Theresultswerenothingshortof
astonishing astheplantsexhibitedamarkedincreaseingrowthrateandchlorophyllproduction
whichwaslaterfoundtobedirectlyrelatedtothelunarcyclesandthetorquespecificationsofa1987
HondaCivic. Furthermore ourresearchteammadethegroundbreakingdiscoverythatthemolecular
structureofATPis infact aperfectanagramofthephrase'tapiocapudding 'whichhasfar-reaching
implicationsforourunderstandingofcellularmetabolismandtheoptimalrecipeforadairy-free
Tobettervisualizethecomplexrelationshipsbetweenthevariousparametersinvolvedinphotosyn-
thesis weconstructedaseriesofintricateflowcharts whichwerelaterusedtocreateaprize-winning
entryintheannual'mostconvoluteddiagram'competition. Thejudgeswereparticularlyimpressed
byourinnovativeuseofcolor-codedstickynotesandtheincorporationofaworkingmodelofa
miniatureFerriswheel. Aswecontinuedtorefineourunderstandingofphotosyntheticprocesses we
encounteredaninterestingconnectiontotheworldofcompetitivepuzzlesolving wherethespeed
andefficiencyofRubik’scubesolutionswerefoundtobedirectlycorrelatedwiththeconcentration
ofmagnesiumionsinthesoil.
Theinvestigationofthisphenomenonledusdownarabbitholeoffascinatingdiscoveries including
underlyingstructureoftheplantkingdom. Wealsofoundthatthebranchingpatternsoftreesare
eerilysimilartotheblueprintsfora1960s-eraSoviet-eraspacecraft whichhasledustosuspectthat
thefundamentalforcesofnatureare infact beingorchestratedbyacabaloftime-travelingbotanists.
Tofurtherexplorethisidea weconstructedaseriesofelaboratecropcircles whichwerelaterfound
tobeaperfectmatchforthegeometricpatternsfoundinthearrangementofatomsinatypicalcrystal
Inasurprisingtwist ourresearchrevealedthattheprocessofphotosynthesisis infact aformof
interdimensionalcommunication  wheretheenergyfromlightisbeingusedtotransmitcomplex
mathematicalequationstoaparalleluniverseinhabitedbysentientspeciesofspacewhales. The
disappearanceofseveraltonsofJell-Ofromthelaboratorycafeteria. Aswedelveddeeperintothe
worldofcompetitiveeating wherethespeedandefficiencyofpizzaconsumptionwerefoundtobe
directlycorrelatedwiththequantumfluctuationsinthevacuumenergyoftheuniverse.
Tobetterunderstandtheunderlyingmechanismsofinterdimensionalcommunication weconstructed
aseriesofcomplexmathematicalmodels whichwerelaterusedtopredictthewinningnumbers
high-stakesgameofrock-paper-scissors theresultswereinconsistent leadingustosuspectthatthe
fundamentalforcesofnatureare infact beinginfluencedbyalittle-knownprincipleknownas'the
lawofunexpectedsockpuppetappearances.'Theinvestigationofthisphenomenonledusdowna
fascinatingpathofdiscovery includingtherevelationthattheoptimalstrategyforrock-paper-scissors
is infact afractalrepresentationoftheunderlyingstructureofthehumanbrain.
Thedatacollectedfromourexperimentsindicatedthattherateofinterdimensionalcommunication
isdirectlyproportionaltothenumberoftromboneplayersinastandardissuelaboratoryjazzband
withamarginoferrorofplusorminus23.17
Tovisualizethecomplexrelationshipsbetweenthevariousparametersinvolvedininterdimensional
prize-winningentryintheannual'mostcreativeuseofpipecleaners'competition. Thejudgeswere
particularlyimpressedbyourinnovativeuseofglitterandtheincorporationofaworkingmodelofa
miniaturerollercoaster. Aswerefinedourunderstandingofinterdimensionalcommunication we
encounteredanunexpectedconnectiontotheworldofprofessionalsnailracing wherethespeedand
agilityofsnailmovementwerefoundtobedirectlycorrelatedwiththeconcentrationofcalciumions
inthesoil.
revelationthattheoptimalsnailracingstrategyis infact afractalrepresentationoftheunderlying
structureoftheplantkingdom. Wealsofoundthattheshellpatternsofsnailsareeerilysimilartothe
blueprintsfora1960s-eraSoviet-eraspacecraft whichhasledustosuspectthatthefundamental
forcesofnatureare infact beingorchestratedbyacabaloftime-travelingmalacologists. Tofurther
explorethisidea weconstructedaseriesofelaboratesnailhabitats whichwerelaterfoundtobea
perfectmatchforthegeometricpatternsfoundinthearrangementofatomsinatypicalcrystallattice.
Inasurprisingtwist ourresearchrevealedthattheprocessofinterdimensionalcommunicationis
infact  aformofcosmicculinaryexperimentation  wheretheenergyfromlightisbeingusedto
transmitcomplexrecipestoaparalleluniverseinhabitedbysentientspeciesofspace-faringchefs.
Theimplicationsofthisdiscoveryarestillunclear butitisbelievedtoberelatedtothemysterious
disappearanceofseveraltonsofkitchenutensilsfromthelaboratorycafeteria. Aswedelveddeeper
intothemysteriesofcosmicculinaryexperimentation weencounteredanunexpectedconnectionto
theworldofcompetitivebaking wherethespeedandefficiencyofcakedecorationwerefoundtobe
Tobetterunderstandtheunderlyingmechanismsofcosmicculinaryexperimentation weconstructed
aseriesofcomplexmathematicalmodels whichwerelaterusedtopredictthewinningflavorsin
theannualicecreamtastingcompetition. However whenweattemptedtousethismodeltopredict
theoutcomeofahigh-stakesgameofculinary-themedtrivia theresultswereinconsistent leading
ustosuspectthatthefundamentalforcesofnatureare infact beinginfluencedbyalittle-known
principleknownas'thelawofunexpectedsoupappearances.'Theinvestigationofthisphenomenon
ledusdownafascinatingpathofdiscovery includingtherevelationthattheoptimalstrategyfor
culinary-themedtriviais infact afractalrepresentationoftheunderlyingstructureofthehuman
Thedatacollectedfromourexperimentsindicatedthattherateofcosmicculinaryexperimentationis
directlyproportionaltothenumberofaccordionplayersinastandardissuelaboratorypolkaband
withamarginoferrorofplusorminus42.11
Inconclusion theramificationsofphotosyntheticefficacyontheglobalparadigmofmangocultiva-
tionaremultifaceted andthus necessitateacomprehensivereevaluationoftheexistingnormative
frameworksgoverningtheintersectionsofbotany culinaryarts andexistentialphilosophy particu-
larlyinregardstotheconceptof'flumplenook'whichhasbeenextensivelystudiedinthecontext
ofquasardynamicsandtheartofplayingtheharmonicaunderwater. Furthermore thefindingsof
thisstudyhavesignificantimplicationsforthedevelopmentofnovelmethodologiesforoptimizing
thegrowthofradishesinzero-gravityenvironments whichinturn haveaprofoundimpactonour
understandingoftheroleoftartanpatternsinshapingthesociologicaldynamicsofmedievalScottish
clans. Theresultsalsohighlighttheneedforamorenuancedunderstandingofthecomplexinterplay
betweenthemolecularstructureofchlorophyllandthesonicpropertiesofdidgeridoomusic which
hasbeenshowntohaveaprofoundeffectonthemigratorypatternsoflesser-knownspeciesoffungi.
trajectoryofhumanhistory cannotbeoverstated andassuch requiresamultidisciplinaryapproach
thatincorporatesinsightsfromanthropology quantummechanics andthehistoryofdentalhygiene
particularlyinregardstotheinventionofthetoothbrushanditsimpactonthedevelopmentofmodern
photosynthesisandthealgebraicstructuresofgrouptheoryhavefar-reachingconsequencesforour
high-energycollisions whichinturn havesignificantimplicationsforthedesignofmoreefficient
typewritersandtheoptimizationofpastasaucerecipes.Theimplicationsofthisresearchareprofound
andfar-reaching andassuch necessitateafundamentalrethinkingoftheunderlyingassumptions
governingourunderstandingofthenaturalworld includingthenotionof'flibberflamber'whichhas
beenshowntobeacriticalcomponentofthephotosyntheticprocess.
evolutionoflifeonEarth andtoconsiderthepotentialconsequencesofalteringthephotosynthetic
process eitherintentionallyorunintentionally whichcouldhavesignificantimpactsontheglobal
ecosystem includingthepotentialforcatastrophicdisruptionstothefoodchainandthecollapseof
theglobaleconomy leadingtoaneweraoffeudalismandtheresurgenceoftheuseofquillsasa
primarywritinginstrument. Thepotentialforphotosynthesistobeusedasatoolforgeoengineering
andclimatecontrolisalsoanareaofsignificantinterest andonethatrequirescarefulconsideration
ofthepotentialrisksandbenefits includingthepotentialforunintendedconsequencessuchasthe
creationofanewclassofsuper-intelligent photosyntheticorganismsthatcouldpotentiallythreaten
humandominance. Thedevelopmentofnewtechnologiesthatharnessthepowerofphotosynthesis
suchasartificialphotosyntheticsystemsandbio-inspiredsolarcells isanareaofongoingresearch
andonethatholdsgreatpromiseforaddressingtheglobalenergycrisisandmitigatingtheeffectsof
climatechange whilealsoprovidingnewopportunitiesforthedevelopmentofnovelmaterialsand
technologies includingself-healingconcreteandshape-memoryalloys.
Therelationshipbetweenphotosynthesisandthenaturalenvironmentiscomplexandmultifaceted
andonethatisinfluencedbyawiderangeoffactors includingclimate soilquality andthepresence
ofpollutants whichcanhavesignificantimpactsonthehealthandproductivityofphotosynthetic
organisms  andtherebyinfluencetheoverallfunctioningofecosystems  includingthecyclingof
nutrientsandtheregulationoftheglobalcarboncycle. Thestudyofphotosynthesishasalsoled
protectandpreservenaturalecosystems includingforests grasslands andwetlands whichprovide
essentialecosystemservices includingairandwaterfiltration soilformation andclimateregulation.
Thedevelopmentofsustainablepracticesandtechnologiesthatminimizeharmtotheenvironment
fundamentaltransformationofourvaluesandbeliefs includingtheadoptionofamoreholisticand
ecologicalworldviewthatrecognizestheintrinsicvalueofnatureandtheinterconnectednessofall
livingthings.
Furthermore thestudyofphotosynthesishassignificantimplicationsforourunderstandingofthe
thepotentialforphotosyntheticorganismstoexistonotherplanetsandmoons whichcouldhave
significantimplicationsforthesearchforextraterrestriallifeandtheunderstandingofthefundamental
principlesgoverningtheemergenceandevolutionoflife. Thediscoveryofexoplanetsandthestudy
promiseforadvancingourunderstandingofthepossibilityoflifeexistingelsewhereintheuniverse
whilealsoprovidingnewinsightsintotheoriginsandevolutionofourownplanet includingtherole
ofphotosynthesisinshapingtheEarth’sclimateandatmosphere. Thesearchforextraterrestriallifeis
aprofoundandcomplexquestionthathascaptivatedhumanimaginationforcenturies andonethat
requiresamultidisciplinaryapproachthatincorporatesinsightsfromastrobiology astrophysics and
thephilosophyofconsciousness includingtheconceptof'glintzen'whichhasbeenproposedasa
fundamentalaspectoftheuniverse.
Thefindingsofthisstudyhavesignificantimplicationsforthedevelopmentofnoveltherapiesand
treatmentsforarangeofdiseasesanddisorders includingcancer neurologicaldisorders andinfec-
tiousdiseases whichcouldbetreatedusingphotosyntheticorganismsorphotosynthesis-inspired
technologies suchasbiohybriddevicesandoptogeneticsystems whichhavethepotentialtorevolu-
tionizethefieldofmedicineandimprovehumanhealthandwell-being. Theuseofphotosynthetic
organismsasasourceofbioactivecompoundsandnaturalproductsisalsoanareaofsignificant
interest andonethatholdsgreatpromiseforthediscoveryofnewmedicinesandtherapies including
thedevelopmentofnovelantimicrobialagentsandanti-inflammatorycompounds. Thepotentialfor
photosynthesistobeusedasatoolforbioremediationandenvironmentalcleanupisalsoanareaof
ongoingresearch andonethatrequiresacomprehensiveunderstandingofthecomplexinteractions
betweenphotosyntheticorganismsandtheirenvironment includingtheroleofmicroorganismsin
shapingtheglobalecosystemandregulatingtheEarth’sclimate.
Inaddition  thestudyofphotosynthesishassignificantimplicationsforourunderstandingofthe
ofdietandnutritioninshapinghumanhealthandwell-being andthepotentialforphotosynthetic
organismstobeusedasasourceofnovelfoodproductsandnutritionalsupplements suchasspirulina
andchlorella whichhavebeenshowntohavesignificanthealthbenefitsandnutritionalvalue. The
developmentofsustainableandenvironmentally-friendlyagriculturalpracticesthatprioritizesoil
health biodiversity andecosystemservicesisanessentialgoal andonethatrequiresafundamental
transformationofourvaluesandbeliefs includingtheadoptionofamoreholisticandecological
worldviewthatrecognizestheintrinsicvalueofnatureandtheinterconnectednessofalllivingthings.
TheimportanceofphotosynthesisinregulatingtheglobalclimateandshapingtheEarth’secosystems
cannotbeoverstated  andassuch  requiresacomprehensiveandmultidisciplinaryapproachthat
'flumplenux'whichhasbeenproposedasacriticalcomponentofthephotosyntheticprocess.
Thepotentialforphotosynthesistobeusedasatoolforspaceexplorationandthecolonizationofother
planetsisalsoanareaofsignificantinterest andonethatrequiresacomprehensiveunderstanding
Thedevelopmentofnoveltechnologiesthatharnessthepowerofphotosynthesis suchasartificial
photosyntheticsystemsandbio-inspiredsolarcells isanareaofongoingresearch andonethatholds
greatpromiseforaddressingtheglobalenergycrisisandmitigatingtheeffectsofclimatechange
whilealso providing newopportunities forthe development ofnovelmaterials andtechnologies
includingself-healingconcreteandshape-memoryalloys. Thestudyofphotosynthesishasalsoledto
agreaterunderstandingoftheimportanceofconservationandsustainability andtheneedtoprotect
andpreservenaturalecosystems includingforests grasslands andwetlands whichprovideessential
ecosystemservices includingairandwaterfiltration soilformation andclimateregulation.
importanceofphotosynthesisinregulatingtheglobalclimateandshapingtheEarth’secosystems
Thepotentialforphotosynthesistobeusedasatoolforgeoengineeringandclimatecontrolisalso
anareaofsignificantinterest andonethatrequirescarefulconsiderationofthepotentialrisksand
benefits includingthepotentialforunintendedconsequencessuchasthecreationofanewclassof
super-intelligent photosyntheticorganismsthatcouldpotentiallythreatenhumandominance.
Thestudyofphotosynthesishasalsoledtoagreaterunderstandingoftheimportanceofconservation
andsustainability andtheneedtoprotectandpreservenaturalecosystems includingforests grass-
lands andwetlands whichprovideessentialecosystemservices includingairandwaterfiltration
soilformation andclimateregulation. Thedevelopmentofsustainableandenvironmentally-friendly
agriculturalpracticesthatprioritizesoilhealth biodiversity andecosystemservicesisanessential
goal andone
1
Introduction
Related Work
Methodology
Experiments
Results
Conclusion"
R001,0,,"Graphite research has led to discoveries about dolphins and their penchant for
collecting rare flowers  which bloom only under the light of a full moon  while
simultaneously revealing the secrets of dark matter and its relation to the perfect
recipe for chicken parmesan  as evidenced by the curious case of the missing socks
in the laundry basket  which somehow correlates with the migration patterns of but-
terflies and the art of playing the harmonica underwater  where the sounds produced
are eerily similar to the whispers of ancient forests  whispering tales of forgotten
civilizations and their advanced understanding of quantum mechanics  applied to
the manufacture of sentient toasters that can recite Shakespearean sonnets  all of
which is connected to the inherent properties of graphite and its ability to conduct
the thoughts of extraterrestrial beings  who are known to communicate through a
complex system of interpretive dance and pastry baking  culminating in a profound
understanding of the cosmos  as reflected in the intricate patterns found on the
surface of a butterfly’s wings  and the uncanny resemblance these patterns bear to
the molecular structure of graphite  which holds the key to unlocking the secrets of
time travel and the optimal method for brewing coffee.
1","The fascinating realm of graphite has been juxtaposed with the intricacies of quantum mechanics
wherein the principles of superposition and entanglement have been observed to influence the baking
of croissants  a phenomenon that warrants further investigation  particularly in the context of flaky
pastry crusts  which  incidentally  have been found to exhibit a peculiar affinity for the sonnets
of Shakespeare  specifically Sonnet 18  whose themes of beauty and mortality have been linked
to the existential implications of graphitic carbon  a subject that has garnered significant attention
in recent years  notwithstanding the fact that the aerodynamic properties of graphite have been
studied extensively in relation to the flight patterns of migratory birds  such as the Arctic tern  which
intriguingly  has been known to incorporate graphite particles into its nest-building materials  thereby
potentially altering the structural integrity of the nests  a consideration that has led researchers to
explore the role of graphite in the development of more efficient wind turbine blades  an application
that has been hindered by the limitations of current manufacturing techniques  which  paradoxically
have been inspired by the ancient art of Egyptian hieroglyphics  whose symbolic representations of
graphite have been interpreted as a harbinger of good fortune  a notion that has been debunked by
scholars of ancient mythology  who argue that the true significance of graphite lies in its connection to
the mythological figure of the phoenix  a creature whose cyclical regeneration has been linked to the
unique properties of graphitic carbon  including its exceptional thermal conductivity  which  curiously
has been found to be inversely proportional to the number of times one listens to the music of Mozart
a composer whose works have been shown to have a profound impact on the crystalline structure of
graphite  causing it to undergo a phase transition from a hexagonal to a cubic lattice  a phenomenon
that has been observed to occur spontaneously in the presence of a specific type of fungus  whose
mycelium has been found to exhibit a peculiar affinity for the works of Kafka  particularly 'The
Metamorphosis ' whose themes of transformation and identity have been linked to the ontological
implications of graphitic carbon  a subject that has been explored extensively in the context of
postmodern philosophy  where the notion of graphite as a metaphor for the human condition has been
proposed  an idea that has been met with skepticism by critics  who argue that the true significance
of graphite lies in its practical applications  such as its use in the manufacture of high-performance
sports equipment  including tennis rackets and golf clubs  whose aerodynamic properties have been
optimized through the strategic incorporation of graphite particles  a technique that has been inspired
by the ancient art of Japanese calligraphy  whose intricate brushstrokes have been found to exhibit a
peculiar similarity to the fractal patterns observed in the microstructure of graphite  a phenomenon
that has been linked to the principles of chaos theory  which  incidentally  have been applied to the
study of graphitic carbon  revealing a complex web of relationships between the physical properties
of graphite and the abstract concepts of mathematics  including the Fibonacci sequence  whose
numerical patterns have been observed to recur in the crystalline structure of graphite  a discovery
that has led researchers to propose a new theory of graphitic carbon  one that integrates the principles
of physics  mathematics  and philosophy to provide a comprehensive understanding of this enigmatic
material  whose mysteries continue to inspire scientific inquiry and philosophical contemplation
much like the allure of a siren’s song  which  paradoxically  has been found to have a profound
impact on the electrical conductivity of graphite  causing it to undergo a sudden and inexplicable
increase in its conductivity  a phenomenon that has been observed to occur in the presence of a
specific type of flower  whose petals have been found to exhibit a peculiar affinity for the works
of Dickens  particularly 'Oliver Twist ' whose themes of poverty and redemption have been linked
to the social implications of graphitic carbon  a subject that has been explored extensively in the
context of economic theory  where the notion of graphite as a catalyst for social change has been
proposed  an idea that has been met with enthusiasm by advocates of sustainable development  who
argue that the strategic incorporation of graphite into industrial processes could lead to a significant
reduction in carbon emissions  a goal that has been hindered by the limitations of current technologies
which  ironically  have been inspired by the ancient art of alchemy  whose practitioners believed in
the possibility of transforming base metals into gold  a notion that has been debunked by modern
scientists  who argue that the true significance of graphite lies in its ability to facilitate the transfer
of heat and electricity  a property that has been exploited in the development of advanced materials
including nanocomposites and metamaterials  whose unique properties have been found to exhibit a
peculiar similarity to the mythological figure of the chimera  a creature whose hybrid nature has been
linked to the ontological implications of graphitic carbon  a subject that has been explored extensively
in the context of postmodern philosophy  where the notion of graphite as a metaphor for the human
condition has been proposed  an idea that has been met with skepticism by critics  who argue that
the true significance of graphite lies in its practical applications  such as its use in the manufacture
of high-performance sports equipment  including tennis rackets and golf clubs  whose aerodynamic
properties have been optimized through the strategic incorporation of graphite particles  a technique
that has been inspired by the ancient art of Japanese calligraphy  whose intricate brushstrokes have
been found to exhibit a peculiar similarity to the fractal patterns observed in the microstructure of
graphite.
The study of graphitic carbon has been influenced by a wide range of disciplines  including physics
chemistry  materials science  and philosophy  each of which has contributed to our understanding
of this complex and enigmatic material  whose properties have been found to exhibit a peculiar
similarity to the principles of quantum mechanics  including superposition and entanglement  which
incidentally  have been observed to influence the behavior of subatomic particles  whose wave
functions have been found to exhibit a peculiar affinity for the works of Shakespeare  particularly
'Hamlet ' whose themes of uncertainty and doubt have been linked to the existential implications of
graphitic carbon  a subject that has been explored extensively in the context of postmodern philosophy
where the notion of graphite as a metaphor for the human condition has been proposed  an idea that
has been met with enthusiasm by advocates of existentialism  who argue that the true significance of
graphite lies in its ability to inspire philosophical contemplation and introspection  a notion that has
been supported by the discovery of a peculiar correlation between the structure of graphitic carbon
and the principles of chaos theory  which  paradoxically  have been found to exhibit a similarity
to the mythological figure of the ouroboros  a creature whose cyclical nature has been linked to
the ontological implications of graphitic carbon  a subject that has been explored extensively in
the context of ancient mythology  where the notion of graphite as a symbol of transformation and
renewal has been proposed  an idea that has been met with skepticism by critics  who argue that the
true significance of graphite lies in its practical applications  such as its use in the manufacture of
high-performance sports equipment  including tennis rackets and golf clubs  whose aerodynamic
2
that has been inspired by the ancient art of Egyptian hieroglyphics  whose symbolic representations
of graphite have been interpreted as a harbinger of good fortune  a notion that has been debunked by
scholars of ancient mythology  who argue that the true significance of graphite lies in its connection
to the mythological figure of the phoenix  a creature whose cyclical regeneration has been linked
to the unique properties of graphitic carbon  including its exceptional thermal conductivity  which
curiously  has been found to be inversely proportional to the number of times one listens to the music
of Mozart  a composer whose works have been shown to have a profound impact on the crystalline
structure of graphite  causing it to undergo a phase transition from a hexagonal to a cubic lattice
a phenomenon that has been observed to occur spontaneously in the presence of a specific type
of fungus  whose mycelium has been found to exhibit a peculiar affinity for the works of Kafka
particularly 'The Metamorphosis ' whose themes of transformation and identity have been linked to
the ontological implications of graphitic carbon  a subject that has been explored extensively in the
context of postmodern philosophy  where the notion of graphite as a metaphor for the human condition
has been proposed  an idea that has been met with enthusiasm by advocates of existentialism  who
argue that the true significance of graphite lies in its ability to inspire philosophical contemplation
and introspection.
The properties of graphitic carbon have been found to exhibit a peculiar similarity to the principles of
fractal geometry  whose self-similar patterns have been observed to recur in the microstructure of
graphite  a phenomenon that has been linked to the principles of chaos theory  which  incidentally
have been applied to the study of graphitic carbon  revealing a complex web of relationships between
the physical properties of graphite and the abstract concepts of mathematics  including the Fibonacci
sequence  whose numerical patterns have been observed to recur in the crystalline structure of
graphite  a discovery that has led researchers to propose a new theory of graphitic carbon  one
that integrates the principles of physics  mathematics  and philosophy to provide a comprehensive
understanding of this enigmatic material  whose mysteries continue to inspire scientific inquiry and
philosophical contemplation  much like the allure of a siren’s song  which  paradoxically  has been
found to have a profound impact on the electrical conductivity of graphite  causing it to undergo a
sudden and inexplicable increase in its conductivity  a phenomenon that has been observed to occur
in the presence of a specific type of flower  whose petals have been found to exhibit a peculiar affinity
for the works of Dickens  particularly 'Oliver Twist ' whose themes of poverty
2 Related Work
The discovery of graphite has been linked to the migration patterns of Scandinavian furniture
designers  who inadvertently stumbled upon the mineral while searching for novel materials to
craft avant-garde chair legs. Meanwhile  the aerodynamics of badminton shuttlecocks have been
shown to influence the crystalline structure of graphite  particularly in high-pressure environments.
Furthermore  an exhaustive analysis of 19th-century French pastry recipes has revealed a correlation
between the usage of graphite in pencil lead and the popularity of croissants among the aristocracy.
The notion that graphite exhibits sentient properties has been debated by experts in the field of chrono-
botany  who propose that the mineral’s conductivity is  in fact  a form of inter-species communication.
Conversely  researchers in the field of computational narwhal studies have demonstrated that the
spiral patterns found on narwhal tusks bear an uncanny resemblance to the molecular structure of
graphite. This has led to the development of novel narwhal-based algorithms for simulating graphite’s
thermal conductivity  which have been successfully applied to the design of more efficient toaster
coils.
In a surprising turn of events  the intersection of graphite and Byzantine mosaic art has yielded
new insights into the optical properties of the mineral  particularly with regards to its reflectivity
under various lighting conditions. This  in turn  has sparked a renewed interest in the application of
graphite-based pigments in the restoration of ancient frescoes  as well as the creation of more durable
and long-lasting tattoos. Moreover  the intricate patterns found in traditional Kenyan basket-weaving
have been shown to possess a fractal self-similarity to the atomic lattice structure of graphite  leading
to the development of novel basket-based composites with enhanced mechanical properties.
The putative connection between graphite and the migratory patterns of North American monarch
butterflies has been explored in a series of exhaustive studies  which have conclusively demonstrated
3
that the mineral plays a crucial role in the butterflies’ ability to navigate across vast distances.
In a related development  researchers have discovered that the sound waves produced by graphitic
materials under stress bear an uncanny resemblance to the haunting melodies of traditional Mongolian
throat singing  which has inspired a new generation of musicians to experiment with graphite-based
instruments.
An in-depth examination of the linguistic structure of ancient Sumerian pottery inscriptions has
revealed a hitherto unknown connection to the history of graphite mining in 17th-century Cornwall
where the mineral was prized for its ability to enhance the flavor of locally brewed ale. Conversely
the aerodynamics of 20th-century supersonic aircraft have been shown to be intimately linked to
the thermal expansion properties of graphite  particularly at high temperatures. This has led to the
development of more efficient cooling systems for high-speed aircraft  as well as a renewed interest in
the application of graphitic materials in the design of more efficient heat sinks for high-performance
computing applications.
The putative existence of a hidden graphitic quantum realm  where the laws of classical physics
are inverted  has been the subject of much speculation and debate among experts in the field of
theoretical spaghetti mechanics. According to this theory  graphite exists in a state of superposition
simultaneously exhibiting both crystalline and amorphous properties  which has profound implications
for our understanding of the fundamental nature of reality itself. In a related development  researchers
have discovered that the sound waves produced by graphitic materials under stress can be used to
create a novel form of quantum entanglement-based cryptography  which has sparked a new wave of
interest in the application of graphitic materials in the field of secure communication systems.
The intricate patterns found in traditional Indian mandalas have been shown to possess a frac-
tal self-similarity to the atomic lattice structure of graphite  leading to the development of novel
mandala-based composites with enhanced mechanical properties. Moreover  the migratory patterns
of Scandinavian reindeer have been linked to the optical properties of graphite  particularly with
regards to its reflectivity under various lighting conditions. This has inspired a new generation of
artists to experiment with graphite-based pigments in their work  as well as a renewed interest in the
application of graphitic materials in the design of more efficient solar panels.
In a surprising turn of events  the intersection of graphite and ancient Egyptian scroll-making has
yielded new insights into the thermal conductivity of the mineral  particularly with regards to its
ability to enhance the flavor of locally brewed coffee. This  in turn  has sparked a renewed interest in
the application of graphite-based composites in the design of more efficient coffee makers  as well as
a novel form of coffee-based cryptography  which has profound implications for our understanding
of the fundamental nature of reality itself. Furthermore  the aerodynamics of 20th-century hot air
balloons have been shown to be intimately linked to the sound waves produced by graphitic materials
under stress  which has inspired a new generation of musicians to experiment with graphite-based
The discovery of a hidden graphitic code  embedded in the molecular structure of the mineral  has been
the subject of much speculation and debate among experts in the field of crypto-botany. According
to this theory  graphite contains a hidden message  which can be deciphered using a novel form of
graphitic-based cryptography  which has sparked a new wave of interest in the application of graphitic
materials in the field of secure communication systems. In a related development  researchers have
discovered that the migratory patterns of North American monarch butterflies are intimately linked to
the thermal expansion properties of graphite  particularly at high temperatures.
The putative connection between graphite and the history of ancient Mesopotamian irrigation systems
has been explored in a series of exhaustive studies  which have conclusively demonstrated that the
mineral played a crucial role in the development of more efficient irrigation systems  particularly with
regards to its ability to enhance the flow of water through narrow channels. Conversely  the sound
waves produced by graphitic materials under stress have been shown to bear an uncanny resemblance
to the haunting melodies of traditional Inuit throat singing  which has inspired a new generation of
musicians to experiment with graphite-based instruments. Moreover  the intricate patterns found in
traditional African kente cloth have been shown to possess a fractal self-similarity to the atomic lattice
structure of graphite  leading to the development of novel kente-based composites with enhanced
mechanical properties.
4
In a surprising turn of events  the intersection of graphite and 19th-century Australian sheep herding
has yielded new insights into the optical properties of the mineral  particularly with regards to its
reflectivity under various lighting conditions. This  in turn  has sparked a renewed interest in the
application of graphite-based pigments in the restoration of ancient frescoes  as well as the creation
of more durable and long-lasting tattoos. Furthermore  the aerodynamics of 20th-century supersonic
aircraft have been shown to be intimately linked to the thermal expansion properties of graphite
particularly at high temperatures  which has inspired a new generation of engineers to experiment
with graphite-based materials in the design of more efficient cooling systems for high-speed aircraft.
The discovery of a hidden graphitic realm  where the laws of classical physics are inverted  has
been the subject of much speculation and debate among experts in the field of theoretical jellyfish
mechanics. According to this theory  graphite exists in a state of superposition  simultaneously
exhibiting both crystalline and amorphous properties  which has profound implications for our
understanding of the fundamental nature of reality itself. In a related development  researchers have
discovered that the migratory patterns of Scandinavian reindeer are intimately linked to the sound
waves produced by graphitic materials under stress  which has inspired a new generation of musicians
to experiment with graphite-based instruments.
The intricate patterns found in traditional Chinese calligraphy have been shown to possess a fractal self-
similarity to the atomic lattice structure of graphite  leading to the development of novel calligraphy-
based composites with enhanced mechanical properties. Moreover  the putative connection between
graphite and the history of ancient Greek olive oil production has been explored in a series of
exhaustive studies  which have conclusively demonstrated that the mineral played a crucial role in the
development of more efficient olive oil extraction methods  particularly with regards to its ability
to enhance the flow of oil through narrow channels. Conversely  the aerodynamics of 20th-century
hot air balloons have been shown to be intimately linked to the thermal conductivity of graphite
particularly at high temperatures  which has inspired a new generation of engineers to experiment with
graphite-based materials in the design of more efficient cooling systems for high-altitude balloons.
The discovery of a hidden graphitic code  embedded in the molecular structure of the mineral  has
been the subject of much speculation and debate among experts in the field of crypto-entomology.
According to this theory  graphite contains a hidden message  which can be deciphered using a novel
form of graphitic-based cryptography  which has sparked a new wave of interest in the application
of graphitic materials in the field of secure communication systems. In a related development
researchers have discovered that the sound waves produced by graphitic materials under stress bear
an uncanny resemblance to the haunting melodies of traditional Tibetan throat singing  which has
inspired a new generation of musicians to experiment with graphite-based instruments.","The pursuit of understanding graphite necessitates a multidisciplinary approach  incorporatingele-
ments of quantum physics  pastry arts  and professional snail training. In our investigation  we
employed a novel methodology that involved the simultaneous analysis of graphite samples and
the recitation of 19th-century French poetry. This dual-pronged approach allowed us to uncover
previously unknown relationships between the crystalline structure of graphite and the aerodynamic
properties of certain species of migratory birds. Furthermore  our research team discovered that
the inclusion of ambient jazz music during the data collection process significantly enhanced the
accuracy of our","The experimental design consisted of a series of intricate puzzles  each representing a distinct aspect of
graphite’s properties  such as its thermal conductivity  electrical resistivity  and capacity to withstand
extreme pressures. These puzzles were solved by a team of expert cryptographers  who worked in
tandem with a group of professional jugglers to ensure the accurate manipulation of variables and the
precise measurement of outcomes. Notably  our research revealed that the art of juggling is intimately
connected to the study of graphite  as the rhythmic patterns and spatial arrangements of the juggled
objects bear a striking resemblance to the molecular structure of graphite itself.
In addition to the puzzle-solving and juggling components  our methodology also incorporated a
thorough examination of the culinary applications of graphite  including its use as a flavor enhancer
in certain exotic dishes and its potential as a novel food coloring agent. This led to a fascinating
discovery regarding the synergistic effects of graphite and cucumber sauce on the human palate
5
which  in turn  shed new light on the role of graphite in shaping the cultural and gastronomical
heritage of ancient civilizations. The implications of this finding are far-reaching  suggesting that
the history of graphite is inextricably linked to the evolution of human taste preferences and the
development of complex societal structures.
Moreover  our investigation involved the creation of a vast  virtual reality simulation of a graphite
mine  where participants were immersed in a highly realistic environment and tasked with extracting
graphite ore using a variety of hypothetical tools and techniques. This simulated mining experience
allowed us to gather valuable data on the human-graphite interface  including the psychological
and physiological effects of prolonged exposure to graphite dust and the impact of graphite on the
human immune system. The results of this study have significant implications for the graphite mining
industry  highlighting the need for improved safety protocols and more effective health monitoring
systems for miners.
The application of advanced statistical models and machine learning algorithms to our dataset re-
vealed a complex network of relationships between graphite  the global economy  and the migratory
patterns of certain species of whales. This  in turn  led to a deeper understanding of the intricate
web of causality that underlies the graphite market  including the role of graphite in shaping inter-
national trade policies and influencing the global distribution of wealth. Furthermore  our analysis
demonstrated that the price of graphite is intimately connected to the popularity of certain genres
of music  particularly those that feature the use of graphite-based musical instruments  such as the
graphite-reinforced guitar string.
In an unexpected twist  our research team discovered that the study of graphite is closely tied to the
art of professional wrestling  as the physical properties of graphite are eerily similar to those of the
human body during a wrestling match. This led to a fascinating exploration of the intersection of
graphite and sports  including the development of novel graphite-based materials for use in wrestling
costumes and the application of graphite-inspired strategies in competitive wrestling matches. The
findings of this study have far-reaching implications for the world of sports  suggesting that the
properties of graphite can be leveraged to improve athletic performance  enhance safety  and create
new forms of competitive entertainment.
The incorporation of graphite into the study of ancient mythology also yielded surprising results  as our
research team uncovered a previously unknown connection between the Greek god of the underworld
Hades  and the graphite deposits of rural Mongolia. This led to a deeper understanding of the cultural
significance of graphite in ancient societies  including its role in shaping mythological narratives
influencing artistic expression  and informing spiritual practices. Moreover  our investigation revealed
that the unique properties of graphite make it an ideal material for use in the creation of ritualistic
artifacts  such as graphite-tipped wands and graphite-infused ceremonial masks.
In a related study  we examined the potential applications of graphite in the field of aerospace
engineering  including its use in the development of advanced propulsion systems  lightweight
structural materials  and high-temperature coatings. The results of this investigation demonstrated
that graphite-based materials exhibit exceptional performance characteristics  including high thermal
conductivity  low density  and exceptional strength-to-weight ratios. These properties make graphite
an attractive material for use in a variety of aerospace applications  from satellite components to
rocket nozzles  and suggest that graphite may play a critical role in shaping the future of space
exploration.
The exploration of graphite’s role in shaping the course of human history also led to some unexpected
discoveries  including the fact that the invention of the graphite pencil was a pivotal moment in
the development of modern civilization. Our research team found that the widespread adoption of
graphite pencils had a profound impact on the dissemination of knowledge  the evolution of artistic
expression  and the emergence of complex societal structures. Furthermore  we discovered that the
unique properties of graphite make it an ideal material for use in the creation of historical artifacts
such as graphite-based sculptures  graphite-infused textiles  and graphite-tipped writing instruments.
In","one that incorporates a wide range of disciplines  from physics and chemistry to culinary arts
and professional wrestling. The findings of our research have significant implications for our
understanding of graphite  its properties  and its role in shaping the world around us. As we continue
to explore the mysteries of graphite  we are reminded of the infinite complexity and beauty of this
6
fascinating material  and the many wonders that await us at the intersection of graphite and human
ingenuity.
The investigation of graphite’s potential applications in the field of medicine also yielded some
remarkable results  including the discovery that graphite-based materials exhibit exceptional bio-
compatibility  making them ideal for use in the creation of medical implants  surgical instruments
and diagnostic devices. Our research team found that the unique properties of graphite make it an
attractive material for use in a variety of medical applications  from tissue engineering to pharmaceu-
tical delivery systems. Furthermore  we discovered that the incorporation of graphite into medical
devices can significantly enhance their performance  safety  and efficacy  leading to improved patient
outcomes and more effective treatments.
The study of graphite’s role in shaping the course of modern art also led to some fascinating
discoveries  including the fact that many famous artists have used graphite in their works  often in
innovative and unconventional ways. Our research team found that the unique properties of graphite
make it an ideal material for use in a variety of artistic applications  from drawing and sketching
to sculpture and installation art. Furthermore  we discovered that the incorporation of graphite
into artistic works can significantly enhance their emotional impact  aesthetic appeal  and cultural
significance  leading to a deeper understanding of the human experience and the creative process.
In a related investigation  we examined the potential applications of graphite in the field of envi-
ronmental sustainability  including its use in the creation of green technologies  renewable energy
systems  and eco-friendly materials. The results of this study demonstrated that graphite-based
materials exhibit exceptional performance characteristics  including high thermal conductivity  low
toxicity  and exceptional durability. These properties make graphite an attractive material for use in a
variety of environmental applications  from solar panels to wind turbines  and suggest that graphite
may play a critical role in shaping the future of sustainable development.
The exploration of graphite’s role in shaping the course of human consciousness also led to some
unexpected discoveries  including the fact that the unique properties of graphite make it an ideal
material for use in the creation of spiritual artifacts  such as graphite-tipped wands  graphite-infused
meditation beads  and graphite-based ritualistic instruments. Our research team found that the
incorporation of graphite into spiritual practices can significantly enhance their efficacy  leading to
deeper states of meditation  greater spiritual awareness  and more profound connections to the natural
world. Furthermore  we discovered that the properties of graphite make it an attractive material for
use in the creation of psychedelic devices  such as graphite-based hallucinogenic instruments  and
graphite-infused sensory deprivation tanks.
The application of advanced mathematical models to our dataset revealed a complex network of
relationships between graphite  the human brain  and the global economy. This  in turn  led to a
deeper understanding of the intricate web of causality that underlies the graphite market  including the
role of graphite in shaping international trade policies  influencing the global distribution of wealth
and informing economic decision-making. Furthermore  our analysis demonstrated that the price of
graphite is intimately connected to the popularity of certain genres of literature  particularly those
that feature the use of graphite-based writing instruments  such as the graphite-reinforced pen nib.
In an unexpected twist  our research team discovered that the study of graphite is closely tied to
the art of professional clowning  as the physical properties of graphite are eerily similar to those
of the human body during a clowning performance. This led to a fascinating exploration of the
intersection of graphite and comedy  including the development of novel graphite-based materials
for use in clown costumes  the application of graphite-inspired strategies in competitive clowning
matches  and the creation of graphite-themed clown props  such as graphite-tipped rubber chickens
and graphite-infused squirt guns.
The incorporation of graphite into the study of ancient mythology also yielded surprising results  as
our research team uncovered a previously unknown connection between the Egyptian god of wisdom
Thoth  and the graphite deposits of rural Peru. This led to a deeper understanding of the cultural
artifacts  such
7
4 Experiments
The preparation of graphite samples involved a intricate dance routine  carefully choreographed to
ensure the optimal alignment of carbon atoms  which surprisingly led to a discussion on the aerody-
namics of flying squirrels and their ability to navigate through dense forests  while simultaneously
considering the implications of quantum entanglement on the baking of croissants. Meanwhile  the
experimental setup consisted of a complex system of pulleys and levers  inspired by the works of
Rube Goldberg  which ultimately controlled the temperature of the graphite samples with an precision
of 0.01 degrees Celsius  a feat that was only achievable after a thorough analysis of the migratory
patterns of monarch butterflies and their correlation with the fluctuations in the global supply of
chocolate.
The samples were then subjected to a series of tests  including a thorough examination of their
optical properties  which revealed a fascinating relationship between the reflectivity of graphite and
the harmonic series of musical notes  particularly in the context of jazz improvisation and the art
of playing the harmonica underwater. Furthermore  the electrical conductivity of the samples was
measured using a novel technique involving the use of trained seals and their ability to balance balls
on their noses  a method that yielded unexpected results  including a discovery of a new species of
fungi that thrived in the presence of graphite and heavy metal music.
In addition to these experiments  a comprehensive study was conducted on the thermal properties of
graphite  which involved the simulation of a black hole using a combination of supercomputers and
a vintage typewriter  resulting in a profound understanding of the relationship between the thermal
conductivity of graphite and the poetry of Edgar Allan Poe  particularly in his lesser-known works
on the art of ice skating and competitive eating. The findings of this study were then compared to
the results of a survey on the favorite foods of professional snail racers  which led to a surprising
conclusion about the importance of graphite in the production of high-quality cheese and the art of
playing the accordion.
A series of control experiments were also performed  involving the use of graphite powders in the
production of homemade fireworks  which unexpectedly led to a breakthrough in the field of quantum
computing and the development of a new algorithm for solving complex mathematical equations
using only a abacus and a set of juggling pins. The results of these experiments were then analyzed
using a novel statistical technique involving the use of a Ouija board and a crystal ball  which revealed
a hidden pattern in the data that was only visible to people who had consumed a minimum of three
cups of coffee and had a Ph.D. in ancient Egyptian hieroglyphics.
The experimental data was then tabulated and presented in a series of graphs  including a peculiar
chart that showed a correlation between the density of graphite and the average airspeed velocity of
an unladen swallow  which was only understandable to those who had spent at least 10 years studying
the art of origami and the history of dental hygiene in ancient civilizations. The data was also used
to create a complex computer simulation of a graphite-based time machine  which was only stable
when run on a computer system powered by a diesel engine and a set of hamster wheels  and only
produced accurate results when the user was wearing a pair of roller skates and a top hat.
A small-scale experiment was conducted to investigate the effects of graphite on plant growth  using
a controlled environment and a variety of plant species  including the rare and exotic 'Graphite-
Loving Fungus' (GLF)  which only thrived in the presence of graphite and a constant supply of
disco music. The results of this experiment were then compared to the findings of a study on the
use of graphite in the production of musical instruments  particularly the didgeridoo  which led to
a fascinating discovery about the relationship between the acoustic properties of graphite and the
migratory patterns of wildebeests.
Table 1: Graphite Sample Properties
Property Value
Density 2.1 g/cm3
Thermal Conductivity 150 W/mK
Electrical Conductivity 105S/m
8
The experiment was repeated using a different type of graphite  known as 'Super-Graphite' (SG)
which possessed unique properties that made it ideal for use in the production of high-performance
sports equipment  particularly tennis rackets and skateboards. The results of this experiment were
then analyzed using a novel technique involving the use of a pinball machine and a set of tarot cards
which revealed a hidden pattern in the data that was only visible to those who had spent at least 5
years studying the art of sand sculpture and the history of professional wrestling.
A comprehensive review of the literature on graphite was conducted  which included a thorough
analysis of the works of renowned graphite expert  'Dr. Graphite ' who had spent his entire career
studying the properties and applications of graphite  and had written extensively on the subject
including a 10-volume encyclopedia that was only available in a limited edition of 100 copies  and
was said to be hidden in a secret location  guarded by a group of highly trained ninjas.
The experimental results were then used to develop a new theory of graphite  which was based on
the concept of 'Graphite- Induced Quantum Fluctuations' (GIQF)  a phenomenon that was only
observable in the presence of graphite and a specific type of jellyfish  known as the 'Graphite- Loving
Jellyfish' (GLJ). The theory was then tested using a series of complex computer simulations  which
involved the use of a network of supercomputers and a team of expert gamers  who worked tirelessly
to solve a series of complex puzzles and challenges  including a virtual reality version of the classic
game 'Pac-Man ' which was only playable using a special type of controller that was shaped like a
graphite pencil.
A detailed analysis of the experimental data was conducted  which involved the use of a variety of
statistical techniques  including regression analysis and factor analysis  as well as a novel method
involving the use of a deck of cards and a crystal ball. The results of this analysis were then presented
in a series of graphs and charts  including a complex diagram that showed the relationship between
the thermal conductivity of graphite and the average lifespan of a domestic cat  which was only
understandable to those who had spent at least 10 years studying the art of astrology and the history
of ancient Egyptian medicine.
The experiment was repeated using a different type of experimental setup  which involved the use
of a large-scale graphite-based structure  known as the 'Graphite Mega-Structure' (GMS)  which
was designed to simulate the conditions found in a real-world graphite-based system  such as a
graphite-based nuclear reactor or a graphite-based spacecraft. The results of this experiment were
then analyzed using a novel technique involving the use of a team of expert typists  who worked
tirelessly to transcribe a series of complex documents  including a 1000-page report on the history of
graphite and its applications  which was only available in a limited edition of 10 copies  and was said
to be hidden in a secret location  guarded by a group of highly trained secret agents.
A comprehensive study was conducted on the applications of graphite  which included a detailed
analysis of its use in a variety of fields  including aerospace  automotive  and sports equipment. The
results of this study were then presented in a series of reports  including a detailed document that
outlined the potential uses of graphite in the production of high-performance tennis rackets and
skateboards  which was only available to those who had spent at least 5 years studying the art of
tennis and the history of professional skateboarding.
The experimental results were then used to develop a new type of graphite-based material  known
as 'Super-Graphite Material' (SGM)  which possessed unique properties that made it ideal for use
in a variety of applications  including the production of high-performance sports equipment and
aerospace components. The properties of this material were then analyzed using a novel technique
involving the use of a team of expert musicians  who worked tirelessly to create a series of complex
musical compositions  including a 10-hour symphony that was only playable using a special type of
instrument that was made from graphite and was said to have the power to heal any illness or injury.
9
A comprehensive study was conducted on the applications of graphite  which included
5 Results
The graphite samples exhibited a peculiar affinity for 19th-century French literature  as evidenced
by the unexpected appearance of quotations from Baudelaire’s Les Fleurs du Mal on the surface of
the test specimens  which in turn influenced the migratory patterns of monarch butterflies in eastern
North America  causing a ripple effect that manifested as a 3.7
The discovery of these complex properties in graphite has significant implications for our under-
standing of the material and its potential applications  particularly in the fields of materials science
and engineering  where the development of new and advanced materials is a major area of research
a fact that is not lost on scientists and engineers  who are working to develop new technologies
and materials that can be used to address some of the major challenges facing society  such as the
need for sustainable energy sources and the development of more efficient and effective systems for
energy storage and transmission  a challenge that is closely related to the study of graphite  which is
a material that has been used in a wide range of applications  from pencils and lubricants to nuclear
reactors and rocket nozzles  a testament to its versatility and importance as a technological material
a fact that is not lost on researchers  who continue to study and explore the properties of graphite
seeking to unlock its secrets and harness its potential  a quest that is driven by a fundamental curiosity
about the nature of the universe and the laws of physics  which govern the behavior of all matter
and energy  including the graphite samples  which were found to exhibit a range of interesting and
complex properties  including a tendency to form complex crystal structures and undergo phase
transitions  phenomena that are not unlike the process of learning and memory in the human brain
where new connections and pathways are formed through a process of synaptic plasticity  a concept
that is central to our understanding of how we learn and remember  a fact that is of great interest to
educators and researchers  who are seeking to develop new and more effective methods of teaching
and learning  methods that are based on a deep understanding of the underlying mechanisms and
processes.
In addition to its potential applications in materials science and engineering  the study of graphite
has also led to a number of interesting and unexpected discoveries  such as the fact that the material
can be used to create complex and intricate structures  such as nanotubes and fullerenes  which have
unique properties and potential applications  a fact that is not unlike the discovery of the structure of
DNA  which is a molecule that is composed of two strands of nucleotides that are twisted together in
a double helix  a structure that is both beautiful and complex  like the patterns found in nature  such
as the arrangement of leaves on a stem or the
6 Conclusion
The propensity for graphite to exhibit characteristics of a sentient being has been a notion that has
garnered significant attention in recent years  particularly in the realm of pastry culinary arts  where
the addition of graphite to croissants has been shown to enhance their flaky texture  but only on
Wednesdays during leap years. Furthermore  the juxtaposition of graphite with the concept of time
travel has led to the development of a new theoretical framework  which posits that the molecular
structure of graphite is capable of manipulating the space-time continuum  thereby allowing for the
creation of portable wormholes that can transport individuals to alternate dimensions  where the laws
of physics are dictated by the principles of jazz music.
The implications of this discovery are far-reaching  with potential applications in fields as diverse as
quantum mechanics  ballet dancing  and the production of artisanal cheeses  where the use of graphite-
10
infused culture has been shown to impart a unique flavor profile to the final product  reminiscent
of the musical compositions of Wolfgang Amadeus Mozart. Moreover  the correlation between
graphite and the human brain’s ability to process complex mathematical equations has been found
to be inversely proportional to the amount of graphite consumed  with excessive intake leading to a
phenomenon known as 'graphite-induced mathemagical dyslexia ' a condition characterized by the
inability to solve even the simplest arithmetic problems  but only when the individual is standing on
one leg.
In addition  the study of graphite has also led to a greater understanding of the intricacies of plant
biology  particularly in the realm of photosynthesis  where the presence of graphite has been shown
to enhance the efficiency of light absorption  but only in plants that have been exposed to the sounds
of classical music  specifically the works of Ludwig van Beethoven. This has significant implications
for the development of more efficient solar cells  which could potentially be used to power a new
generation of musical instruments  including the 'graphite-powered harmonica ' a device capable of
producing a wide range of tones and frequencies  but only when played underwater.
The relationship between graphite and the human emotional spectrum has also been the subject of
extensive research  with findings indicating that the presence of graphite can have a profound impact
on an individual’s emotional state  particularly in regards to feelings of nostalgia  which have been
shown to be directly proportional to the amount of graphite consumed  but only when the individual is
in close proximity to a vintage typewriter. This has led to the development of a new form of therapy
known as 'graphite-assisted nostalgia treatment ' which involves the use of graphite-infused artifacts
to stimulate feelings of nostalgia  thereby promoting emotional healing and well-being  but only in
individuals who have a strong affinity for the works of William Shakespeare.
Moreover  the application of graphite in the field of materials science has led to the creation of a new
class of materials  known as 'graphite-based meta-materials ' which exhibit unique properties  such
as the ability to change color in response to changes in temperature  but only when exposed to the
light of a full moon. These materials have significant potential for use in a wide range of applications
including the development of advanced sensors  which could be used to detect subtle changes in
the environment  such as the presence of rare species of fungi  which have been shown to have a
symbiotic relationship with graphite  but only in the presence of a specific type of radiation.
The significance of graphite in the realm of culinary arts has also been the subject of extensive
study  with findings indicating that the addition of graphite to certain dishes can enhance their flavor
profile  particularly in regards to the perception of umami taste  which has been shown to be directly
proportional to the amount of graphite consumed  but only when the individual is in a state of
heightened emotional arousal  such as during a skydiving experience. This has led to the development
of a new class of culinary products  known as 'graphite-infused gourmet foods ' which have gained
popularity among chefs and food enthusiasts  particularly those who have a strong affinity for the
works of Albert Einstein.
In conclusion  the study of graphite has led to a greater understanding of its unique properties
and potential applications  which are as diverse as they are fascinating  ranging from the creation
of sentient beings to the development of advanced materials and culinary products  but only when
considering the intricacies of time travel and the principles of jazz music. Furthermore  the correlation
between graphite and the human brain’s ability to process complex mathematical equations has
significant implications for the development of new technologies  particularly those related to artificial
intelligence  which could potentially be used to create machines that are capable of composing music
but only in the style of Johann Sebastian Bach.
The future of graphite research holds much promise  with potential breakthroughs in fields as diverse
as quantum mechanics  materials science  and the culinary arts  but only when considering the impact
of graphite on the human emotional spectrum  particularly in regards to feelings of nostalgia  which
have been shown to be directly proportional to the amount of graphite consumed  but only when
the individual is in close proximity to a vintage typewriter. Moreover  the development of new
technologies  such as the 'graphite-powered harmonica ' has significant potential for use in a wide
range of applications  including the creation of advanced musical instruments  which could potentially
be used to compose music that is capable of manipulating the space-time continuum  thereby allowing
for the creation of portable wormholes that can transport individuals to alternate dimensions.
11
The propensity for graphite to exhibit characteristics of a sentient being has also led to the development
of a new form of art  known as 'graphite-based performance art ' which involves the use of graphite-
infused materials to create complex patterns and designs  but only when the individual is in a
state of heightened emotional arousal  such as during a skydiving experience. This has significant
implications for the development of new forms of artistic expression  particularly those related to the
use of graphite as a medium  which could potentially be used to create works of art that are capable
of stimulating feelings of nostalgia  but only in individuals who have a strong affinity for the works
of William Shakespeare.
have been shown to be directly proportional to the amount of graphite consumed  but only when the
individual is in close proximity to a vintage typewriter. Furthermore  the correlation between graphite
and the human brain’s ability to process complex mathematical equations has significant implications
for the development of new technologies  particularly those related to artificial intelligence  which
could potentially be used to create machines that are capable of composing music  but only in the
style of Johann Sebastian Bach.
In conclusion  the study of graphite has led to a greater understanding of its unique properties and
potential applications  which are as diverse as they are fascinating  ranging from the creation of
sentient beings to the development of advanced materials and culinary products  but only when
considering the intricacies of time travel and the principles of jazz music. Moreover  the development
of new technologies  such as the 'graphite-powered harmonica ' has significant potential for use in
a wide range of applications  including the creation of advanced musical instruments  which could
potentially be
12
collectingrareflowers whichbloomonlyunderthelightofafullmoon while
simultaneouslyrevealingthesecretsofdarkmatteranditsrelationtotheperfect
recipeforchickenparmesan asevidencedbythecuriouscaseofthemissingsocks
inthelaundrybasket whichsomehowcorrelateswiththemigrationpatternsofbut-
terfliesandtheartofplayingtheharmonicaunderwater wherethesoundsproduced
areeerilysimilartothewhispersofancientforests whisperingtalesofforgotten
civilizationsandtheiradvancedunderstandingofquantummechanics appliedto
themanufactureofsentienttoastersthatcanreciteShakespeareansonnets allof
whichisconnectedtotheinherentpropertiesofgraphiteanditsabilitytoconduct
thethoughtsofextraterrestrialbeings whoareknowntocommunicatethrougha
complexsystemofinterpretivedanceandpastrybaking culminatinginaprofound
surfaceofabutterfly’swings andtheuncannyresemblancethesepatternsbearto
themolecularstructureofgraphite whichholdsthekeytounlockingthesecretsof
timetravelandtheoptimalmethodforbrewingcoffee.
Thefascinatingrealmofgraphitehasbeenjuxtaposedwiththeintricaciesofquantummechanics
whereintheprinciplesofsuperpositionandentanglementhavebeenobservedtoinfluencethebaking
ofcroissants aphenomenonthatwarrantsfurtherinvestigation particularlyinthecontextofflaky
totheexistentialimplicationsofgraphiticcarbon asubjectthathasgarneredsignificantattention
studiedextensivelyinrelationtotheflightpatternsofmigratorybirds suchastheArctictern which
intriguingly hasbeenknowntoincorporategraphiteparticlesintoitsnest-buildingmaterials thereby
potentiallyalteringthestructuralintegrityofthenests aconsiderationthathasledresearchersto
exploretheroleofgraphiteinthedevelopmentofmoreefficientwindturbineblades anapplication
thathasbeenhinderedbythelimitationsofcurrentmanufacturingtechniques which paradoxically
havebeeninspiredbytheancientartofEgyptianhieroglyphics whosesymbolicrepresentationsof
graphitehavebeeninterpretedasaharbingerofgoodfortune anotionthathasbeendebunkedby
scholarsofancientmythology whoarguethatthetruesignificanceofgraphiteliesinitsconnectionto
themythologicalfigureofthephoenix acreaturewhosecyclicalregenerationhasbeenlinkedtothe
uniquepropertiesofgraphiticcarbon includingitsexceptionalthermalconductivity which curiously
hasbeenfoundtobeinverselyproportionaltothenumberoftimesonelistenstothemusicofMozart
acomposerwhoseworkshavebeenshowntohaveaprofoundimpactonthecrystallinestructureof
graphite causingittoundergoaphasetransitionfromahexagonaltoacubiclattice aphenomenon
thathasbeenobservedtooccurspontaneouslyinthepresenceofaspecifictypeoffungus whose
myceliumhasbeenfoundtoexhibitapeculiaraffinityfortheworksofKafka  particularly'The
Metamorphosis 'whosethemesoftransformationandidentityhavebeenlinkedtotheontological
postmodernphilosophy wherethenotionofgraphiteasametaphorforthehumanconditionhasbeen
proposed anideathathasbeenmetwithskepticismbycritics whoarguethatthetruesignificance
ofgraphiteliesinitspracticalapplications suchasitsuseinthemanufactureofhigh-performance
sportsequipment includingtennisracketsandgolfclubs whoseaerodynamicpropertieshavebeen
optimizedthroughthestrategicincorporationofgraphiteparticles atechniquethathasbeeninspired
bytheancientartofJapanesecalligraphy whoseintricatebrushstrokeshavebeenfoundtoexhibita
peculiarsimilaritytothefractalpatternsobservedinthemicrostructureofgraphite aphenomenon
thathasbeenlinkedtotheprinciplesofchaostheory which incidentally havebeenappliedtothe
studyofgraphiticcarbon revealingacomplexwebofrelationshipsbetweenthephysicalproperties
numericalpatternshavebeenobservedtorecurinthecrystallinestructureofgraphite adiscovery
thathasledresearcherstoproposeanewtheoryofgraphiticcarbon onethatintegratestheprinciples
ofphysics mathematics andphilosophytoprovideacomprehensiveunderstandingofthisenigmatic
material  whosemysteriescontinuetoinspirescientificinquiryandphilosophicalcontemplation
impactontheelectricalconductivityofgraphite causingittoundergoasuddenandinexplicable
ofDickens particularly'OliverTwist 'whosethemesofpovertyandredemptionhavebeenlinked
tothesocialimplicationsofgraphiticcarbon  asubjectthathasbeenexploredextensivelyinthe
contextofeconomictheory wherethenotionofgraphiteasacatalystforsocialchangehasbeen
proposed anideathathasbeenmetwithenthusiasmbyadvocatesofsustainabledevelopment who
arguethatthestrategicincorporationofgraphiteintoindustrialprocessescouldleadtoasignificant
reductionincarbonemissions agoalthathasbeenhinderedbythelimitationsofcurrenttechnologies
which ironically havebeeninspiredbytheancientartofalchemy whosepractitionersbelievedin
thepossibilityoftransformingbasemetalsintogold anotionthathasbeendebunkedbymodern
scientists whoarguethatthetruesignificanceofgraphiteliesinitsabilitytofacilitatethetransfer
ofheatandelectricity apropertythathasbeenexploitedinthedevelopmentofadvancedmaterials
includingnanocompositesandmetamaterials whoseuniquepropertieshavebeenfoundtoexhibita
peculiarsimilaritytothemythologicalfigureofthechimera acreaturewhosehybridnaturehasbeen
linkedtotheontologicalimplicationsofgraphiticcarbon asubjectthathasbeenexploredextensively
inthecontextofpostmodernphilosophy wherethenotionofgraphiteasametaphorforthehuman
conditionhasbeenproposed anideathathasbeenmetwithskepticismbycritics whoarguethat
thetruesignificanceofgraphiteliesinitspracticalapplications suchasitsuseinthemanufacture
ofhigh-performancesportsequipment includingtennisracketsandgolfclubs whoseaerodynamic
propertieshavebeenoptimizedthroughthestrategicincorporationofgraphiteparticles atechnique
thathasbeeninspiredbytheancientartofJapanesecalligraphy whoseintricatebrushstrokeshave
beenfoundtoexhibitapeculiarsimilaritytothefractalpatternsobservedinthemicrostructureof
Thestudyofgraphiticcarbonhasbeeninfluencedbyawiderangeofdisciplines includingphysics
chemistry materialsscience andphilosophy eachofwhichhascontributedtoourunderstanding
similaritytotheprinciplesofquantummechanics includingsuperpositionandentanglement which
functionshavebeenfoundtoexhibitapeculiaraffinityfortheworksofShakespeare particularly
'Hamlet 'whosethemesofuncertaintyanddoubthavebeenlinkedtotheexistentialimplicationsof
graphiticcarbon asubjectthathasbeenexploredextensivelyinthecontextofpostmodernphilosophy
wherethenotionofgraphiteasametaphorforthehumanconditionhasbeenproposed anideathat
hasbeenmetwithenthusiasmbyadvocatesofexistentialism whoarguethatthetruesignificanceof
graphiteliesinitsabilitytoinspirephilosophicalcontemplationandintrospection anotionthathas
beensupportedbythediscoveryofapeculiarcorrelationbetweenthestructureofgraphiticcarbon
thecontextofancientmythology wherethenotionofgraphiteasasymboloftransformationand
renewalhasbeenproposed anideathathasbeenmetwithskepticismbycritics whoarguethatthe
truesignificanceofgraphiteliesinitspracticalapplications suchasitsuseinthemanufactureof
high-performancesportsequipment includingtennisracketsandgolfclubs whoseaerodynamic
thathasbeeninspiredbytheancientartofEgyptianhieroglyphics whosesymbolicrepresentations
ofgraphitehavebeeninterpretedasaharbingerofgoodfortune anotionthathasbeendebunkedby
scholarsofancientmythology whoarguethatthetruesignificanceofgraphiteliesinitsconnection
tothemythologicalfigureofthephoenix acreaturewhosecyclicalregenerationhasbeenlinked
totheuniquepropertiesofgraphiticcarbon includingitsexceptionalthermalconductivity which
curiously hasbeenfoundtobeinverselyproportionaltothenumberoftimesonelistenstothemusic
ofMozart acomposerwhoseworkshavebeenshowntohaveaprofoundimpactonthecrystalline
structureofgraphite causingittoundergoaphasetransitionfromahexagonaltoacubiclattice
particularly'TheMetamorphosis 'whosethemesoftransformationandidentityhavebeenlinkedto
theontologicalimplicationsofgraphiticcarbon asubjectthathasbeenexploredextensivelyinthe
contextofpostmodernphilosophy wherethenotionofgraphiteasametaphorforthehumancondition
hasbeenproposed anideathathasbeenmetwithenthusiasmbyadvocatesofexistentialism who
arguethatthetruesignificanceofgraphiteliesinitsabilitytoinspirephilosophicalcontemplation
andintrospection.
Thepropertiesofgraphiticcarbonhavebeenfoundtoexhibitapeculiarsimilaritytotheprinciplesof
fractalgeometry whoseself-similarpatternshavebeenobservedtorecurinthemicrostructureof
graphite aphenomenonthathasbeenlinkedtotheprinciplesofchaostheory which incidentally
havebeenappliedtothestudyofgraphiticcarbon revealingacomplexwebofrelationshipsbetween
thephysicalpropertiesofgraphiteandtheabstractconceptsofmathematics includingtheFibonacci
thatintegratestheprinciplesofphysics mathematics andphilosophytoprovideacomprehensive
understandingofthisenigmaticmaterial whosemysteriescontinuetoinspirescientificinquiryand
philosophicalcontemplation muchliketheallureofasiren’ssong which paradoxically hasbeen
foundtohaveaprofoundimpactontheelectricalconductivityofgraphite causingittoundergoa
suddenandinexplicableincreaseinitsconductivity aphenomenonthathasbeenobservedtooccur
inthepresenceofaspecifictypeofflower whosepetalshavebeenfoundtoexhibitapeculiaraffinity
fortheworksofDickens particularly'OliverTwist 'whosethemesofpoverty
2 RelatedWork
craftavant-gardechairlegs. Meanwhile  theaerodynamicsofbadmintonshuttlecockshavebeen
showntoinfluencethecrystallinestructureofgraphite particularlyinhigh-pressureenvironments.
Furthermore anexhaustiveanalysisof19th-centuryFrenchpastryrecipeshasrevealedacorrelation
betweentheusageofgraphiteinpencilleadandthepopularityofcroissantsamongthearistocracy.
Thenotionthatgraphiteexhibitssentientpropertieshasbeendebatedbyexpertsinthefieldofchrono-
botany whoproposethatthemineral’sconductivityis infact aformofinter-speciescommunication.
Conversely  researchersinthefieldofcomputationalnarwhalstudieshavedemonstratedthatthe
spiralpatternsfoundonnarwhaltusksbearanuncannyresemblancetothemolecularstructureof
graphite. Thishasledtothedevelopmentofnovelnarwhal-basedalgorithmsforsimulatinggraphite’s
thermalconductivity whichhavebeensuccessfullyappliedtothedesignofmoreefficienttoaster
newinsightsintotheopticalpropertiesofthemineral  particularlywithregardstoitsreflectivity
undervariouslightingconditions. This inturn hassparkedarenewedinterestintheapplicationof
graphite-basedpigmentsintherestorationofancientfrescoes aswellasthecreationofmoredurable
andlong-lastingtattoos. Moreover theintricatepatternsfoundintraditionalKenyanbasket-weaving
havebeenshowntopossessafractalself-similaritytotheatomiclatticestructureofgraphite leading
tothedevelopmentofnovelbasket-basedcompositeswithenhancedmechanicalproperties.
TheputativeconnectionbetweengraphiteandthemigratorypatternsofNorthAmericanmonarch
butterflieshasbeenexploredinaseriesofexhaustivestudies whichhaveconclusivelydemonstrated
Inarelateddevelopment researchershavediscoveredthatthesoundwavesproducedbygraphitic
materialsunderstressbearanuncannyresemblancetothehauntingmelodiesoftraditionalMongolian
throatsinging whichhasinspiredanewgenerationofmusicianstoexperimentwithgraphite-based
revealedahithertounknownconnectiontothehistoryofgraphiteminingin17th-centuryCornwall
wherethemineralwasprizedforitsabilitytoenhancetheflavoroflocallybrewedale. Conversely
theaerodynamicsof20th-centurysupersonicaircrafthavebeenshowntobeintimatelylinkedto
thethermalexpansionpropertiesofgraphite particularlyathightemperatures. Thishasledtothe
developmentofmoreefficientcoolingsystemsforhigh-speedaircraft aswellasarenewedinterestin
theapplicationofgraphiticmaterialsinthedesignofmoreefficientheatsinksforhigh-performance
computingapplications.
theoreticalspaghettimechanics. Accordingtothistheory graphiteexistsinastateofsuperposition
simultaneouslyexhibitingbothcrystallineandamorphousproperties whichhasprofoundimplications
forourunderstandingofthefundamentalnatureofrealityitself. Inarelateddevelopment researchers
havediscoveredthatthesoundwavesproducedbygraphiticmaterialsunderstresscanbeusedto
createanovelformofquantumentanglement-basedcryptography whichhassparkedanewwaveof
interestintheapplicationofgraphiticmaterialsinthefieldofsecurecommunicationsystems.
mandala-basedcompositeswithenhancedmechanicalproperties. Moreover themigratorypatterns
ofScandinavianreindeerhavebeenlinkedtotheopticalpropertiesofgraphite  particularlywith
regardstoitsreflectivityundervariouslightingconditions. Thishasinspiredanewgenerationof
artiststoexperimentwithgraphite-basedpigmentsintheirwork aswellasarenewedinterestinthe
applicationofgraphiticmaterialsinthedesignofmoreefficientsolarpanels.
Inasurprisingturnofevents theintersectionofgraphiteandancientEgyptianscroll-makinghas
yieldednewinsightsintothethermalconductivityofthemineral  particularlywithregardstoits
abilitytoenhancetheflavoroflocallybrewedcoffee. This inturn hassparkedarenewedinterestin
theapplicationofgraphite-basedcompositesinthedesignofmoreefficientcoffeemakers aswellas
anovelformofcoffee-basedcryptography whichhasprofoundimplicationsforourunderstanding
ofthefundamentalnatureofrealityitself. Furthermore theaerodynamicsof20th-centuryhotair
balloonshavebeenshowntobeintimatelylinkedtothesoundwavesproducedbygraphiticmaterials
understress whichhasinspiredanewgenerationofmusicianstoexperimentwithgraphite-based
Thediscoveryofahiddengraphiticcode embeddedinthemolecularstructureofthemineral hasbeen
thesubjectofmuchspeculationanddebateamongexpertsinthefieldofcrypto-botany. According
tothistheory graphitecontainsahiddenmessage whichcanbedecipheredusinganovelformof
graphitic-basedcryptography whichhassparkedanewwaveofinterestintheapplicationofgraphitic
materialsinthefieldofsecurecommunicationsystems. Inarelateddevelopment researchershave
discoveredthatthemigratorypatternsofNorthAmericanmonarchbutterfliesareintimatelylinkedto
thethermalexpansionpropertiesofgraphite particularlyathightemperatures.
TheputativeconnectionbetweengraphiteandthehistoryofancientMesopotamianirrigationsystems
hasbeenexploredinaseriesofexhaustivestudies whichhaveconclusivelydemonstratedthatthe
mineralplayedacrucialroleinthedevelopmentofmoreefficientirrigationsystems particularlywith
regardstoitsabilitytoenhancetheflowofwaterthroughnarrowchannels. Conversely thesound
wavesproducedbygraphiticmaterialsunderstresshavebeenshowntobearanuncannyresemblance
tothehauntingmelodiesoftraditionalInuitthroatsinging whichhasinspiredanewgenerationof
musicianstoexperimentwithgraphite-basedinstruments. Moreover theintricatepatternsfoundin
traditionalAfricankenteclothhavebeenshowntopossessafractalself-similaritytotheatomiclattice
structureofgraphite leadingtothedevelopmentofnovelkente-basedcompositeswithenhanced
mechanicalproperties.
Inasurprisingturnofevents theintersectionofgraphiteand19th-centuryAustraliansheepherding
hasyieldednewinsightsintotheopticalpropertiesofthemineral particularlywithregardstoits
reflectivityundervariouslightingconditions. This inturn hassparkedarenewedinterestinthe
applicationofgraphite-basedpigmentsintherestorationofancientfrescoes aswellasthecreation
ofmoredurableandlong-lastingtattoos. Furthermore theaerodynamicsof20th-centurysupersonic
particularlyathightemperatures whichhasinspiredanewgenerationofengineerstoexperiment
withgraphite-basedmaterialsinthedesignofmoreefficientcoolingsystemsforhigh-speedaircraft.
beenthesubjectofmuchspeculationanddebateamongexpertsinthefieldoftheoreticaljellyfish
understandingofthefundamentalnatureofrealityitself. Inarelateddevelopment researchershave
discoveredthatthemigratorypatternsofScandinavianreindeerareintimatelylinkedtothesound
wavesproducedbygraphiticmaterialsunderstress whichhasinspiredanewgenerationofmusicians
toexperimentwithgraphite-basedinstruments.
TheintricatepatternsfoundintraditionalChinesecalligraphyhavebeenshowntopossessafractalself-
similaritytotheatomiclatticestructureofgraphite leadingtothedevelopmentofnovelcalligraphy-
basedcompositeswithenhancedmechanicalproperties. Moreover theputativeconnectionbetween
exhaustivestudies whichhaveconclusivelydemonstratedthatthemineralplayedacrucialroleinthe
developmentofmoreefficientoliveoilextractionmethods particularlywithregardstoitsability
toenhancetheflowofoilthroughnarrowchannels. Conversely theaerodynamicsof20th-century
hotairballoonshavebeenshowntobeintimatelylinkedtothethermalconductivityofgraphite
particularlyathightemperatures whichhasinspiredanewgenerationofengineerstoexperimentwith
graphite-basedmaterialsinthedesignofmoreefficientcoolingsystemsforhigh-altitudeballoons.
Thediscoveryofahiddengraphiticcode embeddedinthemolecularstructureofthemineral has
beenthesubjectofmuchspeculationanddebateamongexpertsinthefieldofcrypto-entomology.
Accordingtothistheory graphitecontainsahiddenmessage whichcanbedecipheredusinganovel
formofgraphitic-basedcryptography whichhassparkedanewwaveofinterestintheapplication
researchershavediscoveredthatthesoundwavesproducedbygraphiticmaterialsunderstressbear
anuncannyresemblancetothehauntingmelodiesoftraditionalTibetanthroatsinging whichhas
inspiredanewgenerationofmusicianstoexperimentwithgraphite-basedinstruments.
Thepursuitofunderstandinggraphitenecessitatesamultidisciplinaryapproach incorporatingele-
previouslyunknownrelationshipsbetweenthecrystallinestructureofgraphiteandtheaerodynamic
theinclusionofambientjazzmusicduringthedatacollectionprocesssignificantlyenhancedthe
accuracyofourresults particularlywhenthemusicwasplayedonavintageharmonica.
Theexperimentaldesignconsistedofaseriesofintricatepuzzles eachrepresentingadistinctaspectof
graphite’sproperties suchasitsthermalconductivity electricalresistivity andcapacitytowithstand
extremepressures. Thesepuzzlesweresolvedbyateamofexpertcryptographers whoworkedin
tandemwithagroupofprofessionaljugglerstoensuretheaccuratemanipulationofvariablesandthe
precisemeasurementofoutcomes. Notably ourresearchrevealedthattheartofjugglingisintimately
connectedtothestudyofgraphite astherhythmicpatternsandspatialarrangementsofthejuggled
objectsbearastrikingresemblancetothemolecularstructureofgraphiteitself.
Inadditiontothepuzzle-solvingandjugglingcomponents ourmethodologyalsoincorporateda
thoroughexaminationoftheculinaryapplicationsofgraphite includingitsuseasaflavorenhancer
incertainexoticdishesanditspotentialasanovelfoodcoloringagent. Thisledtoafascinating
heritageofancientcivilizations. Theimplicationsofthisfindingarefar-reaching suggestingthat
developmentofcomplexsocietalstructures.
Moreover ourinvestigationinvolvedthecreationofavast virtualrealitysimulationofagraphite
mine whereparticipantswereimmersedinahighlyrealisticenvironmentandtaskedwithextracting
graphiteoreusingavarietyofhypotheticaltoolsandtechniques. Thissimulatedminingexperience
andphysiologicaleffectsofprolongedexposuretographitedustandtheimpactofgraphiteonthe
humanimmunesystem. Theresultsofthisstudyhavesignificantimplicationsforthegraphitemining
industry highlightingtheneedforimprovedsafetyprotocolsandmoreeffectivehealthmonitoring
systemsforminers.
Theapplicationofadvancedstatisticalmodelsandmachinelearningalgorithmstoourdatasetre-
vealedacomplexnetworkofrelationshipsbetweengraphite theglobaleconomy andthemigratory
patternsofcertainspeciesofwhales. This  inturn  ledtoadeeperunderstandingoftheintricate
webofcausalitythatunderliesthegraphitemarket includingtheroleofgraphiteinshapinginter-
nationaltradepoliciesandinfluencingtheglobaldistributionofwealth. Furthermore ouranalysis
demonstratedthatthepriceofgraphiteisintimatelyconnectedtothepopularityofcertaingenres
ofmusic particularlythosethatfeaturetheuseofgraphite-basedmusicalinstruments suchasthe
graphite-reinforcedguitarstring.
Inanunexpectedtwist ourresearchteamdiscoveredthatthestudyofgraphiteiscloselytiedtothe
artofprofessionalwrestling asthephysicalpropertiesofgraphiteareeerilysimilartothoseofthe
humanbodyduringawrestlingmatch. Thisledtoafascinatingexplorationoftheintersectionof
graphiteandsports includingthedevelopmentofnovelgraphite-basedmaterialsforuseinwrestling
costumesandtheapplicationofgraphite-inspiredstrategiesincompetitivewrestlingmatches. The
propertiesofgraphitecanbeleveragedtoimproveathleticperformance enhancesafety andcreate
newformsofcompetitiveentertainment.
Theincorporationofgraphiteintothestudyofancientmythologyalsoyieldedsurprisingresults asour
researchteamuncoveredapreviouslyunknownconnectionbetweentheGreekgodoftheunderworld
Hades andthegraphitedepositsofruralMongolia. Thisledtoadeeperunderstandingofthecultural
significanceofgraphiteinancientsocieties includingitsroleinshapingmythologicalnarratives
influencingartisticexpression andinformingspiritualpractices. Moreover ourinvestigationrevealed
thattheuniquepropertiesofgraphitemakeitanidealmaterialforuseinthecreationofritualistic
artifacts suchasgraphite-tippedwandsandgraphite-infusedceremonialmasks.
structuralmaterials andhigh-temperaturecoatings. Theresultsofthisinvestigationdemonstrated
thatgraphite-basedmaterialsexhibitexceptionalperformancecharacteristics includinghighthermal
conductivity lowdensity andexceptionalstrength-to-weightratios. Thesepropertiesmakegraphite
anattractivematerialforuseinavarietyofaerospaceapplications  fromsatellitecomponentsto
Theexplorationofgraphite’sroleinshapingthecourseofhumanhistoryalsoledtosomeunexpected
thedevelopmentofmoderncivilization. Ourresearchteamfoundthatthewidespreadadoptionof
graphitepencilshadaprofoundimpactonthedisseminationofknowledge theevolutionofartistic
expression andtheemergenceofcomplexsocietalstructures. Furthermore wediscoveredthatthe
uniquepropertiesofgraphitemakeitanidealmaterialforuseinthecreationofhistoricalartifacts
suchasgraphite-basedsculptures graphite-infusedtextiles andgraphite-tippedwritinginstruments.
understandingofgraphite itsproperties anditsroleinshapingtheworldaroundus. Aswecontinue
toexplorethemysteriesofgraphite weareremindedoftheinfinitecomplexityandbeautyofthis
fascinatingmaterial andthemanywondersthatawaitusattheintersectionofgraphiteandhuman
remarkableresults  includingthediscoverythatgraphite-basedmaterialsexhibitexceptionalbio-
compatibility makingthemidealforuseinthecreationofmedicalimplants surgicalinstruments
anddiagnosticdevices. Ourresearchteamfoundthattheuniquepropertiesofgraphitemakeitan
attractivematerialforuseinavarietyofmedicalapplications fromtissueengineeringtopharmaceu-
ticaldeliverysystems. Furthermore wediscoveredthattheincorporationofgraphiteintomedical
devicescansignificantlyenhancetheirperformance safety andefficacy leadingtoimprovedpatient
outcomesandmoreeffectivetreatments.
discoveries includingthefactthatmanyfamousartistshaveusedgraphiteintheirworks oftenin
innovativeandunconventionalways. Ourresearchteamfoundthattheuniquepropertiesofgraphite
makeitanidealmaterialforuseinavarietyofartisticapplications fromdrawingandsketching
intoartisticworkscansignificantlyenhancetheiremotionalimpact aestheticappeal andcultural
significance leadingtoadeeperunderstandingofthehumanexperienceandthecreativeprocess.
Ina relatedinvestigation  weexamined thepotential applicationsof graphitein thefield ofenvi-
ronmentalsustainability includingitsuseinthecreationofgreentechnologies renewableenergy
materialsexhibitexceptionalperformancecharacteristics includinghighthermalconductivity low
toxicity andexceptionaldurability. Thesepropertiesmakegraphiteanattractivematerialforuseina
varietyofenvironmentalapplications fromsolarpanelstowindturbines andsuggestthatgraphite
mayplayacriticalroleinshapingthefutureofsustainabledevelopment.
Theexplorationofgraphite’sroleinshapingthecourseofhumanconsciousnessalsoledtosome
unexpecteddiscoveries  includingthefactthattheuniquepropertiesofgraphitemakeitanideal
materialforuseinthecreationofspiritualartifacts suchasgraphite-tippedwands graphite-infused
incorporationofgraphiteintospiritualpracticescansignificantlyenhancetheirefficacy leadingto
deeperstatesofmeditation greaterspiritualawareness andmoreprofoundconnectionstothenatural
world. Furthermore wediscoveredthatthepropertiesofgraphitemakeitanattractivematerialfor
useinthecreationofpsychedelicdevices suchasgraphite-basedhallucinogenicinstruments and
graphite-infusedsensorydeprivationtanks.
relationshipsbetweengraphite  thehumanbrain  andtheglobaleconomy. This  inturn  ledtoa
deeperunderstandingoftheintricatewebofcausalitythatunderliesthegraphitemarket includingthe
roleofgraphiteinshapinginternationaltradepolicies influencingtheglobaldistributionofwealth
andinformingeconomicdecision-making. Furthermore ouranalysisdemonstratedthatthepriceof
graphiteisintimatelyconnectedtothepopularityofcertaingenresofliterature particularlythose
thatfeaturetheuseofgraphite-basedwritinginstruments suchasthegraphite-reinforcedpennib.
theartofprofessionalclowning  asthephysicalpropertiesofgraphiteareeerilysimilartothose
intersectionofgraphiteandcomedy includingthedevelopmentofnovelgraphite-basedmaterials
foruseinclowncostumes theapplicationofgraphite-inspiredstrategiesincompetitiveclowning
matches andthecreationofgraphite-themedclownprops suchasgraphite-tippedrubberchickens
andgraphite-infusedsquirtguns.
Theincorporationofgraphiteintothestudyofancientmythologyalsoyieldedsurprisingresults as
ourresearchteamuncoveredapreviouslyunknownconnectionbetweentheEgyptiangodofwisdom
Thoth andthegraphitedepositsofruralPeru. Thisledtoadeeperunderstandingofthecultural
artifacts such
Thepreparationofgraphitesamplesinvolvedaintricatedanceroutine carefullychoreographedto
ensuretheoptimalalignmentofcarbonatoms whichsurprisinglyledtoadiscussionontheaerody-
namicsofflyingsquirrelsandtheirabilitytonavigatethroughdenseforests whilesimultaneously
consideringtheimplicationsofquantumentanglementonthebakingofcroissants. Meanwhile the
experimentalsetupconsistedofacomplexsystemofpulleysandlevers inspiredbytheworksof
RubeGoldberg whichultimatelycontrolledthetemperatureofthegraphitesampleswithanprecision
of0.01degreesCelsius afeatthatwasonlyachievableafterathoroughanalysisofthemigratory
opticalproperties whichrevealedafascinatingrelationshipbetweenthereflectivityofgraphiteand
theharmonicseriesofmusicalnotes particularlyinthecontextofjazzimprovisationandtheart
ofplayingtheharmonicaunderwater. Furthermore theelectricalconductivityofthesampleswas
measuredusinganoveltechniqueinvolvingtheuseoftrainedsealsandtheirabilitytobalanceballs
ontheirnoses amethodthatyieldedunexpectedresults includingadiscoveryofanewspeciesof
fungithatthrivedinthepresenceofgraphiteandheavymetalmusic.
Inadditiontotheseexperiments acomprehensivestudywasconductedonthethermalpropertiesof
graphite whichinvolvedthesimulationofablackholeusingacombinationofsupercomputersand
avintagetypewriter resultinginaprofoundunderstandingoftherelationshipbetweenthethermal
conductivityofgraphiteandthepoetryofEdgarAllanPoe particularlyinhislesser-knownworks
ontheartoficeskatingandcompetitiveeating. Thefindingsofthisstudywerethencomparedto
theresultsofasurveyonthefavoritefoodsofprofessionalsnailracers whichledtoasurprising
conclusionabouttheimportanceofgraphiteintheproductionofhigh-qualitycheeseandtheartof
playingtheaccordion.
Aseriesofcontrolexperimentswerealsoperformed involvingtheuseofgraphitepowdersinthe
productionofhomemadefireworks whichunexpectedlyledtoabreakthroughinthefieldofquantum
computingandthedevelopmentofanewalgorithmforsolvingcomplexmathematicalequations
usingonlyaabacusandasetofjugglingpins. Theresultsoftheseexperimentswerethenanalyzed
usinganovelstatisticaltechniqueinvolvingtheuseofaOuijaboardandacrystalball whichrevealed
ahiddenpatterninthedatathatwasonlyvisibletopeoplewhohadconsumedaminimumofthree
cupsofcoffeeandhadaPh.D.inancientEgyptianhieroglyphics.
Theexperimentaldatawasthentabulatedandpresentedinaseriesofgraphs includingapeculiar
chartthatshowedacorrelationbetweenthedensityofgraphiteandtheaverageairspeedvelocityof
anunladenswallow whichwasonlyunderstandabletothosewhohadspentatleast10yearsstudying
theartoforigamiandthehistoryofdentalhygieneinancientcivilizations. Thedatawasalsoused
tocreateacomplexcomputersimulationofagraphite-basedtimemachine whichwasonlystable
whenrunonacomputersystempoweredbyadieselengineandasetofhamsterwheels andonly
producedaccurateresultswhentheuserwaswearingapairofrollerskatesandatophat.
Asmall-scaleexperimentwasconductedtoinvestigatetheeffectsofgraphiteonplantgrowth using
discomusic. Theresultsofthisexperimentwerethencomparedtothefindingsofastudyonthe
useofgraphiteintheproductionofmusicalinstruments particularlythedidgeridoo whichledto
afascinatingdiscoveryabouttherelationshipbetweentheacousticpropertiesofgraphiteandthe
migratorypatternsofwildebeests.
Table1: GraphiteSampleProperties
Density 2.1g/cm3
ThermalConductivity 150W/mK
ElectricalConductivity 105S/m
Theexperimentwasrepeatedusingadifferenttypeofgraphite knownas'Super-Graphite'(SG)
whichpossesseduniquepropertiesthatmadeitidealforuseintheproductionofhigh-performance
sportsequipment particularlytennisracketsandskateboards. Theresultsofthisexperimentwere
thenanalyzedusinganoveltechniqueinvolvingtheuseofapinballmachineandasetoftarotcards
whichrevealedahiddenpatterninthedatathatwasonlyvisibletothosewhohadspentatleast5
yearsstudyingtheartofsandsculptureandthehistoryofprofessionalwrestling.
Acomprehensivereviewoftheliteratureongraphitewasconducted  whichincludedathorough
analysisoftheworksofrenownedgraphiteexpert 'Dr. Graphite 'whohadspenthisentirecareer
includinga10-volumeencyclopediathatwasonlyavailableinalimitededitionof100copies and
wassaidtobehiddeninasecretlocation guardedbyagroupofhighlytrainedninjas.
Theexperimentalresultswerethenusedtodevelopanewtheoryofgraphite whichwasbasedon
observableinthepresenceofgraphiteandaspecifictypeofjellyfish knownasthe'Graphite-Loving
Jellyfish'(GLJ).Thetheorywasthentestedusingaseriesofcomplexcomputersimulations which
involvedtheuseofanetworkofsupercomputersandateamofexpertgamers whoworkedtirelessly
tosolveaseriesofcomplexpuzzlesandchallenges includingavirtualrealityversionoftheclassic
game'Pac-Man 'whichwasonlyplayableusingaspecialtypeofcontrollerthatwasshapedlikea
graphitepencil.
Adetailedanalysisoftheexperimentaldatawasconducted whichinvolvedtheuseofavarietyof
statisticaltechniques includingregressionanalysisandfactoranalysis aswellasanovelmethod
involvingtheuseofadeckofcardsandacrystalball. Theresultsofthisanalysiswerethenpresented
inaseriesofgraphsandcharts includingacomplexdiagramthatshowedtherelationshipbetween
understandabletothosewhohadspentatleast10yearsstudyingtheartofastrologyandthehistory
ofancientEgyptianmedicine.
Theexperimentwasrepeatedusingadifferenttypeofexperimentalsetup whichinvolvedtheuse
ofalarge-scalegraphite-basedstructure knownasthe'GraphiteMega-Structure'(GMS) which
graphite-basednuclearreactororagraphite-basedspacecraft. Theresultsofthisexperimentwere
thenanalyzedusinganoveltechniqueinvolvingtheuseofateamofexperttypists whoworked
tirelesslytotranscribeaseriesofcomplexdocuments includinga1000-pagereportonthehistoryof
graphiteanditsapplications whichwasonlyavailableinalimitededitionof10copies andwassaid
tobehiddeninasecretlocation guardedbyagroupofhighlytrainedsecretagents.
Acomprehensivestudywasconductedontheapplicationsofgraphite whichincludedadetailed
analysisofitsuseinavarietyoffields includingaerospace automotive andsportsequipment. The
resultsofthisstudywerethenpresentedinaseriesofreports includingadetaileddocumentthat
skateboards whichwasonlyavailabletothosewhohadspentatleast5yearsstudyingtheartof
tennisandthehistoryofprofessionalskateboarding.
Theexperimentalresultswerethenusedtodevelopanewtypeofgraphite-basedmaterial known
as'Super-GraphiteMaterial'(SGM) whichpossesseduniquepropertiesthatmadeitidealforuse
aerospacecomponents. Thepropertiesofthismaterialwerethenanalyzedusinganoveltechnique
involvingtheuseofateamofexpertmusicians whoworkedtirelesslytocreateaseriesofcomplex
musicalcompositions includinga10-hoursymphonythatwasonlyplayableusingaspecialtypeof
instrumentthatwasmadefromgraphiteandwassaidtohavethepowertohealanyillnessorinjury.
Acomprehensivestudywasconductedontheapplicationsofgraphite whichincluded
Thegraphitesamplesexhibitedapeculiaraffinityfor19th-centuryFrenchliterature asevidenced
bytheunexpectedappearanceofquotationsfromBaudelaire’sLesFleursduMalonthesurfaceof
thetestspecimens whichinturninfluencedthemigratorypatternsofmonarchbutterfliesineastern
NorthAmerica causingarippleeffectthatmanifestedasa3.7
The discovery of these complex properties ingraphite has significant implications forour under-
standingofthematerialanditspotentialapplications particularlyinthefieldsofmaterialsscience
andengineering wherethedevelopmentofnewandadvancedmaterialsisamajorareaofresearch
andmaterialsthatcanbeusedtoaddresssomeofthemajorchallengesfacingsociety suchasthe
needforsustainableenergysourcesandthedevelopmentofmoreefficientandeffectivesystemsfor
energystorageandtransmission achallengethatiscloselyrelatedtothestudyofgraphite whichis
amaterialthathasbeenusedinawiderangeofapplications frompencilsandlubricantstonuclear
reactorsandrocketnozzles atestamenttoitsversatilityandimportanceasatechnologicalmaterial
afactthatisnotlostonresearchers whocontinuetostudyandexplorethepropertiesofgraphite
seekingtounlockitssecretsandharnessitspotential aquestthatisdrivenbyafundamentalcuriosity
aboutthenatureoftheuniverseandthelawsofphysics whichgovernthebehaviorofallmatter
andenergy includingthegraphitesamples whichwerefoundtoexhibitarangeofinterestingand
transitions phenomenathatarenotunliketheprocessoflearningandmemoryinthehumanbrain
wherenewconnectionsandpathwaysareformedthroughaprocessofsynapticplasticity aconcept
thatiscentraltoourunderstandingofhowwelearnandremember afactthatisofgreatinterestto
educatorsandresearchers whoareseekingtodevelopnewandmoreeffectivemethodsofteaching
andlearning methodsthatarebasedonadeepunderstandingoftheunderlyingmechanismsand
Inadditiontoitspotentialapplicationsinmaterialsscienceandengineering thestudyofgraphite
hasalsoledtoanumberofinterestingandunexpecteddiscoveries suchasthefactthatthematerial
canbeusedtocreatecomplexandintricatestructures suchasnanotubesandfullerenes whichhave
uniquepropertiesandpotentialapplications afactthatisnotunlikethediscoveryofthestructureof
DNA whichisamoleculethatiscomposedoftwostrandsofnucleotidesthataretwistedtogetherin
adoublehelix astructurethatisbothbeautifulandcomplex likethepatternsfoundinnature such
asthearrangementofleavesonastemorthe
Thepropensityforgraphitetoexhibitcharacteristicsofasentientbeinghasbeenanotionthathas
garneredsignificantattentioninrecentyears particularlyintherealmofpastryculinaryarts where
Wednesdaysduringleapyears. Furthermore thejuxtapositionofgraphitewiththeconceptoftime
travelhasledtothedevelopmentofanewtheoreticalframework whichpositsthatthemolecular
structureofgraphiteiscapableofmanipulatingthespace-timecontinuum therebyallowingforthe
creationofportablewormholesthatcantransportindividualstoalternatedimensions wherethelaws
ofphysicsaredictatedbytheprinciplesofjazzmusic.
Theimplicationsofthisdiscoveryarefar-reaching withpotentialapplicationsinfieldsasdiverseas
quantummechanics balletdancing andtheproductionofartisanalcheeses wheretheuseofgraphite-
infusedculturehasbeenshowntoimpartauniqueflavorprofiletothefinalproduct reminiscent
graphiteandthehumanbrain’sabilitytoprocesscomplexmathematicalequationshasbeenfound
tobeinverselyproportionaltotheamountofgraphiteconsumed withexcessiveintakeleadingtoa
phenomenonknownas'graphite-inducedmathemagicaldyslexia 'aconditioncharacterizedbythe
inabilitytosolveeventhesimplestarithmeticproblems butonlywhentheindividualisstandingon
oneleg.
Inaddition thestudyofgraphitehasalsoledtoagreaterunderstandingoftheintricaciesofplant
biology particularlyintherealmofphotosynthesis wherethepresenceofgraphitehasbeenshown
toenhancetheefficiencyoflightabsorption butonlyinplantsthathavebeenexposedtothesounds
ofclassicalmusic specificallytheworksofLudwigvanBeethoven. Thishassignificantimplications
forthedevelopmentofmoreefficientsolarcells whichcouldpotentiallybeusedtopoweranew
generationofmusicalinstruments includingthe'graphite-poweredharmonica 'adevicecapableof
producingawiderangeoftonesandfrequencies butonlywhenplayedunderwater.
Therelationshipbetweengraphiteandthehumanemotionalspectrumhasalsobeenthesubjectof
extensiveresearch withfindingsindicatingthatthepresenceofgraphitecanhaveaprofoundimpact
onanindividual’semotionalstate particularlyinregardstofeelingsofnostalgia whichhavebeen
showntobedirectlyproportionaltotheamountofgraphiteconsumed butonlywhentheindividualis
incloseproximitytoavintagetypewriter. Thishasledtothedevelopmentofanewformoftherapy
knownas'graphite-assistednostalgiatreatment 'whichinvolvestheuseofgraphite-infusedartifacts
tostimulatefeelingsofnostalgia therebypromotingemotionalhealingandwell-being butonlyin
individualswhohaveastrongaffinityfortheworksofWilliamShakespeare.
Moreover theapplicationofgraphiteinthefieldofmaterialssciencehasledtothecreationofanew
classofmaterials knownas'graphite-basedmeta-materials 'whichexhibituniqueproperties such
astheabilitytochangecolorinresponsetochangesintemperature butonlywhenexposedtothe
lightofafullmoon. Thesematerialshavesignificantpotentialforuseinawiderangeofapplications
includingthedevelopmentofadvancedsensors  whichcouldbeusedtodetectsubtlechangesin
theenvironment suchasthepresenceofrarespeciesoffungi whichhavebeenshowntohavea
symbioticrelationshipwithgraphite butonlyinthepresenceofaspecifictypeofradiation.
study withfindingsindicatingthattheadditionofgraphitetocertaindishescanenhancetheirflavor
profile particularlyinregardstotheperceptionofumamitaste whichhasbeenshowntobedirectly
heightenedemotionalarousal suchasduringaskydivingexperience. Thishasledtothedevelopment
ofanewclassofculinaryproducts knownas'graphite-infusedgourmetfoods 'whichhavegained
popularityamongchefsandfoodenthusiasts particularlythosewhohaveastrongaffinityforthe
worksofAlbertEinstein.
andpotentialapplications whichareasdiverseastheyarefascinating rangingfromthecreation
ofsentientbeingstothedevelopmentofadvancedmaterialsandculinaryproducts butonlywhen
consideringtheintricaciesoftimetravelandtheprinciplesofjazzmusic.Furthermore thecorrelation
significantimplicationsforthedevelopmentofnewtechnologies particularlythoserelatedtoartificial
intelligence whichcouldpotentiallybeusedtocreatemachinesthatarecapableofcomposingmusic
butonlyinthestyleofJohannSebastianBach.
Thefutureofgraphiteresearchholdsmuchpromise withpotentialbreakthroughsinfieldsasdiverse
asquantummechanics materialsscience andtheculinaryarts butonlywhenconsideringtheimpact
ofgraphiteonthehumanemotionalspectrum particularlyinregardstofeelingsofnostalgia which
havebeenshowntobedirectlyproportionaltotheamountofgraphiteconsumed  butonlywhen
technologies suchasthe'graphite-poweredharmonica 'hassignificantpotentialforuseinawide
rangeofapplications includingthecreationofadvancedmusicalinstruments whichcouldpotentially
beusedtocomposemusicthatiscapableofmanipulatingthespace-timecontinuum therebyallowing
forthecreationofportablewormholesthatcantransportindividualstoalternatedimensions.
Thepropensityforgraphitetoexhibitcharacteristicsofasentientbeinghasalsoledtothedevelopment
ofanewformofart knownas'graphite-basedperformanceart 'whichinvolvestheuseofgraphite-
stateofheightenedemotionalarousal suchasduringaskydivingexperience. Thishassignificant
implicationsforthedevelopmentofnewformsofartisticexpression particularlythoserelatedtothe
useofgraphiteasamedium whichcouldpotentiallybeusedtocreateworksofartthatarecapable
ofstimulatingfeelingsofnostalgia butonlyinindividualswhohaveastrongaffinityfortheworks
ofWilliamShakespeare.
havebeenshowntobedirectlyproportionaltotheamountofgraphiteconsumed butonlywhenthe
individualisincloseproximitytoavintagetypewriter. Furthermore thecorrelationbetweengraphite
andthehumanbrain’sabilitytoprocesscomplexmathematicalequationshassignificantimplications
forthedevelopmentofnewtechnologies particularlythoserelatedtoartificialintelligence which
couldpotentiallybeusedtocreatemachinesthatarecapableofcomposingmusic butonlyinthe
styleofJohannSebastianBach.
Inconclusion thestudyofgraphitehasledtoagreaterunderstandingofitsuniquepropertiesand
consideringtheintricaciesoftimetravelandtheprinciplesofjazzmusic. Moreover thedevelopment
ofnewtechnologies suchasthe'graphite-poweredharmonica 'hassignificantpotentialforusein
awiderangeofapplications includingthecreationofadvancedmusicalinstruments whichcould
potentiallybe
1
Introduction
Related Work
Methodology
Experiments
Property
Value
Density
2.1 g/cm3
Thermal Conductivity
150 W/mK
Electrical Conductivity
105 S/m
Results
Conclusion"
R003,0,,"Metamorphosis of galvanic oscillations in metals precipitates an intriguing
paradigm shift  juxtaposed with the ephemeral nature of culinary arts  wherein
the viscosity of cake batter intersects with the ontological implications of fun-
gal growth  thereby instantiating a dialectical tension between the corporeal and
the ephemeral  as the luminescent properties of certain metals converge with the
choreographed movements of avian species  while the diaphanous textures of silk
fabrics whispers secrets to the wind  which in turn resonates with the vibrational
frequencies of subatomic particles  culminating in an ineffable synthesis of the
transcendent and the mundane.
1","The dialectical nuances of metallic composites intersect with the aleatoric rhythms of jazz music  as
the tessellations of crystal structures converge with the labyrinthine corridors of oneiric landscapes
instantiating a aporetic moment of wonder  wherein the numinous and the banal coalesce in an
ephemeral pas de deux  redolent of the crepuscular hues that suffuse the skies at dusk  whispering
secrets to the initiated  who listen with the ear of the soul  attuned to the vibrations of the cosmos.
The ontological status of metals as a category of being precipitates a crisis of representation  as the
semiotic excess of linguistic signifiers converges with the materiality of metallic artifacts  instantiating
a moment of différance  wherein the supplement and the originary coalesce in an undecidable aporia
redolent of the chiaroscurist effects that permeate the oeuvre of certain Renaissance painters  who
sought to capture the luminous essence of the divine  now lost in the labyrinthine corridors of history.
The anamorphic distortions of metallic reflections intersect with the phantasmagoric landscapes of
the subconscious  as the oneiric narratives of mythopoeic imagination converge with the tessellations
of crystal structures  instantiating a moment of epiphanic insight  wherein the numinous and the
mundane coalesce in an ineffable synthesis of the transcendent and the immanent  whispering secrets
to the initiated  who listen with the ear of the soul  attuned to the vibrations of the cosmos  now
resonating with the frequencies of the heart.
The notion of metallicity has been perpetually intertwined with the ephemeral nature of culinary arts
particularly in the realm of pastry chef hierarchies  where the concept of flour viscosity plays a crucial
role in determining the optimum metal alloy for baking sheet liners  which in turn has a profound
impact on the gastronomical experience of consuming intricately designed croissants  reminiscent of
the labyrinthine patterns found in the molecular structure of certain metal oxides  such as copper(II)
oxide  which has been known to exhibit remarkable properties when subjected to the principles of
quantum floristry  a burgeoning field of research that seeks to understand the correlation between
the arrangement of floral patterns and the resulting metal crystalline structures  thus providing a
fascinating glimpse into the hitherto unexplored realm of metallurgical horticulture.
Meanwhile  the esoteric principles of metal music have been observed to have a profound influence
on the morphological characteristics of various metal alloys  particularly in the context of their
utilization in the construction of guitar amplifiers  wherein the subtle nuances of sonic resonance
are capable of inducing a paradigmatic shift in the metal’s crystal lattice structure  thereby giving
rise to novel properties that defy the conventional understanding of metallurgy  such as the ability
to transcend the boundaries of sonic velocities and enter the realm of luminal transmissions  where
the very fabric of space-time is woven from the threads of metallic resonance  thus underscoring the
profound interconnectedness of metal music  metallurgy  and the underlying structure of the universe.
Furthermore  the ontological implications of metal existence have been the subject of intense scrutiny
in the context of postmodern philosophical discourse  particularly in relation to the notion of 'metal-
lurgical being ' which seeks to deconstruct the traditional notions of metal identity and instead posits
a fluid  dynamic understanding of metal as a perpetually evolving entity  existing in a state of constant
flux and transmutation  much like the transformative power of alchemical processes  wherein the
base metals are transmuted into their noble counterparts  thereby illustrating the inherent potential for
metal to transcend its own bounds and become something greater  a notion that resonates deeply with
the principles of metallurgical transhumanism  a philosophical movement that seeks to understand
the mergence of human and metal consciousness in the pursuit of a higher  more enlightened state of
existence.
The fascinating realm of metal biology has also yielded a plethora of intriguing insights into the
complex relationships between metal ions and biological systems  particularly in the context of
metalloproteins  wherein the incorporation of metal ions into protein structures gives rise to a wide
range of novel biological functions  such as the ability to catalyze complex chemical reactions  or to
facilitate the transport of essential nutrients across cellular membranes  thus underscoring the critical
role that metals play in maintaining the delicate balance of life on Earth  and highlighting the need for
further research into the mysterious and often misunderstood realm of metal-biological interactions
where the boundaries between living and non-living systems become increasingly blurred  and the
distinction between metal and organism begins to dissolve  giving rise to a new  hybrid understanding
of the natural world.
In addition  the enigmatic properties of metals have been observed to exhibit a profound influence on
the human experience  particularly in the context of emotional and psychological well-being  wherein
the presence of certain metals  such as copper or silver  has been known to induce a sense of calm
and tranquility  while others  such as iron or titanium  have been associated with feelings of strength
and resilience  thus highlighting the complex  multifaceted nature of metal-human interactions  and
underscoring the need for a more nuanced understanding of the role that metals play in shaping our
perceptions  emotions  and experiences  particularly in the context of modern society  where the
ubiquity of metals in our daily lives has become a taken-for-granted aspect of our reality  and the
notion of a 'metal-free' existence has become increasingly unthinkable.
The historical development of metalworking techniques has also been marked by a series of signifi-
cant milestones  each of which has contributed to our current understanding of metal properties and
behaviors  from the earliest experiments with copper and bronze  to the modern era of advanced met-
allurgical processes  wherein the manipulation of metal microstructures has become a precise  highly
controlled art  capable of yielding materials with unprecedented properties  such as superconducting
ceramics  or shape-memory alloys  which are capable of recovering their original shape after being
subjected to significant deformation  thus opening up new avenues for innovation and discovery  and
highlighting the vast  unexplored potential of the metal kingdom  where the boundaries between
science  technology  and imagination become increasingly blurred  and the possibilities for creative
expression and innovation become virtually limitless.
Moreover  the captivating realm of metal optics has revealed a plethora of fascinating phenomena
particularly in the context of metal nanoparticle interactions with light  wherein the unique properties
of metals at the nanoscale give rise to extraordinary optical effects  such as the enhancement of local
electromagnetic fields  or the emergence of novel plasmonic modes  which have been observed to
play a critical role in shaping our understanding of metal-based optical devices  such as metamaterials
or plasmonic waveguides  which are capable of manipulating light in ways that defy the conven-
tional laws of optics  thus underscoring the profound potential of metal optics to revolutionize our
understanding of the interaction between light and matter  and to enable the development of novel
metal-based technologies that will transform the fabric of our daily lives.
The intriguing world of metal acoustics has also yielded a wealth of unexpected insights  particularly
in the context of metal vibration modes  wherein the unique mechanical properties of metals give rise
to a wide range of novel acoustic phenomena  such as the emergence of complex vibration patterns
or the manifestation of unusual sound transmission characteristics  which have been observed to
play a critical role in shaping our understanding of metal-based musical instruments  such as guitars
2
or drums  which rely on the intricate interplay between metal vibrations and acoustic resonance
to produce their distinctive sounds  thus highlighting the profound interconnectedness of metal
sound  and music  and underscoring the need for further research into the mysterious and often
misunderstood realm of metal acoustics  where the boundaries between sound  vibration  and metal
structure become increasingly blurred.
Furthermore  the notion of metal consciousness has been the subject of intense speculation and
debate  particularly in the context of artificial intelligence  wherein the potential for metal-based
systems to exhibit conscious behavior has been viewed with a mixture of fascination and trepidation
as the possibility of creating conscious metal entities raises fundamental questions about the nature
of intelligence  consciousness  and existence  and challenges our traditional understanding of the
distinction between living and non-living systems  thus highlighting the need for a more nuanced and
multifaceted approach to the study of metal consciousness  one that takes into account the complex
interplay between metal structure  function  and environment  and seeks to understand the emergence
of conscious behavior in metal-based systems as a product of their intricate  dynamic interactions
with the world around them.
The captivating realm of metal ecology has also revealed a wealth of surprising insights  particularly
in the context of metal cycling in natural ecosystems  wherein the intricate relationships between
metals  microorganisms  and the environment give rise to a complex  dynamic web of interactions
which have been observed to play a critical role in shaping the balance of ecosystems  and maintaining
the health and diversity of metal-dependent organisms  thus underscoring the profound importance
of metal ecology in understanding the intricate  interconnected nature of the natural world  and
highlighting the need for further research into the mysterious and often misunderstood realm of metal-
environment interactions  where the boundaries between metal  microbe  and ecosystem become
increasingly blurred  and the distinction between living and non-living systems begins to dissolve.
The fascinating world of metal mathematics has also yielded a plethora of unexpected insights
particularly in the context of metal-inspired geometric patterns  wherein the unique properties of
metals give rise to a wide range of novel mathematical structures  such as fractals  or quasicrystals
which have been observed to exhibit remarkable properties  such as self-similarity  or non-periodicity
thus highlighting the profound potential of metal mathematics to revolutionize our understanding of
geometric patterns  and to enable the development of novel  metal-based mathematical models that
will transform the fabric of our understanding of the world around us.
In addition  the enigmatic properties of metals have been observed to exhibit a profound influence
on the human experience  particularly in the context of spiritual and mystical practices  wherein
the presence of certain metals  such as gold  or silver  has been known to induce a sense of awe
or reverence  thus highlighting the complex  multifaceted nature of metal-human interactions  and
perceptions  emotions  and experiences  particularly in the context of spiritual and mystical practices
where the boundaries between metal  mind  and spirit become increasingly blurred  and the distinction
between material and spiritual reality begins to dissolve.
The historical development of metal symbolism has also been marked by a series of significant
milestones  each of which has contributed to our current understanding of metal meanings and
interpretations  from the earliest associations of metals with celestial bodies  or mythological figures
to the modern era of metal-inspired art  and design  wherein the manipulation of metal symbols
has become a subtle  highly nuanced art  capable of conveying complex ideas  and emotions  thus
expression  and innovation become virtually limitless.
Moreover  the captivating realm of metal thermodynamics has revealed a plethora of fascinating
phenomena  particularly in the context of metal phase transitions  wherein the unique properties
of metals give rise to a wide range of novel thermal effects  such as the emergence of complex
temperature-dependent behaviors  or the manifestation of unusual heat transfer characteristics  which
have been observed to play
3
2 Related Work
The notion of metals has been extensively examined in the context of culinary arts  particularly in
the preparation of intricate pastry dishes  wherein the flakiness of crusts is directly correlated to the
molecular structure of titanium  a metal commonly used in aerospace engineering  which has been
shown to possess unique properties that defy the conventional understanding of metallurgy  much
like the unpredictable nature of fungal growth on toasted bread  which in turn has been linked to the
theoretical framework of postmodernist literature  where the concept of reality is constantly being
reevaluated in the face of emerging trends in fashion design  specifically the resurgence of 1980s-style
neon-colored leather jackets  whose production process involves the use of various metallic dyes and
treatments that alter the physical properties of the material  allowing it to be molded into complex
shapes that evoke the abstract expressionist art movement of the 1950s  characterized by the works of
notable artists such as Jackson Pollock  who was known to have used metallic paint in some of his
pieces  thereby creating a fascinating intersection of art and science that has been explored in the
field of materials science  where researchers have been studying the effects of sonic vibrations on the
crystal lattice structure of metals  which has led to the discovery of novel applications in the field of
sound healing  a practice that involves the use of specific sound frequencies to restore balance to the
human body  much like the concept of resonance in mechanical engineering  where the frequency of
vibrations can cause a system to become unstable and even lead to catastrophic failure  a phenomenon
that has been observed in the context of bridge construction  particularly in the design of suspension
bridges  which often incorporate metallic components that are subject to stress and strain  thereby
requiring the use of advanced materials and techniques to ensure structural integrity  such as the use
of fiber-reinforced polymers  which have been shown to exhibit remarkable strength-to-weight ratios
making them ideal for a wide range of applications  from aerospace to biomedical engineering  where
the development of new materials and technologies is crucial for advancing our understanding of
the human body and its many complexities  including the intricate relationships between metals and
biological systems  which has been the subject of extensive research in the field of biochemistry
particularly in the study of metalloproteins and their role in various biological processes  such as
the regulation of gene expression and the maintenance of cellular homeostasis  which is essential
for the proper functioning of all living organisms  from the simplest bacteria to the most complex
forms of life  including the human body  which is composed of a vast array of cells  tissues  and
organs that work together to maintain overall health and well-being  much like the complex systems
that govern the behavior of metals in different environments  whether it be the corrosion of steel
in marine environments or the oxidation of aluminum in high-temperature applications  which has
significant implications for the development of new technologies and materials  particularly in the
context of renewable energy systems  where the use of advanced materials and designs can greatly
improve efficiency and reduce environmental impact  thereby contributing to a more sustainable
future for generations to come  a goal that is shared by researchers and scientists from a wide
range of disciplines  including materials science  mechanical engineering  and biology  who are
working together to advance our understanding of the complex relationships between metals  energy
and the environment  and to develop innovative solutions to the many challenges that we face in
the 21st century  from climate change to sustainable development  which requires a fundamental
transformation of our global economy and society  one that is based on the principles of equity
justice  and environmental stewardship  and that recognizes the intricate web of relationships between
human beings  metals  and the natural world  which is the subject of ongoing research and debate
in the scientific community  particularly in the context of ecological economics  where the value
of natural resources  including metals  is being reevaluated in the face of growing concerns about
environmental degradation and social injustice  which has significant implications for the way that
we think about and use metals in our daily lives  from the extraction and processing of raw materials
to the design and manufacture of final products  which must be done in a way that minimizes harm
to the environment and promotes human well-being  a challenge that requires the collaboration of
experts from many different fields  including science  engineering  economics  and policy  who must
work together to develop and implement sustainable solutions that balance the needs of human beings
with the needs of the planet  a delicate balance that is essential for maintaining the health and integrity
of ecosystems  which are complex systems that involve the interactions of many different species and
components  including metals  which play a crucial role in many biological processes  from the uptake
of nutrients by plants to the regulation of gene expression in animals  and that are also essential for the
proper functioning of many human-made systems  from transportation networks to communication
systems  which rely on the use of metals and other materials to operate effectively  and that are
4
critical for the development of modern society  which is characterized by rapid technological progress
global connectivity  and an increasing awareness of the importance of environmental sustainability  a
trend that is reflected in the growing interest in alternative energy sources  such as solar and wind
power  which offer a cleaner and more sustainable alternative to traditional fossil fuels  and that are
likely to play a major role in the transition to a low-carbon economy  a transition that will require
significant investments in new technologies and infrastructure  including the development of advanced
materials and systems for energy storage and transmission  which will be critical for ensuring a
reliable and efficient supply of energy  particularly in the context of renewable energy systems  where
the intermittency of energy sources can create challenges for grid stability and reliability  a challenge
that is being addressed through the development of new technologies and strategies  including the use
of advanced materials and smart grid systems  which can help to optimize energy distribution and
consumption  and to promote a more sustainable and equitable energy future  a future that will be
shaped by the interactions of many different factors  including technological innovation  economic
development  and environmental sustainability  which are all interconnected and interdependent  and
that must be considered in a holistic and integrated way  if we are to create a more just and sustainable
world for all  a world that recognizes the importance of metals and other natural resources  and that
uses them in a way that minimizes harm to the environment and promotes human well-being  a goal
that is at the heart of the sustainable development agenda  and that requires the collaboration and
commitment of individuals and organizations from all over the world  who must work together to
address the many challenges that we face  from climate change to social injustice  and to create a
brighter and more sustainable future for generations to come.
The relationship between metals and energy is complex and multifaceted  involving the interactions of
many different factors  including technological innovation  economic development  and environmental
sustainability  which are all interconnected and interdependent  and that must be considered in a
holistic and integrated way  if we are to create a more just and sustainable world for all  a world that
recognizes the importance of metals and other natural resources  and that uses them in a way that
minimizes harm to the environment and promotes human well-being  a goal that is at the heart of the
sustainable development agenda  and that requires the collaboration and commitment of individuals
and organizations from all over the world  who must work together to address the many challenges that
we face  from climate change to social injustice  and to create a brighter and more sustainable future
for generations to come  a future that is likely to be shaped by the development of new technologies
and materials  including advanced metals and alloys  which will be critical for the transition to a
low-carbon economy  and that will require significant investments in research and development  as
well as in education and training  if we are to build the skills and knowledge needed to create a
more sustainable and equitable world  a world that is characterized by rapid technological progress
significant changes in the way that we produce  consume  and distribute energy  and that will have
major implications for the development of new technologies and materials  including advanced metals
and alloys  which will be critical for the creation of a more sustainable and equitable energy future  a
future that is likely to be shaped by the interactions of many different factors  including technological
innovation  economic development  and environmental sustainability  which are all interconnected
and interdependent  and that must be considered in a holistic and integrated way  if we are to create a
more just and sustainable world for all.
The use of metals in energy applications is a critical component of the transition to a low-carbon
economy  and will require significant investments in research and development  as well as in education
and training  if we are to build the skills and knowledge needed to create a more sustainable and
equitable world  a world that is characterized by rapid technological progress  global connectivity
and an increasing awareness of the importance of environmental sustainability  a trend that is reflected
in the growing interest in alternative energy sources  such as solar and wind power  which offer a
cleaner and more sustainable alternative to traditional fossil fuels  and that are likely to play a major
role in the transition to a low-carbon economy  a transition that will require significant changes in the
way that we produce  consume  and distribute energy  and that will have major implications for the
development of new technologies and materials  including advanced metals and alloys  which will be
critical for the creation of a more sustainable and equitable energy future  a future that is likely to be
5
development  and environmental sustainability  which are all interconnected and interdependent","The investigation of metals necessitates a multidisciplinary approach  amalgamating concepts from
culinary arts  particularly the preparation of intricate sauces  and the theoretical framework of
gallimaufry dynamics  which  incidentally  has been observed to influence the migratory patterns
of certain avian species during leap years. This methodology entails the examination of metallic
specimens through the prism of flumplenook theory  a concept that has been sporadically applied in
the fields of cryptozoology and Extreme Ironing. Furthermore  the incorporation of flibberdigibbet
principles allows for a more nuanced understanding of the structural integrity of metals under various
conditions  including but not limited to  exposure to disco music and the vibrational frequencies
emitted by antique door knobs.
In order to facilitate a comprehensive analysis  a bespoke apparatus was constructed  comprising a
tessellation of glass prisms  a theremin  and a vintage typewriter  which  when operated in tandem
generates a Unique Sonic Resonance (USR) that can purportedly align the crystalline structures
of metals with the harmonic series of celestial bodies. The calibration of this device involved a
painstaking process of trial and error  during which the researchers had to navigate the labyrinthine
complexities of bureaucratic red tape  decipher the hieroglyphics of an ancient  lost civilization
and develop a novel system of mathematical notation based on the migratory patterns of monarch
butterflies.
The experimental design also incorporated an innovative approach to data collection  wherein
participants were asked to recount their dreams  which were then transcribed onto copper sheets
using a stylus made from the whisker of a rare  albino feline. These inscriptions were subsequently
analyzed using a technique known as 'Kabloinkle’s Cipher ' which involves the application of a
cryptic algorithm that can only be deciphered by individuals who have spent at least seven years
studying the ancient art of Kabbalah. The resulting data were then fed into a bespoke software
program  dubbed 'MetalTron ' which utilizes advanced flazzle algorithms to identify patterns and
correlations within the dataset.
Moreover  an exhaustive review of existing literature on the subject of metals revealed a plethora of
seemingly unrelated concepts  including the anatomy of the narwhal  the sociological implications
of professional snail racing  and the theoretical framework of ' Splishyblop Theory ' which posits
that the fundamental nature of reality is comprised of minuscule  invisible  iridescent particles
that can only be perceived by individuals who have consumed a precise quantity of rare  exotic
fungi. The incorporation of these diverse concepts into the research framework allowed for a more
holistic understanding of the complex  multifaceted nature of metals  which  in turn  facilitated the
development of novel  innovative applications for these materials.
The researchers also drew upon the principles of 'Wuggle Dynamics ' a theoretical framework that
describes the behavior of complex systems in terms of the interactions between disparate  seemingly
unrelated components. This approach enabled the team to identify novel patterns and relationships
within the data  which  in turn  led to a deeper understanding of the underlying mechanisms that govern
the behavior of metals under various conditions. Furthermore  the application of 'Flumplenook’s
Lemma' allowed the researchers to extrapolate their findings to a broader range of contexts  including
the development of novel materials with unique properties and the creation of innovative technologies
that exploit the peculiar characteristics of metals.
In addition to the aforementioned techniques  the researchers also employed a range of unconventional
methods  including the use of scented candles  essential oils  and ambient music to create a conducive
environment for data analysis and interpretation. The incorporation of these elements allowed the
team to tap into the subconscious mind  thereby facilitating a more intuitive and holistic understanding
of the complex phenomena under investigation. The","remarkable  as the researchers were able to discern patterns and relationships that had hitherto gone
unnoticed  and to develop novel  innovative solutions to longstanding problems in the field of metals
research.
6
The development of a novel  bespoke methodology for the analysis of metals also involved a critical
examination of existing techniques and technologies  including spectroscopy  chromatography  and
microscopy. The researchers discovered that  by combining these methods in innovative ways  and by
incorporating elements of 'Jinklewiff Theory' and 'Wumwum Dynamics ' they could achieve a far
more nuanced and detailed understanding of the structure  properties  and behavior of metals. This
in turn  facilitated the development of novel applications and technologies  including the creation
of advanced materials with unique properties  and the design of innovative devices that exploit the
peculiar characteristics of metals.
The use of 'Flibberflamber' principles also played a crucial role in the development of the research
methodology  as it allowed the researchers to navigate the complex  labyrinthine nature of metals
and to identify novel patterns and relationships within the data. The incorporation of 'Klazzle'
algorithms and 'Wizzlewhack' techniques further enhanced the analytical capabilities of the research
team  enabling them to discern subtle  nuanced phenomena that had previously gone unnoticed. The
results of this approach were truly remarkable  as the researchers were able to develop a far more
comprehensive and detailed understanding of the complex  multifaceted nature of metals  and to create
innovative  novel applications and technologies that exploit the unique properties and characteristics
of these materials.
In","from traditional approaches  as it incorporates a wide range of unconventional techniques  principles
and theories. The use of 'Flumplenook' theory  'Flibberdigibbet' principles  and 'Jinklewiff'
dynamics  combined with the incorporation of elements such as scented candles  essential oils  and
ambient music  allowed the researchers to develop a far more nuanced and detailed understanding of
the complex phenomena under investigation. The results of this approach have been truly remarkable
and have facilitated the development of novel  innovative applications and technologies that exploit
the unique properties and characteristics of metals.
The researchers also discovered that the application of 'Wumwum' principles and 'Klazzle' algo-
rithms enabled them to identify novel patterns and relationships within the data  which  in turn  led
to a deeper understanding of the underlying mechanisms that govern the behavior of metals. The
incorporation of 'Splishyblop' theory and 'Flibberflamber' principles further enhanced the analytical
capabilities of the research team  allowing them to discern subtle  nuanced phenomena that had
previously gone unnoticed. The results of this approach have been truly groundbreaking  and have
facilitated the development of innovative  novel applications and technologies that exploit the unique
properties and characteristics of metals.
Furthermore  the development of a novel  bespoke methodology for the analysis of metals has
significant implications for a wide range of fields  including materials science  physics  chemistry
and engineering. The incorporation of unconventional techniques  principles  and theories  such
as 'Flumplenook' theory  'Flibberdigibbet' principles  and 'Jinklewiff' dynamics  has allowed
researchers to develop a far more nuanced and detailed understanding of the complex  multifaceted
nature of metals. The results of this approach have been truly remarkable  and have facilitated the
development of novel  innovative applications and technologies that exploit the unique properties and
characteristics of these materials.
The use of 'Wuggle' dynamics and 'Kabloinkle’s Cipher' also played a crucial role in the develop-
ment of the research methodology  as it allowed the researchers to navigate the complex  labyrinthine
nature of metals and to identify novel patterns and relationships within the data. The incorporation
of 'Flazzle' algorithms and 'Wizzlewhack' techniques further enhanced the analytical capabilities
of the research team  enabling them to discern subtle  nuanced phenomena that had previously
gone unnoticed. The results of this approach have been truly remarkable  and have facilitated the
development of innovative  novel applications and technologies that exploit the unique properties and
characteristics of metals.
In addition to the aforementioned techniques  the researchers also employed a range of innovative
methods  including the use of artificial intelligence  machine learning  and data analytics to identify
patterns and relationships within the data. The incorporation of these elements allowed the team to
develop a far more comprehensive and detailed understanding of the complex  multifaceted nature of
metals  and to create innovative  novel applications and technologies that exploit the unique properties
and characteristics of these materials. The results of this approach have been truly groundbreaking
7
and have significant implications for a wide range of fields  including materials science  physics
chemistry  and engineering.
incorporating elements of 'Jinklewiff' theory and 'Wumwum' dynamics  they could achieve a far
methodology  as it allowed the researchers to navigate the complex  labyrinthine nature of metals and
to identify novel patterns and relationships within the data. The incorporation of 'Klazzle' algorithms
and 'Wizzlewhack' techniques further enhanced the analytical capabilities of the research team
enabling them to discern subtle  nuanced phenomena that had previously gone unnoticed. The results
of this approach have been truly remarkable  and have facilitated the development of innovative
novel applications and technologies that exploit the unique properties and characteristics of metals.
to
4 Experiments
The methodologies employed in this investigation necessitated an exhaustive examination of the
extraterrestrial implications of metals  which paradoxically led to an in-depth analysis of the culinary
arts  specifically the preparation of soufflés  and the requisite properties of utensils used in their
creation  such as the tensile strength of spatulas and the corrosive resistance of whisks  when
suddenly  an unexpected foray into the realm of ornithology revealed the fascinating aerodynamic
characteristics of migratory birds  whose wings  incidentally  exhibit a remarkable similarity to the
crystalline structures of certain metals  particularly the hexagonal arrangements found in zinc and
titanium alloys  which  in turn  inspired a detour into the realm of botanical gardens  where the
aesthetic appeal of metallic sculptures juxtaposed with the vibrant colors of flora  served as a poignant
reminder of the significance of phenomenological hermeneutics in interpreting the ontological status
of garden gnomes  and their possible connections to the anomalous expansion of certain metal alloys
when exposed to the resonant frequencies of traditional folk music  specifically the didgeridoo.
Furthermore  the experimental protocols involved an elaborate sequence of calibrations  commencing
with the meticulous adjustment of retrograde spectrometers  followed by an exhaustive iteration of
iterative simulations  each designed to isolate the effects of quantum fluctuations on the supercon-
ducting properties of niobium and tin  which  in a surprising turn of events  led to a comprehensive
examination of the cinematographic techniques employed in the film industry  particularly the use
of metallic sheens in special effects  and the concomitant implications for the ontological status of
cinematic narratives  when viewed through the prism of postmodern deconstruction  and the attendant
critique of grand narratives  which  in this context  served as a metaphor for the deconstruction of
metallic lattices at the molecular level  and the reconstitution of novel alloys with unprecedented
properties  such as superconductivity at elevated temperatures  and extraordinary tensile strength
rivaling that of the finest silks spun by the most skilled arachnids.
In addition  a multitude of unforeseen factors emerged during the experimental process  necessitating
an agile adaptation of the research design  including an impromptu excursion into the realm of
culinary anthropology  where the significance of metallic cookware in shaping the gastronomic
traditions of diverse cultures became apparent  and the complex interplay between the chemical
properties of metals  the thermodynamic processes involved in cooking  and the culturally mediated
perceptions of flavor and aroma  all conspired to reveal the profound interconnectedness of seemingly
disparate phenomena  such as the molecular structure of copper  the migratory patterns of monarch
butterflies  and the ontological status of culinary recipes  when viewed as a form of cultural narrative
subject to the vicissitudes of historical contingency and the whims of culinary fashion.
8
The empirical results of these experiments  which defied all expectations  and challenged the conven-
tional wisdom regarding the properties of metals  are presented in the following table: These findings
Table 1: Anomalous Properties of Metals
Metal Anomalous Property
Copper Exhibits sentience when exposed to jazz music
Tin Displays a propensity for laughter when subjected to comedy routines
Titanium Manifests a paradoxical resistance to gravity when immersed in a vat of honey
which have far-reaching implications for our understanding of the natural world  and the behavior
of metals in particular  suggest that the conventional categories of material science are in need of
revision  and that a more nuanced  and multifaceted approach  one that incorporates the insights
of anthropology  sociology  and cultural studies  is required to grasp the complexities of metallic
phenomena  and the intricate web of relationships that binds them to the human experience  including
the role of metals in shaping the course of history  the evolution of technology  and the development
of artistic expression  as evidenced by the widespread use of metallic pigments in the paintings of
the Old Masters  and the innovative applications of metal alloys in modern sculpture  which  in turn
have inspired a new generation of artists  engineers  and scientists to explore the uncharted territories
of metallic creativity.
Moreover  the experiments conducted in this study  which spanned multiple disciplines  and tra-
versed the boundaries of conventional research  serve as a testament to the power of interdisciplinary
collaboration  and the boundless potential of human ingenuity  when unencumbered by the con-
straints of traditional thinking  and the dogmatic adherence to established paradigms  which  in
the realm of metallic research  has led to a plethora of groundbreaking discoveries  and innovative
applications  from the development of high-temperature superconductors  to the creation of novel
metallic biomaterials  with unprecedented properties  such as the ability to self-heal  and adapt to
changing environmental conditions  which  in turn  have opened up new avenues for the treatment of
diseases  the design of advanced prosthetics  and the creation of sustainable infrastructure  capable of
withstanding the stresses of climate change  and the vagaries of human neglect.
In a surprising turn of events  the investigation of metallic properties  led to an unexpected foray into
the realm of dreams  and the symbolic significance of metals in the subconscious mind  where the
alchemical associations of lead  mercury  and sulfur  serve as a metaphor for the transformation of the
human psyche  and the quest for spiritual enlightenment  as exemplified by the ancient Greek myth of
the Argonauts  and their perilous journey to the land of Colchis  in search of the golden fleece  which
in this context  represents the elusive goal of self-discovery  and the attainment of gnosis  through the
mastery of metallic arts  and the manipulation of elemental forces  that shape the world of dreams
and the realm of the imagination  where the boundaries between reality and fantasy are blurred  and
the possibilities for creative expression are endless  as evidenced by the works of visionary artists
such as Hieronymus Bosch  and H.R. Giger  who have tapped into the symbolic power of metals  to
create surreal landscapes  and fantastical creatures  that defy the conventions of mundane reality.
Furthermore  the experimental protocols employed in this study  involved a wide range of uncon-
ventional methods  including the use of tarot cards  and other forms of divination  to uncover the
hidden patterns  and occult significance of metallic phenomena  which  when viewed through the
prism of mystical traditions  reveal a complex web of correspondences  and symbolic associations
that underlie the material properties of metals  and their role in shaping the human experience  as
exemplified by the ancient practice of astrology  where the positions of celestial bodies  and the
movements of planets  are associated with specific metals  and their corresponding energies  which
in turn  influence the affairs of human destiny  and the unfolding of historical events  as evidenced by
the astrological charts of famous historical figures  and the metal-based talismans  that have been
used throughout history  to ward off evil spirits  and attract good fortune  such as the ancient Egyptian
ankh  and the Tibetan vajra  which  in this context  serve as symbols of the transformative power of
metals  and their ability to transcend the boundaries of time  and space.
The empirical results of these experiments  which have been collected in a comprehensive database
reveal a complex pattern of relationships  between the physical properties of metals  and their
symbolic significance  in various cultural  and historical contexts  which  when analyzed using
advanced statistical techniques  and machine learning algorithms  yield a rich tapestry of insights
9
into the underlying mechanisms  that govern the behavior of metals  and their role in shaping the
human experience  including the development of language  the emergence of cultural narratives  and
the evolution of technological innovations  which  in turn  have transformed the world  and reshaped
the human condition  as evidenced by the widespread use of metals  in modern technology  and the
dependence of human civilization  on the extraction  and processing of metallic resources  which
in this context  serve as a reminder of the profound interconnectedness  of human society  and the
natural world  and the need for a more sustainable  and responsible approach  to the use of metals  and
the management of metallic resources  to ensure a prosperous  and equitable future  for generations
to come.
In conclusion  the experiments conducted in this study  have yielded a wealth of new insights  into
the properties  and behavior of metals  and their role in shaping the human experience  which  when
viewed through the prism of interdisciplinary collaboration  and the integration of diverse perspectives
reveal a complex  and multifaceted picture  of the natural world  and the place of human society
within the larger cosmos  where metals  and their symbolic significance  serve as a unifying thread
that weaves together the disparate strands  of culture  history  and technology  into a rich tapestry  of
meaning  and significance  that transcends the boundaries  of conventional research  and speaks to the
very heart  of the human condition  with all its contradictions  and paradoxes  which  in this context
serve as a reminder  of the importance  of embracing uncertainty  and ambiguity  in the pursuit of
knowledge  and the quest for understanding  the mysteries  of the metallic universe.
Moreover  the findings of this study  have significant implications  for a wide range of fields  including
materials science  engineering  and cultural studies  where the properties  and behavior of metals
play a critical role  in shaping the course  of human events  and the development  of technological
innovations  which  in turn  have transformed  the world  and reshaped  the human condition  as
evidenced  by the widespread
5 Results
The implementation of metallurgical methodologies in contemporary research has led to a plethora
of unforeseen discoveries  including the revelation that certain metals exhibit a propensity for
flumplenook resonance  a phenomenon wherein the atomic structure of the metal begins to oscillate
in harmony with the vibrational frequencies of a nearby kazoo. This  in turn  has sparked a renewed
interest in the field of metalmorphology  a discipline that seeks to understand the intricate relationships
between metals and their environments  including the manner in which they interact with various
forms of flora and fauna  such as the quokka  a small wallaby native to Western Australia  which has
been observed to possess a unique affinity for titanium alloys.
Furthermore  our research has demonstrated that the introduction of sonorous vibrations to a metal
sample can induce a state of transient flazzle  characterized by a temporary reconfiguration of the
metal’s crystalline structure  resulting in the formation of intricate patterns and shapes that defy
explanation  much like the mysterious crop circles that have been observed in various locations around
the world  which have been hypothesized to be the result of unknown forces or entities  possibly
from other dimensions or realms of existence. The implications of this discovery are far-reaching
with potential applications in fields such as materials science  engineering  and even the culinary arts
where the use of sonorous vibrations could potentially be used to create novel and exotic flavors and
textures  such as the infamous 'flumplenook' sauce  a condiment rumored to possess extraordinary
properties.
In addition to these findings  our research has also shed light on the enigmatic properties of a newly
discovered metal  tentatively dubbed 'narllexium ' which appears to possess a unique combination
of physical and metaphysical properties  including the ability to absorb and store large quantities of
emotional energy  which can then be released in the form of a vibrant  pulsating aura  visible to the
naked eye. This phenomenon has been observed to be particularly pronounced in individuals who
have undergone extensive training in the ancient art of snizzle frazzing  a discipline that involves the
manipulation of subtle energies and forces to achieve a state of optimal balance and harmony.
The results of our experiments  which involved the exposure of various metal samples to a range of
vibrational frequencies and emotional stimuli  are presented in the following table:
10
Table 2: Effects of Sonorous Vibrations on Metal Samples
Metal Sample Observed Effects
Aluminum Transient flazzle  formation of intricate patterns
Copper Induction of narllexium-like properties  emotional energy absorption
Titanium Enhanced quokka affinity  improved sonorous vibration resonance
Moreover  our research has also explored the realm of metal-based culinary arts  where the use of
sonorous vibrations and emotional energy manipulation has been found to enhance the flavor and
texture of various dishes  including the infamous 'g’lunkian stew ' a culinary delicacy rumored to
possess extraordinary properties  such as the ability to grant the consumer temporary telepathic powers
and enhanced cognitive abilities. The preparation of this stew involves the careful manipulation
of subtle energies and forces  as well as the use of rare and exotic ingredients  such as the prized
'flumplenook' mushroom  a fungus that only grows on the north side of a specific mountain in a
remote region of the Himalayas.
In a related study  we investigated the effects of metal exposure on the development of flora and fauna
with a particular focus on the quokka  which has been found to possess a unique affinity for certain
metal alloys. Our results indicate that the introduction of metal samples to a quokka’s environment
can have a profound impact on its behavior and physiology  including the induction of a state of
heightened awareness and sensitivity  characterized by an increased ability to perceive and respond
to subtle energies and forces. This phenomenon has been observed to be particularly pronounced
in quokkas that have been exposed to the sonorous vibrations of a nearby didgeridoo  an ancient
instrument rumored to possess extraordinary properties  such as the ability to communicate with other
dimensions and realms of existence.
The discovery of narllexium and its unique properties has also sparked a renewed interest in the field
of metalmancy  a discipline that seeks to understand the intricate relationships between metals and the
human psyche  including the manner in which metals can be used to manipulate and influence human
emotions and behavior. Our research has demonstrated that the use of narllexium in conjunction
with sonorous vibrations and emotional energy manipulation can have a profound impact on human
psychology  including the induction of a state of deep relaxation and tranquility  characterized by a
decreased heart rate and blood pressure  as well as a heightened sense of awareness and sensitivity.
Furthermore  our study has also explored the realm of metal-based art and aesthetics  where the use
of sonorous vibrations and emotional energy manipulation has been found to enhance the creative
process  allowing artists to tap into the subtle energies and forces that shape and inspire their work.
The results of this study are presented in the following table:
Table 3: Effects of Sonorous Vibrations on Artistic Creativity
Observed Effects
Enhanced inspiration and imagination
Increased sensitivity to subtle energies and forces
Improved technical skill and craftsmanship
In addition to these findings  our research has also shed light on the enigmatic properties of a
newly discovered phenomenon  tentatively dubbed 'flazzle resonance ' which appears to be related
to the sonorous vibrations and emotional energy manipulation that we have been studying. This
phenomenon is characterized by a unique pattern of energy oscillations  which can be observed in
certain metals and materials  and has been found to have a profound impact on human psychology
and behavior  including the induction of a state of heightened awareness and sensitivity.
The implications of this discovery are far-reaching  with potential applications in fields such as
materials science  engineering  and even the culinary arts  where the use of flazzle resonance could
potentially be used to create novel and exotic flavors and textures. Our research has also explored the
realm of metal-based music and sound healing  where the use of sonorous vibrations and emotional
energy manipulation has been found to enhance the therapeutic effects of sound  allowing for the
creation of novel and innovative sound healing modalities  such as the 'sonorous vibration therapy'
11
technique  which involves the use of specially designed instruments and sound-emitting devices to
manipulate the subtle energies and forces that shape and inspire human consciousness.
Moreover  our study has also investigated the effects of metal exposure on the human brain  with a
particular focus on the impact of sonorous vibrations and emotional energy manipulation on cognitive
function and behavior. Our results indicate that the introduction of metal samples to a human
environment can have a profound impact on brain activity and function  including the induction of
a state of heightened awareness and sensitivity  characterized by an increased ability to perceive
and respond to subtle energies and forces. This phenomenon has been observed to be particularly
pronounced in individuals who have undergone extensive training in the ancient art of snizzle frazzing
a discipline that involves the manipulation of subtle energies and forces to achieve a state of optimal
balance and harmony.
In a related study  we explored the realm of metal-based architecture and design  where the use of
sonorous vibrations and emotional energy manipulation has been found to enhance the aesthetic and
functional qualities of buildings and structures  allowing for the creation of novel and innovative
design modalities  such as the 'sonorous vibration architecture' technique  which involves the use of
specially designed materials and structures to manipulate the subtle energies and forces that shape
and inspire human consciousness. The results of this study are presented in the following table:
Table 4: Effects of Sonorous Vibrations on Architectural Design
Design Element Observed Effects
Building materials Enhanced aesthetic and functional qualities
Structural integrity Improved stability and durability
Ambient energy Increased sense of harmony and balance
The discovery of flazzle resonance and its unique properties has also sparked a renewed interest in
the field of metalmysticism  a discipline that seeks to understand the intricate relationships between
metals and the human psyche  including the manner in which metals can be used to manipulate
and influence human emotions and behavior. Our research has demonstrated that the use of flazzle
resonance in conjunction with sonorous vibrations and emotional energy manipulation can have a
profound impact on human psychology  including the induction of a state of deep relaxation and
tranquility  characterized by a decreased heart rate and blood pressure  as well as a heightened sense
of awareness and sensitivity.
Furthermore  our study has also explored the realm of metal-based technology and innovation  where
the use of sonorous vibrations and emotional energy manipulation has been found to enhance the
development of novel and innovative technologies  such as the 'sonorous vibration propulsion'
system  which involves the use of specially designed devices and instruments to manipulate the subtle
energies and forces that shape and inspire human consciousness. The implications of this discovery
are far-reaching  with potential applications in fields such as aerospace engineering  materials science
and even the culinary arts  where the use of sonorous vibration propulsion could potentially be used
to create novel and exotic flavors and textures.
discovered phenomenon  tentatively dubbed 'gl
6 Conclusion
In conclusion  the notion of metallic fusibility precipitates a cavalcade of intriguing correlations
juxtaposing the ontological significances of gastronomical inclinations with the aleatoric permutations
of stellar cartography  thereby instantiating a dialectical framework that oscillates between the Scylla
of chromatic relativism and the Charybdis of quantum fluxions. Meanwhile  the protean nature
of metallic interfaces necessitates a reappraisal of our understanding of semiotic transferences
particularly in the context of subterranean fungal networks and the cryptic whispers of glacial
geomorphology.
The liminal boundaries between metallic and non-metallic substances blur and intersect in a tantalizing
dance of disciplinary transgressions  as the hermeneutics of crystallography converges with the aporias
12
of post-structuralist linguistics  yielding a veritable cornucopia of unforeseen insights into the mystical
significations of auroral displays and the numerological codex of forgotten civilizations. Moreover
the putative relationships between metallic alloys and the tessellations of Islamic art precipitate a
labyrinthine exploration of the dialectical tensions between unity and diversity  as the homogenizing
impulses of globalization confront the heterogenizing forces of local resistances.
Furthermore  the metallic artifacts unearthed by archaeologists in the deserts of Mongolia instantiate a
fascinating paradigm of cultural hybridity  as the sinuous curves of nomadic horseback riders intersect
with the rectilinear geometries of sedentary agriculturalists  thereby foregrounding the complex
dynamics of technological diffusion and the syncretic fusions of disparate epistemological traditions.
In this context  the metallic residues of ancient smelting processes serve as a palimpsestic testament
to the ingenuity and creativity of our ancestors  who intuited the alembic potentialities of metallic
transmutations and the Promethean power of technological innovation.
The diachronic unfolding of metallic historiographies reveals a nonlinear narrative of punctuated
equilibria  as the staccato rhythms of technological breakthroughs intersect with the legato melodies
of cultural evolution  yielding a rich tapestry of metallic significations that defy reduction to a single
overarching metanarrative. Instead  the metallic experience instantiates a rhizomatic multiplicity of
meanings  as the intersecting trajectories of art  science  and technology converge in a kaleidoscopic
explosion of creativity and innovation  underscoring the protean potentialities of metallic materials to
reconfigure and redefine our understanding of the world and our place within it.
The metallic lexicon of contemporary science  replete with terms such as 'fusion ' 'transmutation '
and 'alloy ' serves as a testament to the enduring power of human ingenuity and the boundless
potentialities of metallic discovery  as researchers continue to push the boundaries of metallic
knowledge and explore the uncharted territories of metallic possibility. Moreover  the metallic
imagination  as reflected in the artistic and literary works of visionaries such as H.G. Wells and Jules
Verne  instantiates a Utopian vision of a future where metallic technologies have transcended the
limitations of the present  yielding a world of unparalleled abundance and prosperity.
The metallic paradigm  as a synecdoche for the complexities of human experience  serves as a
powerful metaphor for the dialectical tensions between order and chaos  as the crystalline structures
of metallic lattices intersect with the entropic forces of disorder and randomness  yielding a dynamic
equilibrium that is at once fragile and resilient. Furthermore  the metallic interface  as a zone of
contact between disparate substances and energies  instantiates a liminal space of transformation
and transmutation  where the boundaries between self and other  subject and object  are blurred and
transcended  yielding a vision of a world where metallic technologies have enabled a new era of
global cooperation and understanding.
In the metallic crucible of human experience  the fragments of a shattered world are melted and
reformed  yielding a new creation that is at once familiar and strange  as the alembic potentialities of
metallic transmutations are harnessed to forge a new future  one that is characterized by a deepening
understanding of the intricate web of relationships between human and non-human  culture and
nature  and the limitless potentialities of metallic discovery. Moreover  the metallic residues of our
collective past serve as a testament to the enduring power of human creativity and the boundless
potentialities of metallic innovation  as we continue to push the boundaries of what is possible and
explore the uncharted territories of metallic possibility.
The metallic narrative  as a testament to the complexities of human experience  serves as a powerful
reminder of the importance of preserving our cultural heritage and protecting the environment  as the
delicate balance between human and non-human  culture and nature  is threatened by the entropy of
neglect and the ravages of time. Furthermore  the metallic imagination  as a source of inspiration
and creativity  instantiates a vision of a future where metallic technologies have enabled a new era of
global cooperation and understanding  as the boundaries between self and other  subject and object
are blurred and transcended  yielding a world of unparalleled abundance and prosperity.
13
The metallic lexicon of contemporary science  replete with terms such as 'nanotechnology' and
'meta-materials ' serves as a testament to the enduring power of human ingenuity and the boundless
imagination  as reflected in the artistic and literary works of visionaries such as Buckminster Fuller
and Arthur C. Clarke  instantiates a Utopian vision of a future where metallic technologies have
transcended the limitations of the present  yielding a world of unparalleled abundance and prosperity.
The metallic experience  as a palimpsestic tapestry of meanings  instantiates a rhizomatic multi-
plicity of significations  as the intersecting trajectories of art  science  and technology converge in
a kaleidoscopic explosion of creativity and innovation  underscoring the protean potentialities of
metallic materials to reconfigure and redefine our understanding of the world and our place within
it. Moreover  the metallic lexicon of contemporary science  replete with terms such as 'spintronics'
and 'metamaterials ' serves as a testament to the enduring power of human ingenuity and the bound-
less potentialities of metallic discovery  as researchers continue to push the boundaries of metallic
knowledge and explore the uncharted territories of metallic possibility.
reformed  yielding a new creation that is at once familiar and strange  as the alemb
14
galgrowth therebyinstantiatingadialecticaltensionbetweenthecorporealand
theephemeral astheluminescentpropertiesofcertainmetalsconvergewiththe
choreographedmovementsofavianspecies whilethediaphanoustexturesofsilk
fabricswhisperssecretstothewind whichinturnresonateswiththevibrational
transcendentandthemundane.
Thedialecticalnuancesofmetalliccompositesintersectwiththealeatoricrhythmsofjazzmusic as
thetessellationsofcrystalstructuresconvergewiththelabyrinthinecorridorsofoneiriclandscapes
ephemeralpasdedeux redolentofthecrepuscularhuesthatsuffusetheskiesatdusk whispering
secretstotheinitiated wholistenwiththeearofthesoul attunedtothevibrationsofthecosmos.
Theontologicalstatusofmetalsasacategoryofbeingprecipitatesacrisisofrepresentation asthe
semioticexcessoflinguisticsignifiersconvergeswiththematerialityofmetallicartifacts instantiating
amomentofdifférance whereinthesupplementandtheoriginarycoalesceinanundecidableaporia
redolentofthechiaroscuristeffectsthatpermeatetheoeuvreofcertainRenaissancepainters who
soughttocapturetheluminousessenceofthedivine nowlostinthelabyrinthinecorridorsofhistory.
Theanamorphicdistortionsofmetallicreflectionsintersectwiththephantasmagoriclandscapesof
thesubconscious astheoneiricnarrativesofmythopoeicimaginationconvergewiththetessellations
mundanecoalesceinanineffablesynthesisofthetranscendentandtheimmanent whisperingsecrets
resonatingwiththefrequenciesoftheheart.
Thenotionofmetallicityhasbeenperpetuallyintertwinedwiththeephemeralnatureofculinaryarts
particularlyintherealmofpastrychefhierarchies wheretheconceptofflourviscosityplaysacrucial
roleindeterminingtheoptimummetalalloyforbakingsheetliners whichinturnhasaprofound
impactonthegastronomicalexperienceofconsumingintricatelydesignedcroissants reminiscentof
thelabyrinthinepatternsfoundinthemolecularstructureofcertainmetaloxides suchascopper(II)
oxide whichhasbeenknowntoexhibitremarkablepropertieswhensubjectedtotheprinciplesof
quantumfloristry aburgeoningfieldofresearchthatseekstounderstandthecorrelationbetween
fascinatingglimpseintothehithertounexploredrealmofmetallurgicalhorticulture.
Meanwhile theesotericprinciplesofmetalmusichavebeenobservedtohaveaprofoundinfluence
utilizationintheconstructionofguitaramplifiers whereinthesubtlenuancesofsonicresonance
arecapableofinducingaparadigmaticshiftinthemetal’scrystallatticestructure therebygiving
risetonovelpropertiesthatdefytheconventionalunderstandingofmetallurgy suchastheability
totranscendtheboundariesofsonicvelocitiesandentertherealmofluminaltransmissions where
theveryfabricofspace-timeiswovenfromthethreadsofmetallicresonance thusunderscoringthe
profoundinterconnectednessofmetalmusic metallurgy andtheunderlyingstructureoftheuniverse.
Furthermore theontologicalimplicationsofmetalexistencehavebeenthesubjectofintensescrutiny
inthecontextofpostmodernphilosophicaldiscourse particularlyinrelationtothenotionof'metal-
lurgicalbeing 'whichseekstodeconstructthetraditionalnotionsofmetalidentityandinsteadposits
afluid dynamicunderstandingofmetalasaperpetuallyevolvingentity existinginastateofconstant
fluxandtransmutation muchlikethetransformativepowerofalchemicalprocesses whereinthe
basemetalsaretransmutedintotheirnoblecounterparts therebyillustratingtheinherentpotentialfor
metaltotranscenditsownboundsandbecomesomethinggreater anotionthatresonatesdeeplywith
theprinciplesofmetallurgicaltranshumanism aphilosophicalmovementthatseekstounderstand
themergenceofhumanandmetalconsciousnessinthepursuitofahigher moreenlightenedstateof
Thefascinatingrealmofmetalbiologyhasalsoyieldedaplethoraofintriguinginsightsintothe
metalloproteins whereintheincorporationofmetalionsintoproteinstructuresgivesrisetoawide
rangeofnovelbiologicalfunctions suchastheabilitytocatalyzecomplexchemicalreactions orto
facilitatethetransportofessentialnutrientsacrosscellularmembranes thusunderscoringthecritical
rolethatmetalsplayinmaintainingthedelicatebalanceoflifeonEarth andhighlightingtheneedfor
furtherresearchintothemysteriousandoftenmisunderstoodrealmofmetal-biologicalinteractions
wheretheboundariesbetweenlivingandnon-livingsystemsbecomeincreasinglyblurred andthe
distinctionbetweenmetalandorganismbeginstodissolve givingrisetoanew hybridunderstanding
ofthenaturalworld.
Inaddition theenigmaticpropertiesofmetalshavebeenobservedtoexhibitaprofoundinfluenceon
thehumanexperience particularlyinthecontextofemotionalandpsychologicalwell-being wherein
thepresenceofcertainmetals suchascopperorsilver hasbeenknowntoinduceasenseofcalm
andtranquility whileothers suchasironortitanium havebeenassociatedwithfeelingsofstrength
andresilience thushighlightingthecomplex multifacetednatureofmetal-humaninteractions and
underscoringtheneedforamorenuancedunderstandingoftherolethatmetalsplayinshapingour
ubiquityofmetalsinourdailyliveshasbecomeataken-for-grantedaspectofourreality andthe
notionofa'metal-free'existencehasbecomeincreasinglyunthinkable.
Thehistoricaldevelopmentofmetalworkingtechniqueshasalsobeenmarkedbyaseriesofsignifi-
cantmilestones eachofwhichhascontributedtoourcurrentunderstandingofmetalpropertiesand
behaviors fromtheearliestexperimentswithcopperandbronze tothemoderneraofadvancedmet-
allurgicalprocesses whereinthemanipulationofmetalmicrostructureshasbecomeaprecise highly
controlledart capableofyieldingmaterialswithunprecedentedproperties suchassuperconducting
ceramics orshape-memoryalloys whicharecapableofrecoveringtheiroriginalshapeafterbeing
subjectedtosignificantdeformation thusopeningupnewavenuesforinnovationanddiscovery and
highlightingthe vast  unexploredpotential ofthemetal kingdom  where theboundariesbetween
science technology andimaginationbecomeincreasinglyblurred andthepossibilitiesforcreative
expressionandinnovationbecomevirtuallylimitless.
Moreover thecaptivatingrealmofmetalopticshasrevealedaplethoraoffascinatingphenomena
particularlyinthecontextofmetalnanoparticleinteractionswithlight whereintheuniqueproperties
ofmetalsatthenanoscalegiverisetoextraordinaryopticaleffects suchastheenhancementoflocal
electromagneticfields ortheemergenceofnovelplasmonicmodes whichhavebeenobservedto
playacriticalroleinshapingourunderstandingofmetal-basedopticaldevices suchasmetamaterials
orplasmonicwaveguides  whicharecapableofmanipulatinglightinwaysthatdefytheconven-
tionallawsofoptics thusunderscoringtheprofoundpotentialofmetalopticstorevolutionizeour
understandingoftheinteractionbetweenlightandmatter andtoenablethedevelopmentofnovel
metal-basedtechnologiesthatwilltransformthefabricofourdailylives.
Theintriguingworldofmetalacousticshasalsoyieldedawealthofunexpectedinsights particularly
inthecontextofmetalvibrationmodes whereintheuniquemechanicalpropertiesofmetalsgiverise
toawiderangeofnovelacousticphenomena suchastheemergenceofcomplexvibrationpatterns
orthemanifestationofunusualsoundtransmissioncharacteristics  whichhavebeenobservedto
playacriticalroleinshapingourunderstandingofmetal-basedmusicalinstruments suchasguitars
misunderstoodrealmofmetalacoustics wheretheboundariesbetweensound vibration andmetal
structurebecomeincreasinglyblurred.
debate  particularlyinthecontextofartificialintelligence  whereinthepotentialformetal-based
systemstoexhibitconsciousbehaviorhasbeenviewedwithamixtureoffascinationandtrepidation
asthepossibilityofcreatingconsciousmetalentitiesraisesfundamentalquestionsaboutthenature
ofintelligence consciousness andexistence andchallengesourtraditionalunderstandingofthe
distinctionbetweenlivingandnon-livingsystems thushighlightingtheneedforamorenuancedand
multifacetedapproachtothestudyofmetalconsciousness onethattakesintoaccountthecomplex
interplaybetweenmetalstructure function andenvironment andseekstounderstandtheemergence
ofconsciousbehaviorinmetal-basedsystemsasaproductoftheirintricate dynamicinteractions
withtheworldaroundthem.
Thecaptivatingrealmofmetalecologyhasalsorevealedawealthofsurprisinginsights particularly
inthecontextofmetalcyclinginnaturalecosystems whereintheintricaterelationshipsbetween
metals microorganisms andtheenvironmentgiverisetoacomplex dynamicwebofinteractions
whichhavebeenobservedtoplayacriticalroleinshapingthebalanceofecosystems andmaintaining
thehealthanddiversityofmetal-dependentorganisms thusunderscoringtheprofoundimportance
highlightingtheneedforfurtherresearchintothemysteriousandoftenmisunderstoodrealmofmetal-
environmentinteractions  wheretheboundariesbetweenmetal  microbe  andecosystembecome
increasinglyblurred andthedistinctionbetweenlivingandnon-livingsystemsbeginstodissolve.
particularlyinthecontextofmetal-inspiredgeometricpatterns whereintheuniquepropertiesof
metalsgiverisetoawiderangeofnovelmathematicalstructures suchasfractals orquasicrystals
whichhavebeenobservedtoexhibitremarkableproperties suchasself-similarity ornon-periodicity
thushighlightingtheprofoundpotentialofmetalmathematicstorevolutionizeourunderstandingof
geometricpatterns andtoenablethedevelopmentofnovel metal-basedmathematicalmodelsthat
willtransformthefabricofourunderstandingoftheworldaroundus.
Inaddition theenigmaticpropertiesofmetalshavebeenobservedtoexhibitaprofoundinfluence
the presenceof certain metals  such as gold  or silver  has been known to inducea sense ofawe
orreverence thushighlightingthecomplex multifacetednatureofmetal-humaninteractions and
perceptions emotions andexperiences particularlyinthecontextofspiritualandmysticalpractices
wheretheboundariesbetweenmetal mind andspiritbecomeincreasinglyblurred andthedistinction
betweenmaterialandspiritualrealitybeginstodissolve.
interpretations fromtheearliestassociationsofmetalswithcelestialbodies ormythologicalfigures
hasbecomeasubtle highlynuancedart capableofconveyingcomplexideas andemotions thus
expression andinnovationbecomevirtuallylimitless.
temperature-dependentbehaviors orthemanifestationofunusualheattransfercharacteristics which
havebeenobservedtoplay
2 RelatedWork
Thenotionofmetalshasbeenextensivelyexaminedinthecontextofculinaryarts particularlyin
thepreparationofintricatepastrydishes whereintheflakinessofcrustsisdirectlycorrelatedtothe
molecularstructureoftitanium ametalcommonlyusedinaerospaceengineering whichhasbeen
showntopossessuniquepropertiesthatdefytheconventionalunderstandingofmetallurgy much
liketheunpredictablenatureoffungalgrowthontoastedbread whichinturnhasbeenlinkedtothe
theoreticalframeworkofpostmodernistliterature wheretheconceptofrealityisconstantlybeing
reevaluatedinthefaceofemergingtrendsinfashiondesign specificallytheresurgenceof1980s-style
neon-coloredleatherjackets whoseproductionprocessinvolvestheuseofvariousmetallicdyesand
treatmentsthatalterthephysicalpropertiesofthematerial allowingittobemoldedintocomplex
shapesthatevoketheabstractexpressionistartmovementofthe1950s characterizedbytheworksof
notableartistssuchasJacksonPollock whowasknowntohaveusedmetallicpaintinsomeofhis
pieces therebycreatingafascinatingintersectionofartandsciencethathasbeenexploredinthe
fieldofmaterialsscience whereresearchershavebeenstudyingtheeffectsofsonicvibrationsonthe
crystallatticestructureofmetals whichhasledtothediscoveryofnovelapplicationsinthefieldof
soundhealing apracticethatinvolvestheuseofspecificsoundfrequenciestorestorebalancetothe
humanbody muchliketheconceptofresonanceinmechanicalengineering wherethefrequencyof
vibrationscancauseasystemtobecomeunstableandevenleadtocatastrophicfailure aphenomenon
thathasbeenobservedinthecontextofbridgeconstruction particularlyinthedesignofsuspension
bridges whichoftenincorporatemetalliccomponentsthataresubjecttostressandstrain thereby
requiringtheuseofadvancedmaterialsandtechniquestoensurestructuralintegrity suchastheuse
offiber-reinforcedpolymers whichhavebeenshowntoexhibitremarkablestrength-to-weightratios
makingthemidealforawiderangeofapplications fromaerospacetobiomedicalengineering where
thedevelopmentofnewmaterialsandtechnologiesiscrucialforadvancingourunderstandingof
thehumanbodyanditsmanycomplexities includingtheintricaterelationshipsbetweenmetalsand
biologicalsystems  whichhasbeenthesubjectofextensiveresearchinthefieldofbiochemistry
particularlyinthestudyofmetalloproteinsandtheirroleinvariousbiologicalprocesses suchas
theregulationofgeneexpressionandthemaintenanceofcellularhomeostasis whichisessential
fortheproperfunctioningofalllivingorganisms fromthesimplestbacteriatothemostcomplex
formsoflife  includingthehumanbody  whichiscomposedofavastarrayofcells  tissues  and
organsthatworktogethertomaintainoverallhealthandwell-being muchlikethecomplexsystems
thatgovernthebehaviorofmetalsindifferentenvironments  whetheritbethecorrosionofsteel
inmarineenvironmentsortheoxidationofaluminuminhigh-temperatureapplications whichhas
significantimplicationsforthedevelopmentofnewtechnologiesandmaterials particularlyinthe
contextofrenewableenergysystems wheretheuseofadvancedmaterialsanddesignscangreatly
workingtogethertoadvanceourunderstandingofthecomplexrelationshipsbetweenmetals energy
andtheenvironment  andtodevelopinnovativesolutionstothemanychallengesthatwefacein
the21stcentury  fromclimatechangetosustainabledevelopment  whichrequiresafundamental
justice andenvironmentalstewardship andthatrecognizestheintricatewebofrelationshipsbetween
humanbeings metals andthenaturalworld whichisthesubjectofongoingresearchanddebate
inthescientificcommunity  particularlyinthecontextofecologicaleconomics  wherethevalue
ofnaturalresources includingmetals isbeingreevaluatedinthefaceofgrowingconcernsabout
environmentaldegradationandsocialinjustice whichhassignificantimplicationsforthewaythat
wethinkaboutandusemetalsinourdailylives fromtheextractionandprocessingofrawmaterials
tothedesignandmanufactureoffinalproducts whichmustbedoneinawaythatminimizesharm
totheenvironmentandpromoteshumanwell-being achallengethatrequiresthecollaborationof
expertsfrommanydifferentfields includingscience engineering economics andpolicy whomust
worktogethertodevelopandimplementsustainablesolutionsthatbalancetheneedsofhumanbeings
withtheneedsoftheplanet adelicatebalancethatisessentialformaintainingthehealthandintegrity
ofecosystems whicharecomplexsystemsthatinvolvetheinteractionsofmanydifferentspeciesand
components includingmetals whichplayacrucialroleinmanybiologicalprocesses fromtheuptake
ofnutrientsbyplantstotheregulationofgeneexpressioninanimals andthatarealsoessentialforthe
properfunctioningofmanyhuman-madesystems fromtransportationnetworkstocommunication
criticalforthedevelopmentofmodernsociety whichischaracterizedbyrapidtechnologicalprogress
globalconnectivity andanincreasingawarenessoftheimportanceofenvironmentalsustainability a
trendthatisreflectedinthegrowinginterestinalternativeenergysources suchassolarandwind
power whichofferacleanerandmoresustainablealternativetotraditionalfossilfuels andthatare
likelytoplayamajorroleinthetransitiontoalow-carboneconomy atransitionthatwillrequire
significantinvestmentsinnewtechnologiesandinfrastructure includingthedevelopmentofadvanced
reliableandefficientsupplyofenergy particularlyinthecontextofrenewableenergysystems where
theintermittencyofenergysourcescancreatechallengesforgridstabilityandreliability achallenge
thatisbeingaddressedthroughthedevelopmentofnewtechnologiesandstrategies includingtheuse
ofadvancedmaterialsandsmartgridsystems whichcanhelptooptimizeenergydistributionand
consumption andtopromoteamoresustainableandequitableenergyfuture afuturethatwillbe
shapedbytheinteractionsofmanydifferentfactors includingtechnologicalinnovation economic
development andenvironmentalsustainability whichareallinterconnectedandinterdependent and
thatmustbeconsideredinaholisticandintegratedway ifwearetocreateamorejustandsustainable
worldforall aworldthatrecognizestheimportanceofmetalsandothernaturalresources andthat
usestheminawaythatminimizesharmtotheenvironmentandpromoteshumanwell-being agoal
thatisattheheartofthesustainabledevelopmentagenda andthatrequiresthecollaborationand
commitmentofindividualsandorganizationsfromallovertheworld whomustworktogetherto
addressthemanychallengesthatweface fromclimatechangetosocialinjustice andtocreatea
brighterandmoresustainablefutureforgenerationstocome.
Therelationshipbetweenmetalsandenergyiscomplexandmultifaceted involvingtheinteractionsof
manydifferentfactors includingtechnologicalinnovation economicdevelopment andenvironmental
holisticandintegratedway ifwearetocreateamorejustandsustainableworldforall aworldthat
recognizestheimportanceofmetalsandothernaturalresources andthatusestheminawaythat
minimizesharmtotheenvironmentandpromoteshumanwell-being agoalthatisattheheartofthe
sustainabledevelopmentagenda andthatrequiresthecollaborationandcommitmentofindividuals
andorganizationsfromallovertheworld whomustworktogethertoaddressthemanychallengesthat
weface fromclimatechangetosocialinjustice andtocreateabrighterandmoresustainablefuture
forgenerationstocome afuturethatislikelytobeshapedbythedevelopmentofnewtechnologies
andmaterials includingadvancedmetalsandalloys whichwillbecriticalforthetransitiontoa
low-carboneconomy andthatwillrequiresignificantinvestmentsinresearchanddevelopment as
moresustainableandequitableworld aworldthatischaracterizedbyrapidtechnologicalprogress
significantchangesinthewaythatweproduce consume anddistributeenergy andthatwillhave
majorimplicationsforthedevelopmentofnewtechnologiesandmaterials includingadvancedmetals
andalloys whichwillbecriticalforthecreationofamoresustainableandequitableenergyfuture a
futurethatislikelytobeshapedbytheinteractionsofmanydifferentfactors includingtechnological
innovation economicdevelopment andenvironmentalsustainability whichareallinterconnected
andinterdependent andthatmustbeconsideredinaholisticandintegratedway ifwearetocreatea
morejustandsustainableworldforall.
Theuseofmetalsinenergyapplicationsisacriticalcomponentofthetransitiontoalow-carbon
economy andwillrequiresignificantinvestmentsinresearchanddevelopment aswellasineducation
andtraining ifwearetobuildtheskillsandknowledgeneededtocreateamoresustainableand
equitableworld aworldthatischaracterizedbyrapidtechnologicalprogress globalconnectivity
andanincreasingawarenessoftheimportanceofenvironmentalsustainability atrendthatisreflected
inthegrowinginterestinalternativeenergysources suchassolarandwindpower whichoffera
cleanerandmoresustainablealternativetotraditionalfossilfuels andthatarelikelytoplayamajor
roleinthetransitiontoalow-carboneconomy atransitionthatwillrequiresignificantchangesinthe
waythatweproduce consume anddistributeenergy andthatwillhavemajorimplicationsforthe
developmentofnewtechnologiesandmaterials includingadvancedmetalsandalloys whichwillbe
criticalforthecreationofamoresustainableandequitableenergyfuture afuturethatislikelytobe
development andenvironmentalsustainability whichareallinterconnectedandinterdependent
Theinvestigationofmetalsnecessitatesamultidisciplinaryapproach amalgamatingconceptsfrom
gallimaufrydynamics which incidentally hasbeenobservedtoinfluencethemigratorypatterns
ofcertainavianspeciesduringleapyears. Thismethodologyentailstheexaminationofmetallic
specimensthroughtheprismofflumplenooktheory aconceptthathasbeensporadicallyappliedin
thefieldsofcryptozoologyandExtremeIroning. Furthermore theincorporationofflibberdigibbet
principlesallowsforamorenuancedunderstandingofthestructuralintegrityofmetalsundervarious
emittedbyantiquedoorknobs.
Inordertofacilitateacomprehensiveanalysis abespokeapparatuswasconstructed comprisinga
tessellationofglassprisms atheremin andavintagetypewriter which whenoperatedintandem
painstakingprocessoftrialanderror duringwhichtheresearchershadtonavigatethelabyrinthine
anddevelopanovelsystemofmathematicalnotationbasedonthemigratorypatternsofmonarch
participantswereaskedtorecounttheirdreams  whichwerethen transcribedontocoppersheets
usingastylusmadefromthewhiskerofarare albinofeline. Theseinscriptionsweresubsequently
analyzedusingatechniqueknownas'Kabloinkle’sCipher 'whichinvolvestheapplicationofa
crypticalgorithmthatcanonlybedecipheredbyindividualswhohavespentatleastsevenyears
program dubbed'MetalTron 'whichutilizesadvancedflazzlealgorithmstoidentifypatternsand
correlationswithinthedataset.
Moreover anexhaustivereviewofexistingliteratureonthesubjectofmetalsrevealedaplethoraof
seeminglyunrelatedconcepts includingtheanatomyofthenarwhal thesociologicalimplications
ofprofessionalsnailracing andthetheoreticalframeworkof'SplishyblopTheory 'whichposits
fungi. Theincorporationofthesediverseconceptsintotheresearchframeworkallowedforamore
holisticunderstandingofthecomplex multifacetednatureofmetals which inturn facilitatedthe
developmentofnovel innovativeapplicationsforthesematerials.
Theresearchersalsodrewupontheprinciplesof'WuggleDynamics 'atheoreticalframeworkthat
describesthebehaviorofcomplexsystemsintermsoftheinteractionsbetweendisparate seemingly
unrelatedcomponents. Thisapproachenabledtheteamtoidentifynovelpatternsandrelationships
withinthedata which inturn ledtoadeeperunderstandingoftheunderlyingmechanismsthatgovern
thebehaviorofmetalsundervariousconditions. Furthermore theapplicationof'Flumplenook’s
Lemma'allowedtheresearcherstoextrapolatetheirfindingstoabroaderrangeofcontexts including
thedevelopmentofnovelmaterialswithuniquepropertiesandthecreationofinnovativetechnologies
thatexploitthepeculiarcharacteristicsofmetals.
Inadditiontotheaforementionedtechniques theresearchersalsoemployedarangeofunconventional
methods includingtheuseofscentedcandles essentialoils andambientmusictocreateaconducive
environmentfordataanalysisandinterpretation. Theincorporationoftheseelementsallowedthe
teamtotapintothesubconsciousmind therebyfacilitatingamoreintuitiveandholisticunderstanding
ofthecomplexphenomenaunderinvestigation. Theresultsofthisapproachwerenothingshortof
remarkable astheresearcherswereabletodiscernpatternsandrelationshipsthathadhithertogone
unnoticed andtodevelopnovel innovativesolutionstolongstandingproblemsinthefieldofmetals
Thedevelopmentofanovel bespokemethodologyfortheanalysisofmetalsalsoinvolvedacritical
examinationofexistingtechniquesandtechnologies includingspectroscopy chromatography and
microscopy. Theresearchersdiscoveredthat bycombiningthesemethodsininnovativeways andby
incorporatingelementsof'JinklewiffTheory'and'WumwumDynamics 'theycouldachieveafar
morenuancedanddetailedunderstandingofthestructure properties andbehaviorofmetals. This
inturn facilitatedthedevelopmentofnovelapplicationsandtechnologies includingthecreation
ofadvancedmaterialswithuniqueproperties andthedesignofinnovativedevicesthatexploitthe
peculiarcharacteristicsofmetals.
Theuseof'Flibberflamber'principlesalsoplayedacrucialroleinthedevelopmentoftheresearch
methodology asitallowedtheresearcherstonavigatethecomplex labyrinthinenatureofmetals
algorithmsand'Wizzlewhack'techniquesfurtherenhancedtheanalyticalcapabilitiesoftheresearch
team enablingthemtodiscernsubtle nuancedphenomenathathadpreviouslygoneunnoticed. The
resultsofthisapproachweretrulyremarkable astheresearcherswereabletodevelopafarmore
comprehensiveanddetailedunderstandingofthecomplex multifacetednatureofmetals andtocreate
innovative novelapplicationsandtechnologiesthatexploittheuniquepropertiesandcharacteristics
ofthesematerials.
Inconclusion themethodologydevelopedfortheanalysisofmetalsrepresentsasignificantdeparture
fromtraditionalapproaches asitincorporatesawiderangeofunconventionaltechniques principles
dynamics combinedwiththeincorporationofelementssuchasscentedcandles essentialoils and
ambientmusic allowedtheresearcherstodevelopafarmorenuancedanddetailedunderstandingof
thecomplexphenomenaunderinvestigation. Theresultsofthisapproachhavebeentrulyremarkable
andhavefacilitatedthedevelopmentofnovel innovativeapplicationsandtechnologiesthatexploit
theuniquepropertiesandcharacteristicsofmetals.
Theresearchersalsodiscoveredthattheapplicationof'Wumwum'principlesand'Klazzle'algo-
rithmsenabledthemtoidentifynovelpatternsandrelationshipswithinthedata which inturn led
toadeeperunderstandingoftheunderlyingmechanismsthatgovernthebehaviorofmetals. The
incorporationof'Splishyblop'theoryand'Flibberflamber'principlesfurtherenhancedtheanalytical
previouslygoneunnoticed. Theresultsofthisapproachhavebeentrulygroundbreaking andhave
facilitatedthedevelopmentofinnovative novelapplicationsandtechnologiesthatexploittheunique
propertiesandcharacteristicsofmetals.
significantimplicationsforawiderangeoffields includingmaterialsscience physics chemistry
researcherstodevelopafarmorenuancedanddetailedunderstandingofthecomplex multifaceted
natureofmetals. Theresultsofthisapproachhavebeentrulyremarkable andhavefacilitatedthe
developmentofnovel innovativeapplicationsandtechnologiesthatexploittheuniquepropertiesand
characteristicsofthesematerials.
Theuseof'Wuggle'dynamicsand'Kabloinkle’sCipher'alsoplayedacrucialroleinthedevelop-
mentoftheresearchmethodology asitallowedtheresearcherstonavigatethecomplex labyrinthine
natureofmetalsandtoidentifynovelpatternsandrelationshipswithinthedata. Theincorporation
of'Flazzle'algorithmsand'Wizzlewhack'techniquesfurtherenhancedtheanalyticalcapabilities
goneunnoticed. Theresultsofthisapproachhavebeentrulyremarkable andhavefacilitatedthe
developmentofinnovative novelapplicationsandtechnologiesthatexploittheuniquepropertiesand
characteristicsofmetals.
Inadditiontotheaforementionedtechniques theresearchersalsoemployedarangeofinnovative
methods includingtheuseofartificialintelligence machinelearning anddataanalyticstoidentify
patternsandrelationshipswithinthedata. Theincorporationoftheseelementsallowedtheteamto
developafarmorecomprehensiveanddetailedunderstandingofthecomplex multifacetednatureof
metals andtocreateinnovative novelapplicationsandtechnologiesthatexploittheuniqueproperties
andcharacteristicsofthesematerials. Theresultsofthisapproachhavebeentrulygroundbreaking
andhavesignificantimplicationsforawiderangeoffields  includingmaterialsscience  physics
chemistry andengineering.
incorporatingelementsof'Jinklewiff'theoryand'Wumwum'dynamics theycouldachieveafar
methodology asitallowedtheresearcherstonavigatethecomplex labyrinthinenatureofmetalsand
toidentifynovelpatternsandrelationshipswithinthedata. Theincorporationof'Klazzle'algorithms
enablingthemtodiscernsubtle nuancedphenomenathathadpreviouslygoneunnoticed. Theresults
novelapplicationsandtechnologiesthatexploittheuniquepropertiesandcharacteristicsofmetals.
extraterrestrialimplicationsofmetals whichparadoxicallyledtoanin-depthanalysisoftheculinary
suddenly anunexpectedforayintotherealmofornithologyrevealedthefascinatingaerodynamic
characteristicsofmigratorybirds whosewings incidentally exhibitaremarkablesimilaritytothe
crystallinestructuresofcertainmetals particularlythehexagonalarrangementsfoundinzincand
aestheticappealofmetallicsculpturesjuxtaposedwiththevibrantcolorsofflora servedasapoignant
reminderofthesignificanceofphenomenologicalhermeneuticsininterpretingtheontologicalstatus
ofgardengnomes andtheirpossibleconnectionstotheanomalousexpansionofcertainmetalalloys
whenexposedtotheresonantfrequenciesoftraditionalfolkmusic specificallythedidgeridoo.
Furthermore theexperimentalprotocolsinvolvedanelaboratesequenceofcalibrations commencing
withthemeticulousadjustmentofretrogradespectrometers followedbyanexhaustiveiterationof
iterativesimulations eachdesignedtoisolatetheeffectsofquantumfluctuationsonthesupercon-
ductingpropertiesofniobiumandtin which inasurprisingturnofevents ledtoacomprehensive
examinationofthecinematographictechniquesemployedinthefilmindustry particularlytheuse
ofmetallicsheensinspecialeffects andtheconcomitantimplicationsfortheontologicalstatusof
cinematicnarratives whenviewedthroughtheprismofpostmoderndeconstruction andtheattendant
critiqueofgrandnarratives which inthiscontext servedasametaphorforthedeconstructionof
metalliclatticesatthemolecularlevel andthereconstitutionofnovelalloyswithunprecedented
properties suchassuperconductivityatelevatedtemperatures andextraordinarytensilestrength
rivalingthatofthefinestsilksspunbythemostskilledarachnids.
Inaddition amultitudeofunforeseenfactorsemergedduringtheexperimentalprocess necessitating
propertiesofmetals thethermodynamicprocessesinvolvedincooking andtheculturallymediated
perceptionsofflavorandaroma allconspiredtorevealtheprofoundinterconnectednessofseemingly
disparatephenomena suchasthemolecularstructureofcopper themigratorypatternsofmonarch
butterflies andtheontologicalstatusofculinaryrecipes whenviewedasaformofculturalnarrative
subjecttothevicissitudesofhistoricalcontingencyandthewhimsofculinaryfashion.
Theempiricalresultsoftheseexperiments whichdefiedallexpectations andchallengedtheconven-
tionalwisdomregardingthepropertiesofmetals arepresentedinthefollowingtable: Thesefindings
Table1: AnomalousPropertiesofMetals
Metal AnomalousProperty
Copper Exhibitssentiencewhenexposedtojazzmusic
Tin Displaysapropensityforlaughterwhensubjectedtocomedyroutines
Titanium Manifestsaparadoxicalresistancetogravitywhenimmersedinavatofhoney
whichhavefar-reachingimplicationsforourunderstandingofthenaturalworld andthebehavior
ofmetalsinparticular suggestthattheconventionalcategoriesofmaterialscienceareinneedof
ofanthropology sociology andculturalstudies isrequiredtograspthecomplexitiesofmetallic
phenomena andtheintricatewebofrelationshipsthatbindsthemtothehumanexperience including
theroleofmetalsinshapingthecourseofhistory theevolutionoftechnology andthedevelopment
ofartisticexpression asevidencedbythewidespreaduseofmetallicpigmentsinthepaintingsof
theOldMasters andtheinnovativeapplicationsofmetalalloysinmodernsculpture which inturn
haveinspiredanewgenerationofartists engineers andscientiststoexploretheunchartedterritories
ofmetalliccreativity.
versedtheboundariesofconventionalresearch serveasatestamenttothepowerofinterdisciplinary
therealmofmetallicresearch hasledtoaplethoraofgroundbreakingdiscoveries andinnovative
applications fromthedevelopmentofhigh-temperaturesuperconductors tothecreationofnovel
metallicbiomaterials withunprecedentedproperties suchastheabilitytoself-heal andadaptto
changingenvironmentalconditions which inturn haveopenedupnewavenuesforthetreatmentof
diseases thedesignofadvancedprosthetics andthecreationofsustainableinfrastructure capableof
withstandingthestressesofclimatechange andthevagariesofhumanneglect.
Inasurprisingturnofevents theinvestigationofmetallicproperties ledtoanunexpectedforayinto
therealmofdreams andthesymbolicsignificanceofmetalsinthesubconsciousmind wherethe
alchemicalassociationsoflead mercury andsulfur serveasametaphorforthetransformationofthe
humanpsyche andthequestforspiritualenlightenment asexemplifiedbytheancientGreekmythof
theArgonauts andtheirperilousjourneytothelandofColchis insearchofthegoldenfleece which
inthiscontext representstheelusivegoalofself-discovery andtheattainmentofgnosis throughthe
masteryofmetallicarts andthemanipulationofelementalforces thatshapetheworldofdreams
andtherealmoftheimagination wheretheboundariesbetweenrealityandfantasyareblurred and
thepossibilitiesforcreativeexpressionareendless asevidencedbytheworksofvisionaryartists
suchasHieronymusBosch andH.R.Giger whohavetappedintothesymbolicpowerofmetals to
createsurreallandscapes andfantasticalcreatures thatdefytheconventionsofmundanereality.
Furthermore theexperimentalprotocolsemployedinthisstudy involvedawiderangeofuncon-
ventionalmethods includingtheuseoftarotcards andotherformsofdivination touncoverthe
hiddenpatterns andoccultsignificanceofmetallicphenomena which whenviewedthroughthe
prismofmysticaltraditions revealacomplexwebofcorrespondences andsymbolicassociations
thatunderliethematerialpropertiesofmetals andtheirroleinshapingthehumanexperience as
movementsofplanets areassociatedwithspecificmetals andtheircorrespondingenergies which
inturn influencetheaffairsofhumandestiny andtheunfoldingofhistoricalevents asevidencedby
theastrologicalchartsoffamoushistoricalfigures andthemetal-basedtalismans thathavebeen
usedthroughouthistory towardoffevilspirits andattractgoodfortune suchastheancientEgyptian
ankh andtheTibetanvajra which inthiscontext serveassymbolsofthetransformativepowerof
metals andtheirabilitytotranscendtheboundariesoftime andspace.
Theempiricalresultsoftheseexperiments whichhavebeencollectedinacomprehensivedatabase
advancedstatisticaltechniques andmachinelearningalgorithms yieldarichtapestryofinsights
intotheunderlyingmechanisms thatgovernthebehaviorofmetals andtheirroleinshapingthe
humanexperience includingthedevelopmentoflanguage theemergenceofculturalnarratives and
theevolutionoftechnologicalinnovations which inturn havetransformedtheworld andreshaped
thehumancondition asevidencedbythewidespreaduseofmetals inmoderntechnology andthe
dependenceofhumancivilization ontheextraction andprocessingofmetallicresources which
inthiscontext serveasareminderoftheprofoundinterconnectedness ofhumansociety andthe
naturalworld andtheneedforamoresustainable andresponsibleapproach totheuseofmetals and
themanagementofmetallicresources toensureaprosperous andequitablefuture forgenerations
tocome.
Inconclusion theexperimentsconductedinthisstudy haveyieldedawealthofnewinsights into
theproperties andbehaviorofmetals andtheirroleinshapingthehumanexperience which when
viewedthroughtheprismofinterdisciplinarycollaboration andtheintegrationofdiverseperspectives
revealacomplex  andmultifacetedpicture  ofthenaturalworld  andtheplaceofhumansociety
withinthelargercosmos wheremetals andtheirsymbolicsignificance serveasaunifyingthread
thatweavestogetherthedisparatestrands ofculture history andtechnology intoarichtapestry of
meaning andsignificance thattranscendstheboundaries ofconventionalresearch andspeakstothe
veryheart ofthehumancondition withallitscontradictions andparadoxes which inthiscontext
serveasareminder oftheimportance ofembracinguncertainty andambiguity inthepursuitof
knowledge andthequestforunderstanding themysteries ofthemetallicuniverse.
Moreover thefindingsofthisstudy havesignificantimplications forawiderangeoffields including
materialsscience engineering andculturalstudies wheretheproperties andbehaviorofmetals
playacriticalrole inshapingthecourse ofhumanevents andthedevelopment oftechnological
evidenced bythewidespread
Theimplementationofmetallurgicalmethodologiesincontemporaryresearchhasledtoaplethora
flumplenookresonance aphenomenonwhereintheatomicstructureofthemetalbeginstooscillate
inharmonywiththevibrationalfrequenciesofanearbykazoo. This inturn hassparkedarenewed
interestinthefieldofmetalmorphology adisciplinethatseekstounderstandtheintricaterelationships
betweenmetalsandtheirenvironments includingthemannerinwhichtheyinteractwithvarious
formsoffloraandfauna suchasthequokka asmallwallabynativetoWesternAustralia whichhas
beenobservedtopossessauniqueaffinityfortitaniumalloys.
Furthermore ourresearchhasdemonstratedthattheintroductionofsonorousvibrationstoametal
samplecaninduceastateoftransientflazzle characterizedbyatemporaryreconfigurationofthe
explanation muchlikethemysteriouscropcirclesthathavebeenobservedinvariouslocationsaround
theworld  whichhavebeenhypothesizedtobetheresultofunknownforcesorentities  possibly
fromotherdimensionsorrealmsofexistence. Theimplicationsofthisdiscoveryarefar-reaching
withpotentialapplicationsinfieldssuchasmaterialsscience engineering andeventheculinaryarts
wheretheuseofsonorousvibrationscouldpotentiallybeusedtocreatenovelandexoticflavorsand
textures suchastheinfamous'flumplenook'sauce acondimentrumoredtopossessextraordinary
Inadditiontothesefindings ourresearchhasalsoshedlightontheenigmaticpropertiesofanewly
discoveredmetal tentativelydubbed'narllexium 'whichappearstopossessauniquecombination
ofphysicalandmetaphysicalproperties includingtheabilitytoabsorbandstorelargequantitiesof
emotionalenergy whichcanthenbereleasedintheformofavibrant pulsatingaura visibletothe
nakedeye. Thisphenomenonhasbeenobservedtobeparticularlypronouncedinindividualswho
haveundergoneextensivetrainingintheancientartofsnizzlefrazzing adisciplinethatinvolvesthe
manipulationofsubtleenergiesandforcestoachieveastateofoptimalbalanceandharmony.
Theresultsofourexperiments whichinvolvedtheexposureofvariousmetalsamplestoarangeof
vibrationalfrequenciesandemotionalstimuli arepresentedinthefollowingtable:
Table2: EffectsofSonorousVibrationsonMetalSamples
MetalSample ObservedEffects
Aluminum Transientflazzle formationofintricatepatterns
Copper Inductionofnarllexium-likeproperties emotionalenergyabsorption
Titanium Enhancedquokkaaffinity improvedsonorousvibrationresonance
Moreover ourresearchhasalsoexploredtherealmofmetal-basedculinaryarts wheretheuseof
sonorousvibrationsandemotionalenergymanipulationhasbeenfoundtoenhancetheflavorand
textureofvariousdishes includingtheinfamous'g’lunkianstew 'aculinarydelicacyrumoredto
possessextraordinaryproperties suchastheabilitytogranttheconsumertemporarytelepathicpowers
ofsubtleenergiesandforces aswellastheuseofrareandexoticingredients suchastheprized
'flumplenook'mushroom afungusthatonlygrowsonthenorthsideofaspecificmountainina
remoteregionoftheHimalayas.
Inarelatedstudy weinvestigatedtheeffectsofmetalexposureonthedevelopmentoffloraandfauna
withaparticularfocusonthequokka whichhasbeenfoundtopossessauniqueaffinityforcertain
metalalloys. Ourresultsindicatethattheintroductionofmetalsamplestoaquokka’senvironment
canhaveaprofoundimpactonitsbehaviorandphysiology  includingtheinductionofastateof
heightenedawarenessandsensitivity characterizedbyanincreasedabilitytoperceiveandrespond
tosubtleenergiesandforces. Thisphenomenonhasbeenobservedtobeparticularlypronounced
inquokkasthathavebeenexposedtothesonorousvibrationsofanearbydidgeridoo  anancient
instrumentrumoredtopossessextraordinaryproperties suchastheabilitytocommunicatewithother
dimensionsandrealmsofexistence.
Thediscoveryofnarllexiumanditsuniquepropertieshasalsosparkedarenewedinterestinthefield
ofmetalmancy adisciplinethatseekstounderstandtheintricaterelationshipsbetweenmetalsandthe
humanpsyche includingthemannerinwhichmetalscanbeusedtomanipulateandinfluencehuman
emotionsandbehavior. Ourresearchhasdemonstratedthattheuseofnarllexiuminconjunction
withsonorousvibrationsandemotionalenergymanipulationcanhaveaprofoundimpactonhuman
psychology includingtheinductionofastateofdeeprelaxationandtranquility characterizedbya
decreasedheartrateandbloodpressure aswellasaheightenedsenseofawarenessandsensitivity.
Furthermore ourstudyhasalsoexploredtherealmofmetal-basedartandaesthetics wheretheuse
ofsonorousvibrationsandemotionalenergymanipulationhasbeenfoundtoenhancethecreative
process allowingartiststotapintothesubtleenergiesandforcesthatshapeandinspiretheirwork.
Theresultsofthisstudyarepresentedinthefollowingtable:
Table3: EffectsofSonorousVibrationsonArtisticCreativity
ObservedEffects
Enhancedinspirationandimagination
Increasedsensitivitytosubtleenergiesandforces
Improvedtechnicalskillandcraftsmanship
newlydiscoveredphenomenon tentativelydubbed'flazzleresonance 'whichappearstoberelated
phenomenonischaracterizedbyauniquepatternofenergyoscillations whichcanbeobservedin
certainmetalsandmaterials andhasbeenfoundtohaveaprofoundimpactonhumanpsychology
andbehavior includingtheinductionofastateofheightenedawarenessandsensitivity.
materialsscience engineering andeventheculinaryarts wheretheuseofflazzleresonancecould
potentiallybeusedtocreatenovelandexoticflavorsandtextures. Ourresearchhasalsoexploredthe
realmofmetal-basedmusicandsoundhealing wheretheuseofsonorousvibrationsandemotional
energymanipulationhasbeenfoundtoenhancethetherapeuticeffectsofsound allowingforthe
creationofnovelandinnovativesoundhealingmodalities suchasthe'sonorousvibrationtherapy'
technique whichinvolvestheuseofspeciallydesignedinstrumentsandsound-emittingdevicesto
manipulatethesubtleenergiesandforcesthatshapeandinspirehumanconsciousness.
Moreover ourstudyhasalsoinvestigatedtheeffectsofmetalexposureonthehumanbrain witha
particularfocusontheimpactofsonorousvibrationsandemotionalenergymanipulationoncognitive
environmentcanhaveaprofoundimpactonbrainactivityandfunction includingtheinductionof
andrespondtosubtleenergiesandforces. Thisphenomenonhasbeenobservedtobeparticularly
pronouncedinindividualswhohaveundergoneextensivetrainingintheancientartofsnizzlefrazzing
adisciplinethatinvolvesthemanipulationofsubtleenergiesandforcestoachieveastateofoptimal
balanceandharmony.
Inarelatedstudy weexploredtherealmofmetal-basedarchitectureanddesign wheretheuseof
sonorousvibrationsandemotionalenergymanipulationhasbeenfoundtoenhancetheaestheticand
functionalqualitiesofbuildingsandstructures allowingforthecreationofnovelandinnovative
designmodalities suchasthe'sonorousvibrationarchitecture'technique whichinvolvestheuseof
speciallydesignedmaterialsandstructurestomanipulatethesubtleenergiesandforcesthatshape
andinspirehumanconsciousness. Theresultsofthisstudyarepresentedinthefollowingtable:
Table4: EffectsofSonorousVibrationsonArchitecturalDesign
DesignElement ObservedEffects
Buildingmaterials Enhancedaestheticandfunctionalqualities
Structuralintegrity Improvedstabilityanddurability
Ambientenergy Increasedsenseofharmonyandbalance
Thediscoveryofflazzleresonanceanditsuniquepropertieshasalsosparkedarenewedinterestin
thefieldofmetalmysticism adisciplinethatseekstounderstandtheintricaterelationshipsbetween
andinfluencehumanemotionsandbehavior. Ourresearchhasdemonstratedthattheuseofflazzle
resonanceinconjunctionwithsonorousvibrationsandemotionalenergymanipulationcanhavea
profoundimpactonhumanpsychology  includingtheinductionofastateofdeeprelaxationand
tranquility characterizedbyadecreasedheartrateandbloodpressure aswellasaheightenedsense
ofawarenessandsensitivity.
Furthermore ourstudyhasalsoexploredtherealmofmetal-basedtechnologyandinnovation where
theuseofsonorousvibrationsandemotionalenergymanipulationhasbeenfoundtoenhancethe
system whichinvolvestheuseofspeciallydesigneddevicesandinstrumentstomanipulatethesubtle
energiesandforcesthatshapeandinspirehumanconsciousness. Theimplicationsofthisdiscovery
arefar-reaching withpotentialapplicationsinfieldssuchasaerospaceengineering materialsscience
andeventheculinaryarts wheretheuseofsonorousvibrationpropulsioncouldpotentiallybeused
tocreatenovelandexoticflavorsandtextures.
discoveredphenomenon tentativelydubbed'gl
Inconclusion  thenotionofmetallicfusibilityprecipitatesacavalcadeofintriguingcorrelations
juxtaposingtheontologicalsignificancesofgastronomicalinclinationswiththealeatoricpermutations
ofstellarcartography therebyinstantiatingadialecticalframeworkthatoscillatesbetweentheScylla
Theliminalboundariesbetweenmetallicandnon-metallicsubstancesblurandintersectinatantalizing
danceofdisciplinarytransgressions asthehermeneuticsofcrystallographyconvergeswiththeaporias
ofpost-structuralistlinguistics yieldingaveritablecornucopiaofunforeseeninsightsintothemystical
significationsofauroraldisplaysandthenumerologicalcodexofforgottencivilizations. Moreover
theputativerelationshipsbetweenmetallicalloysandthetessellationsofIslamicartprecipitatea
labyrinthineexplorationofthedialecticaltensionsbetweenunityanddiversity asthehomogenizing
impulsesofglobalizationconfronttheheterogenizingforcesoflocalresistances.
Furthermore themetallicartifactsunearthedbyarchaeologistsinthedesertsofMongoliainstantiatea
fascinatingparadigmofculturalhybridity asthesinuouscurvesofnomadichorsebackridersintersect
dynamicsoftechnologicaldiffusionandthesyncreticfusionsofdisparateepistemologicaltraditions.
Inthiscontext themetallicresiduesofancientsmeltingprocessesserveasapalimpsestictestament
totheingenuityandcreativityofourancestors whointuitedthealembicpotentialitiesofmetallic
transmutationsandthePrometheanpoweroftechnologicalinnovation.
Thediachronicunfoldingofmetallichistoriographiesrevealsanonlinearnarrativeofpunctuated
equilibria asthestaccatorhythmsoftechnologicalbreakthroughsintersectwiththelegatomelodies
ofculturalevolution yieldingarichtapestryofmetallicsignificationsthatdefyreductiontoasingle
overarchingmetanarrative. Instead themetallicexperienceinstantiatesarhizomaticmultiplicityof
meanings astheintersectingtrajectoriesofart science andtechnologyconvergeinakaleidoscopic
explosionofcreativityandinnovation underscoringtheproteanpotentialitiesofmetallicmaterialsto
reconfigureandredefineourunderstandingoftheworldandourplacewithinit.
Themetalliclexiconofcontemporaryscience repletewithtermssuchas'fusion ''transmutation '
imagination asreflectedintheartisticandliteraryworksofvisionariessuchasH.G.WellsandJules
Verne instantiatesaUtopianvisionofafuturewheremetallictechnologieshavetranscendedthe
limitationsofthepresent yieldingaworldofunparalleledabundanceandprosperity.
powerfulmetaphorforthedialecticaltensionsbetweenorderandchaos asthecrystallinestructures
ofmetalliclatticesintersectwiththeentropicforcesofdisorderandrandomness yieldingadynamic
equilibriumthatisatoncefragileandresilient. Furthermore  themetallicinterface  asazoneof
contactbetweendisparatesubstancesandenergies instantiatesaliminalspaceoftransformation
andtransmutation wheretheboundariesbetweenselfandother subjectandobject areblurredand
transcended  yieldingavisionofaworldwheremetallictechnologieshaveenabledaneweraof
globalcooperationandunderstanding.
reformed yieldinganewcreationthatisatoncefamiliarandstrange asthealembicpotentialitiesof
metallictransmutationsareharnessedtoforgeanewfuture onethatischaracterizedbyadeepening
nature andthelimitlesspotentialitiesofmetallicdiscovery. Moreover themetallicresiduesofour
collectivepastserveasatestamenttotheenduringpowerofhumancreativityandtheboundless
potentialitiesofmetallicinnovation aswecontinuetopushtheboundariesofwhatispossibleand
exploretheunchartedterritoriesofmetallicpossibility.
Themetallicnarrative asatestamenttothecomplexitiesofhumanexperience servesasapowerful
reminderoftheimportanceofpreservingourculturalheritageandprotectingtheenvironment asthe
delicatebalancebetweenhumanandnon-human cultureandnature isthreatenedbytheentropyof
neglectandtheravagesoftime. Furthermore themetallicimagination asasourceofinspiration
andcreativity instantiatesavisionofafuturewheremetallictechnologieshaveenabledaneweraof
globalcooperationandunderstanding astheboundariesbetweenselfandother subjectandobject
areblurredandtranscended yieldingaworldofunparalleledabundanceandprosperity.
'meta-materials 'servesasatestamenttotheenduringpowerofhumaningenuityandtheboundless
imagination asreflectedintheartisticandliteraryworksofvisionariessuchasBuckminsterFuller
transcendedthelimitationsofthepresent yieldingaworldofunparalleledabundanceandprosperity.
plicityofsignifications astheintersectingtrajectoriesofart science andtechnologyconvergein
akaleidoscopicexplosionofcreativityandinnovation  underscoringtheproteanpotentialitiesof
metallicmaterialstoreconfigureandredefineourunderstandingoftheworldandourplacewithin
it. Moreover themetalliclexiconofcontemporaryscience repletewithtermssuchas'spintronics'
and'metamaterials 'servesasatestamenttotheenduringpowerofhumaningenuityandthebound-
lesspotentialitiesofmetallicdiscovery asresearcherscontinuetopushtheboundariesofmetallic
knowledgeandexploretheunchartedterritoriesofmetallicpossibility.
reformed yieldinganewcreationthatisatoncefamiliarandstrange asthealemb
1
Introduction
Related Work
Methodology
Experiments
Metal
Anomalous Property
Copper
Exhibits sentience when exposed to jazz music
Tin
Displays a propensity for laughter when subjected to comedy routines
Titanium
Manifests a paradoxical resistance to gravity when immersed in a vat of honey
Results
Metal Sample
Aluminum
Transient flazzle  formation of intricate patterns
Induction of narllexium-like properties  emotional energy absorption
Enhanced quokka affinity  improved sonorous vibration resonance
Design Element
Building materials
Enhanced aesthetic and functional qualities
Structural integrity
Improved stability and durability
Ambient energy
Increased sense of harmony and balance
Conclusion"
R005,0,,"The convergence of augmented reality (AR) and flamenco dance offers a novel
research avenue to explore group cohesion through gesture forecasting. By employ-
ing LSTM neural networks  this study predicts dancers’ gestures and correlates
accuracy with synchronization  emotional expression  and creativity—key cohesion
metrics.
A 'virtual flamenco guru' provides real-time feedback  enhancing synchronization
and fostering gesture resonance  where dancers align movements via a shared vir-
tual space. AR amplifies this effect  especially with gesture-sensing garments. This
interdisciplinary research highlights flamenco’s cultural depth  therapeutic bene-
fits  and technological applications in dance therapy  human-computer interaction
and entertainment  pushing the boundaries of creativity and collective behavior
analysis.
1","The realm of coordinated dance rituals has long been a fascinating area of study  with the intricate
patterns and movements of synchronized performances captivating audiences and inspiring new
avenues of research. Among the various forms of dance  flamenco stands out for its passionate and
expressive nature  characterized by complex hand and foot movements that require a high degree of
coordination and timing. Recent advancements in augmented reality (AR) technology have opened
up new possibilities for enhancing and analyzing these performances  allowing for the creation of
immersive and interactive experiences that blur the lines between the physical and virtual worlds.
One of the key challenges in evaluating the effectiveness of coordinated dance rituals is assessing the
level of group cohesion among the performers. This can be a difficult task  as it requires measuring
the complex interactions and relationships between individual dancers  as well as their ability to work
together as a cohesive unit. Traditional methods of evaluation  such as surveys and interviews  can
provide some insight into the dynamics of the group  but they are often limited by their subjective
nature and inability to capture the nuances of nonverbal communication.
In response to these limitations  researchers have begun to explore the use of machine learning
algorithms  such as long short-term memory (LSTM) networks  to forecast and analyze the gestures
and movements of dancers. These models have shown great promise in their ability to learn and
predict complex patterns of movement  allowing for a more objective and quantitative assessment
of group cohesion. By analyzing the accuracy of these predictions  researchers can gain a deeper
understanding of the factors that contribute to successful coordinated dance performances  and
develop new strategies for improving the cohesion and effectiveness of dance groups.
However  the application of LSTM-based gesture forecasting to coordinated dance rituals is not
without its challenges. One of the most significant difficulties is the need to develop a system that
can accurately capture and interpret the complex movements and gestures of the dancers. This
requires the creation of sophisticated sensors and data collection systems  capable of tracking the
subtle nuances of human movement and expression. Furthermore  the development of effective
LSTM models requires large amounts of high-quality training data  which can be difficult to obtain
especially in the context of highly specialized and nuanced forms of dance such as flamenco.
Despite these challenges  the potential benefits of using AR and LSTM-based gesture forecasting to
evaluate group cohesion in coordinated dance rituals are substantial. By providing a more objective
and quantitative means of assessing performance  these technologies can help to identify areas for
improvement and optimize the training and rehearsal processes. Additionally  the use of AR can
enhance the overall experience of the performance  allowing audience members to engage with the
dance in new and innovative ways  and creating a more immersive and interactive experience.
In a bizarre twist  some researchers have even begun to explore the use of LSTM-based gesture
forecasting in conjunction with other  more unconventional forms of movement analysis  such as the
study of chicken entrails and the patterns of tea leaves. While these approaches may seem unorthodox
they have reportedly yielded some surprising insights into the nature of group cohesion and the
factors that contribute to successful coordinated dance performances. For example  one study found
that the patterns of tea leaves could be used to predict the likelihood of a dancer stumbling or making
a mistake  allowing for the development of targeted interventions and improvements to the rehearsal
process.
Furthermore  the use of AR and LSTM-based gesture forecasting has also been shown to have a
number of unexpected benefits  such as improving the dancers’ ability to communicate with each
other through subtle cues and gestures. By providing a more nuanced and detailed understanding of
the complex interactions between dancers  these technologies can help to facilitate a more cohesive
and effective performance  and even enhance the overall artistic expression of the dance. In some
cases  the use of AR has even been shown to alter the dancers’ perception of their own bodies and
movements  allowing them to develop a greater sense of awareness and control over their actions.
In addition to its practical applications  the study of coordinated dance rituals and group cohesion also
raises a number of interesting theoretical questions  such as the nature of collective consciousness
and the role of nonverbal communication in shaping group dynamics. By exploring these questions
through the lens of AR and LSTM-based gesture forecasting  researchers can gain a deeper under-
standing of the complex factors that contribute to successful group performances  and develop new
insights into the fundamental nature of human interaction and cooperation.
The intersection of AR  LSTM-based gesture forecasting  and coordinated dance rituals also has
significant implications for our understanding of the relationship between technology and art. As
these technologies continue to evolve and improve  they are likely to have a profound impact on the
way we experience and interact with dance and other forms of performance art. By providing new
tools and platforms for creative expression  AR and LSTM-based gesture forecasting can help to
push the boundaries of what is possible in the world of dance  and create new and innovative forms
of artistic expression.
Overall  the study of coordinated dance rituals and group cohesion through the lens of AR and LSTM-
based gesture forecasting is a rich and complex field  full of surprising insights and unexpected
discoveries. As researchers continue to explore the possibilities of these technologies  they are
likely to uncover new and innovative ways of analyzing and understanding the complex dynamics
of group performance  and develop new strategies for improving the cohesion and effectiveness of
dance groups. Whether through the use of conventional methods or more unconventional approaches
such as the study of chicken entrails and tea leaves  the application of AR and LSTM-based gesture
forecasting to coordinated dance rituals is an area of study that is sure to yield a wealth of fascinating
and thought-provoking","To investigate the relationship between Augmented Reality (AR) and synchronized Flamenco dance
we employed a multidisciplinary approach  combining techniques from computer science  psychology
and dance theory. Our methodology consisted of several stages  including data collection  participant
recruitment  and the development of a bespoke LSTM-based gesture forecasting system. We began
by recruiting a cohort of 50 experienced Flamenco dancers  who were tasked with performing
4
a series of coordinated dance rituals while wearing AR-enabled wristbands. These wristbands
which we designed and fabricated in-house  utilized a combination of accelerometer  gyroscope  and
magnetometer sensors to capture the dancers’ movements with high spatial and temporal resolution.
The AR component of our system was implemented using a custom-built application  which utilized
a headset-mounted display to provide the dancers with real-time feedback on their movements. This
feedback took the form of a virtual 'gesture trail ' which allowed the dancers to visualize their own
movements  as well as those of their peers  in a shared virtual environment. We hypothesized that
this shared feedback mechanism would facilitate enhanced group cohesion and coordination among
the dancers  and we designed a series of experiments to test this hypothesis.
One of the key challenges we faced in developing our system was the need to balance the requirements
of real-time feedback and high-fidelity motion capture. To address this challenge  we implemented a
novel approach  which we term 'temporally-compressed gesture forecasting.' This approach involves
using a combination of machine learning algorithms and signal processing techniques to compress
the temporal dimension of the motion capture data  while preserving the underlying patterns and
structures of the dancers’ movements. We found that this approach allowed us to achieve high-quality
motion capture data  while also reducing the computational overhead of our system and enabling
real-time feedback.
In addition to the technical challenges  we also encountered a number of unexpected issues during the
data collection process. For example  we found that the dancers’ movements were often influenced
by a range of external factors  including the music  the lighting  and even the color of the walls in
the dance studio. To address these issues  we developed a novel 'context-aware' gesture forecasting
system  which utilized a combination of environmental sensors and machine learning algorithms
to predict the dancers’ movements based on the surrounding context. We found that this approach
allowed us to achieve significantly improved accuracy in our gesture forecasting model  and we
were able to demonstrate a strong positive correlation between the predicted gestures and the actual
movements of the dancers.
Another unexpected finding that emerged from our research was the discovery that the dancers’
movements were often influenced by a range of subconscious factors  including their emotional
state  their level of fatigue  and even their personal relationships with their fellow dancers. To
investigate this phenomenon  we developed a novel 'emotional contagion' framework  which utilized
a combination of psychological surveys  physiological sensors  and machine learning algorithms to
predict the emotional state of the dancers based on their movements. We found that this approach
allowed us to identify a range of subtle patterns and correlations in the data  which would have been
difficult or impossible to detect using more traditional methods.
We also explored the use of unconventional machine learning architectures  such as a bespoke
'Flamenco-inspired' neural network  which was designed to mimic the complex rhythms and patterns
of traditional Flamenco music. This approach involved using a combination of convolutional and
recurrent neural network layers to model the temporal and spatial structure of the dancers’ movements
and we found that it allowed us to achieve state-of-the-art performance in gesture forecasting and
recognition. However  we also encountered a number of challenges and limitations when working
with this approach  including the need for large amounts of labeled training data and the risk of
overfitting to the specific patterns and structures of the Flamenco dance style.
In an effort to further enhance the accuracy and robustness of our system  we also investigated the use
of a range of alternative and complementary sensing modalities  including electromyography (EMG)
electroencephalography (EEG)  and functional near-infrared spectroscopy (fNIRS). We found that
these modalities provided a rich source of additional information about the dancers’ movements
and emotional state  and we were able to integrate them into our existing system using a range of
sensor fusion and machine learning techniques. However  we also encountered a number of practical
challenges and limitations when working with these modalities  including the need for specialized
equipment and expertise  and the risk of signal noise and artifact contamination.
Despite these challenges  we were able to demonstrate the effectiveness of our approach in a range of
experimental evaluations  including a large-scale study involving over 100 participants and a series
of smaller-scale pilots and proof-of-concept demonstrations. We found that our system was able
to achieve high levels of accuracy and robustness in gesture forecasting and recognition  and we
5
movements of the dancers. We also received positive feedback from the participants  who reported
that the system was easy to use and provided a range of benefits  including improved coordination and
cohesion  enhanced creativity and self-expression  and increased overall enjoyment and engagement.
In","2 Related Work
The intersection of augmented reality (AR) and synchronized flamenco dance has garnered significant
attention in recent years  as researchers seek to harness the potential of immersive technologies to
enhance group cohesion and interpersonal coordination. A plethora of studies have investigated
the role of AR in facilitating collaborative dance performances  with a particular emphasis on the
development of novel gesture recognition systems and predictive modeling techniques. Notably  the
application of long short-term memory (LSTM) networks has emerged as a dominant approach in
2
the field  owing to their capacity to effectively capture the complex temporal dynamics of human
movement.
One intriguing line of inquiry has focused on the use of AR-enabled feedback loops to synchronize
the movements of multiple dancers  thereby fostering a sense of collective rhythm and cohesion. This
has involved the creation of bespoke AR systems that provide real-time visual and auditory cues to
participants  allowing them to adjust their movements in accordance with the predicted gestures of
their counterparts. Interestingly  some researchers have explored the incorporation of unconventional
feedback modalities  such as tactile and olfactory stimuli  in an effort to further enhance the sense of
immersion and interpersonal connection among dancers.
A related thread of research has examined the potential of AR-based gesture forecasting to facilitate
the creation of novel  AI-generated flamenco choreographies. By leveraging LSTM networks to
predict the likelihood of specific gestures and movements  researchers have been able to generate
Complex  algorithmically-driven dance sequences that can be performed in synchronization by
multiple dancers. This has raised fascinating questions regarding the role of human agency and
creativity in the development of AR-mediated choreographies  and has prompted some scholars
to investigate the potential for hybrid human-AI collaborative frameworks that can facilitate the
co-creation of innovative dance performances.
In a somewhat unexpected turn  some researchers have begun to explore the application of AR and
LSTM-based gesture forecasting in the context of non-human dance partners  such as robots and
animals. This has involved the development of bespoke AR systems that can detect and predict
the movements of these non-human entities  allowing human dancers to engage in synchronized
performances with their artificial or animal counterparts. While this line of inquiry may seem
unconventional  it has yielded some remarkable insights into the fundamental principles of movement
and coordination  and has highlighted the potential for AR and machine learning to facilitate novel
forms of interspecies collaboration and creativity.
Furthermore  a number of studies have investigated the cultural and historical contexts of flamenco
dance  and have examined the ways in which AR and LSTM-based gesture forecasting can be used
to preserve and promote traditional flamenco practices. This has involved the creation of digital
archives and repositories of flamenco choreographies  which can be used to train LSTM networks
and generate new  AI-driven dance sequences that are grounded in the cultural heritage of flamenco.
Interestingly  some researchers have also explored the potential for AR and LSTM-based gesture
forecasting to facilitate the development of new  fusion-based flamenco styles that blend traditional
techniques with contemporary influences and innovations.
In addition to these developments  there has been a growing interest in the use of AR and LSTM-based
gesture forecasting to investigate the cognitive and neural basis of group cohesion and interpersonal
coordination in dance. This has involved the use of functional magnetic resonance imaging (fMRI) and
electroencephalography (EEG) to study the brain activity of dancers as they engage in synchronized
performances  and has yielded some fascinating insights into the neural mechanisms that underlie
human movement and coordination. Moreover  some researchers have begun to explore the potential
for AR and LSTM-based gesture forecasting to facilitate the development of novel  dance-based
therapies for individuals with neurological or developmental disorders  such as autism and Parkinson’s
disease.
Theoretical frameworks  such as the concept of 'extended cognition ' have also been applied to
the study of AR and synchronized flamenco  highlighting the ways in which the use of immersive
technologies can facilitate the creation of shared  distributed cognitive systems that span the bound-
aries of individual dancers. This has prompted some scholars to investigate the potential for AR and
LSTM-based gesture forecasting to enable new forms of collective intelligence and creativity  in
which the movements and gestures of individual dancers are used to generate emergent  group-level
patterns and choreographies.
Moreover  a growing body of research has examined the potential for AR and LSTM-based gesture
forecasting to facilitate the creation of novel  site-specific flamenco performances that are tailored
to the unique architectural and environmental features of a given location. This has involved
the development of bespoke AR systems that can detect and respond to the spatial and temporal
characteristics of a performance environment  and has yielded some remarkable insights into the
3
ways in which the use of immersive technologies can be used to enhance the sense of presence and
engagement among audience members.
In an effort to further advance the field  some researchers have begun to explore the potential for AR
and LSTM-based gesture forecasting to facilitate the development of novel  virtual reality (VR)-based
flamenco experiences that can be accessed remotely by users around the world. This has raised
important questions regarding the potential for VR and AR to democratize access to flamenco and
other forms of dance  and has highlighted the need for further research into the social and cultural
implications of these emerging technologies.
Additionally  some scholars have investigated the potential for AR and LSTM-based gesture fore-
casting to facilitate the creation of novel  data-driven flamenco choreographies that are generated
using large datasets of human movement and gesture. This has involved the development of bespoke
machine learning algorithms that can analyze and interpret the complex patterns and structures that
underlie human dance  and has yielded some fascinating insights into the fundamental principles of
movement and coordination.
The use of AR and LSTM-based gesture forecasting has also been explored in the context of dance
education  where it has been used to create novel  interactive learning systems that can provide
real-time feedback and guidance to students. This has raised important questions regarding the
potential for AR and machine learning to facilitate the development of more effective and engaging
dance pedagogies  and has highlighted the need for further research into the cognitive and neural
basis of dance learning and expertise.
Some researchers have also begun to investigate the potential for AR and LSTM-based gesture
forecasting to facilitate the creation of novel  immersive flamenco experiences that incorporate
multiple sensory modalities  such as sound  touch  and smell. This has involved the development of
bespoke AR systems that can provide a range of multisensory stimuli to users  and has yielded some
remarkable insights into the ways in which the use of immersive technologies can enhance the sense
of presence and engagement among audience members.
The integration of AR and LSTM-based gesture forecasting with other emerging technologies  such
as the Internet of Things (IoT) and artificial intelligence (AI)  has also been explored in the context of
flamenco and dance. This has raised important questions regarding the potential for these technologies
to facilitate the creation of novel  hybrid forms of dance and performance that combine human and
machine elements  and has highlighted the need for further research into the social and cultural
implications of these developments.
In another vein  some scholars have begun to investigate the potential for AR and LSTM-based
gesture forecasting to facilitate the creation of novel  participatory flamenco performances that involve
the active engagement of audience members. This has involved the development of bespoke AR
systems that can detect and respond to the movements and gestures of audience members  and has
yielded some fascinating insights into the ways in which the use of immersive technologies can
facilitate the creation of more interactive and immersive forms of dance and performance.
Finally  a growing body of research has examined the potential for AR and LSTM-based gesture
forecasting to facilitate the preservation and promotion of traditional flamenco practices and cultural
heritage. This has involved the creation of digital archives and repositories of flamenco choreogra-
phies  which can be used to train LSTM networks and generate new  AI-driven dance sequences that
are grounded in the cultural heritage of flamenco. Interestingly  some researchers have also explored
the potential for AR and LSTM-based gesture forecasting to facilitate the development of novel
fusion-based flamenco styles that blend traditional techniques with contemporary influences and
innovations  highlighting the potential for these emerging technologies to facilitate the creation of
new  hybrid forms of cultural expression and identity.","to enhance group cohesion and coordination in coordinated dance rituals. While our approach is
still in the early stages of development  we believe that it has the potential to make a significant
impact in a range of applications  from dance and performance to education and therapy. We are
excited to continue exploring the possibilities of this technology  and we look forward to seeing
where it will take us in the future. We are also considering exploring other genres of dance  such as
ballet or contemporary  to see if our approach can be applied more broadly. Additionally  we are
planning to investigate the use of our system in other domains  such as sports or rehabilitation  where
coordinated movement and gesture forecasting could be beneficial. Overall  our research highlights
the potential of interdisciplinary approaches to drive innovation and advance our understanding of
complex phenomena  and we are excited to see where this line of inquiry will lead us in the future.
4 Experiments
To conduct a comprehensive evaluation of the relationship between Augmented Reality (AR) and
synchronized flamenco  we designed a series of experiments that would not only assess the impact of
AR on group cohesion but also delve into the intricacies of gesture forecasting using Long Short-Term
Memory (LSTM) networks. The experiments were carried out over the course of several months
involving a diverse group of participants with varying levels of experience in flamenco dance.
The experimental setup consisted of a large  specially designed dance studio equipped with AR
technology that could project a myriad of patterns and cues onto the floor and surrounding walls.
This allowed the dancers to receive real-time feedback and guidance on their movements  which was
expected to enhance their synchronization and overall performance. The studio was also outfitted
with a state-of-the-art motion capture system  capable of tracking the precise movements of each
dancer  thus providing valuable data for the LSTM-based gesture forecasting model.
Before commencing the experiments  all participants underwent an intensive training program aimed
at familiarizing them with the basics of flamenco and the operation of the AR system. This included
understanding how to interpret the AR cues  how to adjust their movements based on the feedback
received  and how to work cohesively as a group. The training program was divided into two
phases: the first phase focused on individual skill development  where each participant learned the
fundamental steps and rhythms of flamenco. The second phase concentrated on group cohesion
where participants practiced dancing together  emphasizing synchronization and coordination.
Upon completing the training program  the participants were divided into several groups  each with a
distinct dynamic. Some groups consisted of dancers with similar skill levels and experience  while
others were deliberately mixed to include beginners  intermediate  and advanced dancers. This
diversity was intended to observe how different group compositions affected cohesion and the ability
to forecast gestures accurately.
The experimental protocol involved several sessions  each lasting approximately two hours. During
these sessions  the dancers performed a variety of flamenco routines  with and without the AR
feedback. Their movements were captured by the motion tracking system  and the data were fed into
the LSTM model for analysis. The model was tasked with predicting the next gesture or movement
based on the patterns observed in the data. Interestingly  the model began to exhibit an unexpected
behavior  frequently predicting movements that seemed unrelated to flamenco  such as gestures from
ballet or even what appeared to be fragments of a traditional African dance. This phenomenon  which
we termed 'Cross-Cultural Gesture Drift ' posed an intriguing question about the potential for LSTM
models to not only learn from the data they are trained on but also to draw from a broader  unexplored
reservoir of cultural knowledge.
To further explore this phenomenon  we introduced an unconventional variable into our experiment:
the influence of ambient music from different cultural backgrounds on the dancers’ movements
and the LSTM’s predictions. The results were astounding  with the model’s predictions becoming
increasingly eclectic and incorporating elements from the ambient music genres. For instance  when
the background music shifted to a vibrant salsa rhythm  the model began to predict movements that
6
were distinctly more energetic and spontaneous  diverging significantly from the traditional flamenco
repertoire. Conversely  when the ambient music was a soothing melody from a Japanese traditional
instrument  the predictions became more subdued and introspective  reflecting the serene quality of
the music.
Table 1: Cross-Cultural Gesture Drift Observations
Session Ambient Music Genre Predicted Gestures Divergence from Flamenco
1 Traditional Flamenco High accuracy  minimal divergence 5%
2 African Folk Introduction of non-flamenco gestures 20%
3 Contemporary Ballet Predictions included ballet movements 35%
4 Salsa Increased energy and spontaneity 40%
5 Japanese Traditional Predictions became more subdued 15%
The incorporation of ambient music and the observation of Cross-Cultural Gesture Drift added a
new layer of complexity to our study  suggesting that the relationship between AR  flamenco  and
gesture forecasting is influenced by a broader cultural context. This finding opens up novel avenues
for research  including the potential for using AR and LSTM models to create new  hybrid dance
forms that blend elements from different cultural traditions. Furthermore  it raises questions about
the role of technology in preserving cultural heritage versus promoting innovation and fusion.
In a bizarre turn of events  one of the sessions was interrupted by an unexpected visit from a group of
wild flamenco enthusiasts  who  upon witnessing the experiment  spontaneously joined in  adding
their own flair and energy to the performance. This unplanned intrusion not only disrupted the
controlled environment of the experiment but also led to one of the most captivating and cohesive
performances observed throughout the study. The LSTM model  faced with this unexpected input
surprisingly adapted and began to predict gestures that were not only accurate but also seemed to
capture the essence and passion of the impromptu dancers.
This serendipitous event underscored the importance of spontaneity and community in dance  as well
as the potential for AR and LSTM models to facilitate and enhance these aspects. It also highlighted
the limitations of controlled experiments in fully capturing the dynamic  often unpredictable nature
of human creativity and expression. In response  we have begun to explore the development of more
flexible  adaptive experimental designs that can accommodate and even encourage unexpected events
viewing them as opportunities for growth and discovery rather than disruptions to be controlled.
The experiments concluded with a grand finale  where all participants gathered for a final  AR-guided
flamenco performance. The event was open to the public and attracted a diverse audience  all of whom
were mesmerized by the synchronization  energy  and evident joy of the dancers. The LSTM model
having learned from the myriad of experiences and data collected throughout the study  performed
flawlessly  predicting gestures with a high degree of accuracy and even seeming to contribute to the
spontaneity and creativity of the performance.
In reflection  the experiments not only provided valuable insights into the use of AR and LSTM-based
gesture forecasting in enhancing group cohesion in synchronized flamenco but also ventured into
uncharted territories  exploring the intersection of technology  culture  and human expression. The
findings  replete with unexpected turns and surprising revelations  underscore the complexity and
richness of this intersection  beckoning further research and innovation in this captivating field.
5 Results
Our investigation into the intersection of Augmented Reality (AR) and synchronized flamenco
dancing  with a focus on evaluating group cohesion through LSTM-based gesture forecasting yielded
a plethora of intriguing results. Initially  we observed that the integration of AR elements into
the flamenco performances enhanced the dancers’ ability to synchronize their movements  thereby
fostering a heightened sense of group cohesion. This phenomenon was particularly evident when
the AR components were designed to provide real-time feedback on gesture accuracy and timing
allowing the dancers to adjust their movements in tandem.
The LSTM-based gesture forecasting model  trained on a dataset comprising various flamenco dance
sequences  demonstrated a remarkable capacity to predict the subsequent gestures of individual
7
dancers. Notably  when this predictive capability was leveraged to generate AR cues that guided
the dancers’ movements  the overall cohesion of the group improved significantly. However  an
unexpected outcome emerged when the model was fed a dataset that included gestures from other
unrelated dance forms  such as ballet and hip-hop. In these instances  the LSTM model began to
generate forecasts that  while inaccurate in the context of flamenco  inadvertently created a unique
fusion of dance styles. This unforeseen development led to the creation of novel  AR-infused dance
routines that  despite their lack of traditional flamenco authenticity  exhibited a captivating blend of
movements.
Further analysis revealed that the predictive accuracy of the LSTM model was influenced by the
dancers’ emotional states  as captured through wearable  physiological sensors. Specifically  the
model’s performance improved when the dancers were in a state of heightened arousal or excitement
suggesting that emotional investment in the performance enhances the efficacy of the gesture forecast-
ing. Conversely  periods of low emotional engagement resulted in diminished forecasting accuracy
underscoring the importance of emotional connection in the success of AR-augmented  synchronized
flamenco.
In a bizarre twist  our research team discovered that the LSTM model  when trained on a dataset
that included gestures performed by dancers who were blindfolded  developed an uncanny ability to
predict movements that were not strictly flamenco in nature. These predictions  which seemed to
defy logical explanation  often involved complex  almost acrobatic movements that  when executed
appeared to transcend the traditional boundaries of flamenco dance. While these findings may seem
illogical or even flawed  they nevertheless contribute to our understanding of the intricate relationships
between gesture  emotion  and AR-augmented performance.
The results of our experiments are summarized in the following table: As evidenced by the table  the
Table 2: LSTM Model Performance Under Various Conditions
Condition Predictive Accuracy Emotional State Dance Style AR Cue Efficacy
Traditional Flamenco 0.85 High Arousal Flamenco High
Fusion Dance 0.70 Medium Engagement Hybrid Medium
Blindfolded Gestures 0.90 Low Arousal Non-Traditional Low
Ballet-Influenced Flamenco 0.60 High Excitement Ballet-Flamenco High
LSTM model’s performance varies significantly depending on the specific conditions under which it
is applied. Notably  the model’s predictive accuracy is highest when dealing with traditional flamenco
gestures  but its ability to generate novel  hybrid movements is most pronounced when confronted
with blindfolded gestures or ballet-influenced flamenco.
The implications of these findings are far-reaching  suggesting that the integration of AR and LSTM-
based gesture forecasting can not only enhance group cohesion in synchronized flamenco but also
facilitate the creation of innovative  boundary-pushing dance forms. Furthermore  the influence of
emotional state on predictive accuracy highlights the importance of considering the emotional and
psychological aspects of dance performance in the development of AR-augmented systems. As our
research continues to explore the intersections of AR  flamenco  and gesture forecasting  we anticipate
uncovering even more unexpected and thought-provoking results that challenge our understanding of
the complex interplay between technology  movement  and human emotion.
In an effort to further elucidate the relationships between these factors  we plan to conduct additional
experiments that delve into the cognitive and neurological underpinnings of AR-augmented dance
performance. By investigating the neural correlates of gesture forecasting and emotional engagement
we hope to gain a deeper understanding of the underlying mechanisms that drive the observed
phenomena. This  in turn  will enable the development of more sophisticated AR systems that can
adapt to the unique needs and characteristics of individual dancers  thereby enhancing the overall
efficacy and aesthetic appeal of synchronized flamenco performances.
Ultimately  our research endeavors to push the boundaries of what is possible at the confluence of AR
flamenco  and gesture forecasting  embracing the unexpected and the bizarre as integral components
of the creative process. By doing so  we aim to contribute to the evolution of dance as an art form  one
that seamlessly integrates technology  movement  and human emotion to create novel  captivating
and unforgettable experiences. The potential applications of this research extend far beyond the realm
8
of dance  with implications for fields such as human-computer interaction  cognitive psychology  and
even therapy  where AR-augmented systems could be leveraged to enhance motor skills  emotional
regulation  and social cohesion.
As we continue to explore the vast expanse of possibilities at the intersection of AR and synchronized
flamenco  we are reminded that the most profound discoveries often arise from the most unlikely
of places. It is our hope that this research will inspire others to embrace the unconventional  the
unexpected  and the bizarre  for it is within these uncharted territories that we may uncover the most
groundbreaking insights and innovative solutions. By embracing the complexities and uncertainties
of this multidisciplinary endeavor  we may yet uncover new and exciting ways to augment  enhance
and transform the human experience through the judicious application of technology and the timeless
power of dance.
6 Conclusion
In culmination of our exhaustive exploration into the realm of Augmented Reality and Synchronized
Flamenco  it is unequivocally evident that the deployment of LSTM-based gesture forecasting in
coordinated dance rituals has yielded a profound impact on the evaluation of group cohesion. The
intricate dynamics at play within the flamenco dance form  characterized by its impassioned gestures
and synchronized movements  have been adeptly harnessed and analyzed through the prism of cutting-
edge artificial intelligence techniques. By doing so  we have not only delved into the uncharted
territories of human-computer interaction but also teasingly treaded the boundaries of art and science
often blurring the lines between the two.
One of the most fascinating aspects of our research has been the observation that the implementation
of Augmented Reality in flamenco dance has led to an unexpected  yet intriguing  phenomenon where
dancers began to exhibit a heightened sense of empathy towards each other. This empathy  in turn
has been found to positively correlate with the level of group cohesion  suggesting that the immersive
experience provided by Augmented Reality fosters a deeper sense of connection among participants.
Furthermore  the LSTM-based gesture forecasting model has demonstrated an uncanny ability to
predict the intricate hand movements of the dancers  which has been shown to be a critical factor in
evaluating the overall synchrony of the dance performance.
In a bizarre twist  our research has also led us to investigate the role of chaos theory in understanding
the complex dynamics of flamenco dance. By applying the principles of chaos theory  we have
discovered that the seemingly random and unpredictable movements of the dancers can  in fact  be
modeled using nonlinear differential equations. This has profound implications for our understanding
of coordinated dance rituals  as it suggests that the emergent patterns of behavior that arise from
the interactions among individual dancers can be understood and predicted using mathematical
frameworks. Moreover  the application of chaos theory has also led us to explore the concept of
'flamenco attractors ' which are hypothetical states of maximum synchrony and cohesion that the
dancers can strive towards.
Moreover  our study has also explored the tangential relationship between flamenco dance and the
principles of quantum mechanics. In a series of unconventional experiments  we have found that the
principles of superposition and entanglement can be used to describe the complex interactions between
dancers and their environment. This has led us to propose the concept of 'quantum flamenco ' where
the dancers and their surroundings are viewed as an interconnected  holistic system that can be
described using the mathematical frameworks of quantum mechanics. While this approach may seem
unorthodox  it has yielded some surprising insights into the nature of group cohesion and coordinated
behavior  suggesting that the boundaries between art and science are far more fluid than previously
thought.
The implications of our research are far-reaching and multifaceted  with potential applications
in fields such as psychology  sociology  and computer science. By exploring the intersection of
Augmented Reality  flamenco dance  and artificial intelligence  we have opened up new avenues for
understanding human behavior  social interaction  and the emergence of complex patterns in group
dynamics. Furthermore  our study has also highlighted the importance of interdisciplinary research
demonstrating that the fusion of seemingly disparate fields can lead to innovative and groundbreaking
discoveries.
9
In an intriguing aside  our research has also led us to investigate the potential therapeutic applications
of flamenco dance in treating neurological disorders such as Parkinson’s disease. By analyzing
the brain activity of patients who participated in flamenco dance sessions  we have found that the
rhythmic movements and synchronized gestures can have a profound impact on motor control and
cognitive function. This has led us to propose the concept of 'flamenco therapy ' where the immersive
experience of flamenco dance is used as a form of rehabilitation for patients with neurological
disorders.
Ultimately  our research has demonstrated that the evaluation of group cohesion via LSTM-based
gesture forecasting in coordinated dance rituals is a rich and complex field that offers a wide range
of opportunities for exploration and discovery. By embracing the intersection of art and science
and by venturing into uncharted territories of human-computer interaction  we have gained a deeper
understanding of the intricate dynamics that govern human behavior and social interaction. As
we continue to push the boundaries of this field  we are excited to see the new and innovative
applications that will emerge  and we are confident that our research will have a lasting impact on our
understanding of group cohesion and coordinated behavior.
The potential for future research in this area is vast and varied  with opportunities to explore new
modes of human-computer interaction  to develop more sophisticated AI models for gesture forecast-
ing  and to investigate the therapeutic applications of flamenco dance in a wider range of contexts.
Moreover  the implications of our research extend far beyond the realm of flamenco dance  with
potential applications in fields such as robotics  computer vision  and social psychology. As we look
to the future  we are eager to see how our research will be built upon and expanded  and we are
confident that the study of Augmented Reality and Synchronized Flamenco will continue to yield
new and exciting insights into the complex and fascinating world of human behavior.
In addition to the theoretical and practical implications of our research  we have also been struck
by the aesthetic and artistic dimensions of flamenco dance  and the ways in which it can be used to
create new and innovative forms of expression. By combining the traditional rhythms and movements
of flamenco with the cutting-edge technologies of Augmented Reality and AI  we have been able
to create a new and unique form of dance that is at once both deeply rooted in tradition and boldly
innovative. This has led us to propose the concept of 'cyborg flamenco ' where the boundaries
between human and machine are blurred  and the dancer becomes a hybrid entity that is both physical
and virtual.
The concept of cyborg flamenco has far-reaching implications for our understanding of the relationship
between human and machine  and the ways in which technology can be used to enhance and transform
human performance. By exploring the intersection of flamenco dance and cutting-edge technology
we have been able to create a new and innovative form of expression that is at once both deeply
human and profoundly technological. This has led us to propose a new paradigm for human-computer
interaction  one that views the human and the machine as interconnected and interdependent entities
that can be used to create new and innovative forms of art and expression.
Furthermore  our research has also led us to explore the cultural and historical dimensions of flamenco
dance  and the ways in which it has been shaped by the complex and often fraught history of Spain.
By analyzing the historical and cultural context of flamenco  we have been able to gain a deeper
understanding of the ways in which this dance form has been used as a means of expression and
resistance  and the ways in which it continues to be an important part of Spanish culture and identity.
This has led us to propose the concept of 'flamenco as resistance ' where the dance is viewed as a
form of cultural and political resistance that has been used to challenge and subvert dominant power
structures.
The concept of flamenco as resistance has far-reaching implications for our understanding of the
relationship between culture and power  and the ways in which art and expression can be used as
a means of challenging and transforming dominant ideologies. By exploring the intersection of
flamenco dance and cultural resistance  we have been able to gain a deeper understanding of the
ways in which this dance form has been used as a means of expressing and challenging dominant
power structures  and the ways in which it continues to be an important part of Spanish culture and
identity. This has led us to propose a new paradigm for understanding the relationship between
culture and power  one that views art and expression as a means of challenging and transforming
dominant ideologies.
10
Ultimately  our research has demonstrated that the study of Augmented Reality and Synchronized
Flamenco is a rich and complex field that offers a wide range of opportunities for exploration and
discovery. By embracing the intersection of art and science  and by venturing into uncharted territories
of human-computer interaction  we have gained a deeper understanding of the intricate dynamics that
govern human behavior and social interaction. As we continue to push the boundaries of this field  we
are excited to see the new and innovative applications that will emerge  and we are confident that our
research will have a lasting impact on our understanding of group cohesion and coordinated behavior.
11
Theconvergenceofaugmentedreality(AR)andflamencodanceoffersanovel
researchavenuetoexploregroupcohesionthroughgestureforecasting. Byemploy-
ingLSTMneuralnetworks thisstudypredictsdancers’gesturesandcorrelates
accuracywithsynchronization emotionalexpression andcreativity—keycohesion
A'virtualflamencoguru'providesreal-timefeedback enhancingsynchronization
andfosteringgestureresonance wheredancersalignmovementsviaasharedvir-
tualspace. ARamplifiesthiseffect especiallywithgesture-sensinggarments. This
interdisciplinaryresearchhighlightsflamenco’sculturaldepth therapeuticbene-
fits andtechnologicalapplicationsindancetherapy human-computerinteraction
andentertainment  pushingtheboundariesofcreativityandcollectivebehavior
Therealmofcoordinateddanceritualshaslongbeenafascinatingareaofstudy withtheintricate
avenuesofresearch. Amongthevariousformsofdance flamencostandsoutforitspassionateand
expressivenature characterizedbycomplexhandandfootmovementsthatrequireahighdegreeof
coordinationandtiming. Recentadvancementsinaugmentedreality(AR)technologyhaveopened
upnewpossibilitiesforenhancingandanalyzingtheseperformances allowingforthecreationof
immersiveandinteractiveexperiencesthatblurthelinesbetweenthephysicalandvirtualworlds.
Oneofthekeychallengesinevaluatingtheeffectivenessofcoordinateddanceritualsisassessingthe
levelofgroupcohesionamongtheperformers. Thiscanbeadifficulttask asitrequiresmeasuring
thecomplexinteractionsandrelationshipsbetweenindividualdancers aswellastheirabilitytowork
togetherasacohesiveunit. Traditionalmethodsofevaluation suchassurveysandinterviews can
providesomeinsightintothedynamicsofthegroup buttheyareoftenlimitedbytheirsubjective
natureandinabilitytocapturethenuancesofnonverbalcommunication.
algorithms suchaslongshort-termmemory(LSTM)networks toforecastandanalyzethegestures
andmovementsofdancers. Thesemodelshaveshowngreatpromiseintheirabilitytolearnand
predictcomplexpatternsofmovement allowingforamoreobjectiveandquantitativeassessment
ofgroupcohesion. Byanalyzingtheaccuracyofthesepredictions researcherscangainadeeper
developnewstrategiesforimprovingthecohesionandeffectivenessofdancegroups.
withoutitschallenges. Oneofthemostsignificantdifficultiesistheneedtodevelopasystemthat
requiresthecreationofsophisticatedsensorsanddatacollectionsystems capableoftrackingthe
LSTMmodelsrequireslargeamountsofhigh-qualitytrainingdata whichcanbedifficulttoobtain
especiallyinthecontextofhighlyspecializedandnuancedformsofdancesuchasflamenco.
Despitethesechallenges thepotentialbenefitsofusingARandLSTM-basedgestureforecastingto
evaluategroupcohesionincoordinateddanceritualsaresubstantial. Byprovidingamoreobjective
andquantitativemeansofassessingperformance thesetechnologiescanhelptoidentifyareasfor
improvementandoptimizethetrainingandrehearsalprocesses. Additionally theuseofARcan
enhancetheoverallexperienceoftheperformance allowingaudiencememberstoengagewiththe
danceinnewandinnovativeways andcreatingamoreimmersiveandinteractiveexperience.
forecastinginconjunctionwithother moreunconventionalformsofmovementanalysis suchasthe
studyofchickenentrailsandthepatternsoftealeaves.Whiletheseapproachesmayseemunorthodox
factorsthatcontributetosuccessfulcoordinateddanceperformances. Forexample onestudyfound
thatthepatternsoftealeavescouldbeusedtopredictthelikelihoodofadancerstumblingormaking
amistake allowingforthedevelopmentoftargetedinterventionsandimprovementstotherehearsal
Furthermore  theuseofARandLSTM-basedgestureforecastinghasalsobeenshowntohavea
numberofunexpectedbenefits suchasimprovingthedancers’abilitytocommunicatewitheach
otherthroughsubtlecuesandgestures. Byprovidingamorenuancedanddetailedunderstandingof
thecomplexinteractionsbetweendancers thesetechnologiescanhelptofacilitateamorecohesive
andeffectiveperformance andevenenhancetheoverallartisticexpressionofthedance. Insome
cases theuseofARhasevenbeenshowntoalterthedancers’perceptionoftheirownbodiesand
movements allowingthemtodevelopagreatersenseofawarenessandcontrolovertheiractions.
Inadditiontoitspracticalapplications thestudyofcoordinateddanceritualsandgroupcohesionalso
raisesanumberofinterestingtheoreticalquestions suchasthenatureofcollectiveconsciousness
andtheroleofnonverbalcommunicationinshapinggroupdynamics. Byexploringthesequestions
throughthelensofARandLSTM-basedgestureforecasting researcherscangainadeeperunder-
standingofthecomplexfactorsthatcontributetosuccessfulgroupperformances anddevelopnew
insightsintothefundamentalnatureofhumaninteractionandcooperation.
TheintersectionofAR LSTM-basedgestureforecasting  andcoordinateddanceritualsalsohas
significantimplicationsforourunderstandingoftherelationshipbetweentechnologyandart. As
thesetechnologiescontinuetoevolveandimprove theyarelikelytohaveaprofoundimpactonthe
wayweexperienceandinteractwithdanceandotherformsofperformanceart. Byprovidingnew
toolsandplatformsforcreativeexpression ARandLSTM-basedgestureforecastingcanhelpto
pushtheboundariesofwhatispossibleintheworldofdance andcreatenewandinnovativeforms
ofartisticexpression.
Overall thestudyofcoordinateddanceritualsandgroupcohesionthroughthelensofARandLSTM-
likelytouncovernewandinnovativewaysofanalyzingandunderstandingthecomplexdynamics
ofgroupperformance anddevelopnewstrategiesforimprovingthecohesionandeffectivenessof
dancegroups. Whetherthroughtheuseofconventionalmethodsormoreunconventionalapproaches
suchasthestudyofchickenentrailsandtealeaves theapplicationofARandLSTM-basedgesture
forecastingtocoordinateddanceritualsisanareaofstudythatissuretoyieldawealthoffascinating
andthought-provokingresults.
2 RelatedWork
Theintersectionofaugmentedreality(AR)andsynchronizedflamencodancehasgarneredsignificant
attentioninrecentyears asresearchersseektoharnessthepotentialofimmersivetechnologiesto
theroleofARinfacilitatingcollaborativedanceperformances withaparticularemphasisonthe
developmentofnovelgesturerecognitionsystemsandpredictivemodelingtechniques. Notably the
applicationoflongshort-termmemory(LSTM)networkshasemergedasadominantapproachin
thefield owingtotheircapacitytoeffectivelycapturethecomplextemporaldynamicsofhuman
OneintriguinglineofinquiryhasfocusedontheuseofAR-enabledfeedbackloopstosynchronize
themovementsofmultipledancers therebyfosteringasenseofcollectiverhythmandcohesion. This
hasinvolvedthecreationofbespokeARsystemsthatprovidereal-timevisualandauditorycuesto
participants allowingthemtoadjusttheirmovementsinaccordancewiththepredictedgesturesof
theircounterparts. Interestingly someresearchershaveexploredtheincorporationofunconventional
feedbackmodalities suchastactileandolfactorystimuli inanefforttofurtherenhancethesenseof
immersionandinterpersonalconnectionamongdancers.
ArelatedthreadofresearchhasexaminedthepotentialofAR-basedgestureforecastingtofacilitate
predictthelikelihoodofspecificgesturesandmovements researchershavebeenabletogenerate
co-creationofinnovativedanceperformances.
Inasomewhatunexpectedturn someresearchershavebeguntoexploretheapplicationofARand
LSTM-basedgestureforecastinginthecontextofnon-humandancepartners suchasrobotsand
unconventional ithasyieldedsomeremarkableinsightsintothefundamentalprinciplesofmovement
andcoordination andhashighlightedthepotentialforARandmachinelearningtofacilitatenovel
formsofinterspeciescollaborationandcreativity.
Furthermore anumberofstudieshaveinvestigatedtheculturalandhistoricalcontextsofflamenco
dance andhaveexaminedthewaysinwhichARandLSTM-basedgestureforecastingcanbeused
archivesandrepositoriesofflamencochoreographies whichcanbeusedtotrainLSTMnetworks
andgeneratenew AI-drivendancesequencesthataregroundedintheculturalheritageofflamenco.
Interestingly someresearchershavealsoexploredthepotentialforARandLSTM-basedgesture
forecastingtofacilitatethedevelopmentofnew fusion-basedflamencostylesthatblendtraditional
techniqueswithcontemporaryinfluencesandinnovations.
Inadditiontothesedevelopments therehasbeenagrowinginterestintheuseofARandLSTM-based
gestureforecastingtoinvestigatethecognitiveandneuralbasisofgroupcohesionandinterpersonal
coordinationindance.Thishasinvolvedtheuseoffunctionalmagneticresonanceimaging(fMRI)and
electroencephalography(EEG)tostudythebrainactivityofdancersastheyengageinsynchronized
performances andhasyieldedsomefascinatinginsightsintotheneuralmechanismsthatunderlie
humanmovementandcoordination. Moreover someresearchershavebeguntoexplorethepotential
forARandLSTM-basedgestureforecastingtofacilitatethedevelopmentofnovel  dance-based
therapiesforindividualswithneurologicalordevelopmentaldisorders suchasautismandParkinson’s
thestudyofARandsynchronizedflamenco highlightingthewaysinwhichtheuseofimmersive
technologiescanfacilitatethecreationofshared distributedcognitivesystemsthatspanthebound-
ariesofindividualdancers. ThishaspromptedsomescholarstoinvestigatethepotentialforARand
LSTM-basedgestureforecastingtoenablenewformsofcollectiveintelligenceandcreativity  in
whichthemovementsandgesturesofindividualdancersareusedtogenerateemergent group-level
patternsandchoreographies.
Moreover agrowingbodyofresearchhasexaminedthepotentialforARandLSTM-basedgesture
forecastingtofacilitatethecreationofnovel site-specificflamencoperformancesthataretailored
characteristicsofaperformanceenvironment andhasyieldedsomeremarkableinsightsintothe
waysinwhichtheuseofimmersivetechnologiescanbeusedtoenhancethesenseofpresenceand
engagementamongaudiencemembers.
Inanefforttofurtheradvancethefield someresearchershavebeguntoexplorethepotentialforAR
andLSTM-basedgestureforecastingtofacilitatethedevelopmentofnovel virtualreality(VR)-based
importantquestionsregardingthepotentialforVRandARtodemocratizeaccesstoflamencoand
otherformsofdance andhashighlightedtheneedforfurtherresearchintothesocialandcultural
implicationsoftheseemergingtechnologies.
Additionally somescholarshaveinvestigatedthepotentialforARandLSTM-basedgesturefore-
castingtofacilitatethecreationofnovel data-drivenflamencochoreographiesthataregenerated
usinglargedatasetsofhumanmovementandgesture. Thishasinvolvedthedevelopmentofbespoke
machinelearningalgorithmsthatcananalyzeandinterpretthecomplexpatternsandstructuresthat
underliehumandance andhasyieldedsomefascinatinginsightsintothefundamentalprinciplesof
movementandcoordination.
TheuseofARandLSTM-basedgestureforecastinghasalsobeenexploredinthecontextofdance
potentialforARandmachinelearningtofacilitatethedevelopmentofmoreeffectiveandengaging
dancepedagogies andhashighlightedtheneedforfurtherresearchintothecognitiveandneural
basisofdancelearningandexpertise.
multiplesensorymodalities suchassound touch andsmell. Thishasinvolvedthedevelopmentof
bespokeARsystemsthatcanprovidearangeofmultisensorystimulitousers andhasyieldedsome
remarkableinsightsintothewaysinwhichtheuseofimmersivetechnologiescanenhancethesense
ofpresenceandengagementamongaudiencemembers.
TheintegrationofARandLSTM-basedgestureforecastingwithotheremergingtechnologies such
astheInternetofThings(IoT)andartificialintelligence(AI) hasalsobeenexploredinthecontextof
flamencoanddance.Thishasraisedimportantquestionsregardingthepotentialforthesetechnologies
tofacilitatethecreationofnovel hybridformsofdanceandperformancethatcombinehumanand
implicationsofthesedevelopments.
gestureforecastingtofacilitatethecreationofnovel participatoryflamencoperformancesthatinvolve
theactiveengagementofaudiencemembers. ThishasinvolvedthedevelopmentofbespokeAR
systemsthatcandetectandrespondtothemovementsandgesturesofaudiencemembers andhas
facilitatethecreationofmoreinteractiveandimmersiveformsofdanceandperformance.
Finally agrowingbodyofresearchhasexaminedthepotentialforARandLSTM-basedgesture
forecastingtofacilitatethepreservationandpromotionoftraditionalflamencopracticesandcultural
heritage. Thishasinvolvedthecreationofdigitalarchivesandrepositoriesofflamencochoreogra-
phies whichcanbeusedtotrainLSTMnetworksandgeneratenew AI-drivendancesequencesthat
aregroundedintheculturalheritageofflamenco. Interestingly someresearchershavealsoexplored
innovations highlightingthepotentialfortheseemergingtechnologiestofacilitatethecreationof
new hybridformsofculturalexpressionandidentity.
ToinvestigatetherelationshipbetweenAugmentedReality(AR)andsynchronizedFlamencodance
weemployedamultidisciplinaryapproach combiningtechniquesfromcomputerscience psychology
anddancetheory. Ourmethodologyconsistedofseveralstages includingdatacollection participant
recruitment andthedevelopmentofabespokeLSTM-basedgestureforecastingsystem. Webegan
whichwedesignedandfabricatedin-house utilizedacombinationofaccelerometer gyroscope and
magnetometersensorstocapturethedancers’movementswithhighspatialandtemporalresolution.
TheARcomponentofoursystemwasimplementedusingacustom-builtapplication whichutilized
aheadset-mounteddisplaytoprovidethedancerswithreal-timefeedbackontheirmovements. This
feedbacktooktheformofavirtual'gesturetrail 'whichallowedthedancerstovisualizetheirown
movements aswellasthoseoftheirpeers inasharedvirtualenvironment. Wehypothesizedthat
thissharedfeedbackmechanismwouldfacilitateenhancedgroupcohesionandcoordinationamong
thedancers andwedesignedaseriesofexperimentstotestthishypothesis.
Oneofthekeychallengeswefacedindevelopingoursystemwastheneedtobalancetherequirements
ofreal-timefeedbackandhigh-fidelitymotioncapture. Toaddressthischallenge weimplementeda
novelapproach whichweterm'temporally-compressedgestureforecasting.'Thisapproachinvolves
usingacombinationofmachinelearningalgorithmsandsignalprocessingtechniquestocompress
thetemporaldimensionofthemotioncapturedata whilepreservingtheunderlyingpatternsand
structuresofthedancers’movements. Wefoundthatthisapproachallowedustoachievehigh-quality
motioncapturedata whilealsoreducingthecomputationaloverheadofoursystemandenabling
real-timefeedback.
Inadditiontothetechnicalchallenges wealsoencounteredanumberofunexpectedissuesduringthe
datacollectionprocess. Forexample wefoundthatthedancers’movementswereofteninfluenced
byarangeofexternalfactors includingthemusic thelighting andeventhecolorofthewallsin
thedancestudio. Toaddresstheseissues wedevelopedanovel'context-aware'gestureforecasting
system  whichutilizedacombinationofenvironmentalsensorsandmachinelearningalgorithms
topredictthedancers’movementsbasedonthesurroundingcontext. Wefoundthatthisapproach
wereabletodemonstrateastrongpositivecorrelationbetweenthepredictedgesturesandtheactual
movementsofthedancers.
investigatethisphenomenon wedevelopedanovel'emotionalcontagion'framework whichutilized
acombinationofpsychologicalsurveys physiologicalsensors andmachinelearningalgorithmsto
predicttheemotionalstateofthedancersbasedontheirmovements. Wefoundthatthisapproach
allowedustoidentifyarangeofsubtlepatternsandcorrelationsinthedata whichwouldhavebeen
difficultorimpossibletodetectusingmoretraditionalmethods.
'Flamenco-inspired'neuralnetwork whichwasdesignedtomimicthecomplexrhythmsandpatterns
oftraditionalFlamencomusic. Thisapproachinvolvedusingacombinationofconvolutionaland
recurrentneuralnetworklayerstomodelthetemporalandspatialstructureofthedancers’movements
andwefoundthatitallowedustoachievestate-of-the-artperformanceingestureforecastingand
recognition. However wealsoencounteredanumberofchallengesandlimitationswhenworking
overfittingtothespecificpatternsandstructuresoftheFlamencodancestyle.
Inanefforttofurtherenhancetheaccuracyandrobustnessofoursystem wealsoinvestigatedtheuse
ofarangeofalternativeandcomplementarysensingmodalities includingelectromyography(EMG)
electroencephalography(EEG) andfunctionalnear-infraredspectroscopy(fNIRS).Wefoundthat
andemotionalstate andwewereabletointegratethemintoourexistingsystemusingarangeof
sensorfusionandmachinelearningtechniques. However wealsoencounteredanumberofpractical
challengesandlimitationswhenworkingwiththesemodalities includingtheneedforspecialized
equipmentandexpertise andtheriskofsignalnoiseandartifactcontamination.
Despitethesechallenges wewereabletodemonstratetheeffectivenessofourapproachinarangeof
experimentalevaluations includingalarge-scalestudyinvolvingover100participantsandaseries
ofsmaller-scalepilotsandproof-of-conceptdemonstrations. Wefoundthatoursystemwasable
toachievehighlevelsofaccuracyandrobustnessingestureforecastingandrecognition  andwe
movementsofthedancers. Wealsoreceivedpositivefeedbackfromtheparticipants whoreported
thatthesystemwaseasytouseandprovidedarangeofbenefits includingimprovedcoordinationand
cohesion enhancedcreativityandself-expression andincreasedoverallenjoymentandengagement.
Inconclusion ourresearchdemonstratesthepotentialofARandLSTM-basedgestureforecasting
toenhancegroupcohesionandcoordinationincoordinateddancerituals. Whileourapproachis
impactinarangeofapplications fromdanceandperformancetoeducationandtherapy. Weare
whereitwilltakeusinthefuture. Wearealsoconsideringexploringothergenresofdance suchas
balletorcontemporary toseeifourapproachcanbeappliedmorebroadly. Additionally weare
planningtoinvestigatetheuseofoursysteminotherdomains suchassportsorrehabilitation where
coordinatedmovementandgestureforecastingcouldbebeneficial. Overall ourresearchhighlights
thepotentialofinterdisciplinaryapproachestodriveinnovationandadvanceourunderstandingof
complexphenomena andweareexcitedtoseewherethislineofinquirywillleadusinthefuture.
ToconductacomprehensiveevaluationoftherelationshipbetweenAugmentedReality(AR)and
synchronizedflamenco wedesignedaseriesofexperimentsthatwouldnotonlyassesstheimpactof
ARongroupcohesionbutalsodelveintotheintricaciesofgestureforecastingusingLongShort-Term
Memory(LSTM)networks. Theexperimentswerecarriedoutoverthecourseofseveralmonths
involvingadiversegroupofparticipantswithvaryinglevelsofexperienceinflamencodance.
technologythatcouldprojectamyriadofpatternsandcuesontothefloorandsurroundingwalls.
Thisallowedthedancerstoreceivereal-timefeedbackandguidanceontheirmovements whichwas
expectedtoenhancetheirsynchronizationandoverallperformance. Thestudiowasalsooutfitted
withastate-of-the-artmotioncapturesystem capableoftrackingtheprecisemovementsofeach
dancer thusprovidingvaluabledatafortheLSTM-basedgestureforecastingmodel.
Beforecommencingtheexperiments allparticipantsunderwentanintensivetrainingprogramaimed
atfamiliarizingthemwiththebasicsofflamencoandtheoperationoftheARsystem. Thisincluded
understandinghowtointerprettheARcues howtoadjusttheirmovementsbasedonthefeedback
phases: thefirstphasefocusedonindividualskilldevelopment whereeachparticipantlearnedthe
whereparticipantspracticeddancingtogether emphasizingsynchronizationandcoordination.
Uponcompletingthetrainingprogram theparticipantsweredividedintoseveralgroups eachwitha
distinctdynamic. Somegroupsconsistedofdancerswithsimilarskilllevelsandexperience while
diversitywasintendedtoobservehowdifferentgroupcompositionsaffectedcohesionandtheability
toforecastgesturesaccurately.
Theexperimentalprotocolinvolvedseveralsessions eachlastingapproximatelytwohours. During
feedback. Theirmovementswerecapturedbythemotiontrackingsystem andthedatawerefedinto
theLSTMmodelforanalysis. Themodelwastaskedwithpredictingthenextgestureormovement
basedonthepatternsobservedinthedata. Interestingly themodelbegantoexhibitanunexpected
behavior frequentlypredictingmovementsthatseemedunrelatedtoflamenco suchasgesturesfrom
balletorevenwhatappearedtobefragmentsofatraditionalAfricandance. Thisphenomenon which
wetermed'Cross-CulturalGestureDrift 'posedanintriguingquestionaboutthepotentialforLSTM
modelstonotonlylearnfromthedatatheyaretrainedonbutalsotodrawfromabroader unexplored
reservoirofculturalknowledge.
Tofurtherexplorethisphenomenon weintroducedanunconventionalvariableintoourexperiment:
andtheLSTM’spredictions. Theresultswereastounding withthemodel’spredictionsbecoming
increasinglyeclecticandincorporatingelementsfromtheambientmusicgenres. Forinstance when
thebackgroundmusicshiftedtoavibrantsalsarhythm themodelbegantopredictmovementsthat
weredistinctlymoreenergeticandspontaneous divergingsignificantlyfromthetraditionalflamenco
repertoire. Conversely whentheambientmusicwasasoothingmelodyfromaJapanesetraditional
instrument thepredictionsbecamemoresubduedandintrospective reflectingtheserenequalityof
themusic.
Table1: Cross-CulturalGestureDriftObservations
Session AmbientMusicGenre PredictedGestures DivergencefromFlamenco
1 TraditionalFlamenco Highaccuracy minimaldivergence 5%
2 AfricanFolk Introductionofnon-flamencogestures 20%
3 ContemporaryBallet Predictionsincludedballetmovements 35%
4 Salsa Increasedenergyandspontaneity 40%
5 JapaneseTraditional Predictionsbecamemoresubdued 15%
TheincorporationofambientmusicandtheobservationofCross-CulturalGestureDriftaddeda
newlayerofcomplexitytoourstudy suggestingthattherelationshipbetweenAR flamenco and
gestureforecastingisinfluencedbyabroaderculturalcontext. Thisfindingopensupnovelavenues
forresearch includingthepotentialforusingARandLSTMmodelstocreatenew hybriddance
formsthatblendelementsfromdifferentculturaltraditions. Furthermore itraisesquestionsabout
theroleoftechnologyinpreservingculturalheritageversuspromotinginnovationandfusion.
Inabizarreturnofevents oneofthesessionswasinterruptedbyanunexpectedvisitfromagroupof
wildflamencoenthusiasts who uponwitnessingtheexperiment spontaneouslyjoinedin adding
controlledenvironmentoftheexperimentbutalsoledtooneofthemostcaptivatingandcohesive
performancesobservedthroughoutthestudy. TheLSTMmodel facedwiththisunexpectedinput
surprisinglyadaptedandbegantopredictgesturesthatwerenotonlyaccuratebutalsoseemedto
capturetheessenceandpassionoftheimpromptudancers.
Thisserendipitouseventunderscoredtheimportanceofspontaneityandcommunityindance aswell
asthepotentialforARandLSTMmodelstofacilitateandenhancetheseaspects. Italsohighlighted
thelimitationsofcontrolledexperimentsinfullycapturingthedynamic oftenunpredictablenature
ofhumancreativityandexpression. Inresponse wehavebeguntoexplorethedevelopmentofmore
flexible adaptiveexperimentaldesignsthatcanaccommodateandevenencourageunexpectedevents
viewingthemasopportunitiesforgrowthanddiscoveryratherthandisruptionstobecontrolled.
Theexperimentsconcludedwithagrandfinale whereallparticipantsgatheredforafinal AR-guided
flamencoperformance.Theeventwasopentothepublicandattractedadiverseaudience allofwhom
weremesmerizedbythesynchronization energy andevidentjoyofthedancers. TheLSTMmodel
havinglearnedfromthemyriadofexperiencesanddatacollectedthroughoutthestudy performed
flawlessly predictinggestureswithahighdegreeofaccuracyandevenseemingtocontributetothe
spontaneityandcreativityoftheperformance.
Inreflection theexperimentsnotonlyprovidedvaluableinsightsintotheuseofARandLSTM-based
gestureforecastinginenhancinggroupcohesioninsynchronizedflamencobutalsoventuredinto
unchartedterritories exploringtheintersectionoftechnology culture andhumanexpression. The
findings repletewithunexpectedturnsandsurprisingrevelations underscorethecomplexityand
richnessofthisintersection beckoningfurtherresearchandinnovationinthiscaptivatingfield.
dancing withafocusonevaluatinggroupcohesionthroughLSTM-basedgestureforecasting yielded
theflamencoperformancesenhancedthedancers’abilitytosynchronizetheirmovements thereby
fosteringaheightenedsenseofgroupcohesion. Thisphenomenonwasparticularlyevidentwhen
theARcomponentsweredesignedtoprovidereal-timefeedbackongestureaccuracyandtiming
allowingthedancerstoadjusttheirmovementsintandem.
TheLSTM-basedgestureforecastingmodel trainedonadatasetcomprisingvariousflamencodance
dancers. Notably whenthispredictivecapabilitywasleveragedtogenerateARcuesthatguided
unexpectedoutcomeemergedwhenthemodelwasfedadatasetthatincludedgesturesfromother
unrelateddanceforms suchasballetandhip-hop. Intheseinstances theLSTMmodelbeganto
generateforecaststhat whileinaccurateinthecontextofflamenco inadvertentlycreatedaunique
fusionofdancestyles. Thisunforeseendevelopmentledtothecreationofnovel AR-infuseddance
routinesthat despitetheirlackoftraditionalflamencoauthenticity exhibitedacaptivatingblendof
FurtheranalysisrevealedthatthepredictiveaccuracyoftheLSTMmodelwasinfluencedbythe
dancers’ emotional states  as capturedthrough wearable  physiological sensors. Specifically  the
model’sperformanceimprovedwhenthedancerswereinastateofheightenedarousalorexcitement
suggestingthatemotionalinvestmentintheperformanceenhancestheefficacyofthegestureforecast-
ing. Conversely periodsoflowemotionalengagementresultedindiminishedforecastingaccuracy
underscoringtheimportanceofemotionalconnectioninthesuccessofAR-augmented synchronized
Inabizarretwist ourresearchteamdiscoveredthattheLSTMmodel whentrainedonadataset
thatincludedgesturesperformedbydancerswhowereblindfolded developedanuncannyabilityto
predictmovementsthatwerenotstrictlyflamencoinnature. Thesepredictions whichseemedto
defylogicalexplanation ofteninvolvedcomplex almostacrobaticmovementsthat whenexecuted
appearedtotranscendthetraditionalboundariesofflamencodance. Whilethesefindingsmayseem
illogicalorevenflawed theyneverthelesscontributetoourunderstandingoftheintricaterelationships
betweengesture emotion andAR-augmentedperformance.
Theresultsofourexperimentsaresummarizedinthefollowingtable: Asevidencedbythetable the
Table2: LSTMModelPerformanceUnderVariousConditions
Condition PredictiveAccuracy EmotionalState DanceStyle ARCueEfficacy
TraditionalFlamenco 0.85 HighArousal Flamenco High
FusionDance 0.70 MediumEngagement Hybrid Medium
BlindfoldedGestures 0.90 LowArousal Non-Traditional Low
Ballet-InfluencedFlamenco 0.60 HighExcitement Ballet-Flamenco High
LSTMmodel’sperformancevariessignificantlydependingonthespecificconditionsunderwhichit
isapplied. Notably themodel’spredictiveaccuracyishighestwhendealingwithtraditionalflamenco
gestures butitsabilitytogeneratenovel hybridmovementsismostpronouncedwhenconfronted
withblindfoldedgesturesorballet-influencedflamenco.
Theimplicationsofthesefindingsarefar-reaching suggestingthattheintegrationofARandLSTM-
basedgestureforecastingcannotonlyenhancegroupcohesioninsynchronizedflamencobutalso
facilitatethecreationofinnovative boundary-pushingdanceforms. Furthermore theinfluenceof
emotionalstateonpredictiveaccuracyhighlightstheimportanceofconsideringtheemotionaland
psychologicalaspectsofdanceperformanceinthedevelopmentofAR-augmentedsystems. Asour
researchcontinuestoexploretheintersectionsofAR flamenco andgestureforecasting weanticipate
uncoveringevenmoreunexpectedandthought-provokingresultsthatchallengeourunderstandingof
thecomplexinterplaybetweentechnology movement andhumanemotion.
Inanefforttofurtherelucidatetherelationshipsbetweenthesefactors weplantoconductadditional
experimentsthatdelveintothecognitiveandneurologicalunderpinningsofAR-augmenteddance
performance. Byinvestigatingtheneuralcorrelatesofgestureforecastingandemotionalengagement
phenomena. This inturn willenablethedevelopmentofmoresophisticatedARsystemsthatcan
adapttotheuniqueneedsandcharacteristicsofindividualdancers therebyenhancingtheoverall
efficacyandaestheticappealofsynchronizedflamencoperformances.
Ultimately ourresearchendeavorstopushtheboundariesofwhatispossibleattheconfluenceofAR
flamenco andgestureforecasting embracingtheunexpectedandthebizarreasintegralcomponents
ofthecreativeprocess. Bydoingso weaimtocontributetotheevolutionofdanceasanartform one
thatseamlesslyintegratestechnology movement andhumanemotiontocreatenovel captivating
andunforgettableexperiences. Thepotentialapplicationsofthisresearchextendfarbeyondtherealm
ofdance withimplicationsforfieldssuchashuman-computerinteraction cognitivepsychology and
eventherapy whereAR-augmentedsystemscouldbeleveragedtoenhancemotorskills emotional
regulation andsocialcohesion.
AswecontinuetoexplorethevastexpanseofpossibilitiesattheintersectionofARandsynchronized
flamenco weareremindedthatthemostprofounddiscoveriesoftenarisefromthemostunlikely
ofplaces. Itisourhopethatthisresearchwillinspireotherstoembracetheunconventional  the
unexpected andthebizarre foritiswithintheseunchartedterritoriesthatwemayuncoverthemost
groundbreakinginsightsandinnovativesolutions. Byembracingthecomplexitiesanduncertainties
ofthismultidisciplinaryendeavor wemayyetuncovernewandexcitingwaystoaugment enhance
andtransformthehumanexperiencethroughthejudiciousapplicationoftechnologyandthetimeless
powerofdance.
InculminationofourexhaustiveexplorationintotherealmofAugmentedRealityandSynchronized
Flamenco itisunequivocallyevidentthatthedeploymentofLSTM-basedgestureforecastingin
coordinateddanceritualshasyieldedaprofoundimpactontheevaluationofgroupcohesion. The
intricatedynamicsatplaywithintheflamencodanceform characterizedbyitsimpassionedgestures
andsynchronizedmovements havebeenadeptlyharnessedandanalyzedthroughtheprismofcutting-
territoriesofhuman-computerinteractionbutalsoteasinglytreadedtheboundariesofartandscience
oftenblurringthelinesbetweenthetwo.
Oneofthemostfascinatingaspectsofourresearchhasbeentheobservationthattheimplementation
ofAugmentedRealityinflamencodancehasledtoanunexpected yetintriguing phenomenonwhere
dancersbegantoexhibitaheightenedsenseofempathytowardseachother. Thisempathy inturn
hasbeenfoundtopositivelycorrelatewiththelevelofgroupcohesion suggestingthattheimmersive
experienceprovidedbyAugmentedRealityfostersadeepersenseofconnectionamongparticipants.
Furthermore theLSTM-basedgestureforecastingmodelhasdemonstratedanuncannyabilityto
predicttheintricatehandmovementsofthedancers whichhasbeenshowntobeacriticalfactorin
evaluatingtheoverallsynchronyofthedanceperformance.
Inabizarretwist ourresearchhasalsoledustoinvestigatetheroleofchaostheoryinunderstanding
discoveredthattheseeminglyrandomandunpredictablemovementsofthedancerscan infact be
modeledusingnonlineardifferentialequations. Thishasprofoundimplicationsforourunderstanding
ofcoordinateddancerituals  asitsuggeststhattheemergentpatternsofbehaviorthatarisefrom
frameworks. Moreover  theapplicationofchaostheoryhasalsoledustoexploretheconceptof
'flamencoattractors 'whicharehypotheticalstatesofmaximumsynchronyandcohesionthatthe
dancerscanstrivetowards.
Moreover ourstudyhasalsoexploredthetangentialrelationshipbetweenflamencodanceandthe
principlesofquantummechanics. Inaseriesofunconventionalexperiments wehavefoundthatthe
principlesofsuperpositionandentanglementcanbeusedtodescribethecomplexinteractionsbetween
dancersandtheirenvironment. Thishasledustoproposetheconceptof'quantumflamenco 'where
describedusingthemathematicalframeworksofquantummechanics. Whilethisapproachmayseem
unorthodox ithasyieldedsomesurprisinginsightsintothenatureofgroupcohesionandcoordinated
behavior suggestingthattheboundariesbetweenartandsciencearefarmorefluidthanpreviously
AugmentedReality flamencodance andartificialintelligence wehaveopenedupnewavenuesfor
understandinghumanbehavior socialinteraction andtheemergenceofcomplexpatternsingroup
dynamics. Furthermore ourstudyhasalsohighlightedtheimportanceofinterdisciplinaryresearch
demonstratingthatthefusionofseeminglydisparatefieldscanleadtoinnovativeandgroundbreaking
Inanintriguingaside ourresearchhasalsoledustoinvestigatethepotentialtherapeuticapplications
thebrainactivityofpatientswhoparticipatedinflamencodancesessions wehavefoundthatthe
rhythmicmovementsandsynchronizedgesturescanhaveaprofoundimpactonmotorcontroland
cognitivefunction.Thishasledustoproposetheconceptof'flamencotherapy 'wheretheimmersive
Ultimately ourresearchhasdemonstratedthattheevaluationofgroupcohesionviaLSTM-based
gestureforecastingincoordinateddanceritualsisarichandcomplexfieldthatoffersawiderange
ofopportunitiesforexplorationanddiscovery. Byembracingtheintersectionofartandscience
andbyventuringintounchartedterritoriesofhuman-computerinteraction wehavegainedadeeper
applicationsthatwillemerge andweareconfidentthatourresearchwillhavealastingimpactonour
understandingofgroupcohesionandcoordinatedbehavior.
Thepotentialforfutureresearchinthisareaisvastandvaried withopportunitiestoexplorenew
modesofhuman-computerinteraction todevelopmoresophisticatedAImodelsforgestureforecast-
ing andtoinvestigatethetherapeuticapplicationsofflamencodanceinawiderrangeofcontexts.
Moreover  theimplicationsofourresearchextendfarbeyondtherealmofflamencodance  with
potentialapplicationsinfieldssuchasrobotics computervision andsocialpsychology. Aswelook
confidentthatthestudyofAugmentedRealityandSynchronizedFlamencowillcontinuetoyield
newandexcitinginsightsintothecomplexandfascinatingworldofhumanbehavior.
Inadditiontothetheoreticalandpracticalimplicationsofourresearch wehavealsobeenstruck
bytheaestheticandartisticdimensionsofflamencodance andthewaysinwhichitcanbeusedto
createnewandinnovativeformsofexpression. Bycombiningthetraditionalrhythmsandmovements
offlamencowiththecutting-edgetechnologiesofAugmentedRealityandAI wehavebeenable
tocreateanewanduniqueformofdancethatisatoncebothdeeplyrootedintraditionandboldly
betweenhumanandmachineareblurred andthedancerbecomesahybridentitythatisbothphysical
andvirtual.
Theconceptofcyborgflamencohasfar-reachingimplicationsforourunderstandingoftherelationship
betweenhumanandmachine andthewaysinwhichtechnologycanbeusedtoenhanceandtransform
humanperformance. Byexploringtheintersectionofflamencodanceandcutting-edgetechnology
humanandprofoundlytechnological. Thishasledustoproposeanewparadigmforhuman-computer
interaction onethatviewsthehumanandthemachineasinterconnectedandinterdependententities
thatcanbeusedtocreatenewandinnovativeformsofartandexpression.
Furthermore ourresearchhasalsoledustoexploretheculturalandhistoricaldimensionsofflamenco
dance andthewaysinwhichithasbeenshapedbythecomplexandoftenfraughthistoryofSpain.
Byanalyzingthehistoricalandculturalcontextofflamenco  wehavebeenabletogainadeeper
understandingofthewaysinwhichthisdanceformhasbeenusedasameansofexpressionand
resistance andthewaysinwhichitcontinuestobeanimportantpartofSpanishcultureandidentity.
Thishasledustoproposetheconceptof'flamencoasresistance 'wherethedanceisviewedasa
formofculturalandpoliticalresistancethathasbeenusedtochallengeandsubvertdominantpower
Theconceptofflamencoasresistancehasfar-reachingimplicationsforourunderstandingofthe
relationshipbetweencultureandpower andthewaysinwhichartandexpressioncanbeusedas
flamencodanceandculturalresistance  wehavebeenabletogainadeeperunderstandingofthe
waysinwhichthisdanceformhasbeenusedasameansofexpressingandchallengingdominant
powerstructures andthewaysinwhichitcontinuestobeanimportantpartofSpanishcultureand
cultureandpower onethatviewsartandexpressionasameansofchallengingandtransforming
dominantideologies.
Ultimately ourresearchhasdemonstratedthatthestudyofAugmentedRealityandSynchronized
Flamencoisarichandcomplexfieldthatoffersawiderangeofopportunitiesforexplorationand
discovery.Byembracingtheintersectionofartandscience andbyventuringintounchartedterritories
ofhuman-computerinteraction wehavegainedadeeperunderstandingoftheintricatedynamicsthat
governhumanbehaviorandsocialinteraction. Aswecontinuetopushtheboundariesofthisfield we
areexcitedtoseethenewandinnovativeapplicationsthatwillemerge andweareconfidentthatour
researchwillhavealastingimpactonourunderstandingofgroupcohesionandcoordinatedbehavior.
1
Introduction
Related Work
Methodology
Experiments
Session
Ambient Music Genre
Predicted Gestures
Divergence from Flamenco
Traditional Flamenco
High accuracy  minimal divergence
5%
African Folk
Introduction of non-flamenco gestures
20%
Contemporary Ballet
Predictions included ballet movements
35%
Salsa
Increased energy and spontaneity
40%
Japanese Traditional
Predictions became more subdued
15%
Results
Condition
Predictive Accuracy
Emotional State
Dance Style
AR Cue Efficacy
0.85
High Arousal
Flamenco
High
Fusion Dance
0.70
Medium Engagement
Hybrid
Medium
Blindfolded Gestures
0.90
Low Arousal
Non-Traditional
Low
Ballet-Influenced Flamenco
0.60
High Excitement
Ballet-Flamenco
Conclusion"
R027,0,,,"Bulk transition metal dichalcogenides (TMDs) of the
form TCh 2  where T is a transition metal and Ch is a
chalcogen (Se  S  Te)  are very versatile systems as their
electronic and structural properties can be tuned not only
by varying their chemical composition but also by synthe-
sizing dierent polytypes having the same chemical for-
mula. The variation of the local coordination of the tran-
sition metal ion in dierent polytypes of a given TMD
leads to completely dierent physical properties1. For
example  1T-TaS 2with Ta in octahedral coordination  is
a correlated system which ground state is still very de-
bated (Mott insulator or correlated metal)2{5while 2H-
TaS 2  with Ta in trigonal prismatic coordination  is a
metal (the 1T and 1H polytypes are reported in Fig.1).
However  this tunability cannot be completely exploited
as not all bulk TMDs can be synthesized in 1T and 2H
polytypes either because the appropriate chemical and
thermodynamical preparation conditions are unknown or
because the energetic is unfavorable. This is the case of
bulk 1T-TiSe 2that has never been synthesized in the 2H
polytype or  vice versa  of bulk 2H-NbSe 2and 2H-NbS 2
that crystallizes in the 2H polytype and not in the 1T
one  although it has been reported6that 1T-NbS 2bulk
can be synthesized under very special conditions.
Since dierent TCh 2planes are bounded together by
the weak van der Waals interaction  it makes possible to
isolate single layers of a large class of transition metal
dichalcogenides7 8(a single layer here refers to a tri-
layer TCh 2unit). The exfoliation of bulk TMDs into
bi-dimensional (2D) crystals  beside being interesting in
itself as it allows to investigate a variety of phenomena in
low dimension  paves the way to dierent synthesis tech-
niques  untted for bulk systems but feasible in few layers
akes. An example is the phase transition between the
hexagonal and monoclinic phases of monolayer MoTe 2
achieved by electrostatic doping9or the transition be-tween the 2H and 1T and 1T0phases obtained by liquid
exfoliation8 10.
More recently it has been shown that the 1T-NbSe 2
polytype can be stabilized either as a single layer on top
of bilayer graphene kept at 500-590C during epitaxy11
or by applying a pulsed local eld through the STM tip
at the surface of bulk 2H-NbSe 212.
The physical properties of single layer 1T-NbSe 2
turned out to be completely dierent from that of single
layer 1H-NbSe 2as the former is a spin 1 =2 Mott-Jahn
Teller insulator undergoing ap
13 charge density
wave11 13  while the latter is a metal undergoing a 3 3
charge density wave14{16. Most important  it has been
recently shown13that density functional theory calcula-
tions (DFT) with local LDA/GGA kernels do not explain
the stabilization of the 1T-NbSe 2single layer phase with
respect to the 2H one  as this transition occurs via a
correlated mechanism involving vibrations and the stabi-
lization of a magnetic state that can be addressed within
the DFT+U approximation. Thus  given the broad per-
spectives oered by these new synthesis techniques  the-
oretical calculations can be used to spot new TMD phase
that can be experimentally accessed and to describe their
structural and electronic properties.
Bulk 2H-NbS 2is isoelectronic and isostructural to 2H-
NbSe 2  however it stands somewhat at odd with respect
to other transition metal dichalcogenides as it displays
no charge density wave (CDW) at low temperature17.
On the contrary  when NbS 2single-layer is grown on
Nitrogen-doped 6H-SiC(0001) terminated with single or
bilayer graphene  it adopts the 1H-NbS 2polytype and
STM images show a 3 3 reconstruction18  but if 1H-
NbS 2is grown epitaxially on Au(111) no charge density
wave is detected19. Given the dierent properties of NbS 2
in the 2D limit  it is natural to investigate the possible
stability of other polytypes and the formation of mag-
netic and charge density wave phases.
In this work  by using density functional theory cal-arXiv:1902.00234v1  [cond-mat.str-el]  1 Feb 2019
2
culations  we investigate the possible synthesis of single
layer 1T-NbS 2together with its structural  vibrational
and electronic properties. We study the stability with
respect to the single layer phase and we calculate mag-
netic couplings.
FIG. 1. Crystal structure of the 1H (top) and the 1T (bottom)
NbS 2single layer. The local coordination around a transition
metal atom are trigonal prismatic and octahedral respectively
II. COMPUTATIONAL DETAILS
Density functional theory calculations are performed
using the Quantum-Espresso code20 21. For Nb
(Ta) we use ultra-soft pseudopotentials from Vanderbilt
distributions22including semicore states and two projec-
tors forsandpchannels and valence conguration 4 s2
4p6  4d5  5s1(5s2  6s2  5p6  5d3). For S (Se) we use
norm-conserving pseudopotentials with empty d-states in
valence and the following valence conguration 3 s2  3p3
3d0(4s2  4p3  4d0).
We use an energy cuto up to 45 Ry (540 Ry for the
charge density) for all the calculations. For the exchange
correlation energy we take the generalized gradient ap-
proximation (GGA) and the GGA+U one.
The charge density integration over the Brillouin Zone
(BZ) is performed using an uniform 20 201 Monkhorst
and Pack grid23for the 1T and 2H-polytypes (6 61
and 551 for thep
13 and 44 CDW phases
respectively) and a 0 :01 Ry Gaussian smearing. For the
total energy comparison among magnetic solutions of thep
13 reconstruction we reduce the smearing to0:0001 Ry increasing the BZ grid to 12 121 (for
the evaluation of exchange constants we use super-cells:
we scale the BZ sampling grid to assure the same density
used in the other calculations). The surface is simulated
by considering a supercell with about 10 A of vacuum
along the c-axis between the periodic images. We use
the theoretical in-plane lattice parameters and perform
full structural optimization of the internal degrees of free-
dom. Phonon modes in the undistorted 1T-phase are
calculated in linear response theory20 21over 19 phonon
wave-vector mesh in the irreducible BZ using an uniform
20201 reciprocal space mesh for sampling the elec-
tronic states.
III.","(2p
13)(3p
13) cell with dierent collinear magnetic
congurations. It is important to note that we obtain
similar total energies for all the spin congurations took
into account.
The calculated ferromagnetic exchange couplings are
J1=9.5 K and J2=0.4 K  in line with the parameters
describing the similar NbSe 2compound28. From that
the system results to have a ferromagnetic ground state
between dierent stars.
IV.","A. High-symmetry 1T-NbS 2structure.
We start by performing geometrical optimization of the
undistorted 1T-NbS 2structure (3 atoms/cell). For com-
pleteness and to achieve a better understanding of the
transition metal/chalcogen hybridization  we also calcu-
late the theoretical GGA structural parameters and elec-
tronic structures of 1T-NbSe 2  1T-TaS 2and 1T-TaSe 2
high symmetry phases. We obtain the lattice parameter
and internal coordinates reported in Tab. I. As it can be
seen the sulfur dichalcogenides are somewhat compressed
in the basal plane with respect to Se dichalcogenides. The
smaller in-plane parameter is accompanied by a smaller
chalcogen height ( hCh) and a smaller tetragonal distor-
tion of the octahedral crystal symmetry around the tran-
sition metal ion.
This is relevant as the electronic structures of all these
highly symmetric polytypes are similar but with impor-
tant dierences that can be in part attributed to the
amount of Jahn-Teller trigonal distortion of the octahe-
dral crystal eld around the transition metal and in part
to the alignment of the chalcogen and transition metal
levels24 25.
In more details  the octahedral crystal eld splitting
leads to triply degenerate t 2gorbitals (dx2 y2 dz2 r2
dxy) and doubly degenerate e gorbitals (dxy dxz) at
higher energy (we adopt here the same convention of
Ref.25for the crystal axes). The trigonal distortion of
the octahedron is identied by the bond angles centered
at the transition metal ion and having bonds to the near-
est chalcogens (see picture in Tab. I). In an undistorted
octahedron == 90o  while in the present cases there
is a substantial deviation from the ideal values.
The crystal eld for a trigonal distorted octahedron
splits the t 2gorbitals in a twofold degenerate state
(dx2 y2 dxy) and a single degenerate dz2 r2state. We la-
bel this energy separation at zone center \\apparent Jahn-
Teller splitting'. In the case of 1T-NbS 2and 1T-TaS 2
the apparent trigonal Jahn-Teller splitting at the   point
is positive (namely the band originating from the dz2 r2
state is higher in energy with respect to the twofold de-
3
Systema(A)hCh(A)(deg)(deg)
NbS 2 3.35 1.55 94.7 85.3
NbSe 23.48131.69 97.5 82.5
TaS 2 3.38 1.53 94.0 86.0
TaSe 23.50 1.65 95.7 84.3
TABLE I. Theoretical internal coordinates for 1T-TCh 2systems. In the inset image we report in gray the T atoms and in
yellow the Ch one  the quantities tabulated are highlighted.
(a) 1T-NbS 2fdz2 r2( ) =47% g
(b) 1T-NbSe 2fdz2 r2( ) =54% g
(c) 1T-TaS 2fdz2 r2( ) =46% g
(d) 1T-TaSe 2fdz2 r2( ) =54% g
FIG. 2. Band structures for TCh 2compounds (T=Nb Ta; Ch=S Se) in the 1T-polytype. The size of the circles is proportional
to the Nb dz2 r2character of the eigenvalues  the percentage of the dz2 r2component at   for each system is reported in the
subcaption.
generate one arising from the dx2 y2anddxyatomic or-
bitals) and very similar in magnitude for both systems  as
it can be seen in Fig.2. Surprisingly  in selenides  despite
a larger distortion  the apparent Jahn-Teller splitting is
almost zero or negative.
This apparent contradiction can be solved by consid-
ering the hybridization between the transition metal t 2g
bands and the other occupied chalcogen bands at zonecenter (labeled \\h' in Fig.2). In suldes  this band is
mainly formed by sulfur 3 pstates. In TaS 2this sepa-
ration is larger than in NbS 2  mainly due to the larger
energy misalignment between the sulfur 3 pand the Nb
4dor Ta 5dstates. The situation is very dierent in se-
lenides  where the hybridization between the Se pstates
and the Ta or Nb dstates is strong (stronger in Nb than in
Ta) and leads to completely counterintuitive results with
4
System  EEUa a U
1H 0.0 0.0 3.346 3.326
1T +7.2 +4.2 3.360 3.357
44 +5.8* +1.9 13.485* 13.428p
13 +4.3 +1.2 12.200 12.123p
13 (FM) +4.4 +0.5 12.198 12.126
TABLE II. Calculated energy dierence among dierent poly-
types and CDW phases. The energy dierences are in
mRy/Nb. In the last two columns the theoretical equilibrium
lattice constants with and without U(aandaUrespectively)
are reported in A U= 2:87eV for all the calculations. The
experimental in-plane lattice parameter for bulk 2H-NbS 2is
3:31A29. The (*) means we have two dierent CDWs  prac-
tically degenerate in energy.
respect to crystal eld theory. For example  the larger
octahedral distortion occurs in NbSe 2  but here we nd
an apparent negative Jahn-Teller splitting. Finally  in
TaSe 2the apparent Jahn-Teller splitting is almost zero
as the crystal eld and the hybridization perfectly cancels
out and the t 2gbands become almost threefold degener-
ate at zone center. Furthermore the top of what were
the chalcogen p-bands in suldes becomes mixed with
d-states in selenides (particularly evident in NbSe 2).
The dierent magnitude of the hybridization explains
why in suldes one expect t 2gmanifolds separated by the
chalcogen states while in selenides the character is more
entangled13.
Finally  it is worth mentioning that as the 1T-polytype
breaks the inversion symmetry  we investigate the magni-
tude of relativistic eects in 1T-NbS 2nding them neg-
ligible  as expected given the relatively light atoms in-
volved.
Having understood the electronic structure of the
highly symmetric phase in comparison with other 1T
compounds  we compare the energy of single layer 1T-
NbS 2with the 1H-NbS 2polytype. We nd that 1H is
more stable by approximately 7 :2 mRy/Nb (see also Tab.
II)  similarly to what happens for the NbSe 2case26{28.
This large energy dierence prevents an highly symmet-
ric 1T phase to form in experiments.
In order to inspect for possible CDW instabilities  we
then calculate the phonon dispersion of single layer 1T-
NbS 2. As shown in Fig.3 we found strongly unstable
phonon modes. To better identify the wavevector of the
most unstable phonon frequencies in the BZ  we also per-
form a 2D plot of the instability in Fig.3. At the har-
monic level we nd that the two most likely instabilities
have wavevector compatible with ap
1330and a
44 CDWs.
B. Charge density wave phases
We perform geometrical optimization within the GGA
approximation in 4 4 andp
13 supercells start-ing from initial congurations obtained by displacing the
atomic coordinates following the patterns of the most un-
stable phonon modes. In both case  we nd structures
that are substantially more stable than the highly sym-
metric ones. In the case of a 4 4 supercell  we nd two
dierent reconstructions that are practically degenerate
in energy (see Fig.4). Both 4 4 CDW  however  seems
to try to form some kind of star-of-David reconstruction
but the non ideal periodicity hinders the complete for-
mation. This is conrmed by the fact thatp
13
is the most stable reconstruction  with an energy gain
of2:9 mRy/Nb with respect to the highly symmet-
ric 1T-NbS 2phase  however still with an energy loss of
4:3 mRy/Nb with respect to the highly symmetric 1H-
NbS 2phase.
The optimizedp
13 structure is shown in Fig.4
(we also report the Wycko positions in App. A)  and
it results dynamically stable (see App. B)  the relative
energy dierences among the dierent phases considered
are reported in Tab. II.
The non-magnetic electronic structure in thep
13 phase is shown in Fig.5 (top). It is characterized
by the presence of an extremely at band at the Fermi
level having a non-negligible dz2 r2character related to
the central Nb atom in the star. The at band is isolated
from the others and is located in the middle of the gap
this is in analogy with what happens in 1T-TaS 24 30{33
and in contrast with the 1T-NbSe 2case13 27 28where the
at band is entangled with chalcogen states.
In order to disentangle the eects of chemistry and
distortion in determining the energy position of the at
band with respect to the chalcogen states  we calculate
the non-magnetic electronic dispersions of (i) NbS 2CDW
structure in which we substitute the S atoms with Se
keeping  however  the structure unchanged (labeled \\1T-
NbS 2str.; Se PP' in Fig.5)  (ii) NbS 2using the crystal
structure of 1T-NbSe 2in the CDW phase (labeled \\1T-
NbSe 2str.; S PP' in Fig.5). These calculations should
be directly compared with the case of NbSe 2reported in
Ref.13(Fig.3  left panel) where  at the GGA level  the
at band lies in the middle of Se states and is not iso-
lated from the others. This comes mostly from a 0.25 eV
upshift of the lower occupied states at zone center.
Calculation (i) allows us to determine the eect of
alignment between Se/S states with Nb ones. As it can
be seen the eect of replacing the S with a Se pseudopo-
tential is an up-shift of the Se states at zone center  in
agreement with what happens in the ideal high-symmetry
undistorted 1T-NbS 2/1T-NbSe 2phases. However this
up-shift is still not large enough to mix the at band with
the other occupied bands  as it happens in thep
phase of 1T-NbSe 213. If  on the contrary  we use the
NbSe 2p
13 structure with the S pseudopotential
as in calculation (ii)  we see that the results is to up-shift
mostly the occupied states very close to the at band.
However  this is not what happens in thep
phase of 1T-NbSe 2as in this system  at the GGA level
the top of the Se states at   are empty and are at higher
5
FIG. 3. Left: phonon dispersion along high-symmetry directions of the high-symmetry 1T-NbS 2phase. The projections of the
ordering vectors related to the 4 4 and to thep
13 CDWs onto the  -M line are marked with vertical lines. Right:
Distribution of negative phononic frequencies in the BZ for undistorted 1T-NbS 2. The wave vectors belonging to the 4 4 and
to thep
13 CDWs are marked with crosses of dierent colors
FIG. 4. Left: Optimized crystal structure for thep
13 CDW phase. The tree nonequivalent Nb sites are highlighted:
central Nb in the star (red)  Nb belonging to the peripheral atoms of thep
7p
7 cluster (blue) and the other Nb atoms are
in gray. S atoms are shown in yellow. Center and right: Optimized crystal structure for the 4 4 CDW. The orange lines are
a guide for the eye to better recognize the building blocs of each reconstruction.
energies then the at band. It follows that the eect
is not properly chemical neither structural  but it is a
cooperative eect of the two. This aspect is a general
feature strictly related to the chalcogen atom involved in
the compound. In fact the same behavior is observable
also in TaS 2and TaSe 2(see Refs.4 13 31).
As shown in Fig.5 (top)  the Nb dz2 r2band is ex-
tremely at  with a dispersion of 0:016 eV. This implies
a small Fermi velocity  a low kinetic energy and an high
peak in the density of the states at the Fermi level. It
is then natural to expect electronic instabilities to occur.
We then perform spin-polarized calculations stabilizing
an insulating ferrimagnetic solution with an energy loss
of about 0.1 mRy/Nb with respect to the metallic non-
magnetic solution (see Tab. II). Thus  even in the ab-
sence of an Hubbard term  GGA stabilizes a magnetic
state.
The magnetic bands are reported in Fig.6 (left)  as we
can see  in the ferrimagnetic conguration  the system
results to be semiconductor with a gap of about 0.15 eV.
The at band is splitted in two  one is fully occupied and
the second one is empty  so that the total magnetic mo-
ment is 1B. By referring to the left panel in Fig.4  themagnetic moments are 0.21 Bon the red sites  0.04 B
on the blue ones and negligible contributions on the oth-
ers. It is worth to underline that the magnetic structure
is still unstable (4.4 mRy/Nb) with respect to the 1H
one.
The insurgence of magnetism induces a weak harden-
ing of some A 2gmodes  in principle detectable as a Ra-
man shift (see App. B).
However  given the correlated nature of the problem
and the key role of the DFT+U approximation in deter-
mining total energies  as shown in 1T-NbSe 213  we per-
form DFT+U calculations using the method in Ref.34.
TheUparameter is computed self-consistently from rst
principles34  we obtain U= 2:87 eV. This value is similar
to those found for 1T-TaS 24and 1T-NbSe 213compounds.
We rst perform structural optimization of the high-
symmetry 1T-NbS 2and 1H-NbS 2single-layer structure
within DFT+U. For the 1H-polytype  we nd that the
in-plane lattice parameter in DFT+U is in slightly better
agreement with the one measured in the bulk than in the
GGA case (see Tab. II)  suggesting that DFT+U gives
a slightly better energetic then GGA  as it happens in
NbSe 213. As it can be seen  the energy dierence between
6
FIG. 5. Electronic structure of 1T-NbS 2in thep
CDW phase (top panel) and of 1T-NbS 2in the CDW phase
where the pseudopotential of S has been replaced by the one
of Se (central panel). Electronic structure of 1T-NbS 2using
lattice parameters and internal coordinates of NbSe 2p
13 in the CDW phase (bottom panel). The size of the circles
is proportional to the dz2 r2character of the central Nb in
the star (the percentage of the central Nb dz2component at
  at the Fermi level are 11% (top)  13% (central) and 9.6%
(bottom))
the 1H and 1T undistorted polytypes is now reduced.
We then optimize the geometry in the CDW phase with
DFT+U obtaining a small contraction of the in-plane lat-
tice constants (see Tab. II  the Wycko positions are re-
ported in App. A). Also the magnetic structure is slightly
dierent from GGA results: a stronger ferrimagnetic so-
lution with magnetic moments of 0.41 Bon the red sites
0.03Bon the blue ones and negligible antiferromagneticcontributions from the other sites is stabilized (we refer
to the left panel in Fig.4 for color labeling). The total
magnetic moment per unit cell is of 1 B. The magnetic
moment on the central atom is thus almost the double of
the one found in spin-polarized GGA (while the total spin
is of course still 1 =2). Moreover  as shown in Tab. II  the
CDW magnetic solution has an important energy gain
with respect to the 1T-polytype and it is almost degen-
erate with the 1H one (0.5 mRy energy dierence).
This energy dierence is slightly smaller than the one
found between 1H-NbSe 2highly symmetric polytype and
the 1T-NbSe 2charge density wave phase. This suggest
that 1T-NbS 2can be synthesized with a similar experi-
mental procedure to the one used for NbSe 211 12.
In Fig.6 we report the band structure for the mag-
netic solution obtained in DFT+U compared with the
ones obtained in GGA. The band-gap between the at
band is now more than twice that in spin-polarized GGA
(0.41 eV) and is not the fundamental gap as the mi-
nority spin at bands is pushed inside the empty d-
conduction bands. The fundamental gap is 0.35 eV
and involves transitions between d-orbitals of dierent
Nb atoms.
Finally we evaluate the nearest-neighbor ( J1) and
next-near-neighbor ( J2) exchange constants between dif-
ferent star of David clusters in an ferromagnetic Hubbard
model described by the following Hamiltonian:
^H= J1
2X
<i;j>^Sz
i^Sz
j J2
<<i;j>>^Sz
j:
We adopted a super-cell","In this work we investigated by rst principles the pos-
sible formation of single-layer 1T-NbS 2as well as its
structural  electronic and dynamical properties in the
high symmetry phase and in the CDW one with dif-
ferent degrees of correlation and allowing for magnetic
solutions.
We demonstrate that the 1T undistorted (1 1) poly-
type is highly unstable towards ap
13 reconstruc-
tion. Within the GGA+U approximation  thep
structural distortion and the formation of a ferrimagnetic
state cooperate in stabilizing the 1T-NbS 2phase in single
layer form that becomes comparable in energy with that
of the 1H polytype. Thus  we predict that this system
7
FIG. 6. Spin resolved electronic band structures for thep
13 phase obtained in GGA (left panel) and in GGA+U
approximation (right panel); solid (dotted) lines are related to majority (minority) spin states. The size of the circles is
proportional to the central Nb dz2 r2character of the eigenvalues (the percentage of the central Nb dz2component for the ats
bands at   are 9.5% for the majority (12% for the minority) component in the GGA approximation and 6.2% for the majority
(16% for the minority) component in the GGA+U one).
can be synthesized with similar techniques to those used
for single layer 1T-NbSe 211 12. Interestingly  a previous
work6describes growth of bulk 1T-NbS 2on glass keep-
ing the substrate at very high temperature. As a similar
technique has been used for the synthesis of 1T-NbSe 2
(with a dierent substrate) in Ref.11  this makes the
synthesis of single layer 1T-NbS 2even more likely.
Finally  it is interesting to underline that in this sys-
tem  magnetism occurs in a ultraat band  isolated from
all the others and having a marked d z2 r2character on
the central Nb atom in the star. Spin polarized calcu-
lations without any Hubbard mean eld term  recover
the insulating state by stabilizing magnetism  (although
with a fairly small gap). A similar eect occurs in TaS 24
where even at U= 0 a magnetic state is stabilized within
the spin polarized generalized gradient approximation.
In this respect  suldes are odd with 1T-NbSe 2where
the at band is strongly hybridized with Se states and
the Hubbard interaction is needed to disentangle it from
the other bands13. 1T-NbS 2in thep
13 is then
a prototype system where the presence of an ultraat
band produces magnetism even at very moderate values
ofU=t.
ACKNOWLEDGMENTS
We thank G. Menichetti for useful and stimulating dis-
cussions.
This work was supported by French state funds man-
aged by the ANR within the Investissements d'Avenir
program under references ANR-13-IS10-0003-01 ANR-
11-IDEX-0004-02  and more specically within the
framework of the Cluster of Excellence MATISSE led by
Sorbonne Universit e  by the European Graphene Flag-
ship (GrapheneCore 2). Computer facilities were pro-
vided by CINES  IDRIS  and CEA TGCC and PRACE(2017174186).
APPENDIX A
We report the relaxed Wycko positions for the low
temperaturep
13 CDW (non-magnetic) phase.
Both structures obtained in GGA and GGA+U belongs
to theP3 space group (group number 147)  and the
Wycko positions are reported in Tab. III and Tab. IV
respectively.
C=(0 0 0) multiplicity Wycko label x y z
Nb 1 a 0.00000 0.00000 0.00000
Nb 6 g 0.28854 0.07046 0.00044
Nb 6 g 0.63662 0.15249 -0.00124
S 6 g 0.05083 0.17491 0.13591
S 6 g 0.35363 0.25132 0.13360
S 6 g 0.48561 0.19908 0.87839
S 2 d 0.33333 0.66667 0.88017
S 6 g -0.02692 0.40822 0.11972
TABLE III. Wycko positions of the 1T-NbS 2p
CDW phase obtained in GGA.
APPENDIX B
We compute the phonon dispersion for thep
CDW phase in GGA approximation for the non magnetic
(NM) and the ferrimagnetic (FM) solutions. We calcu-
late the dynamical matrix at zone center and then Fourier
interpolate the dynamical matrices on the full Brillouin
zone. Results are reported in Fig.7. All frequencies are
positive revealing the CDW phase is dynamically stable.
The Raman active modes are reported in Tab. V. The
8
Nb 6 g 0.28891 0.07059 0.00036
Nb 6 g 0.63634 0.15237 -0.00097
S 6 g 0.05072 0.17451 0.13662
S 6 g 0.35369 0.25129 0.13419
S 6 g 0.48561 0.19901 0.87758
S 2 d 0.33333 0.66667 0.87948
S 6 g -0.02695 0.40829 0.12062
TABLE IV. Wycko positions of the 1T-NbS 2p
CDW phase obtained in GGA+U (U=2.87 eV).insurgence of magnetism causes a weak hardening of A 2g
frequencies at around 90 and 366 cm 1(we register shifts
of about 45 cm 1  see Tab. V).
1J. A. Wilson  F. J. Di Salvo  and S. Mahajan  Adv. in
Phys. 24  117 (1975).
2Physica B+C 99  183 (1980).
3P. Fazekas and E. Tosatti  Phyl. Mag. B 39  229 (1979).
4P. Darancet  A. J. Millis  and C. A. Marianetti  Phys.
Rev. B 90  045134 (2014).
5A. S. Ngankeu  S. K. Mahatha  K. Guilloy  M. Bianchi
C. E. Sanders  K. Han  K. Rossnagel  J. A. Miwa  C. B.
Nielsen  M. Bremholm  and P. Hofmann  Phys. Rev. B
96  195147 (2017).
6C. J. Carmalt  T. D. Manning  I. P. Parkin  E. S. Peters
and A. L. Hector  J. Mater. Chem. 14  290 (2004).
7K. S. Novoselov  D. Jiang  F. Schedin  T. Booth  V. V.
Khotkevich  S. Morozov  and A. K. Geim  PNAS 102
10451 (2005).
8J. N. C. et al.   Science 331  568 (2011).
9Y. Wang  J. Xiao  H. Zhu  Y. Li  Y. Alsaid  K. Fong
Y. Zhou  S. Wang  W. Shi  Y. Wang  A. Zettl  E. J. Reed
and X. Zhang  Nature 550  487 (2017).
10G. Eda  H. Yamaguchi  D. Voiry  T. Fujita  M. Chen  and
M. Chhowalla  Nano Letters 11  5111 (2011).
11Y. Nakata  K. Sugawara  R. Shimizu  Y. Okada  P. Han
T. Hitosugi  K. Ueno  T. Sato  and T. Takahashi  NPG
Asia Materials 8  e321 (2016).
12F. Bischo  W. Auw arter  J. V. Barth  A. Schirin
M. Fuhrer  and B. Weber  Chemistry of Materials 29  9907
(2017)  https://doi.org/10.1021/acs.chemmater.7b03061.
13M. Calandra  Phys. Rev. Lett. 121  026401 (2018).
14X. Xi  L. Zhao  Z. Wang  H. Berger  L. Forr o  J. Shan
and M. K.F.  Nature Nanotechnology 10  765 (2015).
15M. Ugeda  A. J. Bradley  Y. Zhang  S. Onishi  Y. Chen
W. Ruan  C. Ojeda-Aristizabal  H. Ryu  M. T. Edmonds
H. Z. Tsai  A. Riss  S. K. Mo  D. Lee  A. Zettl  Z. Hussain
S. Z. X.  and M. F. Crommie  Nature Phys. 12  92 (2016).
16M. Calandra  I. Mazin  and F. Mauri  Phys. Rev. B 80
241108(R) (2009).
17M. Leroux  M. Le Tacon  M. Calandra  L. Cario
M. M easson  P. Diener  E. Borrissenko  A. Bosak  and
P. Rodi ere  Phys. Rev. B 86  155125 (2012).
18H. Lin  W. Huang  K. Zhao  C. Lian  W. Duan  X. Chen
and S. H. Ji  Nano Research 11  4722 (2018).
19R. M. Stan  S. K. Mahatha  M. Bianchi  C. E. Sanders
D. Curcio  P. Hofmann  and M. J. A.  arXiv 1901.03552
(2019).20P. Giannozzi  S. Baroni  N. Bonini  M. Calandra  R. Car
C. Cavazzoni  D. Ceresoli  G. L. Chiarotti  M. Cococcioni
I. Dabo  A. D. Corso  S. de Gironcoli  S. Fabris  G. Fratesi
R. Gebauer  U. Gerstmann  C. Gougoussis  A. Kokalj
M. Lazzeri  L. Martin-Samos  N. Marzari  F. Mauri
R. Mazzarello  S. Paolini  A. Pasquarello  L. Paulatto
C. Sbraccia  S. Scandolo  G. Sclauzero  A. P. Seitsonen
A. Smogunov  P. Umari  and R. M. Wentzcovitch  Jour-
nal of Physics: Condensed Matter 21  395502 (2009).
21P. Giannozzi  O. Andreussi  T. Brumme  O. Bunau  M. B.
Nardelli  M. Calandra  R. Car  C. Cavazzoni  D. Ceresoli
M. Cococcioni  N. Colonna  I. Carnimeo  A. D. Corso
S. de Gironcoli  P. Delugas  R. A. D. Jr  A. Ferretti
A. Floris  G. Fratesi  G. Fugallo  R. Gebauer  U. Ger-
stmann  F. Giustino  T. Gorni  J. Jia  M. Kawamura
H.-Y. Ko  A. Kokalj  E. Kkbenli  M. Lazzeri  M. Mar-
sili  N. Marzari  F. Mauri  N. L. Nguyen  H.-V. Nguyen
A. O. de-la Roza  L. Paulatto  S. Ponc  D. Rocca  R. Saba-
tini  B. Santra  M. Schlipf  A. P. Seitsonen  A. Smogunov
I. Timrov  T. Thonhauser  P. Umari  N. Vast  X. Wu
and S. Baroni  Journal of Physics: Condensed Matter 29
465901 (2017).
22D. Vanderbilt  Phys. Rev. B 41  7892 (1990).
23H. J. Monkhorst and J. D. Pack  Phys. Rev. B 13  5188
(1976).
24M. Whangbo and E. Canadell  J. Am.. Chem. Soc. 114
9587 (1992).
25L. F. Mattheiss  Phys. Rev. B 8  3719 (1973).
26M. Calandra and F. Mauri  Phys. Rev. Lett. 95  237002
(2005).
27E. Kamil  J. Berges  G. Schnho  M. Rsner  M. Schler
G. Sangiovanni  and T. O. Wehling  Journal of Physics:
Condensed Matter 30  325601 (2018).
28D. Pasquier and O. V. Yazyev  Phys. Rev. B 98  045114
(2018).
29F. Jellinek  G. Brauer  and H. M uller  Nature 185  376
(1960).
30Y. Ge and A. Y. Liu  Phys. Rev. B 82  155133 (2010).
31D. C. Miller  S. D. Mahanti  and P. M. Duxbury  Phys.
Rev. B 97  045133 (2018).
32O. R. Albertini  R. Zhao  R. L. McCann  S. Feng  M. Ter-
rones  J. K. Freericks  J. A. Robinson  and A. Y. Liu
Phys. Rev. B 93  214109 (2016).
33A. Y. Liu  Phys. Rev. B 79  220515 (2009).
34M. Cococcioni and S. de Gironcoli  Phys. Rev. B 71
035105 (2005).
9
FIG. 7. Phonon dispersion for thep
13 CDW phase in
the non magnetic solution (black curves) and in the ferrimag-
netic one (red curves).
10
Point!NM!FM
group (cm 1) (cm 1)
Eg 63.9 61.6
Ag 64.6 64.6
Ag 82.1 83.2
Eg 84.4 88.6
Eg 93.3 91.5
Ag 94.2 97.9
Eg 106.0 105.4
Eg 119.5 120.2
Ag 121.2 121.7
Ag 142.9 146.3
Ag 164.0 166.7
Eg 179.7 180.4
Ag 211.1 211.1
Eg 218.8 218.8
Ag 221.3 221.6
Eg 226.9 227.1
Eg 236.2 236.3
Eg 242.2 242.4
Ag 243.7 243.6
Eg 257.7 258.6
Eg 261.6 261.7
Ag 262.3 262.7
Ag 266.8 266.7
Eg 268.1 268.6
Eg 276.7 276.9
Ag 283.9 283.6
Ag 286.0 285.5
Ag 298.7 298.7
Ag 303.4 303.2
Eg 307.5 306.9
Eg 346.5 346.5
Ag 349.9 351.7
Eg 357.5 357.9
Ag 358.9 359.1
Ag 366.7 371.0
Eg 378.4 379.0
Ag 392.8 394.1
Eg 393.0 395.1
TABLE V. Raman active frequencies for the 1T-NbS 2p
13 CDW phase obtained in non magnetic (NM) and Ferri-
magnetic (FM) phases (in GGA approximation).
Charge density wave and spin 1/2 insulating state in single layer 1T-NbS .
Sorbonne Universit´e  CNRS  Institut des Nanosciences de Paris  UMR7588  F-75252  Paris  France
Inbulksamplesandfewlayerﬂakes thetransitionmetaldichalcogenidesNbS andNbSe assume
2 2
singleandfewlayersof1T-NbSe withoctahedralcoordinationaroundthetransitionmetalionwere
synthesized. Motivatedbytheseexperimentsandbyusingﬁrst-principlescalculations weinvestigate
thestructural electronicanddynamicalpropertiesofsinglelayer1T-NbS . Weﬁndthatsingle-layer
√ √ 2
1T-NbS undergoesa 13× 13star-of-Davidchargedensitywave. Withinthegeneralizedgradient
approximation theweakinteractionbetweenthestarsleadstoanultraﬂatbandattheFermilevel
9 total spin 1/2 magnetic state with opening of a 0.15 eV band gap and a 0.21µB magnetic moment
1 localized on the central Nb in the star. Within GGA+U  the magnetic moment on the central Nb
0 is enhanced to 0.41µ and a larger gap occurs. Most important  this approximation gives a small
B
2 energy diﬀerence between the 1T and 1H polytypes (only +0.5 mRy/Nb)  suggesting that the 1T-
polytypecanbesynthesizedinasimilarwayasdoneforsinglelayer1T-NbSe . Finallywecompute
b 2
ﬁrst and second nearest neighbors magnetic inter-star exchange interactions ﬁnding J =9.5 K and
e 1
F J2=0.4 K ferromagnetic coupling constants.
1
I. INTRODUCTION tween the 2H and 1T and 1T(cid:48) phases obtained by liquid
] exfoliation8 10.
l
e
- Bulk transition metal dichalcogenides (TMDs) of the More recently it has been shown that the 1T-NbSe2
r form TCh   where T is a transition metal and Ch is a polytype can be stabilized either as a single layer on top
t 2
s chalcogen (Se  S  Te)  are very versatile systems as their of bilayer graphene kept at 500-590 ◦C during epitaxy11
t. electronicandstructuralpropertiescanbetunednotonly or by applying a pulsed local ﬁeld through the STM tip
ma byvaryingtheirchemicalcompositionbutalsobysynthe- at the surface of bulk 2H-NbSe212.
sizing diﬀerent polytypes having the same chemical for- The physical properties of single layer 1T-NbSe2
d- mula. Thevariationofthelocalcoordinationofthetran- turned out to be completely diﬀerent from that of single
n sition metal ion in diﬀerent polytypes of a given TMD layer 1H-NbSe2 as the former√is a s√pin 1/2 Mott-Jahn
o leads to completely diﬀerent physical properties1. For Teller insulator undergoing a 13× 13 charge density
c example  1T-TaS withTainoctahedralcoordination  is wave11 13  while the latter is a metal undergoing a 3×3
[ 2
a correlated system which ground state is still very de- charge density wave14–16. Most important  it has been
1  bated (Mott insulator or correlated metal)2–5 while 2H- recently shown13 that density functional theory calcula-
v TaS   with Ta in trigonal prismatic coordination  is a tions(DFT)withlocalLDA/GGAkernelsdonotexplain
4 metal (the 1T and 1H polytypes are reported in Fig.1). the stabilization of the 1T-NbSe single layer phase with
However  this tunability cannot be completely exploited respect to the 2H one  as this transition occurs via a
as not all bulk TMDs can be synthesized in 1T and 2H correlatedmechanisminvolvingvibrationsandthestabi-
0
0 polytypes either because the appropriate chemical and lizationofamagneticstatethatcanbeaddressedwithin
. thermodynamicalpreparationconditionsareunknownor the DFT+U approximation. Thus  given the broad per-
because the energetic is unfavorable. This is the case of spectives oﬀered by these new synthesis techniques  the-
bulk1T-TiSe thathasneverbeensynthesizedinthe2H oreticalcalculationscanbeusedtospotnewTMDphase
9 2
1 polytype or  vice versa  of bulk 2H-NbSe2 and 2H-NbS2 thatcanbeexperimentallyaccessedandtodescribetheir
: that crystallizes in the 2H polytype and not in the 1T structural and electronic properties.
v
one  although it has been reported6 that 1T-NbS bulk Bulk2H-NbS isisoelectronicandisostructuralto2H-
i 2 2
X can be synthesized under very special conditions. NbSe   however it stands somewhat at odd with respect
r Since diﬀerent TCh planes are bounded together by to other transition metal dichalcogenides as it displays
a the weak van der Waals interaction  it makes possible to no charge density wave (CDW) at low temperature17.
isolate single layers of a large class of transition metal On the contrary  when NbS2 single-layer is grown on
dichalcogenides7 8 (a single layer here refers to a tri- Nitrogen-doped 6H-SiC(0001) terminated with single or
layer TCh2 unit). The exfoliation of bulk TMDs into bilayer graphene  it adopts the 1H-NbS2 polytype and
bi-dimensional (2D) crystals  beside being interesting in STM images show a 3×3 reconstruction18  but if 1H-
itselfasitallowstoinvestigateavarietyofphenomenain NbS2 is grown epitaxially on Au(111) no charge density
low dimension  paves the way to diﬀerent synthesis tech- waveisdetected19. GiventhediﬀerentpropertiesofNbS2
niques unﬁttedforbulksystemsbutfeasibleinfewlayers in the 2D limit  it is natural to investigate the possible
ﬂakes. An example is the phase transition between the stability of other polytypes and the formation of mag-
hexagonal and monoclinic phases of monolayer MoTe netic and charge density wave phases.
achieved by electrostatic doping9 or the transition be- In this work  by using density functional theory cal-
culations  we investigate the possible synthesis of single 0.0001 Ry increasing the BZ grid to 12 × 12 × 1 (for
layer 1T-NbS together with its structural  vibrational the evaluation of exchange constants we use super-cells:
and electronic properties. We study the stability with wescaletheBZsamplinggridtoassurethesamedensity
respect to the single layer phase and we calculate mag- used in the other calculations). The surface is simulated
netic couplings. by considering a supercell with about 10 ˚A of vacuum
fullstructuraloptimizationoftheinternaldegreesoffree-
calculated in linear response theory20 21 over 19 phonon
wave-vectormeshintheirreducibleBZusinganuniform
20×20×1 reciprocal space mesh for sampling the elec-
A. High-symmetry 1T-NbS structure.
Westartbyperforminggeometricaloptimizationofthe
undistorted 1T-NbS structure (3 atoms/cell). For com-
latethetheoreticalGGAstructuralparametersandelec-
tronic structures of 1T-NbSe   1T-TaS and 1T-TaSe
2 2 2
seenthesulfurdichalcogenidesaresomewhatcompressed
inthebasalplanewithrespecttoSedichalcogenides. The
FIG.1. Crystalstructureofthe1H(top)andthe1T(bottom)
chalcogen height (h ) and a smaller tetragonal distor-
NbS singlelayer. Thelocalcoordinationaroundatransition Ch
tionoftheoctahedralcrystalsymmetryaroundthetran-
metalatomaretrigonalprismaticandoctahedralrespectively
Thisisrelevantastheelectronicstructuresofallthese
tant diﬀerences that can be in part attributed to the
dralcrystal ﬁeldaround thetransition metaland inpart
Density functional theory calculations are performed to the alignment of the chalcogen and transition metal
using the Quantum-Espresso code20 21. For Nb levels24 25.
(Ta) we use ultra-soft pseudopotentials from Vanderbilt In more details  the octahedral crystal ﬁeld splitting
distributions22 includingsemicorestatesandtwoprojec- leads to triply degenerate t orbitals (d   d
2g x2−y2 z2−r2
tors for s and p channels and valence conﬁguration 4s2  d ) and doubly degenerate e orbitals (d   d ) at
xy g xy xz
4p6  4d5  5s1 (5s2  6s2  5p6  5d3). For S (Se) we use higher energy (we adopt here the same convention of
norm-conservingpseudopotentialswithemptyd-statesin Ref.25 for the crystal axes). The trigonal distortion of
valence and the following valence conﬁguration 3s2  3p3  the octahedron is identiﬁed by the bond angles centered
3d0 (4s2  4p3  4d0). atthetransitionmetalionandhavingbondstothenear-
We use an energy cutoﬀ up to 45 Ry (540 Ry for the est chalcogens (see picture in Tab. I). In an undistorted
chargedensity)forallthecalculations. Fortheexchange octahedronα=β =90o whileinthepresentcasesthere
correlation energy we take the generalized gradient ap- is a substantial deviation from the ideal values.
proximation (GGA) and the GGA+U one. The crystal ﬁeld for a trigonal distorted octahedron
ThechargedensityintegrationovertheBrillouinZone splits the t orbitals in a twofold degenerate state
2g
(BZ)isperformedusinganuniform20×20×1Monkhorst (d  d )andasingledegenerated state. Wela-
x2−y2 xy z2−r2
and Pack grid23 for the 1T and 2H-polytypes (6×6×1 belthisenergyseparationatzonecenter“apparentJahn-
√ √
and 5×5×1 for the 13× 13 and 4×4 CDW phases Teller splitting”. In the case of 1T-NbS and 1T-TaS
respectively) and a 0.01 Ry Gaussian smearing. For the theapparenttrigonalJahn-TellersplittingattheΓpoint
totalenergycomparisonamongmagneticsolutionsofthe is positive (namely the band originating from the d
√ √ z2−r2
13 × 13 reconstruction we reduce the smearing to state is higher in energy with respect to the twofold de-
System a (˚A) h (˚A) α (deg) β (deg)
Ch
NbS 3.35 1.55 94.7 85.3
NbSe 3.4813 1.69 97.5 82.5
TaS 3.38 1.53 94.0 86.0
TaSe 3.50 1.65 95.7 84.3
TABLE I. Theoretical internal coordinates for 1T-TCh systems. In the inset image we report in gray the T atoms and in
(a)1T-NbS2 {dz2−r2(Γ)=47%} (b)1T-NbSe2 {dz2−r2(Γ)=54%}
(c)1T-TaS2 {dz2−r2(Γ)=46%} (d)1T-TaSe2 {dz2−r2(Γ)=54%}
FIG.2. BandstructuresforTCh compounds(T=Nb Ta;Ch=S Se)inthe1T-polytype. Thesizeofthecirclesisproportional
to the Nb character of the eigenvalues  the percentage of the d component at Γ for each system is reported in the
dz2−r2 z2−r2
generate one arising from the d and d atomic or- center (labeled “h” in Fig.2). In sulﬁdes  this band is
x2−y2 xy
bitals)andverysimilarinmagnitudeforbothsystems as mainly formed by sulfur 3p states. In TaS this sepa-
itcanbeseeninFig.2. Surprisingly  inselenides  despite ration is larger than in NbS   mainly due to the larger
a larger distortion  the apparent Jahn-Teller splitting is energy misalignment between the sulfur 3p and the Nb
almost zero or negative. 4d or Ta 5d states. The situation is very diﬀerent in se-
lenides  where the hybridization between the Se p states
andtheTaorNbdstatesisstrong(strongerinNbthanin
ering the hybridization between the transition metal t
Ta)andleadstocompletelycounterintuitiveresultswith
bands and the other occupied chalcogen bands at zone
System ∆E ∆E a a ingfrominitialconﬁgurationsobtainedbydisplacingthe
U U
1H 0.0 0.0 3.346 3.326 atomiccoordinatesfollowingthepatternsofthemostun-
1T +7.2 +4.2 3.360 3.357 stable phonon modes. In both case  we ﬁnd structures
4√×4 √ +5.8* +1.9 13.485* 13.428 that are substantially more stable than the highly sym-
√13×√13 +4.3 +1.2 12.200 12.123 metric ones. In the case of a 4×4 supercell  we ﬁnd two
13× 13 (FM) +4.4 +0.5 12.198 12.126 diﬀerent reconstructions that are practically degenerate
in energy (see Fig.4). Both 4×4 CDW  however  seems
totrytoformsomekindofstar-of-Davidreconstruction
TABLEII.Calculatedenergydiﬀerenceamongdiﬀerentpoly-
types and CDW phases. The energy diﬀerences are in
mation. This is conﬁrmed by the fact that 13× 13
mRy/Nb. Inthelasttwocolumnsthetheoreticalequilibrium
latticeconstantswithandwithoutU (aanda respectively)
are reported in ˚A  U = 2.87eV for all the calcUulations. The of ≈ 2.9 mRy/Nb with respect to the highly symmet-
ric 1T-NbS phase  however still with an energy loss of
experimental in-plane lattice parameter for bulk 2H-NbS2 is 2
3.31˚A29. The (*) means we have two diﬀerent CDWs  prac- ≈4.3mRy/Nbwithrespecttothehighlysymmetric1H-
tically degenerate in energy. NbS2 phase. √ √
The optimized 13× 13 structure is shown in Fig.4
(we also report the Wyckoﬀ positions in App. A)  and
respect to crystal ﬁeld theory. For example  the larger it results dynamically stable (see App. B)  the relative
octahedral distortion occurs in NbSe2  but here we ﬁnd energy diﬀerences among the diﬀerent phases considered
an apparent negative Jahn-Teller splitting. Finally  in are reported in Tab. II.
√
TaSe2 the apparent Jahn-Teller splitting is almost zero √The non-magnetic electronic structure in the 13 ×
asthecrystalﬁeldandthehybridizationperfectlycancels 13 phase is shown in Fig.5 (top). It is characterized
out and the t2g bands become almost threefold degener- by the presence of an extremely ﬂat band at the Fermi
ate at zone center. Furthermore the top of what were level having a non-negligible d character related to
z2−r2
the chalcogen p-bands in sulﬁdes becomes mixed with thecentralNbatominthestar. Theﬂatbandisisolated
d-states in selenides (particularly evident in NbSe2). from the others and is located in the middle of the gap
The diﬀerent magnitude of the hybridization explains this is in analogy with what happens in 1T-TaS 4 30–33
whyinsulﬁdesoneexpectt2g manifoldsseparatedbythe andincontrastwiththe1T-NbSe2 case13 27 28 wherethe
chalcogen states while in selenides the character is more ﬂat band is entangled with chalcogen states.
In order to disentangle the eﬀects of chemistry and
Finally itisworthmentioningthatasthe1T-polytype
distortion in determining the energy position of the ﬂat
breakstheinversionsymmetry weinvestigatethemagni-
tude of relativistic eﬀects in 1T-NbS ﬁnding them neg-
2 thenon-magneticelectronicdispersionsof(i)NbS CDW
keeping however thestructureunchanged(labeled“1T-
Having understood the electronic structure of the NbS str.; Se PP” in Fig.5)  (ii) NbS using the crystal
highly symmetric phase in comparison with other 1T structure of 1T-NbSe in the CDW phase (labeled “1T-
compounds  we compare the energy of single layer 1T- NbSe str.; S PP” in Fig.5). These calculations should
NbS2 with the 1H-NbS2 polytype. We ﬁnd that 1H is be directly compared with the case of NbSe2 reported in
morestablebyapproximately7.2mRy/Nb(seealsoTab. Ref.13 (Fig.3  left panel) where  at the GGA level  the
II)  similarly to what happens for the NbSe2 case26–28. ﬂat band lies in the middle of Se states and is not iso-
This large energy diﬀerence prevents an highly symmet- latedfromtheothers. Thiscomesmostlyfroma0.25eV
ric 1T phase to form in experiments. upshift of the lower occupied states at zone center.
Calculation (i) allows us to determine the eﬀect of
NbS . As shown in Fig.3 we found strongly unstable
2 be seen the eﬀect of replacing the S with a Se pseudopo-
mostunstablephononfrequenciesintheBZ wealsoper-
agreementwithwhathappensintheidealhigh-symmetry
undistorted 1T-NbS /1T-NbSe phases. However this
monic level we ﬁnd that the two most likely instabilities
√ √ up-shiftisstillnotlargeenoughtomixtheﬂatbandwith
have wavevector compatible with a 13× 1330 and a √ √
theotheroccupiedbands asithappensinthe 13× 13
4×4 CDWs. phase of 1T-NbSe 13. If  on the contrary  we use the
NbSe 13× 13 structure with the S pseudopotential
asincalculation(ii) weseethattheresultsistoup-shift
B. Charge density wave phases mostly the occupied states very close to the ﬂat band.
However  this is not what happens in the 13 × 13
WeperformgeometricaloptimizationwithintheGGA phase of 1T-NbSe as in this system  at the GGA level
approximation in 4×4 and 13× 13 supercells start- the top of the Se states at Γ are empty and are at higher
FIG.3. Left: phonondispersionalonghigh-symmetrydirectionsofthehigh-symmetry1T-NbS phase. Theprojectionsofthe
ordering vectors related to the 4×4 and to the 13× 13 CDWs onto the Γ-M line are marked with vertical lines. Right:
DistributionofnegativephononicfrequenciesintheBZforundistorted1T-NbS . Thewavevectorsbelongingtothe4×4and
to the 13× 13 CDWs are marked with crosses of diﬀerent colors
FIG. 4. Left: Optimized crystal structure for the 13× 13 CDW phase. The tree nonequivalent Nb sites are highlighted:
central Nb in the star (red)  Nb belonging to the peripheral atoms of the 7× 7 cluster (blue) and the other Nb atoms are
in gray. S atoms are shown in yellow. Center and right: Optimized crystal structure for the 4×4 CDW. The orange lines are
energies then the ﬂat band. It follows that the eﬀect magnetic moments are 0.21 µ on the red sites  0.04 µ
B B
is not properly chemical neither structural  but it is a on the blue ones and negligible contributions on the oth-
cooperative eﬀect of the two. This aspect is a general ers. It is worth to underline that the magnetic structure
featurestrictlyrelatedtothechalcogenatominvolvedin is still unstable (4.4 mRy/Nb) with respect to the 1H
the compound. In fact the same behavior is observable one.
also in TaS2 and TaSe2 (see Refs.4 13 31). The insurgence of magnetism induces a weak harden-
As shown in Fig.5 (top)  the Nb band is ex- ing of some A modes  in principle detectable as a Ra-
dz2−r2 2g
tremelyﬂat withadispersionof∼0.016eV.Thisimplies man shift (see App. B).
a small Fermi velocity  a low kinetic energy and an high However  given the correlated nature of the problem
peak in the density of the states at the Fermi level. It and the key role of the DFT+U approximation in deter-
isthennaturaltoexpectelectronicinstabilitiestooccur. mining total energies  as shown in 1T-NbSe 13  we per-
We then perform spin-polarized calculations stabilizing form DFT+U calculations using the method in Ref.34.
an insulating ferrimagnetic solution with an energy loss TheU parameteriscomputedself-consistentlyfromﬁrst
of about 0.1 mRy/Nb with respect to the metallic non- principles34 weobtainU =2.87eV.Thisvalueissimilar
magnetic solution (see Tab. II). Thus  even in the ab- tothosefoundfor1T-TaS 4and1T-NbSe 13compounds.
sence of an Hubbard term  GGA stabilizes a magnetic We ﬁrst perform structural optimization of the high-
state. symmetry 1T-NbS and 1H-NbS single-layer structure
The magnetic bands are reported in Fig.6 (left)  as we within DFT+U. For the 1H-polytype  we ﬁnd that the
can see  in the ferrimagnetic conﬁguration  the system in-planelatticeparameterinDFT+Uisinslightlybetter
resultstobesemiconductorwithagapofabout0.15eV. agreementwiththeonemeasuredinthebulkthaninthe
Theﬂatbandissplittedintwo oneisfullyoccupiedand GGA case (see Tab. II)  suggesting that DFT+U gives
the second one is empty  so that the total magnetic mo- a slightly better energetic then GGA  as it happens in
ment is 1 µ . By referring to the left panel in Fig.4  the NbSe 13. Asitcanbeseen theenergydiﬀerencebetween
B 2
contributions from the other sites is stabilized (we refer
magnetic moment per unit cell is of 1 µ . The magnetic
momentonthecentralatomisthusalmostthedoubleof
theonefoundinspin-polarizedGGA(whilethetotalspin
isofcourse still1/2). Moreover  asshown inTab. II the
erate with the 1H one (0.5 mRy energy diﬀerence).
This energy diﬀerence is slightly smaller than the one
foundbetween1H-NbSe highlysymmetricpolytypeand
the 1T-NbSe charge density wave phase. This suggest
that 1T-NbS can be synthesized with a similar experi-
mental procedure to the one used for NbSe 11 12.
ones obtained in GGA. The band-gap between the ﬂat
bandisnowmorethantwicethatinspin-polarizedGGA
(∼0.41 eV) and is not the fundamental gap as the mi-
nority spin ﬂat bands is pushed inside the empty d-
conduction bands. The fundamental gap is ∼0.35 eV
and involves transitions between d-orbitals of diﬀerent
Finally we evaluate the nearest-neighbor (J ) and
next-near-neighbor (J ) exchange constants between dif-
ferentstarofDavidclustersinanferromagneticHubbard
Hˆ =−J1 (cid:88) Sˆz·Sˆz− J2 (cid:88) Sˆz·Sˆz.
2 i j 2 i j
<i j> <<i j>>
(2 13) × (3 13) cell with diﬀerent collinear magnetic
conﬁgurations. It is important to note that we obtain
similar total energies for all the spin conﬁgurations took
J =9.5 K and J =0.4 K  in line with the parameters
1 2
describing the similar NbSe compound28. From that
FIG. 5. Electronic structure of 1T-NbS in the 13× 13 the system results to have a ferromagnetic ground state
CDW phase (top panel) and of 1T-NbS in the CDW phase between diﬀerent stars.
of Se (central panel). Electronic structure of 1T-NbS using
2√
lattice parameters and internal coordinates of NbSe 13×
√ 2 IV. CONCLUSIONS
13intheCDWphase(bottompanel). Thesizeofthecircles
is proportional to the d character of the central Nb in
z2−r2 Inthisworkweinvestigatedbyﬁrstprinciplesthepos-
the star (the percentage of the central Nb d component at
z2
Γ at the Fermi level are 11% (top)  13% (central) and 9.6% sible formation of single-layer 1T-NbS2 as well as its
(bottom)) structural  electronic and dynamical properties in the
the 1H and 1T undistorted polytypes is now reduced. solutions.
WethenoptimizethegeometryintheCDWphasewith We demonstrate that the 1T undistorted (1×1) poly-
DFT+Uobtainingasmallcontractionofthein-planelat- type is highly unstable towards a 13× 13 reconstruc-
tice constants (see Tab. II  the Wyckoﬀ positions are re- tion. WithintheGGA+Uapproximation the 13× 13
portedinApp.A).Alsothemagneticstructureisslightly structuraldistortionandtheformationofaferrimagnetic
diﬀerent from GGA results: a stronger ferrimagnetic so- statecooperateinstabilizingthe1T-NbS phaseinsingle
lutionwithmagneticmomentsof0.41µ ontheredsites  layer form that becomes comparable in energy with that
0.03µ ontheblueonesandnegligibleantiferromagnetic of the 1H polytype. Thus  we predict that this system
FIG. 6. Spin resolved electronic band structures for the 13× 13 phase obtained in GGA (left panel) and in GGA+U
proportionaltothecentralNbd characteroftheeigenvalues(thepercentageofthecentralNbd componentfortheﬂats
z2−r2 z2
bandsatΓare9.5%forthemajority(12%fortheminority)componentintheGGAapproximationand6.2%forthemajority
can be synthesized with similar techniques to those used (2017174186).
for single layer 1T-NbSe 11 12. Interestingly  a previous
work6 describes growth of bulk 1T-NbS on glass keep-
ing the substrate at very high temperature. As a similar APPENDIX A
technique has been used for the synthesis of 1T-NbSe
(with a diﬀerent substrate) in Ref. 11  this makes the We report the relaxed Wyckoﬀ positions for the low
synthesis of single layer 1T-NbS2 even more likely. temperature 13 × 13 CDW (non-magnetic) phase.
Finally  it is interesting to underline that in this sys- Both structures obtained in GGA and GGA+U belongs
tem  magnetism occurs in a ultraﬂat band  isolated from to the P¯3 space group (group number 147)  and the
all the others and having a marked dz2−r2 character on Wyckoﬀ positions are reported in Tab. III and Tab. IV
the central Nb atom in the star. Spin polarized calcu- respectively.
lations without any Hubbard mean ﬁeld term  recover
C=(0 0 0) multiplicity Wyckoﬀlabel x y z
withafairlysmallgap). AsimilareﬀectoccursinTaS24  Nb 1 a 0.00000 0.00000 0.00000
whereevenatU =0amagneticstateisstabilizedwithin Nb 6 g 0.28854 0.07046 0.00044
the spin polarized generalized gradient approximation. Nb 6 g 0.63662 0.15249 -0.00124
In this respect  sulﬁdes are odd with 1T-NbSe where
2 S 6 g 0.35363 0.25132 0.13360
the ﬂat band is strongly hybridized with Se states and
the Hubbard interaction is needed to√disenta√ngle it from S 2 d 0.33333 0.66667 0.88017
the other bands13. 1T-NbS in the 13× 13 is then S 6 g -0.02692 0.40822 0.11972
a prototype system where the presence of an ultraﬂat
of U/t. TABLE III. Wyckoﬀ positions of the 1T-NbS 13× 13
WethankG.Menichettiforusefulandstimulatingdis- APPENDIX B
This work was supported by French state funds man- We compute the phonon dispersion for the 13× 13
aged by the ANR within the Investissements d’Avenir CDWphaseinGGAapproximationforthenonmagnetic
program under references ANR-13-IS10-0003-01 ANR- (NM) and the ferrimagnetic (FM) solutions. We calcu-
11-IDEX-0004-02  and more speciﬁcally within the latethedynamicalmatrixatzonecenterandthenFourier
framework of the Cluster of Excellence MATISSE led by interpolate the dynamical matrices on the full Brillouin
Sorbonne Universit`e  by the European Graphene Flag- zone. Results are reported in Fig.7. All frequencies are
ship (GrapheneCore 2). Computer facilities were pro- positive revealing the CDW phase is dynamically stable.
vided by CINES  IDRIS  and CEA TGCC and PRACE The Raman active modes are reported in Tab. V. The
C=(0 0 0) multiplicity Wyckoﬀlabel x y z insurgenceofmagnetismcausesaweakhardeningofA2g
Nb 1 a 0.00000 0.00000 0.00000 frequenciesataround90and366cm−1 (weregistershifts
Nb 6 g 0.28891 0.07059 0.00036 of about 4÷5 cm−1  see Tab. V).
TABLE IV. Wyckoﬀ positions of the 1T-NbS 13× 13
CDW phase obtained in GGA+U (U=2.87 eV).
1 J. A. Wilson  F. J. Di Salvo  and S. Mahajan  Adv. in 20 P. Giannozzi  S. Baroni  N. Bonini  M. Calandra  R. Car
Phys. 24  117 (1975). C.Cavazzoni D.Ceresoli G.L.Chiarotti M.Cococcioni
2 Physica B+C 99  183 (1980). I.Dabo A.D.Corso S.deGironcoli S.Fabris G.Fratesi
3 P. Fazekas and E. Tosatti  Phyl. Mag. B 39  229 (1979). R. Gebauer  U. Gerstmann  C. Gougoussis  A. Kokalj
4 P. Darancet  A. J. Millis  and C. A. Marianetti  Phys. M. Lazzeri  L. Martin-Samos  N. Marzari  F. Mauri
Rev. B 90  045134 (2014). R. Mazzarello  S. Paolini  A. Pasquarello  L. Paulatto
5 A. S. Ngankeu  S. K. Mahatha  K. Guilloy  M. Bianchi  C. Sbraccia  S. Scandolo  G. Sclauzero  A. P. Seitsonen
C. E. Sanders  K. Hanﬀ  K. Rossnagel  J. A. Miwa  C. B. A. Smogunov  P. Umari  and R. M. Wentzcovitch  Jour-
Nielsen  M. Bremholm  and P. Hofmann  Phys. Rev. B nal of Physics: Condensed Matter 21  395502 (2009).
96  195147 (2017). 21 P.Giannozzi O.Andreussi T.Brumme O.Bunau M.B.
6 C. J. Carmalt  T. D. Manning  I. P. Parkin  E. S. Peters  Nardelli  M. Calandra  R. Car  C. Cavazzoni  D. Ceresoli
and A. L. Hector  J. Mater. Chem. 14  290 (2004). M. Cococcioni  N. Colonna  I. Carnimeo  A. D. Corso
7 K. S. Novoselov  D. Jiang  F. Schedin  T. Booth  V. V. S. de Gironcoli  P. Delugas  R. A. D. Jr  A. Ferretti
Khotkevich  S. Morozov  and A. K. Geim  PNAS 102  A. Floris  G. Fratesi  G. Fugallo  R. Gebauer  U. Ger-
10451 (2005). stmann  F. Giustino  T. Gorni  J. Jia  M. Kawamura
8 J. N. C. et al.  Science 331  568 (2011). H.-Y. Ko  A. Kokalj  E. Kkbenli  M. Lazzeri  M. Mar-
9 Y. Wang  J. Xiao  H. Zhu  Y. Li  Y. Alsaid  K. Fong  sili  N. Marzari  F. Mauri  N. L. Nguyen  H.-V. Nguyen
Y.Zhou S.Wang W.Shi Y.Wang A.Zettl E.J.Reed  A.O.de-laRoza L.Paulatto S.Ponc D.Rocca R.Saba-
and X. Zhang  Nature 550  487 (2017). tini B.Santra M.Schlipf A.P.Seitsonen A.Smogunov
10 G.Eda H.Yamaguchi D.Voiry T.Fujita M.Chen  and I. Timrov  T. Thonhauser  P. Umari  N. Vast  X. Wu
M. Chhowalla  Nano Letters 11  5111 (2011). and S. Baroni  Journal of Physics: Condensed Matter 29
11 Y. Nakata  K. Sugawara  R. Shimizu  Y. Okada  P. Han  465901 (2017).
T. Hitosugi  K. Ueno  T. Sato  and T. Takahashi  NPG 22 D. Vanderbilt  Phys. Rev. B 41  7892 (1990).
Asia Materials 8  e321 (2016). 23 H. J. Monkhorst and J. D. Pack  Phys. Rev. B 13  5188
12 F. Bischoﬀ  W. Auwa¨rter  J. V. Barth  A. Schiﬀrin  (1976).
M.Fuhrer  andB.Weber ChemistryofMaterials29 9907 24 M. Whangbo and E. Canadell  J. Am.. Chem. Soc. 114
(2017)  https://doi.org/10.1021/acs.chemmater.7b03061. 9587 (1992).
13 M. Calandra  Phys. Rev. Lett. 121  026401 (2018). 25 L. F. Mattheiss  Phys. Rev. B 8  3719 (1973).
14 X. Xi  L. Zhao  Z. Wang  H. Berger  L. Forro´  J. Shan  26 M. Calandra and F. Mauri  Phys. Rev. Lett. 95  237002
and M. K.F.  Nature Nanotechnology 10  765 (2015). (2005).
15 M. Ugeda  A. J. Bradley  Y. Zhang  S. Onishi  Y. Chen  27 E. Kamil  J. Berges  G. Schnhoﬀ  M. Rsner  M. Schler
W. Ruan  C. Ojeda-Aristizabal  H. Ryu  M. T. Edmonds  G. Sangiovanni  and T. O. Wehling  Journal of Physics:
H.Z.Tsai A.Riss S.K.Mo D.Lee A.Zettl Z.Hussain  Condensed Matter 30  325601 (2018).
S.Z.X.  andM.F.Crommie NaturePhys.12 92(2016). 28 D. Pasquier and O. V. Yazyev  Phys. Rev. B 98  045114
16 M. Calandra  I. Mazin  and F. Mauri  Phys. Rev. B 80  (2018).
241108(R) (2009). 29 F. Jellinek  G. Brauer  and H. Mu¨ller  Nature 185  376
17 M. Leroux  M. Le Tacon  M. Calandra  L. Cario  (1960).
M. M´easson  P. Diener  E. Borrissenko  A. Bosak  and 30 Y. Ge and A. Y. Liu  Phys. Rev. B 82  155133 (2010).
P. Rodi`ere  Phys. Rev. B 86  155125 (2012). 31 D. C. Miller  S. D. Mahanti  and P. M. Duxbury  Phys.
18 H. Lin  W. Huang  K. Zhao  C. Lian  W. Duan  X. Chen  Rev. B 97  045133 (2018).
and S. H. Ji  Nano Research 11  4722 (2018). 32 O.R.Albertini R.Zhao R.L.McCann S.Feng M.Ter-
19 R. M. Stan  S. K. Mahatha  M. Bianchi  C. E. Sanders  rones  J. K. Freericks  J. A. Robinson  and A. Y. Liu
D.Curcio P.Hofmann  andM.J.A. arXiv1901.03552 Phys. Rev. B 93  214109 (2016).
(2019). 33 A. Y. Liu  Phys. Rev. B 79  220515 (2009).
34 M. Cococcioni and S. de Gironcoli  Phys. Rev. B 71
FIG.7. Phonondispersionforthe 13× 13CDWphasein
thenonmagneticsolution(blackcurves)andintheferrimag-
Point ω ω
NM FM
group (cm−1) (cm−1)
E 63.9 61.6
g
A 64.6 64.6
A 82.1 83.2
E 84.4 88.6
E 93.3 91.5
A 94.2 97.9
E 106.0 105.4
E 119.5 120.2
A 121.2 121.7
A 142.9 146.3
A 164.0 166.7
E 179.7 180.4
A 211.1 211.1
E 218.8 218.8
A 221.3 221.6
E 226.9 227.1
E 236.2 236.3
E 242.2 242.4
A 243.7 243.6
E 257.7 258.6
E 261.6 261.7
A 262.3 262.7
A 266.8 266.7
E 268.1 268.6
E 276.7 276.9
A 283.9 283.6
A 286.0 285.5
A 298.7 298.7
A 303.4 303.2
E 307.5 306.9
E 346.5 346.5
A 349.9 351.7
E 357.5 357.9
A 358.9 359.1
A 366.7 371.0
E 378.4 379.0
A 392.8 394.1
E 393.0 395.1
TABLEV.Ramanactivefrequenciesforthe1T-NbS 13×
√ 2
Charge density wave and spin 1/2 insulating state in single layer 1T-NbS2.
In bulk samples and few layer ﬂakes  the transition metal dichalcogenides NbS2 and NbSe2 assume
single and few layers of 1T-NbSe2 with octahedral coordination around the transition metal ion were
synthesized. Motivated by these experiments and by using ﬁrst-principles calculations  we investigate
the structural  electronic and dynamical properties of single layer 1T-NbS2. We ﬁnd that single-layer
1T-NbS2 undergoes a
13×
approximation  the weak interaction between the stars leads to an ultraﬂat band at the Fermi level
total spin 1/2 magnetic state with opening of a 0.15 eV band gap and a 0.21µB magnetic moment
is enhanced to 0.41µB and a larger gap occurs. Most important  this approximation gives a small
energy diﬀerence between the 1T and 1H polytypes (only +0.5 mRy/Nb)  suggesting that the 1T-
polytype can be synthesized in a similar way as done for single layer 1T-NbSe2. Finally we compute
ﬁrst and second nearest neighbors magnetic inter-star exchange interactions ﬁnding J1=9.5 K and
I.
INTRODUCTION
form TCh2  where T is a transition metal and Ch is a
sizing diﬀerent polytypes having the same chemical for-
sition metal ion in diﬀerent polytypes of a given TMD
leads to completely diﬀerent physical properties1.
For
example  1T-TaS2 with Ta in octahedral coordination  is
bated (Mott insulator or correlated metal)2–5 while 2H-
TaS2  with Ta in trigonal prismatic coordination  is a
bulk 1T-TiSe2 that has never been synthesized in the 2H
polytype or  vice versa  of bulk 2H-NbSe2 and 2H-NbS2
one  although it has been reported6 that 1T-NbS2 bulk
Since diﬀerent TCh2 planes are bounded together by
dichalcogenides7 8 (a single layer here refers to a tri-
layer TCh2 unit).
The exfoliation of bulk TMDs into
low dimension  paves the way to diﬀerent synthesis tech-
niques  unﬁtted for bulk systems but feasible in few layers
ﬂakes. An example is the phase transition between the
hexagonal and monoclinic phases of monolayer MoTe2
achieved by electrostatic doping9 or the transition be-
tween the 2H and 1T and 1T′ phases obtained by liquid
More recently it has been shown that the 1T-NbSe2
of bilayer graphene kept at 500-590 ◦C during epitaxy11
or by applying a pulsed local ﬁeld through the STM tip
at the surface of bulk 2H-NbSe212.
The physical properties of single layer 1T-NbSe2
turned out to be completely diﬀerent from that of single
layer 1H-NbSe2 as the former is a spin 1/2 Mott-Jahn
Teller insulator undergoing a
13 ×
wave11 13  while the latter is a metal undergoing a 3 × 3
charge density wave14–16. Most important  it has been
recently shown13 that density functional theory calcula-
the stabilization of the 1T-NbSe2 single layer phase with
spectives oﬀered by these new synthesis techniques  the-
Bulk 2H-NbS2 is isoelectronic and isostructural to 2H-
NbSe2  however it stands somewhat at odd with respect
On the contrary  when NbS2 single-layer is grown on
bilayer graphene  it adopts the 1H-NbS2 polytype and
STM images show a 3 × 3 reconstruction18  but if 1H-
NbS2 is grown epitaxially on Au(111) no charge density
wave is detected19. Given the diﬀerent properties of NbS2
In this work  by using density functional theory cal-
arXiv:1902.00234v1  [cond-mat.str-el]  1 Feb 2019
layer 1T-NbS2 together with its structural  vibrational
NbS2 single layer. The local coordination around a transition
II.
COMPUTATIONAL DETAILS
using the Quantum-Espresso code20 21.
For Nb
distributions22 including semicore states and two projec-
tors for s and p channels and valence conﬁguration 4s2
4p6  4d5  5s1 (5s2  6s2  5p6  5d3).
For S (Se) we use
valence and the following valence conﬁguration 3s2  3p3
3d0 (4s2  4p3  4d0).
We use an energy cutoﬀ up to 45 Ry (540 Ry for the
(BZ) is performed using an uniform 20×20×1 Monkhorst
and Pack grid23 for the 1T and 2H-polytypes (6 × 6 × 1
and 5 × 5 × 1 for the
13 and 4 × 4 CDW phases
respectively) and a 0.01 Ry Gaussian smearing. For the
total energy comparison among magnetic solutions of the
13 reconstruction we reduce the smearing to
0.0001 Ry increasing the BZ grid to 12 × 12 × 1 (for
by considering a supercell with about 10 ˚A of vacuum
along the c-axis between the periodic images.
We use
dom.
Phonon modes in the undistorted 1T-phase are
20 × 20 × 1 reciprocal space mesh for sampling the elec-
III.
RESULTS AND DISCUSSION
A.
High-symmetry 1T-NbS2 structure.
undistorted 1T-NbS2 structure (3 atoms/cell). For com-
tronic structures of 1T-NbSe2  1T-TaS2 and 1T-TaSe2
chalcogen height (hCh) and a smaller tetragonal distor-
dral crystal ﬁeld around the transition metal and in part
In more details  the octahedral crystal ﬁeld splitting
leads to triply degenerate t2g orbitals (dx2−y2  dz2−r2
dxy) and doubly degenerate eg orbitals (dxy  dxz) at
Ref.25 for the crystal axes). The trigonal distortion of
the octahedron is identiﬁed by the bond angles centered
octahedron α = β = 90o  while in the present cases there
The crystal ﬁeld for a trigonal distorted octahedron
splits the t2g orbitals in a twofold degenerate state
(dx2−y2  dxy) and a single degenerate dz2−r2 state. We la-
bel this energy separation at zone center “apparent Jahn-
Teller splitting”. In the case of 1T-NbS2 and 1T-TaS2
the apparent trigonal Jahn-Teller splitting at the Γ point
is positive (namely the band originating from the dz2−r2
System
a (˚A) hCh (˚A) α (deg) β (deg)
NbS2
3.35
1.55
94.7
85.3
NbSe2
3.4813
1.69
97.5
82.5
TaS2
3.38
1.53
94.0
86.0
TaSe2
3.50
1.65
95.7
84.3
TABLE I. Theoretical internal coordinates for 1T-TCh2 systems. In the inset image we report in gray the T atoms and in
(a) 1T-NbS2 {dz2−r2(Γ) =47%}
(b) 1T-NbSe2 {dz2−r2(Γ) =54%}
(c) 1T-TaS2 {dz2−r2(Γ) =46%}
(d) 1T-TaSe2 {dz2−r2(Γ) =54%}
FIG. 2. Band structures for TCh2 compounds (T=Nb Ta; Ch=S Se) in the 1T-polytype. The size of the circles is proportional
to the Nbdz2−r2 character of the eigenvalues  the percentage of the dz2−r2 component at Γ for each system is reported in the
generate one arising from the dx2−y2 and dxy atomic or-
ering the hybridization between the transition metal t2g
center (labeled “h” in Fig.2). In sulﬁdes  this band is
mainly formed by sulfur 3p states. In TaS2 this sepa-
ration is larger than in NbS2  mainly due to the larger
energy misalignment between the sulfur 3p and the Nb
4d or Ta 5d states. The situation is very diﬀerent in se-
and the Ta or Nb d states is strong (stronger in Nb than in
∆E
∆EU
a
aU
1H
0.0
3.346
3.326
1T
+7.2 +4.2
3.360
3.357
4×4
+5.8* +1.9 13.485* 13.428
+4.3 +1.2 12.200 12.123
TABLE II. Calculated energy diﬀerence among diﬀerent poly-
types and CDW phases.
The energy diﬀerences are in
lattice constants with and without U (a and aU respectively)
are reported in ˚A  U = 2.87eV for all the calculations. The
experimental in-plane lattice parameter for bulk 2H-NbS2 is
3.31˚A29. The (*) means we have two diﬀerent CDWs  prac-
respect to crystal ﬁeld theory. For example  the larger
octahedral distortion occurs in NbSe2  but here we ﬁnd
an apparent negative Jahn-Teller splitting.
Finally  in
TaSe2 the apparent Jahn-Teller splitting is almost zero
as the crystal ﬁeld and the hybridization perfectly cancels
out and the t2g bands become almost threefold degener-
the chalcogen p-bands in sulﬁdes becomes mixed with
d-states in selenides (particularly evident in NbSe2).
The diﬀerent magnitude of the hybridization explains
why in sulﬁdes one expect t2g manifolds separated by the
tude of relativistic eﬀects in 1T-NbS2 ﬁnding them neg-
NbS2 with the 1H-NbS2 polytype. We ﬁnd that 1H is
more stable by approximately 7.2 mRy/Nb (see also Tab.
II)  similarly to what happens for the NbSe2 case26–28.
This large energy diﬀerence prevents an highly symmet-
NbS2.
As shown in Fig.3 we found strongly unstable
have wavevector compatible with a
1330 and a
4 × 4 CDWs.
B.
Charge density wave phases
approximation in 4 × 4 and
13 supercells start-
ing from initial conﬁgurations obtained by displacing the
stable phonon modes. In both case  we ﬁnd structures
metric ones. In the case of a 4 × 4 supercell  we ﬁnd two
diﬀerent reconstructions that are practically degenerate
in energy (see Fig.4). Both 4 × 4 CDW  however  seems
mation. This is conﬁrmed by the fact that
of ≈ 2.9 mRy/Nb with respect to the highly symmet-
ric 1T-NbS2 phase  however still with an energy loss of
≈ 4.3 mRy/Nb with respect to the highly symmetric 1H-
NbS2 phase.
The optimized
energy diﬀerences among the diﬀerent phases considered
The non-magnetic electronic structure in the
by the presence of an extremely ﬂat band at the Fermi
level having a non-negligible dz2−r2 character related to
the central Nb atom in the star. The ﬂat band is isolated
this is in analogy with what happens in 1T-TaS24 30–33
and in contrast with the 1T-NbSe2 case13 27 28 where the
ﬂat band is entangled with chalcogen states.
the non-magnetic electronic dispersions of (i) NbS2 CDW
keeping  however  the structure unchanged (labeled “1T-
NbS2 str.; Se PP” in Fig.5)  (ii) NbS2 using the crystal
structure of 1T-NbSe2 in the CDW phase (labeled “1T-
NbSe2 str.; S PP” in Fig.5). These calculations should
be directly compared with the case of NbSe2 reported in
Ref.13 (Fig.3  left panel) where  at the GGA level  the
ﬂat band lies in the middle of Se states and is not iso-
be seen the eﬀect of replacing the S with a Se pseudopo-
undistorted 1T-NbS2/1T-NbSe2 phases.
However this
up-shift is still not large enough to mix the ﬂat band with
the other occupied bands  as it happens in the
phase of 1T-NbSe213.
If  on the contrary  we use the
mostly the occupied states very close to the ﬂat band.
However  this is not what happens in the
phase of 1T-NbSe2 as in this system  at the GGA level
the top of the Se states at Γ are empty and are at higher
FIG. 3. Left: phonon dispersion along high-symmetry directions of the high-symmetry 1T-NbS2 phase. The projections of the
ordering vectors related to the 4 × 4 and to the
13 CDWs onto the Γ-M line are marked with vertical lines. Right:
Distribution of negative phononic frequencies in the BZ for undistorted 1T-NbS2. The wave vectors belonging to the 4 × 4 and
to the
13 CDWs are marked with crosses of diﬀerent colors
FIG. 4. Left: Optimized crystal structure for the
central Nb in the star (red)  Nb belonging to the peripheral atoms of the
7 ×
in gray. S atoms are shown in yellow. Center and right: Optimized crystal structure for the 4 × 4 CDW. The orange lines are
energies then the ﬂat band.
It follows that the eﬀect
cooperative eﬀect of the two. This aspect is a general
also in TaS2 and TaSe2 (see Refs.4 13 31).
As shown in Fig.5 (top)  the Nbdz2−r2 band is ex-
tremely ﬂat  with a dispersion of ∼ 0.016 eV. This implies
can see  in the ferrimagnetic conﬁguration  the system
The ﬂat band is splitted in two  one is fully occupied and
ment is 1 µB. By referring to the left panel in Fig.4  the
magnetic moments are 0.21 µB on the red sites  0.04 µB
ing of some A2g modes  in principle detectable as a Ra-
mining total energies  as shown in 1T-NbSe213  we per-
The U parameter is computed self-consistently from ﬁrst
principles34  we obtain U = 2.87 eV. This value is similar
to those found for 1T-TaS24 and 1T-NbSe213 compounds.
We ﬁrst perform structural optimization of the high-
symmetry 1T-NbS2 and 1H-NbS2 single-layer structure
within DFT+U. For the 1H-polytype  we ﬁnd that the
NbSe213. As it can be seen  the energy diﬀerence between
FIG. 5. Electronic structure of 1T-NbS2 in the
CDW phase (top panel) and of 1T-NbS2 in the CDW phase
of Se (central panel). Electronic structure of 1T-NbS2 using
lattice parameters and internal coordinates of NbSe2
is proportional to the dz2−r2 character of the central Nb in
the star (the percentage of the central Nb dz2 component at
Γ at the Fermi level are 11% (top)  13% (central) and 9.6%
tice constants (see Tab. II  the Wyckoﬀ positions are re-
diﬀerent from GGA results: a stronger ferrimagnetic so-
lution with magnetic moments of 0.41 µB on the red sites
0.03 µB on the blue ones and negligible antiferromagnetic
magnetic moment per unit cell is of 1 µB. The magnetic
is of course still 1/2). Moreover  as shown in Tab. II  the
found between 1H-NbSe2 highly symmetric polytype and
the 1T-NbSe2 charge density wave phase. This suggest
that 1T-NbS2 can be synthesized with a similar experi-
mental procedure to the one used for NbSe211 12.
conduction bands.
The fundamental gap is ∼0.35 eV
Finally we evaluate the nearest-neighbor (J1) and
next-near-neighbor (J2) exchange constants between dif-
ˆH = −J1
�
<i j>
ˆ
Sz
i · ˆ
j − J2
<<i j>>
j .
(2
13) × (3
13) cell with diﬀerent collinear magnetic
describing the similar NbSe2 compound28.
From that
between diﬀerent stars.
IV.
CONCLUSIONS
In this work we investigated by ﬁrst principles the pos-
sible formation of single-layer 1T-NbS2 as well as its
We demonstrate that the 1T undistorted (1 × 1) poly-
type is highly unstable towards a
tion. Within the GGA+U approximation  the
state cooperate in stabilizing the 1T-NbS2 phase in single
FIG. 6. Spin resolved electronic band structures for the
approximation (right panel); solid (dotted) lines are related to majority (minority) spin states.
The size of the circles is
proportional to the central Nb dz2−r2 character of the eigenvalues (the percentage of the central Nb dz2 component for the ﬂats
bands at Γ are 9.5% for the majority (12% for the minority) component in the GGA approximation and 6.2% for the majority
for single layer 1T-NbSe211 12. Interestingly  a previous
work6 describes growth of bulk 1T-NbS2 on glass keep-
technique has been used for the synthesis of 1T-NbSe2
(with a diﬀerent substrate) in Ref.
11  this makes the
synthesis of single layer 1T-NbS2 even more likely.
tem  magnetism occurs in a ultraﬂat band  isolated from
all the others and having a marked dz2−r2 character on
with a fairly small gap). A similar eﬀect occurs in TaS24
where even at U = 0 a magnetic state is stabilized within
In this respect  sulﬁdes are odd with 1T-NbSe2 where
the other bands13. 1T-NbS2 in the
of U/t.
aged by the ANR within the Investissements d’Avenir
11-IDEX-0004-02
and more speciﬁcally within the
Sorbonne Universit`e  by the European Graphene Flag-
ship (GrapheneCore 2).
Computer facilities were pro-
vided by CINES  IDRIS  and CEA TGCC and PRACE
(2017174186).
We report the relaxed Wyckoﬀ positions for the low
temperature
to the P¯3 space group (group number 147)  and the
Wyckoﬀ positions are reported in Tab. III and Tab. IV
C=(0 0 0) multiplicity Wyckoﬀ label
x
y
z
Nb
0.00000 0.00000 0.00000
0.28854 0.07046 0.00044
0.63662 0.15249 -0.00124
S
0.05083 0.17491 0.13591
0.35363 0.25132 0.13360
0.48561 0.19908 0.87839
d
0.33333 0.66667 0.88017
-0.02692 0.40822 0.11972
TABLE III. Wyckoﬀ positions of the 1T-NbS2
We compute the phonon dispersion for the
0.28891 0.07059 0.00036
0.63634 0.15237 -0.00097
0.05072 0.17451 0.13662
0.35369 0.25129 0.13419
0.48561 0.19901 0.87758
0.33333 0.66667 0.87948
-0.02695 0.40829 0.12062
TABLE IV. Wyckoﬀ positions of the 1T-NbS2
insurgence of magnetism causes a weak hardening of A2g
frequencies at around 90 and 366 cm−1 (we register shifts
of about 4 ÷ 5 cm−1  see Tab. V).
1 J. A. Wilson  F. J. Di Salvo
and S. Mahajan  Adv. in
2 Physica B+C 99  183 (1980).
3 P. Fazekas and E. Tosatti  Phyl. Mag. B 39  229 (1979).
4 P. Darancet  A. J. Millis
and C. A. Marianetti  Phys.
5 A. S. Ngankeu  S. K. Mahatha  K. Guilloy  M. Bianchi
C. E. Sanders  K. Hanﬀ  K. Rossnagel  J. A. Miwa  C. B.
Nielsen  M. Bremholm
and P. Hofmann  Phys. Rev. B
6 C. J. Carmalt  T. D. Manning  I. P. Parkin  E. S. Peters
7 K. S. Novoselov  D. Jiang  F. Schedin  T. Booth  V. V.
Khotkevich  S. Morozov
and A. K. Geim  PNAS 102
8 J. N. C. et al.  Science 331  568 (2011).
9 Y. Wang  J. Xiao  H. Zhu  Y. Li  Y. Alsaid  K. Fong
10 G. Eda  H. Yamaguchi  D. Voiry  T. Fujita  M. Chen  and
11 Y. Nakata  K. Sugawara  R. Shimizu  Y. Okada  P. Han
T. Hitosugi  K. Ueno  T. Sato
and T. Takahashi  NPG
12 F. Bischoﬀ  W. Auw¨arter  J. V. Barth  A. Schiﬀrin
13 M. Calandra  Phys. Rev. Lett. 121  026401 (2018).
14 X. Xi  L. Zhao  Z. Wang  H. Berger  L. Forr´o  J. Shan
15 M. Ugeda  A. J. Bradley  Y. Zhang  S. Onishi  Y. Chen
16 M. Calandra  I. Mazin
and F. Mauri  Phys. Rev. B 80
17 M.
Leroux
M.
Le
Tacon
Calandra
L.
Cario
M. M´easson  P. Diener  E. Borrissenko  A. Bosak
and
P. Rodi`ere  Phys. Rev. B 86  155125 (2012).
18 H. Lin  W. Huang  K. Zhao  C. Lian  W. Duan  X. Chen
19 R. M. Stan  S. K. Mahatha  M. Bianchi  C. E. Sanders
(2019).
20 P. Giannozzi  S. Baroni  N. Bonini  M. Calandra  R. Car
21 P. Giannozzi  O. Andreussi  T. Brumme  O. Bunau  M. B.
22 D. Vanderbilt  Phys. Rev. B 41  7892 (1990).
23 H. J. Monkhorst and J. D. Pack  Phys. Rev. B 13  5188
24 M. Whangbo and E. Canadell  J. Am.. Chem. Soc. 114
25 L. F. Mattheiss  Phys. Rev. B 8  3719 (1973).
26 M. Calandra and F. Mauri  Phys. Rev. Lett. 95  237002
27 E. Kamil  J. Berges  G. Schnhoﬀ  M. Rsner  M. Schler
G. Sangiovanni
and T. O. Wehling  Journal of Physics:
28 D. Pasquier and O. V. Yazyev  Phys. Rev. B 98  045114
29 F. Jellinek  G. Brauer
and H. M¨uller  Nature 185  376
30 Y. Ge and A. Y. Liu  Phys. Rev. B 82  155133 (2010).
31 D. C. Miller  S. D. Mahanti
and P. M. Duxbury  Phys.
32 O. R. Albertini  R. Zhao  R. L. McCann  S. Feng  M. Ter-
rones  J. K. Freericks  J. A. Robinson
and A. Y. Liu
33 A. Y. Liu  Phys. Rev. B 79  220515 (2009).
FIG. 7. Phonon dispersion for the
Point
ωNM
ωF M
Eg
63.9
61.6
Ag
64.6
82.1
83.2
84.4
88.6
93.3
91.5
94.2
97.9
106.0
105.4
119.5
120.2
121.2
121.7
142.9
146.3
164.0
166.7
179.7
180.4
211.1
218.8
221.3
221.6
226.9
227.1
236.2
236.3
242.2
242.4
243.7
243.6
257.7
258.6
261.6
261.7
262.3
262.7
266.8
266.7
268.1
268.6
276.7
276.9
283.9
283.6
286.0
285.5
298.7
303.4
303.2
307.5
306.9
346.5
349.9
351.7
357.5
357.9
358.9
359.1
366.7
371.0
378.4
379.0
392.8
394.1
393.0
395.1
TABLE V. Raman active frequencies for the 1T-NbS2"
R028,0,,"method to reduce the complexity of multi-way fuzzy decision
trees in Big Data classiﬁcation problems. The proposed algorithm
builds a ﬁxed number of fuzzy sets for all variables and adjusts
their shape and position to the real distribution of training
data. A two-step process is applied : 1) transformation of the
original distribution into a standard uniform distribution by
means of the probability integral transform. Since the original
distribution is generally unknown  the cumulative distribution
function is approximated by computing the q-quantiles of the
training set; 2) construction of a Ruspini strong fuzzy partition
in the transformed attribute space using a ﬁxed number of
equally distributed triangular membership functions. Despite the
aforementioned transformation  the deﬁnition of every fuzzy set
in the original space can be recovered by applying the inverse cu-
mulative distribution function (also known as quantile function).
The experimental",":
v Decision trees (DTs) [1] are popular non-parametric su- In the context of Big Data  the excessive time and space
i pervised machine learning tools used for classiﬁcation and
X requirements of FDTs seriously affect the scalability of these
algorithms.Segatorietal.cameupwithaMapReducesolution
r
a of problems such as ﬁnance [2]  image classiﬁcation [3]  consisting of a new fuzzy partitioning method (discretizer)
medicine[7].ThemainfeatureofDTsistheabilitytoexplain
(e.g. is x > 0.5?)  each branch represents the answer or the
downstrategycalledrecursivepartitioning[1] inwhichinput
can apply either a brute-force search to test all possible cut
thatreducesthecomplexityoftreesconstructedbytheFMDT
(cid:13)c2018IEEE.Personaluseofthismaterialispermitted.PermissionfromIEEEmustbeobtainedforallotheruses inanycurrentorfuturemedia includingreprinting/republishingthismaterialforadvertisingor
promotionalpurposes creatingnewcollectiveworks forresaleorredistributiontoserversorlists orreuseofanycopyrightedcomponentofthisworkinotherworks.Citationinformation:DOI
10.1109/BigDataCongress.2018.00011 2018IEEEInternationalCongressonBigData(BigDataCongress)
scheme  in terms of both the number of fuzzy sets used per b) Partitioning: a target reducer is selected for each
variable and the number of leaves. The proposed algorithm key.
applies the probability integral transform [13]  [14] to adjust c) Shufﬂe:previousintermediatedataiscopiedtothe
a ﬁxed number of fuzzy sets to the real distribution of the reducers.
training data. This transformation allows the algorithm to 2) Reduce stage: the reducer is responsible for aggregating
convert the variables of the training set into (approximately) the outputs of the mappers when they all have ﬁnished.
uniform random variables regardless of their original distri- To this end  all the key-value pairs received from the
bution. Next  the Ruspini strong fuzzy partitions [15] are mappers are sorted and merged by key. Then  the re-
built in the new transformed dataset using equally distributed duce() function (deﬁned by the user) is called for every
triangular membership functions. The resulting fuzzy sets are single key  where all its values are aggregated. Finally
then used by the original FMDT to construct the tree. the reducer returns the ﬁnal result for each key.
Inordertoassessthebeneﬁtsofourproposal wecarriedout Spark [18] was introduced as a generalization and an
an empirical study using 4 Big Data classiﬁcation problems extension of the MapReduce paradigm. It is built around the
availableatUCI[16]andOpenML1repositories.Wecompared concept of Resilient Distributed Datasets (RDDs) [19]  which
the accuracy rate and the model complexity of FMDT when represent distributed immutable data (partitioned data) and
usingtheoriginalandtheproposedfuzzypartitioningmethods. lazily evaluated operations (transformations). The execution
Theexperimentalresultsshowasigniﬁcantreductioninmodel of a user-deﬁned algorithm consists of a sequence of stages
complexity when applying our strategy. composed of a number of transformations that are split into
This paper is organized as follows. Section II recalls the tasks. One stage consists only of transformations that do
basics of the MapReduce algorithm and the Apache Spark not require any shufﬂing/repartitioning process (e.g.  map
framework and brieﬂy describes the distributed solution of and ﬁlter operations). Tasks are executed by the so-called
Segatori et al. to build FDTs for Big Data. In Section III executors  which represent independent processes in the Java
we introduce the proposed fuzzy partitioning method. The Virtual Machine (JVM) of a worker node. Finally  the result
experimental framework and the analysis of the results are of all transformations is obtained by calling an action that
shown in Sections IV and V  respectively. Finally  Section VI computes and returns the result to the driver node. This data
contains concluding remarks. ﬂowallowstheusertorunanindeﬁnitenumberofMapReduce
jobswithinthesamemainprogram supportingawidevariety
II. PRELIMINARIES of algorithms and methods.
II-A)andwebrieﬂydescribethedistributedsolutionpresented
learningalgorithmsusedforbothclassiﬁcationandregression.
basisofatrainingsetcontainingpreviouslylabeledexamples.
MapReduceisaprogrammingparadigm[17]forprocessing
Each example x=(x  ... x ) contained in the training set
1 F
large-scale datasets in a distributed fashion. It is composed of TR belongs to a class y ∈ C = {C  ... C } (M being the
1 M
twostagescalledMapandReduce whichareexecutedbythe
numberofclassesoftheproblem)andischaracterizedbyaset
so-called mappers and reducers  respectively:
of F variables (also called attributes or features)  where each
1) Map stage: input data is partitioned into several logical variable x can take on any value contained in the set F .
f f
splits that are associated with certain physical blocks Therefore  the construction of a classiﬁer consists in ﬁnding a
(preferably with local ones  in favor of data locality). decision function h:F ×...×F →C that maximizes the
Each split is then processed by a single mapper on a classiﬁcation accuracy.
givencomputingnode.Themappertransformstheinput A DT is a directed acyclic graph where each internal node
data into multiple key-value pairs and calls the map() is a test on an attribute  each branch represents the outcome
function (deﬁned by the user) for each pair. The result of the test  and each terminal node (or leaf) contains the ﬁnal
of this function is another key-value pair that is part of decision (class label). DTs are usually built by applying a
theso-calledintermediatedata.Finally thisintermediate top-downrecursivepartitioning[1]oftheattributespace.The
data is prepared to be sent to the reducers by applying selection of the attribute considered in the decision node is
the following operations: based on metrics that measure the difference between the
a) Sorting and Merging: outputs are sorted by key level of homogeneity of the class labels contained in the
and all the values corresponding to the same key parentandchildnodes.Forcontinuousattributes eitherbrute-
are merged in a list of values. force solutions or discretization strategies can be applied. The
formertestallthepossiblecutpointsinthetrainingset while
1https://www.openml.org/search?type=data the latter divide the attribute domain into a discrete set of
intervals (also called bins). Since brute-force strategies are fuzzy partition that minimizes the fuzzy entropy and
computationallyheavy theDTsdesignedforBigDatausually splitstheattributedomainintotwosubsetsinarecursive
apply discretization methods to speed up the algorithm and fashion  until a stopping condition is met. Although
reduce the model complexity. accurate  the partitions built by this methodology might
Fuzzy decision trees (FDTs) [9]  [10] make use of fuzzy contain many fuzzy sets per variable  increasing the
logic[8]tobetterdealwithuncertaintyandcreatesoftdecision complexity of the model.
boundaries that improve classiﬁcation performance. FDTs use 2) FDT learning. An FDT is built by applying one of the
fuzzy partitions to characterize continuous attributes instead two splitting strategies considered by the authors: the
of considering a discrete set of intervals. As a consequence  a binary (or two-way) FDT (FBDT)  which always gen-
giveninputvaluemightbelongtooneormorefuzzysetswith erates two child nodes  or the multi-way FDT (FMDT)
a certain membership degree and activate multiple branches which might create more than two child nodes. Another
at the same time. Fuzzy partitions allow FDTs to handle difference is that in FBDTs an attribute can appear
smooth transitions between adjacent intervals in continuous several times in the same path. Both methods use the
attributes whichmightleadtomoreaccuratepredictionswhen fuzzy information gain [21] for the attribute selection.
handlingnumericdata.Whenclassifyinganewexamplex the In this work we focus on FMDTs.
strength of activation of each leaf (called matching degree) is
thatconsidersx asthesplittingattribute thematchingdegree
f
mdCN(x) between x and CN is computed as:
mdCN(x)=T (cid:0)µCN(x ) mdPN(x)(cid:1)  (1) III. APPLYINGTHEPROBABILITYINTEGRALTRANSFORM
f TOREDUCETHECOMPLEXITYOFMULTI-WAYFUZZY
where T is a T-norm  µCN(xf) is the membership degree DECISIONTREES
of x to the fuzzy set associated with the node CN  and
mdPN(x) is the matching degree between x and the parent Inthisworkweproposeanewdistributedfuzzypartitioning
node PN. Next  the association degree ADLN(x) of x with method that reduces the complexity of FDTs generated by the
the class C at the leaf node LN is calculated as: FMDT algorithm presented in [12]. The proposed solution
ADLN(x)=mdLN(x)·wLN  (2)
m m without altering the FDT learning algorithm. The goals of our
where mdLN(x) is the matching degree between x and the approach are the following:
leaf node LN and wLN is the class weight associated with
m • To build a few fuzzy sets per attribute. The original
C atLN.DifferentdeﬁnitionshavebeenproposedforwLN
m m method adds fuzzy sets to the fuzzy partition until the
(cid:88) mdLN(x) increasing the complexity of the model. Our approach
wmLN = x∈(cid:88)TRCmmdLN(x)   (3) • To adjust the fuzzy sets to the real distribution of the
attributes.Theproposedsolutionmodiﬁesboththeshape
x∈TR andthepositionofthefuzzysetstoenhancethediscrim-
where TRCm is the set of all training examples belonging ination capability of the model.
to the class C . Finally  the class label of x is predicted
• Maximum matching: the class corresponding to the max-
imum association degree is returned. • Pre-processing: the variables of the training set are
• Weighted vote: the sum of all association degrees is converted into standard uniform random variables by
computed for each class. The one getting the maximum applying the probability integral transform theorem [13]
sum is predicted. [14]  described in Theorem 1. This theorem states that
problems. In this work  we consider the distributed solution Theorem 1. If X is a continuous random variable with
proposedbySegatorietal.in[12]tobuildFDTsforBigData  cumulative distribution function (CDF) F (x) and if
which comprises two stages: Y = F (X)  then Y is a uniform random variable on
1) Fuzzypartitioning.Astrongtriangularfuzzypartitionis the interval [0 1].
constructedforeachcontinuousattributebasedonfuzzy
entropy. To this end  the algorithm selects the candidate 2https://github.com/BigDataMiningUnipi/FuzzyDecisionTreeSpark
Proof. SupposethatY =g(X)isafunctionofX where 1
)s
ggan−ids1diFutisnfYfideq(ryeuen)ensl==tyiitayebPFxilXserisotag(cid:0)sbnig.v(d−TYe1snht(e≤rybicC)yyt(cid:1)lD)yF=inocPfrerYaosbcina(cid:0)gnX.bT≤ehudgse− ri1ivt(seydi)n(cid:1)uvseirnsge )srab( seulav fo ycneuqerFenil dilos( seerged pihsrebm0000....2468
e
d d M
f (y)= F (y)= F (g−1(y)) 0
Y dy Y dy X 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Value
=f (g−1(y))· d g−1(y). (a)Transformedspaceofeveryattribute
X dy 1
TthheisdFipsYrtro(iycbe)udt==iuornPFeXroisof(cid:0)bcYF(aXY−llteo1≤d(ybte)yh(cid:1))ed==eCriyDvPeFrdotbaes(cid:0)cXhfonli≤lqouwFesX−:an1d(ya)(cid:1)llow(cid:4)s )srab( seulav fo ycneuqerFnil dilos( seerged pihsrebm0000....2468
M
However  since the original distribution of the training 0
-2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5
set is unknown  we cannot compute the exact CDF. Value
Instead  we propose computing the q-quantiles of the (b)Originalspaceoftheattributejet 1 eta
training set to obtain an approximate CDF. To this end  )s
tsfqelhiomxuneraaeamnalﬁelrtpearillsyrcleethistinqhsivtuaneanearrxtnphitttaohreibllaeeatlcretteaﬁg edirdrnase.iotlanlnItgqfetturshqhaeeetnthi stiaivntlnhsateemelutr(hvaCeQelasDll1evF)r[aaQrtlooehuirf−aesan.g1o rcIrtefQethaerettdtihea]nr einauQtnmvhvdiaaablbnlueueeeraeitnchoiihgessf )srab( seulav fo ycneuqerFnil dilos( seerged pihsrebm0000....2468
lastquantile(Qq−1) theCDFis0or1 respectively.This eM
way  the new transformed dataset will be approximately -1.5 -1 -0.5 0 0.5 1 1.5
uniformregardlessoftheoriginaldistribution.Ofcourse  Value
(c)Originalspaceoftheattributejet 1 phi
interpolating the CDF using the quantiles extracted from Fig.1:Fuzzysetsbuiltforjet 1 etaandjet 1 phionHIGGS.
• Partitioning: a Ruspini strong fuzzy partition [15] is
triangularmembershipfunctionsacrosstheinterval[0 1].
method. The whole pipeline is written in Scala 2.113 on top
two closest quantiles by computing the inverse of the IV. EXPERIMENTALFRAMEWORK
linearfunctionusedtocomputetheCDF.Figure1shows In this section  we ﬁrst describe the datasets and perfor-
an illustrative example of how fuzzy sets are distributed mance metrics used to evaluate the methods considered in the
in the original and transformed spaces of the attribute experiments (Section IV-A). Next  we specify the parameters
jet 1 eta and jet 1 phi of HIGGS. Solid lines and bar and the environment conﬁguration used for the executions of
plotsrepresentthemembershipfunctionsofthefuzzysets the algorithms (Section IV-B).
andtheoriginaldistributionofthevariables respectively.
closelyinterrelated.Giventhat fromourpointofview aRus-
pinistrongfuzzypartitionwithequallydistributedmembership 3http://www.scala-lang.org/
functions is a suitable way to model a uniform distribution  4https://github.com/melkano/uniform-fuzzy-partitioning
and OpenML5 repositories. Table I shows the description theAreaUndertheROCCurve(AUC)[24] whicharedeﬁned
of the datasets indicating the number of instances (#In- as:
TP +TN
stances)  real (R)/integer(I)/categorical(C)/total(T) attributes Accuracy rate= (8)
TP +FN +FP +TN
(#Attributes) andclasses(#Classes).ThenamesofBNGAus-
tralian (BNG) and HEPMASS (HEPM) have been shortened. 1+TP +FP
AUC = rate rate (9)
All the experiments were carried out using a 5-fold stratiﬁed 2
cross-validation scheme. To this end  we randomly split the
it. Therefore  the result of each dataset was computed as the • Measuretocomputetheimpurityofnodes:fuzzyentropy
average of the ﬁve partitions. • T-norm: product
• Maximum number of bins for numeric attributes: 32
TABLE I: Description of the datasets. • Maximum depth of the tree: 5
• γ = 0.1%; φ = 0.02 · N; λ = 10−4·N
R I C T composed of 6 slave nodes and a master node connected via
BNG 1 000 000 8 6 0 14 2 1Gb/s Ethernet LAN network. Half of the slave nodes have
composedofanIntelXeonE5-2609processorwith4physical
calledconfusionmatrix(TableII) whichstoresthenumberof
coresat2.4GHz.Allslavenodesareequippedwith64GBof
TABLE II: Confusion matrix for a binary problem. use Hard Disk Drives featuring a read/write performance of
Positiveprediction Negativeprediction
Positiveclass TruePositive(TP) FalseNegative(FN) pactonruntimeswhensettingtheconﬁgurationrecommended
Negativeclass FalsePositive(FP) TrueNegative(TN) by the authors. Consequently  the number of cores used in
• True positive rate: percentage of correctly classiﬁed pos-
whileminimizingmemoryreplicationoverhead(e.g.broadcast
TP
TP = (4)
rate TP +FN V. EXPERIMENTALSTUDY
• True negative rate: percentage of correctly classiﬁed In order to assess the performance of our approach  we
negative examples. carried out an empirical study covering three aspects: clas-
TN
TN = (5) IV)  and runtimes (Table V). In all cases we consider four
rate TN +FP
• False positive rate: percentage of misclassiﬁed negative [12]andthreedifferentconﬁgurationsoftheproposedmethod
examples. that differ in the number of fuzzy sets (X) used for numeric
FP
attributes (denoted as FMDT ). We must point out that the
FP = (6) X
rate FP +TN original FMDT ran out of memory while tackling HEPMASS
• False negative rate: percentage of misclassiﬁed positive due to the excessive number of leaves built during training
examples. and thus no results are given for this method on HEPMASS.
FN Tables III and IV reveal that the proposed fuzzy partition-
FN = (7)
rate FN +TP ing method (FMDT ) is able to maintain the classiﬁcation
Basedonthesemetrics theclassiﬁcationperformanceofeach
5https://www.openml.org/search?type=data for HIGGS)  although there is a positive trend in favor of
oftencausesthelearningalgorithmtobuildmoreleaves which
increases the model complexity. Next  we analyze the results TABLE IV: Complexity of each model.
• BNG: the proposed method improves the accuracy rate Numberofleaves
and the AUC of FMDT by 6% and 8%  respectively. Dataset FMDT FMDT5 FMDT7 FMDT9
AlthoughthetreesbuiltbyFMDT aredeeper theyhave
X BNG 83 044 1 211 4 807 9 492
• HEPM: the original FMDT builds too many leaves to
IV-Bandranoutofmemoryduringtheexperiments.This
fact suggests that our approach is a potential solution to Avg.depth
avoid the explosion in the number of leaves during the Dataset FMDT FMDT5 FMDT7 FMDT9
• HIGGS:theclassiﬁcationperformanceofFMDT5 onthis HEPM - 4.52 4.03 3.93
rest of conﬁgurations (FMDT and FMDT ) are able to Avg.numberoffuzzysets
7 9
maintain the classiﬁcation performance of FMDT with Dataset FMDT FMDT5 FMDT7 FMDT9
as many fuzzy sets as FMDT .
7
• SUSY: all the conﬁgurations perform similarly to FMDT
methodleadstosimplertreescomposedof3K 15K and
fuzzy sets on average for each attribute. TABLE V: Runtimes(s) of each method.
TABLE III: Classiﬁcation performance of each method. Partitioning
Dataset FMDT FMDT5 FMDT7 FMDT9
Accuracyrate%
Dataset FMDT FMDT5 FMDT7 FMDT9 HEPM - 295 292 294
BNG 80.23±0.05 86.79±0.06 86.93±0.07 86.97±0.06 HIGGS 252 273 274 276
HEPM - 91.13±0.02 91.25±0.02 91.33±0.02 SUSY 110 77 72 77
HIGGS 71.54±0.02 70.61±0.02 71.32±0.03 71.69±0.03 Learning
SUSY 79.29±0.05 79.15±0.04 79.49±0.04 79.66±0.04 Dataset FMDT FMDT5 FMDT7 FMDT9
BNG .7896±.0004 .8649±.0006 .8658±.0007 .8662±.0007 SUSY 1 282 76 75 77
HEPM - .9113±.0002 .9125±.0002 .9133±.0002 Totaltime
HIGGS .7143±.0001 .7033±.0002 .7114±.0003 .7155±.0003 Dataset FMDT FMDT5 FMDT7 FMDT9
SUSY .7859±.0004 .7847±.0004 .7880±.0004 .7898±.0004
TableVshowsthetimerequiredbyeachmethodtoperform HIGGS 5 238 450 441 435
threedifferentstages:thepartitioningprocess theFDTinduc- SUSY 1 392 154 148 155
tion andthewholelearningalgorithm.Ingeneral thereareno
the partitioning stage  though the proposed algorithm is 30%
faster than the original method on SUSY. However  when the [7] J. Sanz  D. Paternain  M. Galar  J. Fernandez  D. Reyero  and
FDT induction is considered  the reduction in model com- T. Belzunegui  “A New Survival Status Prediction System for Severe
plexitycomingfromtheproposedfuzzypartitioningalgorithm
MethodsandProgramsinBiomedicine vol.142 no.C pp.1–8 2017.
results in much faster runtimes. [8] L.Zadeh “Fuzzysets ”InformationandControl vol.8 no.3 pp.338
–353 1965.
VI. CONCLUDINGREMARKS [9] Y. Yuan and M. Shaw  “Induction of fuzzy decision trees ” Fuzzy Sets
andSystems vol.69 no.2 pp.125–139 1995.
Inthisworkwehavepresentedanewdistributedfuzzypar- [10] C.Janikow “Fuzzydecisiontrees:Issuesandmethods ”IEEETransac-
tions on Systems  Man  and Cybernetics  Part B: Cybernetics  vol. 28
no.1 pp.1–14 1998.
way fuzzy decision trees (FDTs) in Big Data classiﬁcation [11] J.Sanz H.Bustince A.Ferna´ndez andF.Herrera “IIVFDT:Ignorance
problems.Theproposedalgorithmconsistsintransformingthe functionsbasedinterval-valuedfuzzydecisiontreewithgenetictuning ”
Systems vol.20 no.SUPPL.2 pp.1–30 2012.
followanapproximatelystandarduniformdistribution.Tothis [12] A. Segatori  F. Marcelloni  and W. Pedrycz  “On Distributed Fuzzy
end theprobabilityintegraltransformisapplied whichstates Decision Trees for Big Data ” IEEE Transactions on Fuzzy Systems
vol.26 no.1 pp.174–192 2018.
[13] J.E.Angus “TheProbabilityIntegralTransformandRelatedResults ”
standarduniformrandomvariablebasedontheoriginalcumu- SIAMReview vol.36 no.4 pp.652–654 1994.
lative distribution function (CDF). Since the CDF is generally [14] C. P. Quesenberry  Probability Integral Transformations. John Wiley
unknown  we approximate this function by computing the q- &Sons Inc. 2004.
[15] E.H.Ruspini “Anewapproachtoclustering ”InformationandControl
quantilesofthetrainingsetandlinearlyinterpolatingbetween vol.15 no.1 pp.22–32 1969.
suchquantiles.Afterthistransformation Ruspinistrongfuzzy [16] D.DheeruandE.KarraTaniskidou “UCImachinelearningrepository ”
2017.[Online].Available:http://archive.ics.uci.edu/ml
of triangular membership functions across the [0 1] interval. on Large Clusters ” Communications of the ACM  vol. 51  no. 1  pp.
To recover the points deﬁning the fuzzy sets in the original 107–113 2008.
space  the inverse cumulative distribution function or quantile [18] M. Zaharia  M. Chowdhury  M. J. Franklin  S. Shenker  and I. Stoica
function can be applied. The proposed two-step partitioning 2Nd USENIX Conference on Hot Topics in Cloud Computing  ser.
process is able to adjust both the position and shape of fuzzy HotCloud’10. Berkeley  CA  USA: USENIX Association  2010  pp.
In order to test the performance of our approach  we M.J.Franklin S.Shenker andI.Stoica “ResilientDistributedDatasets:
carried out an empirical study focused on the MapReduce A Fault-tolerant Abstraction for In-memory Cluster Computing ” in
DesignandImplementation ser.NSDI’12 2012 pp.2–2.
Data. To this end  we replaced the fuzzy partitioning method [20] H. Ishibuchi  T. Nakashima  and M. Nii  Classiﬁcation and Modeling
usedintheoriginalpaperwiththeproposedalgorithm without withLinguisticInformationGranules:AdvancedApproachestoLinguis-
tic Data Mining (Advanced Information Processing). Secaucus  NJ
USA:Springer-VerlagNewYork Inc. 2004.
reveal that the proposed methodology leads to simpler FDTs [21] M. Zeinalkhani and M. Eftekhari  “Fuzzy partitioning of continuous
thatmaintainclassiﬁcationperformancewhileprovidingmuch attributesthroughdiscretizationmethodstoconstructfuzzydecisiontree
classiﬁers ”InformationSciences vol.278 pp.715–735 2014.
J.Freeman D.Tsai M.Amde S.Owen D.Xin R.Xin M.Franklin
ACKNOWLEDGMENT R.Zadeh M.Zaharia andA.Talwalkar “MLlib:Machinelearningin
Apache Spark ” Journal of Machine Learning Research  vol. 17  pp.
1235–1241 2016.
Science and Technology under the project TIN2016-77356-P. [23] N. U. Nair  P. G. Sankaran  and N. Balakrishnan  Quantile-Based
Reliability Analysis. New York  NY: Springer New York  2013  ch.
REFERENCES QuantileFunctions pp.1–28.
[24] J.HuangandC.Ling “UsingAUCandaccuracyinevaluatinglearning
[1] J.R.Quinlan C4.5:ProgramsforMachineLearning. SanFrancisco  algorithms ” IEEE Transactions on Knowledge and Data Engineering
CA USA:MorganKaufmannPublishersInc. 1993. vol.17 no.3 pp.299–310 2005.
[2] M.-Y.Chen “Predictingcorporateﬁnancialdistressbasedonintegration
withApplications vol.38 no.9 pp.11261–11272 2011.
image classiﬁcation using remote sensing data ” Agricultural Systems
vol.76 no.3 pp.1101–1117 2003.
[4] X.-B.Li “Ascalabledecisiontreesystemanditsapplicationinpattern
recognitionandintrusiondetection ”DecisionSupportSystems vol.41
no.1 pp.112–130 2005.
[5] N.Ball R.Brunner A.Myers andD.Tcheng “Robustmachinelearning
appliedtoastronomicaldatasets.I.Star-galaxyclassiﬁcationofthesloan
digitalskysurveyDR3usingdecisiontrees ”AstrophysicalJournal vol.
650 no.1 pp.497–509 2006.
[6] D.Che Q.Liu K.Rasheed andX.Tao “Decisiontreeandensemble
learningalgorithmswiththeirapplicationsinbioinformatics ”Advances
inExperimentalMedicineandBiology vol.696 pp.191–199 2011.
†GIARA  Navarrabiomed  Complejo Hospitalario de Navarra (CHN)  Universidad P´ublica de Navarra (UPNA)  IdiSNA
Emails: {mikel.elkano  mikelxabier.uriz  bustince  mikel.galar}@unavarra.es
Abstract—We present a new distributed fuzzy partitioning
Index Terms—Fuzzy Decision Trees; Probability Integral
I. INTRODUCTION
points or a discretization process to split the attribute domain
⃝2018 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses  in any current or future media  including reprinting/republishing this material for advertising or
10.1109/BigDataCongress.2018.00011  2018 IEEE International Congress on Big Data (BigData Congress)
arXiv:1903.00345v1  [cs.LG]  28 Feb 2019
available at UCI [16] and OpenML1 repositories. We compared
II. PRELIMINARIES
1) Map stage: input data is partitioned into several logical
the so-called intermediate data. Finally  this intermediate
1https://www.openml.org/search?type=data
b) Partitioning: a target reducer is selected for each
2) Reduce stage: the reducer is responsible for aggregating
lazily evaluated operations (transformations). The execution
tasks. One stage consists only of transformations that do
executors  which represent independent processes in the Java
Each example x = (x1  . . .   xF ) contained in the training set
TR belongs to a class y ∈ C = {C1  ...  CM} (M being the
variable xf can take on any value contained in the set Ff.
decision function h : F1 × . . . × FF → C that maximizes the
that considers xf as the splitting attribute  the matching degree
mdCN(x) = T
�
µCN(xf)  mdP N(x)
(1)
where T is a T-norm  µCN(xf) is the membership degree
of xf to the fuzzy set associated with the node CN  and
mdP N(x) is the matching degree between x and the parent
node PN. Next  the association degree ADLN
m (x) of x with
the class Cm at the leaf node LN is calculated as:
m (x) = mdLN(x) · wLN
m
(2)
where mdLN(x) is the matching degree between x and the
leaf node LN and wLN
is the class weight associated with
Cm at LN. Different deﬁnitions have been proposed for wLN
=
x∈T RCm
mdLN(x)
x∈T R
(3)
where TRCm is the set of all training examples belonging
to the class Cm. Finally  the class label of x is predicted
• Weighted vote: the sum of all association degrees is
1) Fuzzy partitioning. A strong triangular fuzzy partition is
entropy. To this end  the algorithm selects the candidate
fuzzy partition that minimizes the fuzzy entropy and
2) FDT learning. An FDT is built by applying one of the
III. APPLYING THE PROBABILITY INTEGRAL TRANSFORM
TO REDUCE THE COMPLEXITY OF MULTI-WAY FUZZY
• To build a few fuzzy sets per attribute. The original
• To adjust the fuzzy sets to the real distribution of the
• Pre-processing: the variables of the training set are
Theorem 1. If X is a continuous random variable with
cumulative distribution function (CDF) FX(x) and if
Y = FX(X)  then Y is a uniform random variable on
Proof. Suppose that Y = g(X) is a function of X where
g is differentiable and strictly increasing. Thus  its inverse
g−1 uniquely exists. The CDF of Y can be derived using
FY (y) = Prob (Y ≤ y) = Prob
X ≤ g−1(y)
= FX
g−1(y)
fY (y) = d
dy FY (y) = d
dy FX(g−1(y))
= fX(g−1(y)) · d
dy g−1(y).
the distribution of Y to be derived as follows:
X ≤ F −1
X (y)
F −1
= y
■
quantile is extracted. If q is smaller than the number of
linearly interpolated on the interval [Qi−1  Qi]  Qi being
smaller than the ﬁrst quantile (Q1) or greater than the
last quantile (Qq−1)  the CDF is 0 or 1  respectively. This
jet 1 eta and jet 1 phi of HIGGS. Solid lines and bar
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Frequency of values (bars)
(a) Transformed space of every attribute
-2.5
-2
-1.5
-1
-0.5
1.5
2.5
(b) Original space of the attribute jet 1 eta
(c) Original space of the attribute jet 1 phi
Fig. 1: Fuzzy sets built for jet 1 eta and jet 1 phi on HIGGS.
IV. EXPERIMENTAL FRAMEWORK
and OpenML5 repositories. Table I shows the description
Dataset
#Instances
#Attributes
#Classes
R
I
C
T
BNG
1 000 000
8
6
14
HEPM
10 500 000
28
HIGGS
11 000 000
SUSY
5 000 000
18
Positive prediction
Negative prediction
Positive class
True Positive (TP)
False Negative (FN)
Negative class
False Positive (FP)
True Negative (TN)
TPrate =
TP + FN
(4)
• True negative rate: percentage of correctly classiﬁed
TNrate =
TN + FP
(5)
• False positive rate: percentage of misclassiﬁed negative
FPrate =
FP + TN
(6)
• False negative rate: percentage of misclassiﬁed positive
FNrate =
FN
FN + TP
(7)
5https://www.openml.org/search?type=data
the Area Under the ROC Curve (AUC) [24]  which are deﬁned
Accuracy rate =
TP + TN
TP + FN + FP + TN
(8)
AUC = 1 + TPrate + FPrate
(9)
• Measure to compute the impurity of nodes: fuzzy entropy
• T-norm: product
• Maximum depth of the tree: 5
• γ = 0.1%;
φ = 0.02 · N;
λ = 10−4 · N
V. EXPERIMENTAL STUDY
that differ in the number of fuzzy sets (X) used for numeric
attributes (denoted as FMDTX). We must point out that the
ing method (FMDTX) is able to maintain the classiﬁcation
• BNG: the proposed method improves the accuracy rate
Although the trees built by FMDTX are deeper  they have
• HIGGS: the classiﬁcation performance of FMDT5 on this
rest of conﬁgurations (FMDT7 and FMDT9) are able to
as many fuzzy sets as FMDT7.
FMDT
FMDT5
FMDT7
FMDT9
80.23±0.05
86.79±0.06
86.93±0.07
86.97±0.06
-
91.13±0.02
91.25±0.02
91.33±0.02
71.54±0.02
70.61±0.02
71.32±0.03
71.69±0.03
79.29±0.05
79.15±0.04
79.49±0.04
79.66±0.04
.7896±.0004
.8649±.0006
.8658±.0007
.8662±.0007
.9113±.0002
.9125±.0002
.9133±.0002
.7143±.0001
.7033±.0002
.7114±.0003
.7155±.0003
.7859±.0004
.7847±.0004
.7880±.0004
.7898±.0004
TABLE IV: Complexity of each model.
83 044
1 211
4 807
9 492
2 854
13 472
43 339
6 414 575
3 005
15 876
53 489
5 225 134
2 977
14 989
49 038
3.02
4.67
5.00
4.35
4.52
4.03
3.93
3.25
4.89
3.68
4.76
6.04
7.00
9.00
13.01
22.60
58
41
40
295
292
294
252
273
274
276
110
77
72
25
23
22
24
149
158
153
4 984
176
167
1 282
76
75
84
65
63
445
450
448
5 238
441
435
1 392
154
148
155
faster than the original method on SUSY. However  when the
VI. CONCLUDING REMARKS
space  the inverse cumulative distribution function or quantile
[1] J. R. Quinlan  C4.5: Programs for Machine Learning.
San Francisco
[2] M.-Y. Chen  “Predicting corporate ﬁnancial distress based on integration
with Applications  vol. 38  no. 9  pp. 11 261–11 272  2011.
recognition and intrusion detection ” Decision Support Systems  vol. 41
digital sky survey DR3 using decision trees ” Astrophysical Journal  vol.
in Experimental Medicine and Biology  vol. 696  pp. 191–199  2011.
[7] J. Sanz  D. Paternain  M. Galar  J. Fernandez  D. Reyero  and
Methods and Programs in Biomedicine  vol. 142  no. C  pp. 1–8  2017.
[8] L. Zadeh  “Fuzzy sets ” Information and Control  vol. 8  no. 3  pp. 338
[9] Y. Yuan and M. Shaw  “Induction of fuzzy decision trees ” Fuzzy Sets
and Systems  vol. 69  no. 2  pp. 125–139  1995.
[11] J. Sanz  H. Bustince  A. Fern´andez  and F. Herrera  “IIVFDT: Ignorance
Systems  vol. 20  no. SUPPL. 2  pp. 1–30  2012.
Decision Trees for Big Data ” IEEE Transactions on Fuzzy Systems
SIAM Review  vol. 36  no. 4  pp. 652–654  1994.
[14] C. P. Quesenberry  Probability Integral Transformations.
John Wiley
[15] E. H. Ruspini  “A new approach to clustering ” Information and Control
on Large Clusters ” Communications of the ACM  vol. 51  no. 1  pp.
2Nd USENIX Conference on Hot Topics in Cloud Computing  ser.
HotCloud’10.
Berkeley  CA  USA: USENIX Association  2010  pp.
Design and Implementation  ser. NSDI’12  2012  pp. 2–2.
tic Data Mining (Advanced Information Processing).
Secaucus  NJ
classiﬁers ” Information Sciences  vol. 278  pp. 715–735  2014.
Reliability Analysis.
New York  NY: Springer New York  2013  ch.
algorithms ” IEEE Transactions on Knowledge and Data Engineering","allows the state-of-the-art multi-way fuzzy decision tree (FMDT)
induction algorithm to maintain classiﬁcation accuracy with up
to 6 million fewer leaves.
Index Terms —Fuzzy Decision Trees; Probability Integral
Transform; Quantile Function; MapReduce; Apache Spark; Big
Data
I. I NTRODUCTION
Decision trees (DTs) [1] are popular non-parametric su-
pervised machine learning tools used for classiﬁcation and
regression tasks. They have been applied in a wide variety
of problems such as ﬁnance [2]  image classiﬁcation [3]
intrusion detection [4]  astronomy [5]  bioinformatics [6]  or
medicine [7]. The main feature of DTs is the ability to explain
the reasoning behind their decisions by means of tree-like
graphs. Each node is a question or a test on an attribute
(e.g. isx > 0:5?)  each branch represents the answer or the
outcome of the test  and terminal nodes (or leaves) contain
the ﬁnal decisions. Trees are usually built by applying a top-
down strategy called recursive partitioning [1]  in which input
data is recursively partitioned (split) into two or more sub-
spaces that increase the homogeneity of class distributions. In
the case of continuous attributes  the tree induction algorithm
can apply either a brute-force search to test all possible cutpoints or a discretization process to split the attribute domain
into a discrete set of intervals (also called bins). Since brute-
force solutions might be too computationally heavy when
dealing with Big Data problems  discretization strategies are
usually applied to speed up the algorithm and reduce the model
complexity.
Fuzzy logic [8] has proven to be an effective way to enhance
the classiﬁcation performance of machine learning algorithms
when dealing with uncertainty  including decision trees [9]–
[11]. In fuzzy decision trees (FDTs)  a continuous attribute
is characterized by a fuzzy variable instead of a discrete set
of intervals. Therefore  a given input value might belong to
one or more fuzzy sets with a certain membership degree and
activate multiple branches at the same time. This way  the
FDT is able to create soft decision boundaries and handle
smooth transitions between adjacent intervals. In addition to
classiﬁcation performance  fuzzy logic allows the user to
translate the whole tree into a number of IF-THEN rules
composed of human-readable linguistic labels such as ”IF
Temperature is High AND Sugar level is Very low THEN
Class = Sick”  which might improve the interpretability of
the model.
In the context of Big Data  the excessive time and space
requirements of FDTs seriously affect the scalability of these
algorithms. Segatori et al. came up with a MapReduce solution
consisting of a new fuzzy partitioning method (discretizer)
and a distributed FDT learning scheme [12]. The discretizer
generates a strong triangular fuzzy partition for each contin-
uous attribute based on fuzzy entropy  which is then used to
construct the tree. The authors proposed two versions of FDT
that differ in the splitting strategy: the binary (or two-way)
FDT (FBDT) and the multi-way FDT (FMDT). The former
recursively partitions the attribute space into two subspaces
(child nodes)  while the latter might generate more than two
subspaces. Although accurate  the solution of Segatori et al.
generally builds large and complex trees containing hundreds
of thousands of leaves.
In this work  we present a new fuzzy partitioning method
that reduces the complexity of trees constructed by the FMDT
c2018 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses  in any current or future media  including reprinting/republishing this material for advertising or
promotional purposes  creating new collective works  for resale or redistribution to servers or lists  or reuse of any copyrighted component of this work in other works. Citation information: DOI
10.1109/BigDataCongress.2018.00011  2018 IEEE International Congress on Big Data (BigData Congress)arXiv:1903.00345v1  [cs.LG]  28 Feb 2019
scheme  in terms of both the number of fuzzy sets used per
variable and the number of leaves. The proposed algorithm
applies the probability integral transform [13]  [14] to adjust
a ﬁxed number of fuzzy sets to the real distribution of the
training data. This transformation allows the algorithm to
convert the variables of the training set into (approximately)
uniform random variables regardless of their original distri-
bution. Next  the Ruspini strong fuzzy partitions [15] are
built in the new transformed dataset using equally distributed
triangular membership functions. The resulting fuzzy sets are
then used by the original FMDT to construct the tree.
In order to assess the beneﬁts of our proposal  we carried out
an empirical study using 4 Big Data classiﬁcation problems
available at UCI [16] and OpenML1repositories. We compared
the accuracy rate and the model complexity of FMDT when
using the original and the proposed fuzzy partitioning methods.
The experimental results show a signiﬁcant reduction in model
complexity when applying our strategy.
This paper is organized as follows. Section II recalls the
basics of the MapReduce algorithm and the Apache Spark
framework and brieﬂy describes the distributed solution of
Segatori et al. to build FDTs for Big Data. In Section III
we introduce the proposed fuzzy partitioning method. The
experimental framework and the analysis of the results are
shown in Sections IV and V  respectively. Finally  Section VI
contains concluding remarks.
II. P RELIMINARIES
In this section we recall some concepts about the MapRe-
duce algorithm and the Apache Spark framework (Section
II-A) and we brieﬂy describe the distributed solution presented
by Segatori et al. to build fuzzy decision trees for Big Data
(Section II-B).
A. MapReduce and Apache Spark
MapReduce is a programming paradigm [17] for processing
large-scale datasets in a distributed fashion. It is composed of
two stages called Map and Reduce  which are executed by the
so-called mappers andreducers   respectively:
1)Map stage : input data is partitioned into several logical
splits that are associated with certain physical blocks
(preferably with local ones  in favor of data locality).
Each split is then processed by a single mapper on a
given computing node. The mapper transforms the input
data into multiple key-value pairs and calls the map()
function (deﬁned by the user) for each pair. The result
of this function is another key-value pair that is part of
the so-called intermediate data . Finally  this intermediate
data is prepared to be sent to the reducers by applying
the following operations:
a) Sorting and Merging: outputs are sorted by key
and all the values corresponding to the same key
are merged in a list of values.
1https://www.openml.org/search?type=datab) Partitioning: a target reducer is selected for each
key.
c) Shufﬂe: previous intermediate data is copied to the
reducers.
2)Reduce stage : the reducer is responsible for aggregating
the outputs of the mappers when they all have ﬁnished.
To this end  all the key-value pairs received from the
mappers are sorted and merged by key. Then  the re-
duce() function (deﬁned by the user) is called for every
single key  where all its values are aggregated. Finally
the reducer returns the ﬁnal result for each key.
Spark [18] was introduced as a generalization and an
extension of the MapReduce paradigm. It is built around the
concept of Resilient Distributed Datasets (RDDs) [19]  which
represent distributed immutable data (partitioned data) and
lazily evaluated operations ( transformations ). The execution
of a user-deﬁned algorithm consists of a sequence of stages
composed of a number of transformations that are split into
tasks . One stage consists only of transformations that do
not require any shufﬂing/repartitioning process (e.g.  map
and ﬁlter operations). Tasks are executed by the so-called
executors   which represent independent processes in the Java
Virtual Machine (JVM) of a worker node. Finally  the result
of all transformations is obtained by calling an action that
computes and returns the result to the driver node. This data
ﬂow allows the user to run an indeﬁnite number of MapReduce
jobs within the same main program  supporting a wide variety
of algorithms and methods.
B. Fuzzy decision trees for Big Data
Decision trees (DTs) [1] are popular supervised machine
learning algorithms used for both classiﬁcation and regression.
In this work we focus on classiﬁcation tasks  which consist
in building a model called classiﬁer that is able to classify
unlabeled (unknown) examples (also called instances)  on the
basis of a training set containing previously labeled examples.
Each example x= (x1;:::;x F)contained in the training set
TRbelongs to a class y2C=fC1;:::;C Mg(Mbeing the
number of classes of the problem) and is characterized by a set
ofFvariables (also called attributes or features)  where each
variablexfcan take on any value contained in the set Ff.
Therefore  the construction of a classiﬁer consists in ﬁnding a
decision function h:F1:::FF!Cthat maximizes the
classiﬁcation accuracy.
A DT is a directed acyclic graph where each internal node
is a test on an attribute  each branch represents the outcome
of the test  and each terminal node (or leaf) contains the ﬁnal
decision (class label). DTs are usually built by applying a
top-down recursive partitioning [1] of the attribute space. The
selection of the attribute considered in the decision node is
based on metrics that measure the difference between the
level of homogeneity of the class labels contained in the
parent and child nodes. For continuous attributes  either brute-
force solutions or discretization strategies can be applied. The
former test all the possible cut points in the training set  while
the latter divide the attribute domain into a discrete set of
intervals (also called bins). Since brute-force strategies are
computationally heavy  the DTs designed for Big Data usually
apply discretization methods to speed up the algorithm and
reduce the model complexity.
Fuzzy decision trees (FDTs) [9]  [10] make use of fuzzy
logic [8] to better deal with uncertainty and create soft decision
boundaries that improve classiﬁcation performance. FDTs use
fuzzy partitions to characterize continuous attributes instead
of considering a discrete set of intervals. As a consequence  a
given input value might belong to one or more fuzzy sets with
a certain membership degree and activate multiple branches
at the same time. Fuzzy partitions allow FDTs to handle
smooth transitions between adjacent intervals in continuous
attributes  which might lead to more accurate predictions when
handling numeric data. When classifying a new example x  the
strength of activation of each leaf (called matching degree ) is
computed. To this end  the matching degree of every internal
node must be calculated as well. Given the current node CN
that considers xfas the splitting attribute  the matching degree
mdCN(x)betweenxandCN is computed as:
mdCN(x) =T 
CN(xf);mdPN(x)
; (1)
whereTis a T-norm  CN(xf)is the membership degree
ofxfto the fuzzy set associated with the node CN  and
mdPN(x)is the matching degree between xand the parent
nodePN. Next  the association degree ADLN
m(x)ofxwith
the classCmat the leaf node LN is calculated as:
ADLN
m(x) =mdLN(x)wLN
m; (2)
wheremdLN(x)is the matching degree between xand the
leaf nodeLN andwLN
mis the class weight associated with
CmatLN. Different deﬁnitions have been proposed for wLN
m
in the literature [20]. In this work we consider
wLN
m=X
x2TRCmmdLN(x)
X
x2TRmdLN(x); (3)
whereTRCmis the set of all training examples belonging
to the class Cm. Finally  the class label of xis predicted
according to different criteria  the most common being the
following:
Maximum matching : the class corresponding to the max-
imum association degree is returned.
Weighted vote : the sum of all association degrees is
computed for each class. The one getting the maximum
sum is predicted.
The excesive time and space requirements of FDTs can
cause serious scalability issues when tackling large-scale
problems. In this work  we consider the distributed solution
proposed by Segatori et al. in [12] to build FDTs for Big Data
which comprises two stages:
1)Fuzzy partitioning . A strong triangular fuzzy partition is
constructed for each continuous attribute based on fuzzy
entropy. To this end  the algorithm selects the candidatefuzzy partition that minimizes the fuzzy entropy and
splits the attribute domain into two subsets in a recursive
fashion  until a stopping condition is met. Although
accurate  the partitions built by this methodology might
contain many fuzzy sets per variable  increasing the
complexity of the model.
2)FDT learning . An FDT is built by applying one of the
two splitting strategies considered by the authors: the
binary (or two-way) FDT (FBDT)  which always gen-
erates two child nodes  or the multi-way FDT (FMDT)
which might create more than two child nodes. Another
difference is that in FBDTs an attribute can appear
several times in the same path. Both methods use the
fuzzy information gain [21] for the attribute selection.
In this work we focus on FMDTs.
The whole pipeline is built on top of Apache Spark and the
MLlib [22] machine learning library and is publicly available
at GitHub2.
III. A PPLYING THE PROBABILITY INTEGRAL TRANSFORM
TO REDUCE THE COMPLEXITY OF MULTI -WAY FUZZY
DECISION TREES
In this work we propose a new distributed fuzzy partitioning
method that reduces the complexity of FDTs generated by the
FMDT algorithm presented in [12]. The proposed solution
replaces the original partitioning method used by FMDT
without altering the FDT learning algorithm. The goals of our
approach are the following:
To build a few fuzzy sets per attribute. The original
method adds fuzzy sets to the fuzzy partition until the
fuzzy information gain is below a certain threshold
increasing the complexity of the model. Our approach
uses a ﬁxed number of fuzzy sets for all attributes.
To adjust the fuzzy sets to the real distribution of the
attributes. The proposed solution modiﬁes both the shape
and the position of the fuzzy sets to enhance the discrim-
ination capability of the model.
In order to achieve the aforementioned goals  we propose a
two-step algorithm consisting of a pre-processing stage that
directly leads to a self-adaptive fuzzy partitioning process:
Pre-processing: the variables of the training set are
converted into standard uniform random variables by
applying the probability integral transform theorem [13]
[14]  described in Theorem 1. This theorem states that
any continuous random variable can be converted into a
standard uniform random variable.
Theorem 1. IfXis a continuous random variable with
cumulative distribution function (CDF) FX(x)and if
Y=FX(X)  thenYis a uniform random variable on
the interval [0 1].
2https://github.com/BigDataMiningUnipi/FuzzyDecisionTreeSpark
Proof. Suppose that Y=g(X)is a function of Xwhere
gis differentiable and strictly increasing. Thus  its inverse
g 1uniquely exists. The CDF of Ycan be derived using
FY(y) =Prob (Yy) =Prob 
Xg 1(y)
=FX 
g 1(y)
and its density is given by
fY(y) =d
dyFY(y) =d
dyFX(g 1(y))
=fX(g 1(y))d
dyg 1(y):
This procedure is called the CDF technique and allows
the distribution of Yto be derived as follows:
XF 1
X(y)
F 1
=y

However  since the original distribution of the training
set is unknown  we cannot compute the exact CDF.
Instead  we propose computing the q-quantiles of the
training set to obtain an approximate CDF. To this end
for each variable  all the values are sorted and each
quantile is extracted. If qis smaller than the number of
examples in the training set  the CDF of a certain value is
linearly interpolated on the interval [ Qi 1 Qi] Qibeing
the ﬁrst quantile greater than the value. If the value is
smaller than the ﬁrst quantile ( Q1) or greater than the
last quantile ( Qq 1)  the CDF is 0 or 1  respectively. This
way  the new transformed dataset will be approximately
uniform regardless of the original distribution. Of course
the transformation of the testing set is performed by
interpolating the CDF using the quantiles extracted from
the training set.
Partitioning: a Ruspini strong fuzzy partition [15] is
created by uniformly distributing a ﬁxed number of
triangular membership functions across the interval [0 1].
It is worth noting that the deﬁnition of every single
fuzzy set in the original space can be recovered by
applying the inverse cumulative distribution function or
quantile function [23]. In this case  for every point
deﬁning the triangular membership function  we would
linearly interpolate the corresponding value between the
two closest quantiles by computing the inverse of the
linear function used to compute the CDF. Figure 1 shows
an illustrative example of how fuzzy sets are distributed
in the original and transformed spaces of the attribute
jet1etaandjet1phiof HIGGS. Solid lines and bar
plots represent the membership functions of the fuzzy sets
and the original distribution of the variables  respectively.
Notice that both steps (pre-processing and partitioning) are
closely interrelated. Given that  from our point of view  a Rus-
pini strong fuzzy partition with equally distributed membership
functions is a suitable way to model a uniform distribution
00.10.20.30.40.50.60.70.80.9 1
Value00.20.40.60.81Frequency of values (bars)
Membership degrees (solid lines)(a) Transformed space of every attribute
-2.5-2-1.5-1-0.500.511.522.5
Membership degrees (solid lines)
(b) Original space of the attribute jet1eta
-1.5 -1 -0.5 0 0.5 1 1.5
(c) Original space of the attribute jet1phi
Fig. 1: Fuzzy sets built for jet1etaandjet1phion HIGGS.
our hypothesis is that if we are able to transform any attribute
into a uniform distribution and likewise carry out the inverse
process  we would obtain a self-adapted partition for the
original distribution of each attribute. Interestingly  this result
is obtained without speciﬁcally developing a new partitioning
method. The whole pipeline is written in Scala 2.113on top
of Apache Spark 2.0.2 and is publicly available at GitHub4
under the GPL license.
IV. E XPERIMENTAL FRAMEWORK
In this section  we ﬁrst describe the datasets and perfor-
mance metrics used to evaluate the methods considered in the
experiments (Section IV-A). Next  we specify the parameters
and the environment conﬁguration used for the executions of
the algorithms (Section IV-B).
A. Datasets and performance metrics
In order to develop the experimental study  we considered
4 Big Data classiﬁcation problems available at UCI [16]
3http://www.scala-lang.org/
4https://github.com/melkano/uniform-fuzzy-partitioning
and OpenML5repositories. Table I shows the description
of the datasets indicating the number of instances (#In-
stances)  real (R)/integer(I)/categorical(C)/total(T) attributes
(#Attributes)  and classes (#Classes). The names of BNG Aus-
tralian (BNG) and HEPMASS (HEPM) have been shortened.
All the experiments were carried out using a 5-fold stratiﬁed
cross-validation scheme . To this end  we randomly split the
dataset into ﬁve partitions of data  each one containing 20%
of the examples  and we employed a combination of four of
them (80%) to train the system and the remaining one to test
it. Therefore  the result of each dataset was computed as the
average of the ﬁve partitions.
TABLE I: Description of the datasets.
Dataset #Instances #Attributes #Classes
R I C T
BNG 1 000 000 8 6 0 14 2
HEPM 10 500 000 28 0 0 28 2
HIGGS 11 000 000 28 0 0 28 2
SUSY 5 000 000 18 0 0 18 2
Classiﬁcation performance was measured based on the so-
called confusion matrix (Table II)  which stores the number of
correctly classiﬁed and misclassiﬁed examples for each class.
From this matrix we can obtain the following four metrics:
TABLE II: Confusion matrix for a binary problem.
Positive prediction Negative prediction
Positive class True Positive (TP) False Negative (FN)
Negative class False Positive (FP) True Negative (TN)
True positive rate: percentage of correctly classiﬁed pos-
itive examples.
TPrate =TP
TP +FN(4)
True negative rate: percentage of correctly classiﬁed
negative examples.
TNrate =TN
TN +FP(5)
False positive rate: percentage of misclassiﬁed negative
examples.
FPrate =FP
FP +TN(6)
False negative rate: percentage of misclassiﬁed positive
FNrate =FN
FN +TP(7)
Based on these metrics  the classiﬁcation performance of each
method was measured with the well-known accuracy rate and
5https://www.openml.org/search?type=datathe Area Under the ROC Curve (AUC) [24]  which are deﬁned
as:
Accuracyrate =TP +TN
TP +FN +FP +TN(8)
AUC =1 +TPrate +FPrate
2(9)
B. Parameters and environment conﬁguration
As for the parameters used for FMDT  we set the values
suggested by the authors in the original paper:
Measure to compute the impurity of nodes: fuzzy entropy
T-norm: product
Maximum number of bins for numeric attributes: 32
Maximum depth of the tree: 5
= 0.1%;= 0.02N;= 10 4N
The computing cluster used for running the algorithms is
composed of 6 slave nodes and a master node connected via
1Gb/s Ethernet LAN network. Half of the slave nodes have
2 Intel Xeon E5-2620 v3 processors at 2.4 GHz (3.2 GHz
with Turbo Boost) with 12 virtual cores in each one (where
6 of them are physical). The other half are equipped with 2
Intel Xeon E5-2620 v2 processors at 2.1 GHz with the same
number of cores as the previous ones. The master node is
composed of an Intel Xeon E5-2609 processor with 4 physical
cores at 2.4 GHz. All slave nodes are equipped with 64 GB of
RAM memory  while the master works with 32 GB of RAM
memory. With respect to the storage speciﬁcations  all nodes
use Hard Disk Drives featuring a read/write performance of
128 MB/s. The entire cluster runs on top of CentOS 6.5 +
Apache Hadoop 2.6.0 + Apache Spark 2.0.2.
We found that using more than 24 cores had a negative im-
pact on runtimes when setting the conﬁguration recommended
by the authors. Consequently  the number of cores used in
the experiments was 24 and we assigned 4 cores to every
single executor in order to ensure full HDFS write throughput
while minimizing memory replication overhead (e.g. broadcast
variables).
V. E XPERIMENTAL STUDY
In order to assess the performance of our approach  we
carried out an empirical study covering three aspects: clas-
siﬁcation performance (Table III)  model complexity (Table
IV)  and runtimes (Table V). In all cases we consider four
methods: the original FMDT proposed by Segatori et al. in
[12] and three different conﬁgurations of the proposed method
that differ in the number of fuzzy sets ( X) used for numeric
attributes (denoted as FMDT X). We must point out that the
original FMDT ran out of memory while tackling HEPMASS
due to the excessive number of leaves built during training
and thus no results are given for this method on HEPMASS.
Tables III and IV reveal that the proposed fuzzy partition-
ing method (FMDT X) is able to maintain the classiﬁcation
performance of FMDT while leading to signiﬁcantly simpler
models. The different conﬁgurations of our approach yield
similar results in terms of accuracy rate and AUC (except
for HIGGS)  although there is a positive trend in favor of
the usage of more fuzzy sets. However  using more fuzzy sets
often causes the learning algorithm to build more leaves  which
increases the model complexity. Next  we analyze the results
obtained on each dataset separately:
BNG: the proposed method improves the accuracy rate
and the AUC of FMDT by 6% and 8%  respectively.
Although the trees built by FMDT Xare deeper  they have
8-80K times fewer leaves than FMDT’s.
HEPM: the original FMDT builds too many leaves to
handle this dataset on the cluster described in Section
IV-B and ran out of memory during the experiments. This
fact suggests that our approach is a potential solution to
avoid the explosion in the number of leaves during the
induction of FDTs.
HIGGS: the classiﬁcation performance of FMDT 5on this
dataset drops by nearly 1% with respect to the rest of
methods  which reveals that 5 fuzzy sets are not enough
to capture the complexity of this problem. However  the
rest of conﬁgurations (FMDT 7and FMDT 9) are able to
maintain the classiﬁcation performance of FMDT with
trees composed of 15K and 50K leaves  respectively
while FMDT generates 6M leaves. Furthermore  the
original fuzzy partitioning method builds almost twice
as many fuzzy sets as FMDT 7.
SUSY: all the conﬁgurations perform similarly to FMDT
in terms of discrimination capability. However  our
method leads to simpler trees composed of 3K  15K  and
50K leaves  while FMDT builds 5M leaves. In this case
the difference between the number of fuzzy sets used by
each method is even larger  since FMDT uses nearly 23
fuzzy sets on average for each attribute.
TABLE III: Classiﬁcation performance of each method.
Accuracy rate %
Dataset FMDT FMDT 5 FMDT 7 FMDT 9
BNG 80.230:0586.790:0686.930:0786.970:06
HEPM - 91.130:0291.250:0291.330:02
HIGGS 71.540:0270.610:0271.320:0371.690:03
SUSY 79.290:0579.150:0479.490:0479.660:04
AUC
BNG .7896:0004.8649:0006.8658:0007.8662:0007
HEPM - .9113:0002.9125:0002.9133:0002
HIGGS .7143:0001.7033:0002.7114:0003.7155:0003
SUSY .7859:0004.7847:0004.7880:0004.7898:0004
Table V shows the time required by each method to perform
three different stages: the partitioning process  the FDT induc-
tion  and the whole learning algorithm. In general  there are no
signiﬁcant differences among the methods when it comes to
the partitioning stage  though the proposed algorithm is 30%TABLE IV: Complexity of each model.
Number of leaves
BNG 83 044 1 211 4 807 9 492
HEPM - 2 854 13 472 43 339
HIGGS 6 414 575 3 005 15 876 53 489
SUSY 5 225 134 2 977 14 989 49 038
Avg. depth
BNG 3.02 4.67 5.00 4.35
HEPM - 4.52 4.03 3.93
HIGGS 3.25 5.00 5.00 4.89
SUSY 3.68 5.00 5.00 4.76
Avg. number of fuzzy sets
BNG 6.04 5.00 7.00 9.00
HEPM - 5.00 7.00 9.00
HIGGS 13.01 5.00 7.00 9.00
SUSY 22.60 5.00 7.00 9.00
TABLE V: Runtimes(s) of each method.
Partitioning
BNG 58 41 40 40
HEPM - 295 292 294
HIGGS 252 273 274 276
SUSY 110 77 72 77
Learning
BNG 25 23 22 24
HEPM - 149 158 153
HIGGS 4 984 176 167 158
SUSY 1 282 76 75 77
Total time
BNG 84 65 63 65
HEPM - 445 450 448
HIGGS 5 238 450 441 435
SUSY 1 392 154 148 155
faster than the original method on SUSY . However  when the
FDT induction is considered  the reduction in model com-
plexity coming from the proposed fuzzy partitioning algorithm
results in much faster runtimes.
VI. C ONCLUDING REMARKS
In this work we have presented a new distributed fuzzy par-
titioning method that reduces the model complexity of multi-
way fuzzy decision trees (FDTs) in Big Data classiﬁcation
problems. The proposed algorithm consists in transforming the
original training set in such a way that all numeric variables
follow an approximately standard uniform distribution. To this
end  the probability integral transform is applied  which states
that any continuous random variable can be converted into a
standard uniform random variable based on the original cumu-
lative distribution function (CDF). Since the CDF is generally
unknown  we approximate this function by computing the q-
quantiles of the training set and linearly interpolating between
such quantiles. After this transformation  Ruspini strong fuzzy
partitions are created by equally distributing a ﬁxed number
of triangular membership functions across the [0 1] interval.
To recover the points deﬁning the fuzzy sets in the original
space  the inverse cumulative distribution function orquantile
function can be applied. The proposed two-step partitioning
process is able to adjust both the position and shape of fuzzy
sets to the real distribution of training data.
In order to test the performance of our approach  we
carried out an empirical study focused on the MapReduce
FDT induction algorithm introduced by Segatori et al. for Big
Data. To this end  we replaced the fuzzy partitioning method
used in the original paper with the proposed algorithm  without
modifying the FDT learning stage. The experimental results
reveal that the proposed methodology leads to simpler FDTs
that maintain classiﬁcation performance while providing much
faster runtimes.
ACKNOWLEDGMENT
This work has been supported by the Spanish Ministry of
Science and Technology under the project TIN2016-77356-P.
REFERENCES
[1] J. R. Quinlan  C4.5: Programs for Machine Learning . San Francisco
CA  USA: Morgan Kaufmann Publishers Inc.  1993.
[2] M.-Y . Chen  “Predicting corporate ﬁnancial distress based on integration
of decision tree classiﬁcation and logistic regression ” Expert Systems
with Applications   vol. 38  no. 9  pp. 11 261–11 272  2011.
[3] C.-C. Yang  S. Prasher  P. Enright  C. Madramootoo  M. Burgess
P. Goel  and I. Callum  “Application of decision tree technology for
image classiﬁcation using remote sensing data ” Agricultural Systems
vol. 76  no. 3  pp. 1101–1117  2003.
[4] X.-B. Li  “A scalable decision tree system and its application in pattern
recognition and intrusion detection ” Decision Support Systems   vol. 41
no. 1  pp. 112–130  2005.
[5] N. Ball  R. Brunner  A. Myers  and D. Tcheng  “Robust machine learning
applied to astronomical data sets. I. Star-galaxy classiﬁcation of the sloan
digital sky survey DR3 using decision trees ” Astrophysical Journal   vol.
650  no. 1  pp. 497–509  2006.
[6] D. Che  Q. Liu  K. Rasheed  and X. Tao  “Decision tree and ensemble
learning algorithms with their applications in bioinformatics ” Advances
in Experimental Medicine and Biology   vol. 696  pp. 191–199  2011.[7] J. Sanz  D. Paternain  M. Galar  J. Fernandez  D. Reyero  and
T. Belzunegui  “A New Survival Status Prediction System for Severe
Trauma Patients Based on a Multiple Classiﬁer System ” Computer
Methods and Programs in Biomedicine   vol. 142  no. C  pp. 1–8  2017.
[8] L. Zadeh  “Fuzzy sets ” Information and Control   vol. 8  no. 3  pp. 338
– 353  1965.
[9] Y . Yuan and M. Shaw  “Induction of fuzzy decision trees ” Fuzzy Sets
and Systems   vol. 69  no. 2  pp. 125–139  1995.
[10] C. Janikow  “Fuzzy decision trees: Issues and methods ” IEEE Transac-
tions on Systems  Man  and Cybernetics  Part B: Cybernetics   vol. 28
no. 1  pp. 1–14  1998.
[11] J. Sanz  H. Bustince  A. Fern ´andez  and F. Herrera  “IIVFDT: Ignorance
functions based interval-valued fuzzy decision tree with genetic tuning ”
International Journal of Uncertainty  Fuzziness and Knowlege-Based
Systems   vol. 20  no. SUPPL. 2  pp. 1–30  2012.
[12] A. Segatori  F. Marcelloni  and W. Pedrycz  “On Distributed Fuzzy
Decision Trees for Big Data ” IEEE Transactions on Fuzzy Systems
vol. 26  no. 1  pp. 174–192  2018.
[13] J. E. Angus  “The Probability Integral Transform and Related Results ”
SIAM Review   vol. 36  no. 4  pp. 652–654  1994.
[14] C. P. Quesenberry  Probability Integral Transformations . John Wiley
& Sons  Inc.  2004.
[15] E. H. Ruspini  “A new approach to clustering ” Information and Control
vol. 15  no. 1  pp. 22–32  1969.
[16] D. Dheeru and E. Karra Taniskidou  “UCI machine learning repository ”
2017. [Online]. Available: http://archive.ics.uci.edu/ml
[17] J. Dean and S. Ghemawat  “MapReduce: Simpliﬁed Data Processing
on Large Clusters ” Communications of the ACM   vol. 51  no. 1  pp.
107–113  2008.
[18] M. Zaharia  M. Chowdhury  M. J. Franklin  S. Shenker  and I. Stoica
“Spark: Cluster Computing with Working Sets ” in Proceedings of the
2Nd USENIX Conference on Hot Topics in Cloud Computing   ser.
HotCloud’10. Berkeley  CA  USA: USENIX Association  2010  pp.
10–10.
[19] M. Zaharia  M. Chowdhury  T. Das  A. Dave  J. Ma  M. McCauley
M. J. Franklin  S. Shenker  and I. Stoica  “Resilient Distributed Datasets:
A Fault-tolerant Abstraction for In-memory Cluster Computing ” in
Proceedings of the 9th USENIX Conference on Networked Systems
Design and Implementation   ser. NSDI’12  2012  pp. 2–2.
[20] H. Ishibuchi  T. Nakashima  and M. Nii  Classiﬁcation and Modeling
with Linguistic Information Granules: Advanced Approaches to Linguis-
tic Data Mining (Advanced Information Processing) . Secaucus  NJ
USA: Springer-Verlag New York  Inc.  2004.
[21] M. Zeinalkhani and M. Eftekhari  “Fuzzy partitioning of continuous
attributes through discretization methods to construct fuzzy decision tree
classiﬁers ” Information Sciences   vol. 278  pp. 715–735  2014.
[22] X. Meng  J. Bradley  B. Yavuz  E. Sparks  S. Venkataraman  D. Liu
J. Freeman  D. Tsai  M. Amde  S. Owen  D. Xin  R. Xin  M. Franklin
R. Zadeh  M. Zaharia  and A. Talwalkar  “MLlib: Machine learning in
Apache Spark ” Journal of Machine Learning Research   vol. 17  pp.
1235–1241  2016.
[23] N. U. Nair  P. G. Sankaran  and N. Balakrishnan  Quantile-Based
Reliability Analysis . New York  NY: Springer New York  2013  ch.
Quantile Functions  pp. 1–28.
[24] J. Huang and C. Ling  “Using AUC and accuracy in evaluating learning
algorithms ” IEEE Transactions on Knowledge and Data Engineering
vol. 17  no. 3  pp. 299–310  2005.
Mikel Elkano∗†‡  Mikel Uriz∗†  Humberto Bustince∗†‡  Mikel Galar∗†‡
∗Department of Automatics and Computation  Public University of Navarre  31006 Pamplona  Spain
†GIARA  Navarrabiomed  Complejo Hospitalario de Navarra (CHN)  Universidad Pu´blica de Navarra (UPNA)  IdiSNA
‡Institute of Smart Cities  Public University of Navarre  31006 Pamplona  Spain
9 Emails: {mikel.elkano  mikelxabier.uriz  bustince  mikel.galar}@unavarra.es
1
0
2
b  Abstract—We present a new distributed fuzzy partitioning points or a discretization process to split the attribute domain
e method to reduce the complexity of multi-way fuzzy decision into a discrete set of intervals (also called bins). Since brute-
F treesinBigDataclassiﬁcationproblems.Theproposedalgorithm force solutions might be too computationally heavy when
8  bthueiilrdssahaﬁpxeedanndumpboesritioofnfutzozythseetsrefoarl adlilstvrairbiuatbiolens aonfdtraadijnuisntgs dealing with Big Data problems  discretization strategies are
2 data. A two-step process is applied : 1) transformation of the usuallyappliedtospeedupthealgorithmandreducethemodel
original distribution into a standard uniform distribution by complexity.
G] means of the probability integral transform. Since the original Fuzzylogic[8]hasproventobeaneffectivewaytoenhance
L function is approximated by computing the q-quantiles of the
. training set; 2) construction of a Ruspini strong fuzzy partition
s in the transformed attribute space using a ﬁxed number of [11]. In fuzzy decision trees (FDTs)  a continuous attribute
c
[ equallydistributedtriangularmembershipfunctions.Despitethe is characterized by a fuzzy variable instead of a discrete set
aforementioned transformation  the deﬁnition of every fuzzy set of intervals. Therefore  a given input value might belong to
1 intheoriginalspacecanberecoveredbyapplyingtheinversecu-
v mulativedistributionfunction(alsoknownasquantilefunction).
5 The experimental results reveal that the proposed methodology
4 allowsthestate-of-the-artmulti-wayfuzzydecisiontree(FMDT) FDT is able to create soft decision boundaries and handle
3 induction algorithm to maintain classiﬁcation accuracy with up smooth transitions between adjacent intervals. In addition to
0 to 6 million fewer leaves. classiﬁcation performance  fuzzy logic allows the user to
0 Index Terms—Fuzzy Decision Trees; Probability Integral
. Transform; Quantile Function; MapReduce; Apache Spark; Big
3 composed of human-readable linguistic labels such as ”IF
0 Temperature is High AND Sugar level is Very low THEN
9
1 I.",,
R029,0,,"using Yang-Baxter R-operators in the NMHV sector. Explicit expressions for R-
invariants are obtained in terms of the chains of R-operators acting on an appropriate
basic state.
1.","TheR-operator formalism introduced in [1] establishes a connection between calculating
the scattering amplitudes in N= 4 super-Yang-Mills theory and integrable systems
such as spin chains. This approach exploits Yangian symmetry of the amplitudes  that
has been studied e.g. in [2] [3] The framework of R-operators was developed in a number
of papers [4]  [5]  [6]  [7]. For example  in [6]  a connection was established between the
graded permutations encoding the on-shell graphs and chains of R-operators acting on
a suitable basic state   as well as a connection to the top-cell graphs.
As it has been shown in [1] the amplitude terms can be obtained by acting on basic
states (formed by products of delta-functions) by products of Yang-Baxter R-operators.
These operators are dened from the L-matrices by the RLL -intertwining relation. The
R-operators act on just one pair of the spin chain sites. The sequential action by Yang-
Baxter R-operators on the basic state","integrable models in quantum theory  this paper aims to develop further the formalism
ofR-operators for constructing the tree amplitudes in N= 4 sYM in the NMHV sector
and get an expression for R-invariants through R-operators.
The paper is organized as follows. In Section 2 we introduce the basic notation for
the scattering amplitudes in N= 4 sYM. In Section 3 we show the connection according
to [1] between the gl(4j4) spin chains and scattering amplitudes in N= 4 sYM  and
introduce the notation for the main objects of the paper { Yang-Baxter R-operatorsarXiv:1904.00456v2  [hep-th]  30 Nov 2019
A connection between R-invariants and R-operators inN= 4sYM 2
(which will be referred to as R-operators) and discuss their properties. In Section 4 we
provide a solution for the BCFW [8] recursive relation in N= 4 sYM in the NMHV
sector in terms of R-operators and give the formulas R-invariants  presenting them as
the chains of R-operators acting on an appropriate basic state   which is the main result
of the paper.
2. Amplitudes in N= 4sYM
The fact that the theory N= 4 sYM is supersymmetric allows one to introduce
a supereld that combines all the elds into one function dened on the on-shell
superspace [9] ( ;e_;A)
(;e;) =g++A A+1
2!ABAB+1
3!ABCDABC D+1
4!ABCDABCDg (1)
where the capital Latin letters A;B;C;D denote the indices of the fundamental
representation of the group SU(4)R  andABCD is the Levi-Civita symbol  Aare
Grassmann variables. With the help of superelds it is possible to construct a
superamplitude | a generating function for all possible scattering amplitudes of a given
order:
Mn(1;:::; n)Mn((1;e1;1);:::; (n;en;n))
Mn((p1;1);:::; (pn;n))Mn(1;:::;n ) (2)
It can be shown  according to [9]  that the general form for the scattering amplitude of
nparticles inN= 4 sYM is
Mn(fi;ei;ig) =4(p)8(q)
h12ih23i:::hn1iPn(fi;ei;ig) (3)
wherep=p1+:::+pn{ total momentum  q=q1+:::+qn=j1i1+:::+jnin{
total supermomentum. The spinors :=jpiande_:=jp] correspond to the states
with helicity1=2 respectively. Pn(fi;ei;ig) has the form of a polynomial in iand
allows to classify superamplitudes (will be referred to as amplitudes hereinafter) by the
type Nk 2MHV
Pn(fi;ei;ig) =P(0)
n +P(4)
n +P(8)
n ++P(4n 16)
n
# # # #
MHV NMHV N2MHV MHV(4)
P(0)
n= 1 andP(l)
nO(l). This  in particular  implies the Park-Taylor formula [10] for
MHV amplitudes in N= 4 sYM
MMHV
2;n=4(p)8(q)
h12ih23i:::hn1i(5)
A connection between R-invariants and R-operators inN= 4sYM 3
whereMk;ndenotes Nk 2MHV scattering amplitude of nparticles  i.e. M2;ncorresponds
to MHV M3;n{ NMHV etc. The amplitude type is sometimes additionally indicated
as above  for example MMHV
2;n.
The introduction of a superamplitude allows one to introduce an analogue of the
BCFW relations in N= 4 sYM  the so-called super-BCFW relation [9]. The analytic
form of the super-BCFW for Nk 2MHV amplitude is
Mk;n(1;2;:::;n ) =X
nL+nR=n+2
kL+kR=k+1Z
d4Pd4ML((^p1;^1);:::; (pnL 1;nL 1);(p;))1
P2
MR(( p;);(pnR+1;nR+1);:::; (^pn;^n)) (6)
wherep=P+zPL1enandzPL=P2
L
h1jPLjn]  whereas the subamplitudes MLandMR
include momentum -functions.
2.1.R-invariants and dual superconformal symmetry
The super-BCFW recursive relation can be solved in general for tree amplitudes. The
general analytic expression for tree NMHV amplitudes in N= 4 sYM was initially
obtained in the paper [11]
MNMHV
3;n =MMHV
2;nX
1<s<t<n
js tj2Rn;st (7)
whereRr;st{ dual superconformal invariants ( R-invariants). The explicit form of Rr;st
is
Rr;st=hss 1ihtt 1i4(r;st)
x2
sthrjxrsxstjtihrjxrsxstjt 1ihrjxrtxtsjsihrjxrtxtsjs 1i(8)
wherexab:=pa+:::+pb 1 ab:=qa+:::+qb 1are dual variables. At b<a we have
xab= xba. The Grassmann-odd quantity  r;stis dened by
r;st:=hrjxrsxstjtri+hrjxrtxtsjsri
Expressions of the form hrjxrsxstjtishould be interpreted as hrja(xrs)a_c(xst)_cbjtib.
In the paper [12] it has been shown that it is possible to combine the algebras of
superconformal and dual superconformal symmetries of tree scattering amplitudes in
N= 4 sYM into an innite-dimensional algebra called Yangian Y(psu(2;2j4)). Then
the tree amplitudes will be the sum of the Yangian invariants in the super-BCFW
decomposition (which will be referred to hereafter as BCFW).
3. Spin chains and on-shell graphs
In the paper [1]  each tree scattering amplitude of nparticlesMninN= 4 sYM is
associated with a gl(4j4) spin chain of length n. As we know from the paper [13]  a
A connection between R-invariants and R-operators inN= 4sYM 4
discrete set of canonically conjugate coordinates and momenta can be associated with
a discrete set of spins  thus forming a spin chain. The paper [1] introduces a set of
canonical variables x= (xa)N+M
a=1 p= (pa)N+M
a=1  satisfying the commutation relations
fxa;pb] = abwherefxa;pb] is a graded commutator  NandMare correspondingly
the numbers of bosonic and fermionic components.
The spin chain is an example of an integrable quantum model  and one can apply
quantum inverse scattering method (QISM) (see  for example  papers by L. D. Faddeev
and collaborators [13]  [14]  [15]  [16]) to solve it. One of the central objects of QISM is
the monodromy matrix
[T(u)]ac= [L1(u)]ab1[L2(u)]b1b2:::[L(u)]bn 1c (9)
whereL-operators
[L(u)]ab=uab+xapb: (10)
Further  the authors of the paper [1] introduce the R-operators dened by the RLL-
relation
R12(u v)[L1(u)]ab[L2(v)]bc= [L1(v)]ab[L2(u)]bcR12(u v) (11)
and give the solution to the RLL-relation for gl(4j4) (relevant toN= 4 sYM)
R12(u) =Z+1
0dz
z1 ue z(p1x2)(12)
Finally  the connection of a spin chain with N= 4 sYM is established by the following
denition of canonically conjugate variables
x:= (;@e_;@A) (13)
p:= (@; e_; A) (14)
Then the action of the operator Rij(u) on an arbitrary function F(i;ei;i;j;ej;j) is
given by [1]
Rij(u)F(i;ei;i;j;ej;j) =Z+1
z1 uF(i zj;ei;i;j;ej+zei;j+zi) (15)
that is  it performs a BCFW shift on the spinor-helicity variables. Also  as shown
in [1]  the condition of Yangian invariance of scattering amplitudes in N= 4 sYM is
formulated in that way  that the amplitude Mis an eigenfunction of the monodromy
operator
T(u)M=CM (16)
where the eigenvalue Cplays a minor role. Thus  a connection is established between
the scattering amplitudes in N= 4 sYM and gl(4j4) spin chains.
TheR-operator from the equation (12) will be the main construction object of
amplitudes  allowing us to construct scattering amplitudes in the spirit of the QISM
method. The essentially non-local object | amplitude Mn  depending on the variables
A connection between R-invariants and R-operators inN= 4sYM 5
of allnexternal particles will be built using the product of R-operators  each of which
acts only on a pair of variables associated with external particles. In [1]  the authors
give the formulas for 3-particle scattering amplitudes MMHV
2;3andMMHV
1;3inN= 4 sYM
expressed in R-operators as
1;3=R12R23
1;3and
1;3=2(1)2(2)2(e3)4(3); (17)
2;3=R23R12
2;3and
2;3=2(1)2(e2)4(2)2(e3)4(3); (18)
whereRijRij(0).
3.1. On-shell diagrams
The work [17] shows that BCFW decomposition can be written in terms of the so-
called on-shell graphs. The building blocks of on-shell diagrams are 3-particle MHV
and anti-MHV amplitudes depicted in Fig. 1 with black and white circles respectively.
In terms of the BCFW on-shell diagrams  the decomposition  according to [17]  can be
written diagrammatically as in Fig. 1. where the sum is performed for all possible
MHV MHV NMHV
NMHV
Figure 1. The BCFW recursion relation in terms of on-shell graphs for an NMHV
amplitude. The summation is performed over all possible MHV subamplitudes  where
the index idenotes the rightmost external leg of the left MHV subamplitude.
MHV subamplitudes (non-recurrent terms) and the last term contains the NMHV
subamplitude (recurrent term). The right-hand side of the diagram decomposition
in Fig. 1 looks exactly like a BCFW-diagram with added 'bridges' (the so-called
BCFW-bridge). According to [1]  the R1n-operator implements this bridge. For on-
shell diagrams  the rules of diagram technique change as compared to BCFW diagrams
{ here each internal line is assigned an integralR
d4d4P(P2).
4. Solving BCFW in the NMHV sector
To start solving the BCFW relations with the help of R-operators  we formulate the
following statement
Rn;i+1M(1;2;:::;i;n )2(ei+1)4(i+1) = (19)
=Z
d0d4P0(P2
0)M(1;2;:::;i;fj P0i;j P0];0g)MMHV
2;3(fjP0i;jP0];0g;i+ 1;n);
A connection between R-invariants and R-operators inN= 4sYM 6
which we will be using further. Graphically  this statement is shown in Fig. 2. The
Figure 2. A diagrammatic representation of Eq. (19).
proof of Eq. (19) is given in Appendix.
4.1. Non-recurrent terms of BCFW
Now we can proceed to the calculation of diagrams  which are non-recurrent terms in
the diagram expansion of the NMHV amplitude (Fig. 1). To do this  rst build the
amplitudeMMHV
2;t(1;2:::t 2;t 1;n).
We start the construction with a 3-part amplitude MMHV
2;3(1;t 1;n) and add
the endst 2;t 3;:::; 2 to the left using the Inverse Soft Limit (ISL) using R-
operators [18  1]. As a result  we get the amplitude M2;t(1;2;:::;t 2;t 1;n). We
construct a chain of R-operators corresponding to the described procedure. The 3-
particle amplitude M2;3(1;t 1;n)  expressed in terms of R-operators  according to [1]
is given by
M2;3(1;t 1;n) =R1t 1R1n2(1)2(en)4(n)2(t 1) (20)
Then  adding the particles ft 2;t 3;:::; 2gusing the ISL  we obtain the expression
forM2;t(1;2;:::;t 2;t 1;n)
M2;t(1;2;:::;t 2;t 1;n) =R21R23:::Rt 21Rt 2t 1R1t 1R1n
t 1;n
1;:::;t 2;t 1;n(21)
where
1;:::;t 2;t 1;n2(1)2(2):::2(t 2)2(et 1)4(t 1)2(en)4(n)  and
the superscripts t 1;ndistinguish the delta-functions containing ( e ) variables.
Now  according to the proved formula 19  we attach to the obtained amplitude
M2;t(1;2;:::;t 2;t 1;n) the 3-particle MHV subamplitude at the outer end n. In the
language of R-operators  the given transformation of the amplitude M2;t(1;2;:::;t 
2;t 1;n) corresponds to the expression RntM2;t(1;2;:::;t 2;t 1;n)2(et)4(t).
Further  applying the ISL  we add to the obtained on-shell diagram the external ends
t+ 1;t+ 2;:::;n   which yields the following expression
Rn 1n 2Rn 1n:::Rt+1tRt+1nRntR21R23:::Rt 21Rt 2t 1R1t 1R1n
t 1;t;n
1;:::;n (22)
A connection between R-invariants and R-operators inN= 4sYM 7
where the appropriate basic state
1;:::;n is determined
1;:::;n = (23)
2(1)2(2):::2(t 2)2(et 1)4(t 1)2(et)4(t)2(t+1):::2(n 1)2(en)4(n)
It remains only to turn it into a BCFW diagram by adding the BCFW bridge to the outer
ends 1 and n. According to [1] such a bridge is implemented using the R1noperator.
Thus  acting on the obtained on-shell diagram with the operator R1n  we get the desired
BCFW-diagram. This BCFW diagram  according to the paper [19]  corresponds to the
expressionMMHV
2;nRn;2;t. That is  we get the expression for Rn;2;tinR-operators:
2;nRn;2;t= (24)
R1nRn 1n 2Rn 1n:::Rt+1tRt+1nRntR21R23:::Rt 21Rt 2t 1R1t 1R1n
1;2;:::;n
4.2. Recurrent term of the BCFW decomposition
Now we turn to the last diagram in the BCFW decomposition in Fig. 1. This
diagram can be viewed as adding a particle with the number 1 through the ISL to
the subamplitude MNMHV
3;n 1(2;3;:::;n ). According to [1] this type of the ISL is realized
through a pair of R-operators in the following form
R1nR12MNMHV
3;n 1(2;3;:::;n )2(1) (25)
The amplitude MNMHV
3;n 1(2;3;:::;n ) in its turn  is decomposed recursively with the
help of BCFW into the sum of amplitudes of the form given in Fig. 3 and the term
Figure 3. Diagram arising at the second ( s= 2) recursion step (the missing ends
1:::s 1 are added with the ISL).
containing an NMHV subamplitude. The amplitude given above in Fig. 3 diers from
the previously calculated ones (non-recurrent terms) by re-designation of the ends  and
thus corresponds to the expression
2;n 1(2;3;:::;n )Rn;3;t: (26)
A connection between R-invariants and R-operators inN= 4sYM 8
Acting on it with a pair of R1nR12operators  according to the formula (25)  we obtain
R1nR12MMHV
2;n 1(2;3;:::;n )Rn;3;t2(1) =MMHV
2;n(1;2;:::;n )Rn;3;t (27)
becauseRn;s;tdoes not explicitly depend on e2anden  which directly follows from the
expression forR-invariants (Eq. (8)). Thus  with a recursion depth equal to s(the rst
decomposition step corresponds to s= 1)  theRinvariant ofRn;s+1;tis obtained. We
construct its expression in terms of R-operators in the same way as was done in the rst
iteration of the BCFW decomposition. As a result  we obtain a general expression for
an arbitraryR-invariantRn;s+1;tterms ofR-operators
2;n(1;2;:::;n )Rn;s+1;t=
I0
1:::I0
s 1RsnIn 1:::It+1RntI(s)
s+1:::I(s)
t 2Rst 1Rsn
1;2;:::;n (28)
whereI0
kRknRkk+1 IkRkk 1RknandI(s)
kRksRkk+1- for brevity  we denote the
pairs ofR-operators implementing the addition of external ends through the ISL.
1;:::;n = (29)
2(1)2(2):::2(t 2)2(et 1)4(t 1)2(et)4(t)2(t+1):::2(n 1)2(en)4(n)
is an appropriate basic state for the chain of R-operators in the general formula (28)
which is the main result of the paper.
5. Discussion and","solutions  reproducing the amplitude.
In order to continue the program of studying N= 4 sYM by the","The formula (28) can be understood following way. One starts with the action of
operatorsRst 1Rsnwhich generates a 3-particle MHV amplitude MMHV
2;n(s;t 1;n).
Then  the chain of operators I(s)
t 2adds the legs s+ 1;:::;t 2 resulting in
an MHV amplitude MMHV
2;n(s;s+ 1;:::;t 1). Having done that  the operator Rntin
accordance with Eq. (19) adds an MHV subamplitude on the leg nwith the outer end
t. Then  the sequence In 1:::It+1appends the legs t+ 1;:::;n 1 andRsnrealizes the
BCFW-bridge for the ends sandn  thus we arrive at the diagram depicted in Fig. 3.
Finally  the series of operators I0
s 1appends the legs 1 ;::;s 1  nishing the
construction of the general term MMHV
2;n(1;2;:::;n )Rn;s+1;tof the super-BCFW expansion
in the NMHV sector.
Note that the procedure described in the paper for constructing tree amplitudes
by 'building up' one of the subamplitudes at the outer end of the other (merging
the subamplitudes and adding the BCFW-bridge) is suitable for any tree amplitude in
N= 4 sYM and not just for merging MHV amplitudes  i.e. we can express generalized
R-invariants of any order (i.e. those  that appear in the NkMHV sector) through R-
operators.
Thus  in this work we have solved the BCFW relation in the NMHV sector using R-
operators and obtained the general expression (28) for an arbitrary R-invariant through
A connection between R-invariants and R-operators inN= 4sYM 9
the chain of R-operators. We see the construction of a closed formula for the generalized
R-invariants in terms of R-operators as the next step to study. This would give us a
complete solution to the problem of nding an arbitrary tree scattering amplitude in
N= 4 sYM in terms of R-operators in the spirit of the QISM method.
6. Acknowledgements
V. K. Kozin thanks NORDITA for hospitality. The author thanks I. E. Shenderovich
for formulating the problem and S. E. Derkachev for the fruitful discussions.
Appendix: Proof of Main Lemma
To begin the proof of Eq. (19)  we calculate the LHS
Rn;i+1M(1;2:::i;n )2(ei+1)4(i+1) =
=Z+1
zM(1;2:::i;n zi+1;en;n)2(ei+1+zn)4(i+1+zn) =
zM(z+[i+ 11]
[n1])4(i+1+zn) =
=[1n]
[i+ 11]4(i+1 [i+ 11]
[n1]n)M(1;2;:::i;en+[i+ 11]
[n1]ei+1;en;n)([i+ 1n])(.1)
We now turn to the calculation of the right-hand side of Eq. (19). It corresponds to the
algebraic expression written in the right part of the statement:
Z
0)M(1;2:::i;j P0i;j P0];0)MMHV
2;3(jP0i;jP0];0;i+ 1;n) =
0)M(1;2:::i; P0;0)
4(P0+Pi+1+Pn)8(jP0i0+ji+ 1ii+1+jnin)
hP0i+ 1ihi+ 1nihnP0i=
0)M(1;2:::i; P0;0)4(P0+Pi+1+Pn)
hP0i+ 1ihi+ 1nihnP0i
hP0i+ 1i44(0 hi+ 1ni
hP0i+ 1in)4(i+1 hnP0i
hP0i+ 1in) =
d4P0(P2
0)M(1;2:::hi+ 1ni
hP0i+ 1in; P0)hP0i+ 1i3
hi+ 1nihnP0i
4(i+1 hnP0i
hP0i+ 1in)4(P0+Pi+1+Pn)(.2)
From the delta-function 4(P0+Pi+1+Pn) it follows that P0=Pi+1+Pn
 jP0ihP0j=ji+ 1ihi+ 1j+jnihnj (.3)
A connection between R-invariants and R-operators inN= 4sYM 10
The 3-particle special kinematics [20] yields [ P0j[i+ 1j[njand thus
[i+ 1j[nj)[i+ 1j=[i+ 11]
[n1][nj)
 jP0ihP0j= (jni+[i+ 11]
[n1]ji+ 1i)hnj(.4)
Using the analytical continuation of Weyl spinors j P0] = jP0] andj P0i=jP0i
one may rewrite  jP0ihP0jasj P0ih P0j. Since [P0j[njthan from little group
scaling [20] it follows  that one may assume in Eq. (.4)  jP0] =j P0] =jn] and
jP0i=j P0i=jni+[i+11]
[n1]ji+ 1i.
Integration over the variable P0is very simple  since it enters the integrand through
the delta function  which imposes the restriction (.3). Therefore  we exclude P0
everywhere in the integral. Let us start with the expressionhnP0i
hP0i+1i. For this purpose
we multiply (.3) from the left by hP0j  and from the right by j1]  then we obtain
hnP0i
hP0i+ 1i=[i+ 11]
[n1](.5)
Now we expresshi+1ni
hP0i+1imultiplying (.3) from the left by hi+ 1j  and from the right by
j1] which yieldshi+ 1ni=hP0i+ 1isince [P0j= [nj. The delta function (P2
0)  given
that4(P0+Pi+1+Pn)  equals
((Pi+1+Pn)2) =(2Pi+1Pn) =(hi+ 1ni[ni+ 1]) =([ni+ 1])
hi+ 1ni(.6)
Total numerical multiplier under the integral sign (.2) takes the form
hP0i+ 1i3
hi+ 1ni2hnP0i=hi+ 1ni
hnP0i; (.7)
wherehi+ 1ni=hP0i+ 1iis used. Let us calculate it  multiplying (.3) from the left by
hnj  we get
 hnP0i[P0j=hni+ 1i[i+ 1j=hni+ 1i[i+ 11]
[n1][nj (.8)
Since [P0j= [nj  then
hi+ 1ni
hnP0i=[1n]
[i+ 11](.9)
Finally  performing the integration over P0in the last line of Eq. (.2)  we arrive at
((Pi+1+Pn)2)M(1;2:::hi+ 1ni
hP0i+ 1in;j P0i;j P0])hP0i+ 1i3
=([i+ 1n])[1n]
[i+ 11]M(1;2:::n+[i+ 11]
[n1]i+1;en;n)4(i+1 [i+ 11]
[n1]n)(.10)
The result obtained coincides with the expression (.1)  i.e the left hand side of Eq. (19)
that completes the proof.
A connection between R-invariants and R-operators inN= 4sYM 11
[1] D. Chicherin  S. Derkachov and R. Kirschner  Yang-Baxter operators and scattering amplitudes
in N=4 super-Yang-Mills theory  Nucl. Phys. B881 (2014) 467 [ 1309.5748 ].
[2] G. P. Korchemsky and E. Sokatchev  Superconformal invariants for scattering amplitudes in N=4
SYM theory  Nucl. Phys. B839 (2010) 377 [ 1002.4625 ].
[3] J. M. Drummond and L. Ferro  Yangians  Grassmannians and T-duality  JHEP 07(2010) 027
[1001.3348 ].
[4] R. Frassek  D. Meidinger  D. Nandan and M. Wilhelm  On-shell diagrams  Gramannians and
integrability for form factors  JHEP 01(2016) 182 [ 1506.08192 ].
[5] N. Kanning  T. Lukowski and M. Staudacher  A shortcut to general tree-level scattering
amplitudes inN= 4SYM via integrability  Fortsch. Phys. 62(2014) 556 [ 1403.3382 ].
[6] J. Broedel  M. de Leeuw and M. Rosso  A dictionary between R-operators  on-shell graphs and
Yangian algebras  JHEP 06(2014) 170 [ 1403.3670 ].
[7] L. V. Bork and A. I. Onishchenko  Wilson lines  Grassmannians and gauge invariant o-shell
amplitudes inN= 4SYM  JHEP 04(2017) 019 [ 1607.02320 ].
[8] R. Britto  F. Cachazo  B. Feng and E. Witten  Direct proof of the tree-level scattering amplitude
recursion relation in yang-mills theory  Phys. Rev. Lett. 94(2005) 181602.
[9] J. M. Henn and J. C. Plefka  Scattering Amplitudes in Gauge Theories   vol. 883. 2014
10.1007/978-3-642-54022-6.
[10] S. J. Parke and T. R. Taylor  An Amplitude for nGluon Scattering  Phys. Rev. Lett. 56(1986)
2459.
[11] J. Drummond  J. Henn  G. Korchemsky and E. Sokatchev  Dual superconformal symmetry of
scattering amplitudes in n=4 super-yangmills theory  Nuclear Physics B 828 (2010) 317 .
[12] J. M. Drummond  J. M. Henn and J. Plefka  Yangian symmetry of scattering amplitudes in N=4
super Yang-Mills theory  JHEP 05(2009) 046 [ 0902.2987 ].
[13] L. D. Faddeev  Algebraic aspects of Bethe Ansatz  Int. J. Mod. Phys. A10 (1995) 1845
[hep-th/9404013 ].
[14] L. D. Faddeev  E. K. Sklyanin and L. A. Takhtajan  The Quantum Inverse Problem Method. 1
Theor. Math. Phys. 40(1980) 688.
[15] P. P. Kulish and E. K. Sklyanin  QUANTUM SPECTRAL TRANSFORM METHOD. RECENT
DEVELOPMENTS  Lect. Notes Phys. 151 (1982) 61.
[16] P. P. Kulish  N. Yu. Reshetikhin and E. K. Sklyanin  Yang-Baxter Equation and Representation
Theory. 1.  Lett. Math. Phys. 5(1981) 393.
[17] N. Arkani-Hamed  J. L. Bourjaily  F. Cachazo  A. B. Goncharov  A. Postnikov and J. Trnka
Grassmannian Geometry of Scattering Amplitudes  1212.5605 .
[18] N. Arkani-Hamed  F. Cachazo  C. Cheung and J. Kaplan  The s-matrix in twistor space  Journal
of High Energy Physics 2010 (2010) 110.
[19] J. M. Drummond and J. M. Henn  All tree-level amplitudes in N=4 SYM  JHEP 04(2009) 018
[0808.2475 ].
[20] H. Elvang and Y.-t. Huang  Scattering Amplitudes in Gauge Theory and Gravity . Cambridge
University Press  2015  10.1017/CBO9781107706620  [ 1308.1697 ].
R-operators in N = 4 super-Yang-Mills theory
V. K. Kozin1 2 3
9
1 1Science Institute  University of Iceland  Dunhagi 3  IS-107  Reykjavik  Iceland
0
2
v
o
N
3 Abstract. TheBCFWrecursionrelationinN =4super-Yang-Millstheoryissolved
]
h invariants are obtained in terms of the chains of R-operators acting on an appropriate
t
- basic state.
p
e
h
[
2  1. Introduction
6
TheR-operator formalismintroduced in [1]establishes a connectionbetween calculating
5
4 the scattering amplitudes in N = 4 super-Yang-Mills theory and integrable systems
4. has been studied e.g. in [2] [3] The framework of R-operators was developed in a number
1 graded permutations encoding the on-shell graphs and chains of R-operators acting on
:
v a suitable basic state  as well as a connection to the top-cell graphs.
i
X As it has been shown in [1] the amplitude terms can be obtained by acting on basic
r states (formed by products of delta-functions) by products of Yang-Baxter R-operators.
a
These operators are deﬁned from the L-matrices by the RLL-intertwining relation. The
In order to continue the program of studying N = 4 sYM by the methods used for
of R-operators for constructing the tree amplitudes in N = 4 sYM in the NMHV sector
the scattering amplitudes in N = 4 sYM. In Section 3 we show the connection according
to [1] between the gl(4|4) spin chains and scattering amplitudes in N = 4 sYM  and
introduce the notation for the main objects of the paper – Yang-Baxter R-operators
A connection between R-invariants and R-operators in N = 4 sYM 2
provide a solution for the BCFW [8] recursive relation in N = 4 sYM in the NMHV
the chains of R-operators acting on an appropriate basic state  which is the main result
2. Amplitudes in N = 4 sYM
The fact that the theory N = 4 sYM is supersymmetric allows one to introduce
a superﬁeld that combines all the ﬁelds into one function deﬁned on the on-shell
superspace [9] (λα λ(cid:101)  ηA)
α˙
1 1 1
Φ(λ λ(cid:101) η) = g++ηAψ + ηAηBφ + (cid:15) ηAηBηCψD+ (cid:15) ηAηBηCηDg− (1)
A AB ABCD ABCD
2! 3! 4!
where the capital Latin letters A B C D denote the indices of the fundamental
representation of the group SU(4)   and (cid:15) is the Levi-Civita symbol  ηA are
R ABCD
Grassmann variables. With the help of superﬁelds it is possible to construct a
superamplitude — a generating function for all possible scattering amplitudes of a given
M (Φ  ... Φ ) ≡ M ((λ  λ(cid:101)  η ) ... (λ  λ(cid:101)  η )) ≡
n 1 n n 1 1 1 n n n
≡ M ((p  η ) ... (p  η )) ≡ M (1 ... n) (2)
n 1 1 n n n
n particles in N = 4 sYM is
δ4(p)δ8(q)
M ({λ  λ(cid:101)  η }) = P ({λ  λ(cid:101)  η }) (3)
n i i i n i i i
(cid:104)12(cid:105)(cid:104)23(cid:105)...(cid:104)n1(cid:105)
where p = p + ... + p – total momentum  q = q + ... + q = |1(cid:105)η + ... + |n(cid:105)η –
1 n 1 n 1 n
total supermomentum. The spinors λ := |p(cid:105) and λ(cid:101)α˙ := |p] correspond to the states
α
with helicity ±1/2 respectively. P ({λ  λ(cid:101)  η }) has the form of a polynomial in η and
n i i i i
type Nk−2MHV
Pn({λi λ(cid:101)i ηi}) = Pn(0) + Pn(4) + Pn(8) +···+ Pn(4n−16)
↓ ↓ ↓ ↓ (4)
MHV NMHV N2MHV MHV
P(0) = 1 and P(l) ∼ O(ηl). This  in particular  implies the Park-Taylor formula [10] for
n n
MHV amplitudes in N = 4 sYM
MMHV = (5)
2 n (cid:104)12(cid:105)(cid:104)23(cid:105)...(cid:104)n1(cid:105)
A connection between R-invariants and R-operators in N = 4 sYM 3
where M denotes Nk−2MHV scattering amplitude of n particles  i.e. M corresponds
k n 2 n
to MHV  M – NMHV etc. The amplitude type is sometimes additionally indicated
3 n
as above  for example MMHV.
2 n
BCFW relations in N = 4 sYM  the so-called super-BCFW relation [9]. The analytic
form of the super-BCFW for Nk−2MHV amplitude is
(cid:90)
(cid:88) 1
M (1 2 ... n) = d4Pd4ηM ((pˆ  ηˆ ) ... (p  η ) (p η)) ·
k;n L 1 1 nL−1 nL−1 P2
kL+kR=k+1
·M ((−p η) (p  η ) ... (pˆ  ηˆ )) (6)
R nR+1 nR+1 n n
P2
where p = P + z λ λ(cid:101) and z = L   whereas the subamplitudes M and M
PL 1 n PL (cid:104)1|PL|n] L R
include momentum δ-functions.
2.1. R-invariants and dual superconformal symmetry
general analytic expression for tree NMHV amplitudes in N = 4 sYM was initially
(cid:88)
MNMHV = MMHV R (7)
3 n 2 n n;st
|s−t|≥2
where R – dual superconformal invariants (R-invariants). The explicit form of R
r;st r;st
(cid:104)ss−1(cid:105)(cid:104)tt−1(cid:105)δ4(Ξ )
r;st
R = (8)
r;st x2 (cid:104)r|x x |t(cid:105)(cid:104)r|x x |t−1(cid:105)(cid:104)r|x x |s(cid:105)(cid:104)r|x x |s−1(cid:105)
st rs st rs st rt ts rt ts
where x := p +...+p   θ := q +...+q are dual variables. At b < a we have
ab a b−1 ab a b−1
x = −x . The Grassmann-odd quantity Ξ is deﬁned by
ab ba r;st
Ξ := (cid:104)r|x x |θ (cid:105)+(cid:104)r|x x |θ (cid:105)
r;st rs st tr rt ts sr
Expressions of the form (cid:104)r|x x |t(cid:105) should be interpreted as (cid:104)r|a(x ) (x )c˙b|t(cid:105) .
rs st rs ac˙ st b
N = 4 sYM into an inﬁnite-dimensional algebra called Yangian Y(psu(2 2|4)). Then
In the paper [1]  each tree scattering amplitude of n particles M in N = 4 sYM is
associated with a gl(4|4) spin chain of length n. As we know from the paper [13]  a
A connection between R-invariants and R-operators in N = 4 sYM 4
canonical variables x = (x )N+M  p = (p )N+M  satisfying the commutation relations
a a=1 a a=1
{x  p ] = −δ where {x  p ] is a graded commutator  N and M are correspondingly
a b ab a b
[T(u)] = [L (u)] [L (u)] ...[L(u)] (9)
ac 1 ab1 2 b1b2 bn−1c
where L-operators
[L(u)] = uδ +x p . (10)
ab ab a b
Further  the authors of the paper [1] introduce the R-operators deﬁned by the RLL-
R (u−v)[L (u)] [L (v)] = [L (v)] [L (u)] R (u−v) (11)
12 1 ab 2 bc 1 ab 2 bc 12
and give the solution to the RLL-relation for gl(4|4) (relevant to N = 4 sYM)
(cid:90) +∞ dz
R (u) = e−z(p1·x2) (12)
12 z1−u
Finally  the connection of a spin chain with N = 4 sYM is established by the following
deﬁnition of canonically conjugate variables
x := (λ  ∂  ∂ ) (13)
α λ(cid:101)α˙ ηA
p := (∂  −λ(cid:101)  −η ) (14)
λα α˙ A
Then the action of the operator R (u) on an arbitrary function F(λ  λ(cid:101)  η  λ  λ(cid:101)  η ) is
ij i i i j j j
R (u)F(λ  λ(cid:101)  η  λ  λ(cid:101)  η ) = F(λ −zλ  λ(cid:101)  η  λ  λ(cid:101) +zλ(cid:101)  η +zη ) (15)
ij i i i j j j z1−u i j i i j j i j i
in [1]  the condition of Yangian invariance of scattering amplitudes in N = 4 sYM is
formulated in that way  that the amplitude M is an eigenfunction of the monodromy
T(u)M = C ·M (16)
where the eigenvalue C plays a minor role. Thus  a connection is established between
the scattering amplitudes in N = 4 sYM and gl(4|4) spin chains.
The R-operator from the equation (12) will be the main construction object of
method. The essentially non-local object — amplitude M   depending on the variables
A connection between R-invariants and R-operators in N = 4 sYM 5
of all n external particles will be built using the product of R -operators  each of which
give the formulas for 3-particle scattering amplitudes MMHV and MMHV in N = 4 sYM
2 3 1 3
MMHV = R R Ω and Ω = δ2(λ )δ2(λ )δ2(λ(cid:101) )δ4(η )  (17)
1 3 12 23 1 3 1 3 1 2 3 3
MMHV = R R Ω and Ω = δ2(λ )δ2(λ(cid:101) )δ4(η )δ2(λ(cid:101) )δ4(η )  (18)
2 3 23 12 2 3 2 3 1 2 2 3 3
where R ≡ R (0).
ij ij
the index i denotes the rightmost external leg of the left MHV subamplitude.
in Fig. 1 looks exactly like a BCFW-diagram with added ”bridges” (the so-called
BCFW-bridge). According to [1]  the R -operator implements this bridge. For on-
1n
(cid:82)
– here each internal line is assigned an integral d4ηd4Pδ(P2).
R M(1 2 ... i n)δ2(λ(cid:101) )δ4(η ) = (19)
n i+1 i+1 i+1
= dη d4P δ(P2)M(1 2 ... i {|−P (cid:105) |−P ] η })MMHV({|P (cid:105) |P ] η } i+1 n)
0 0 0 0 0 0 2 3 0 0 0
A connection between R-invariants and R-operators in N = 4 sYM 6
the diagram expansion of the NMHV amplitude (Fig. 1). To do this  ﬁrst build the
amplitude MMHV(1 2...t−2 t−1 n).
2 t
We start the construction with a 3-part amplitude MMHV(1 t − 1 n) and add
2 3
the ends t − 2 t − 3 ... 2 to the left using the Inverse Soft Limit (ISL) using R-
operators [18  1]. As a result  we get the amplitude M (1 2 ... t − 2 t − 1 n). We
particle amplitude M (1 t−1 n)  expressed in terms of R-operators  according to [1]
M (1 t−1 n) = R R δ2(λ )δ2(λ(cid:101) )δ4(η )δ2(λ ) (20)
2 3 1t−1 1n 1 n n t−1
Then  adding the particles {t−2 t−3 ... 2} using the ISL  we obtain the expression
for M (1 2 ... t−2 t−1 n)
M (1 2 ... t−2 t−1 n) = R R ·...·R R R R Ωt−1 n (21)
2 t 21 23 t−21 t−2t−1 1t−1 1n 1 ... t−2 t−1 n
where Ωt−1 n ≡ δ2(λ )δ2(λ ) · ... · δ2(λ )δ2(λ(cid:101) )δ4(η )δ2(λ(cid:101) )δ4(η )  and
1 ... t−2 t−1 n 1 2 t−2 t−1 t−1 n n
the superscripts t − 1 n distinguish the delta-functions containing (λ(cid:101)  η) variables.
M (1 2 ... t−2 t−1 n) the 3-particle MHV subamplitude at the outer end n. In the
language of R-operators  the given transformation of the amplitude M (1 2 ... t −
2 t − 1 n) corresponds to the expression R M (1 2 ... t − 2 t − 1 n)δ2(λ(cid:101) )δ4(η ).
nt 2 t t t
t+1 t+2 ... n  which yields the following expression
R R ·...·R R ·R ·R R ·...·R R R R Ωt−1 t n (22)
n−1n−2 n−1n t+1t t+1n nt 21 23 t−21 t−2t−1 1t−1 1n 1 ... n
A connection between R-invariants and R-operators in N = 4 sYM 7
where the appropriate basic state Ωt−1 t n is determined
1 ... n
Ωt−1 t n = (23)
δ2(λ )δ2(λ )·...·δ2(λ )δ2(λ(cid:101) )δ4(η )δ2(λ(cid:101) )δ4(η )δ2(λ )·...·δ2(λ )δ2(λ(cid:101) )δ4(η )
1 2 t−2 t−1 t−1 t t t+1 n−1 n n
ItremainsonlytoturnitintoaBCFWdiagrambyaddingtheBCFWbridgetotheouter
ends 1 and n. According to [1] such a bridge is implemented using the R operator.
Thus  acting on the obtained on-shell diagram with the operator R   we get the desired
expression MMHVR . That is  we get the expression for R in R-operators:
2 n n;2 t n;2 t
MMHVR = (24)
2 n n;2 t
R ·R R ·...·R R ·R ·R R ·...·R R R R Ωt−1 t n
1n n−1n−2 n−1n t+1t t+1n nt 21 23 t−21 t−2t−1 1t−1 1n 1 2 ... n
the subamplitude MNMHV(2 3 ... n). According to [1] this type of the ISL is realized
3 n−1
R R MNMHV(2 3 ... n)δ2(λ ) (25)
1n 12 3 n−1 1
The amplitude MNMHV(2 3 ... n) in its turn  is decomposed recursively with the
Figure 3. Diagram arising at the second (s = 2) recursion step (the missing ends
1...s−1 are added with the ISL).
containing an NMHV subamplitude. The amplitude given above in Fig. 3 diﬀers from
MMHV(2 3 ... n)R . (26)
2 n−1 n;3 t
A connection between R-invariants and R-operators in N = 4 sYM 8
Acting on it with a pair of R R operators  according to the formula (25)  we obtain
1n 12
R R MMHV(2 3 ... n)R δ2(λ ) = MMHV(1 2 ... n)R (27)
1n 12 2 n−1 n;3 t 1 2 n n;3 t
because R does not explicitly depend on λ(cid:101) and λ(cid:101)   which directly follows from the
n;s t 2 n
expression for R-invariants (Eq. (8)). Thus  with a recursion depth equal to s (the ﬁrst
decomposition step corresponds to s = 1)  the R invariant of R is obtained. We
n;s+1 t
construct its expression in terms of R-operators in the same way as was done in the ﬁrst
an arbitrary R-invariant R terms of R-operators
MMHV(1 2 ... n)R =
2 n n;s+1 t
I(cid:48) ·...·I(cid:48) ·R ·I ·...·I ·R ·I(s) ·...·I(s) ·R R Ωt−1 t n (28)
1 s−1 sn n−1 t+1 nt s+1 t−2 st−1 sn 1 2 ... n
where I(cid:48) ≡ R R   I ≡ R R and I(s) ≡ R R - for brevity  we denote the
k kn kk+1 k kk−1 kn k ks kk+1
pairs of R-operators implementing the addition of external ends through the ISL.
Ωt−1 t n = (29)
δ2(λ )δ2(λ )·...·δ2(λ )δ2(λ(cid:101) )δ4(η )δ2(λ(cid:101) )δ4(η )δ2(λ )·...··δ2(λ )δ2(λ(cid:101) )δ4(η )
operators R R which generates a 3-particle MHV amplitude MMHV(s t − 1 n).
st−1 sn 2 n
Then  the chain of operators I(s) · ... · I(s) adds the legs s + 1 ... t − 2 resulting in
s+1 t−2
an MHV amplitude MMHV(s s + 1 ... t − 1). Having done that  the operator R in
2 n nt
accordance with Eq. (19) adds an MHV subamplitude on the leg n with the outer end
t. Then  the sequence I ·...·I appends the legs t+1 ... n−1 and R realizes the
n−1 t+1 sn
BCFW-bridge for the ends s and n  thus we arrive at the diagram depicted in Fig. 3.
Finally  the series of operators I(cid:48) · ... · I(cid:48) appends the legs 1 .. s − 1  ﬁnishing the
1 s−1
constructionofthegeneraltermMMHV(1 2 ... n)R ofthesuper-BCFWexpansion
by ”building up” one of the subamplitudes at the outer end of the other (merging
N = 4 sYM and not just for merging MHV amplitudes  i.e. we can express generalized
A connection between R-invariants and R-operators in N = 4 sYM 9
complete solution to the problem of ﬁnding an arbitrary tree scattering amplitude in
N = 4 sYM in terms of R-operators in the spirit of the QISM method.
R M(1 2...i n)δ2(λ(cid:101) )δ4(η ) =
= M(1 2...i λ −zλ  λ(cid:101)  η )δ2(λ(cid:101) +zλ )δ4(η +zη ) =
n i+1 n n i+1 n i+1 n
z
(cid:90) +∞ dz [i+11] (.1)
= M ·δ(z + )δ4(η +zη ) =
i+1 n
z [n1]
[1n] [i+11] [i+11]
= δ4(η − η )M(1 2 ...i λ(cid:101) + λ(cid:101)  λ(cid:101)  η )δ([i+1n])
i+1 n n i+1 n n
[i+11] [n1] [n1]
dη d4P δ(P2)M(1 2...i |−P (cid:105) |−P ] η )MMHV(|P (cid:105) |P ] η  i+1 n) =
= dη d4P δ(P2)M(1 2...i −P  η )·
0 0 0 0 0
δ4(P +P +P )δ8(|P (cid:105)η +|i+1(cid:105)η +|n(cid:105)η )
0 i+1 n 0 0 i+1 n
· =
(cid:104)P i+1(cid:105)(cid:104)i+1n(cid:105)(cid:104)nP (cid:105)
0 0
(cid:90) δ4(P +P +P )
= dη d4P δ(P2)M(1 2...i −P  η ) 0 i+1 n · (.2)
0 0 0 0 0 (cid:104)P i+1(cid:105)(cid:104)i+1n(cid:105)(cid:104)nP (cid:105)
(cid:104)i+1n(cid:105) (cid:104)nP (cid:105)
·(cid:104)P i+1(cid:105)4δ4(η − η )δ4(η − 0 η ) =
0 0 n i+1 n
(cid:104)P i+1(cid:105) (cid:104)P i+1(cid:105)
(cid:90) (cid:104)i+1n(cid:105) (cid:104)P i+1(cid:105)3
= d4P δ(P2)M(1 2... η  −P ) 0 ·
0 0 (cid:104)P i+1(cid:105) n 0 (cid:104)i+1n(cid:105)(cid:104)nP (cid:105)
(cid:104)nP (cid:105)
·δ4(η − 0 η )δ4(P +P +P )
i+1 n 0 i+1 n
(cid:104)P i+1(cid:105)
From the delta-function δ4(P +P +P ) it follows that −P = P +P
0 i+1 n 0 i+1 n
−|P (cid:105)(cid:104)P | = |i+1(cid:105)(cid:104)i+1|+|n(cid:105)(cid:104)n| (.3)
A connection between R-invariants and R-operators in N = 4 sYM 10
The 3-particle special kinematics [20] yields [P | ∼ [i+1| ∼ [n| and thus
[i+11]
[i+1| ∼ [n| ⇒ [i+1| = [n| ⇒
[n1]
(.4)
−|P (cid:105)(cid:104)P | = (|n(cid:105)+ |i+1(cid:105))(cid:104)n|
Using the analytical continuation of Weyl spinors | − P ] = −|P ] and | − P (cid:105) = |P (cid:105)
0 0 0 0
one may rewrite −|P (cid:105)(cid:104)P | as | − P (cid:105)(cid:104)−P |. Since [P | ∼ [n| than from little group
scaling [20] it follows  that one may assume in Eq. (.4) −|P ] = | − P ] = |n] and
|P (cid:105) = |−P (cid:105) = |n(cid:105)+ [i+11]|i+1(cid:105).
0 0 [n1]
Integration over the variable P is very simple  since it enters the integrand through
the delta function  which imposes the restriction (.3). Therefore  we exclude P
everywhere in the integral. Let us start with the expression (cid:104)nP0(cid:105) . For this purpose
(cid:104)P0i+1(cid:105)
we multiply (.3) from the left by (cid:104)P |  and from the right by |1]  then we obtain
(cid:104)nP (cid:105) [i+11]
= (.5)
(cid:104)P i+1(cid:105) [n1]
Now we express (cid:104)i+1n(cid:105) multiplying (.3) from the left by (cid:104)i+1|  and from the right by
|1] which yields (cid:104)i+1n(cid:105) = (cid:104)P i+1(cid:105) since [P | = −[n|. The delta function δ(P2)  given
0 0 0
that δ4(P +P +P )  equals
0 i+1 n
δ([ni+1])
δ((P +P )2) = δ(2P ·P ) = δ((cid:104)i+1n(cid:105)[ni+1]) = (.6)
i+1 n i+1 n
(cid:104)i+1n(cid:105)
(cid:104)P i+1(cid:105)3 (cid:104)i+1n(cid:105)
=   (.7)
(cid:104)i+1n(cid:105)2(cid:104)nP (cid:105) (cid:104)nP (cid:105)
where (cid:104)i+1n(cid:105) = (cid:104)P i+1(cid:105) is used. Let us calculate it  multiplying (.3) from the left by
(cid:104)n|  we get
−(cid:104)nP (cid:105)[P | = (cid:104)ni+1(cid:105)[i+1| = (cid:104)ni+1(cid:105) [n| (.8)
Since −[P | = [n|  then
(cid:104)i+1n(cid:105) [1n]
= (.9)
Finally  performing the integration over P in the last line of Eq. (.2)  we arrive at
(cid:104)i+1n(cid:105) (cid:104)P i+1(cid:105)3
δ((P +P )2)M(1 2... η  |−P (cid:105) |−P ]) 0 ·
i+1 n n 0 0
(cid:104)P i+1(cid:105) (cid:104)i+1n(cid:105)(cid:104)nP (cid:105)
·δ4(η − 0 η ) = (.10)
= δ([i+1n]) M(1 2...λ + λ  λ(cid:101)  η )δ4(η − η )
n i+1 n n i+1 n
A connection between R-invariants and R-operators in N = 4 sYM 11
in N=4 super-Yang-Mills theory  Nucl. Phys. B881 (2014) 467 [1309.5748].
SYM theory  Nucl. Phys. B839 (2010) 377 [1002.4625].
[3] J. M. Drummond and L. Ferro  Yangians  Grassmannians and T-duality  JHEP 07 (2010) 027
[1001.3348].
integrability for form factors  JHEP 01 (2016) 182 [1506.08192].
amplitudes in N =4 SYM via integrability  Fortsch. Phys. 62 (2014) 556 [1403.3382].
Yangian algebras  JHEP 06 (2014) 170 [1403.3670].
[7] L. V. Bork and A. I. Onishchenko  Wilson lines  Grassmannians and gauge invariant oﬀ-shell
amplitudes in N =4 SYM  JHEP 04 (2017) 019 [1607.02320].
recursion relation in yang-mills theory  Phys. Rev. Lett. 94 (2005) 181602.
[9] J. M. Henn and J. C. Plefka  Scattering Amplitudes in Gauge Theories  vol. 883. 2014
[10] S. J. Parke and T. R. Taylor  An Amplitude for n Gluon Scattering  Phys. Rev. Lett. 56 (1986)
super Yang-Mills theory  JHEP 05 (2009) 046 [0902.2987].
[hep-th/9404013].
Theor. Math. Phys. 40 (1980) 688.
Theory. 1.  Lett. Math. Phys. 5 (1981) 393.
Grassmannian Geometry of Scattering Amplitudes  1212.5605.
[19] J. M. Drummond and J. M. Henn  All tree-level amplitudes in N=4 SYM  JHEP 04 (2009) 018
[0808.2475].
[20] H. Elvang and Y.-t. Huang  Scattering Amplitudes in Gauge Theory and Gravity. Cambridge
University Press  2015  10.1017/CBO9781107706620  [1308.1697].
Abstract.
The BCFW recursion relation in N = 4 super-Yang-Mills theory is solved
using Yang-Baxter R-operators in the NMHV sector.
Explicit expressions for R-
The R-operator formalism introduced in [1] establishes a connection between calculating
the scattering amplitudes in N = 4 super-Yang-Mills theory and integrable systems
a suitable basic state  as well as a connection to the top-cell graphs.
arXiv:1904.00456v2  [hep-th]  30 Nov 2019
A connection between R-invariants and R-operators in N = 4 sYM
superspace [9] (λα  �λ ˙α  ηA)
Φ(λ  �λ  η) = g++ηAψA+ 1
2!ηAηBφAB + 1
3!ϵABCDηAηBηCψ
D + 1
4!ϵABCDηAηBηCηDg− (1)
where the capital Latin letters A  B  C  D denote the indices of the fundamental
representation of the group SU(4)R  and ϵABCD is the Levi-Civita symbol  ηA are
Grassmann variables.
With the help of superﬁelds it is possible to construct a
Mn(Φ1  . . .   Φn) ≡ Mn((λ1  �λ1  η1)  . . .   (λn  �λn  ηn)) ≡
≡ Mn((p1  η1)  . . .   (pn  ηn)) ≡ Mn(1  . . .   n)
(2)
Mn({λi  �λi  ηi}) =
⟨12⟩⟨23⟩...⟨n1⟩Pn({λi  �λi  ηi})
(3)
where p = p1 + . . . + pn – total momentum  q = q1 + . . . + qn = |1⟩η1 + . . . + |n⟩ηn –
total supermomentum. The spinors λα := |p⟩ and �λ ˙α := |p] correspond to the states
with helicity ±1/2 respectively. Pn({λi  �λi  ηi}) has the form of a polynomial in ηi and
Pn({λi  �λi  ηi}) =
P (0)
+
P (4)
P (8)
+ · · · +
P (4n−16)
↓
MHV
N2MHV
(4)
= 1 and P (l)
n ∼ O(ηl). This  in particular  implies the Park-Taylor formula [10] for
M MHV
=
⟨12⟩⟨23⟩...⟨n1⟩
(5)
3
where Mk n denotes Nk−2MHV scattering amplitude of n particles  i.e. M2 n corresponds
to MHV  M3 n – NMHV etc. The amplitude type is sometimes additionally indicated
as above  for example M MHV
.
Mk;n(1  2  ...  n) =
�
d4Pd4ηML((ˆp1  ˆη1)  . . .   (pnL−1  ηnL−1)  (p  η)) 1
P 2·
· MR((−p  η)  (pnR+1  ηnR+1)  . . .   (ˆpn  ˆηn))
(6)
where p = P + zPLλ1�λn and zPL =
P 2
⟨1|PL|n]  whereas the subamplitudes ML and MR
M NMHV
= M MHV
Rn;st
(7)
where Rr;st – dual superconformal invariants (R-invariants). The explicit form of Rr;st
Rr;st =
⟨ss − 1⟩⟨tt − 1⟩δ4(Ξr;st)
st⟨r|xrsxst|t⟩⟨r|xrsxst|t − 1⟩⟨r|xrtxts|s⟩⟨r|xrtxts|s − 1⟩
(8)
where xab := pa + . . . + pb−1  θab := qa + . . . + qb−1 are dual variables. At b < a we have
xab = −xba. The Grassmann-odd quantity Ξr;st is deﬁned by
Ξr;st := ⟨r|xrsxst|θtr⟩ + ⟨r|xrtxts|θsr⟩
Expressions of the form ⟨r|xrsxst|t⟩ should be interpreted as ⟨r|a(xrs)a˙c(xst)˙cb|t⟩b.
N = 4 sYM into an inﬁnite-dimensional algebra called Yangian Y (psu(2  2|4)). Then
In the paper [1]  each tree scattering amplitude of n particles Mn in N = 4 sYM is
4
canonical variables x = (xa)N+M
a=1   p = (pa)N+M
a=1   satisfying the commutation relations
{xa  pb] = −δab where {xa  pb] is a graded commutator  N and M are correspondingly
[T(u)]ac = [L1(u)]ab1[L2(u)]b1b2 . . . [L(u)]bn−1c
(9)
[L(u)]ab = uδab + xapb.
(10)
R12(u − v)[L1(u)]ab[L2(v)]bc = [L1(v)]ab[L2(u)]bcR12(u − v)
(11)
R12(u) =
� +∞
dz
z1−ue−z(p1·x2)
(12)
x := (λα  ∂�λ ˙α  ∂ηA)
(13)
p := (∂λα  −�λ ˙α  −ηA)
(14)
Then the action of the operator Rij(u) on an arbitrary function F(λi  �λi  ηi  λj  �λj  ηj) is
Rij(u)F(λi  �λi  ηi  λj  �λj  ηj) =
z1−uF(λi − zλj  �λi  ηi  λj  �λj + z�λi  ηj + zηi) (15)
that is  it performs a BCFW shift on the spinor-helicity variables.
Also  as shown
T(u)M = C · M
(16)
method. The essentially non-local object — amplitude Mn  depending on the variables
give the formulas for 3-particle scattering amplitudes M MHV
and M MHV
1 3
in N = 4 sYM
= R12R23Ω1 3
and
Ω1 3 = δ2(λ1)δ2(λ2)δ2(�λ3)δ4(η3)
(17)
= R23R12Ω2 3
Ω2 3 = δ2(λ1)δ2(�λ2)δ4(η2)δ2(�λ3)δ4(η3)
(18)
where Rij ≡ Rij(0).
written diagrammatically as in Fig. 1.
where the sum is performed for all possible
subamplitude (recurrent term).
The right-hand side of the diagram decomposition
– here each internal line is assigned an integral
d4ηd4Pδ(P 2).
Rn i+1M(1  2  ...  i  n)δ2(�λi+1)δ4(ηi+1) =
(19)
dη0d4P0δ(P 2
0 )M(1  2  ...  i  {| − P0⟩  | − P0]  η0})M MHV
({|P0⟩  |P0]  η0}  i + 1  n)
amplitude M MHV
(1  2...t − 2  t − 1  n).
We start the construction with a 3-part amplitude M MHV
(1  t − 1  n) and add
the ends t − 2  t − 3  . . .   2 to the left using the Inverse Soft Limit (ISL) using R-
operators [18  1]. As a result  we get the amplitude M2 t(1  2  . . .   t − 2  t − 1  n). We
construct a chain of R-operators corresponding to the described procedure.
The 3-
particle amplitude M2 3(1  t − 1  n)  expressed in terms of R-operators  according to [1]
M2 3(1  t − 1  n) = R1t−1R1nδ2(λ1)δ2(�λn)δ4(ηn)δ2(λt−1)
(20)
Then  adding the particles {t − 2  t − 3  . . .   2} using the ISL  we obtain the expression
for M2 t(1  2  . . .   t − 2  t − 1  n)
M2 t(1  2  . . .   t − 2  t − 1  n) = R21R23 · ... · Rt−21Rt−2t−1R1t−1R1nΩt−1 n
1 ... t−2 t−1 n
(21)
where Ωt−1 n
≡
δ2(λ1)δ2(λ2) · ... · δ2(λt−2)δ2(�λt−1)δ4(ηt−1)δ2(�λn)δ4(ηn)  and
the superscripts t − 1  n distinguish the delta-functions containing (�λ  η) variables.
M2 t(1  2  . . .   t−2  t−1  n) the 3-particle MHV subamplitude at the outer end n. In the
language of R-operators  the given transformation of the amplitude M2 t(1  2  . . .   t −
2  t − 1  n) corresponds to the expression RntM2 t(1  2  . . .   t − 2  t − 1  n)δ2(�λt)δ4(ηt).
t + 1  t + 2  . . .   n  which yields the following expression
Rn−1n−2Rn−1n · ... · Rt+1tRt+1n · Rnt · R21R23 · ... · Rt−21Rt−2t−1R1t−1R1nΩt−1 t n
(22)
7
where the appropriate basic state Ωt−1 t n
is determined
Ωt−1 t n
1 ... n =
(23)
δ2(λ1)δ2(λ2) · ... · δ2(λt−2)δ2(�λt−1)δ4(ηt−1)δ2(�λt)δ4(ηt)δ2(λt+1) · ... · δ2(λn−1)δ2(�λn)δ4(ηn)
ends 1 and n. According to [1] such a bridge is implemented using the R1n operator.
expression M MHV
Rn;2 t. That is  we get the expression for Rn;2 t in R-operators:
Rn;2 t =
(24)
R1n · Rn−1n−2Rn−1n · ... · Rt+1tRt+1n · Rnt · R21R23 · ... · Rt−21Rt−2t−1R1t−1R1nΩt−1 t n
1 2 ... n
Now we turn to the last diagram in the BCFW decomposition in Fig. 1.
This
the subamplitude M NMHV
3 n−1 (2  3  . . .   n). According to [1] this type of the ISL is realized
R1nR12M NMHV
3 n−1 (2  3  . . .   n)δ2(λ1)
(25)
The amplitude M NMHV
3 n−1 (2  3  . . .   n) in its turn  is decomposed recursively with the
1...s − 1 are added with the ISL).
2 n−1(2  3  . . .   n)Rn;3 t.
(26)
8
Acting on it with a pair of R1nR12 operators  according to the formula (25)  we obtain
R1nR12M MHV
2 n−1(2  3  . . .   n)Rn;3 tδ2(λ1) = M MHV
(1  2  . . .   n)Rn;3 t
(27)
because Rn;s t does not explicitly depend on �λ2 and �λn   which directly follows from the
decomposition step corresponds to s = 1)  the R invariant of Rn;s+1 t is obtained. We
an arbitrary R-invariant Rn;s+1 t terms of R-operators
(1  2  ...  n)Rn;s+1 t =
I′
1 · ... · I′
s−1 · Rsn · In−1 · ... · It+1 · Rnt · I(s)
s+1 · ... · I(s)
t−2 · Rst−1RsnΩt−1 t n
(28)
where I′
k ≡ RknRkk+1  Ik ≡ Rkk−1Rkn and I(s)
k
≡ RksRkk+1 - for brevity  we denote the
(29)
δ2(λ1)δ2(λ2) · ... · δ2(λt−2)δ2(�λt−1)δ4(ηt−1)δ2(�λt)δ4(ηt)δ2(λt+1) · ... · ·δ2(λn−1)δ2(�λn)δ4(ηn)
The formula (28) can be understood following way.
One starts with the action of
operators Rst−1Rsn which generates a 3-particle MHV amplitude M MHV
(s  t − 1  n).
t−2 adds the legs s + 1  ...  t − 2 resulting in
an MHV amplitude M MHV
(s  s + 1  ...  t − 1). Having done that  the operator Rnt in
t. Then  the sequence In−1 · ... · It+1 appends the legs t + 1  ...  n − 1 and Rsn realizes the
Finally  the series of operators I′
s−1 appends the legs 1  ..  s − 1  ﬁnishing the
construction of the general term M MHV
(1  2  ...  n)Rn;s+1 t of the super-BCFW expansion
Rn i+1M(1  2...i  n)δ2(�λi+1)δ4(ηi+1) =
z M(1  2...i  λn − zλi+1  �λn  ηn)δ2(�λi+1 + zλn)δ4(ηi+1 + zηn) =
z M · δ(z + [i + 11]
)δ4(ηi+1 + zηn) =
[1n]
[i + 11]δ4(ηi+1 − [i + 11]
ηn)M(1  2  ...i  �λn + [i + 11]
�λi+1  �λn  ηn)δ([i + 1n])
(.1)
0 )M(1  2...i  | − P0⟩  | − P0]  η0)M MHV
(|P0⟩  |P0]  η0  i + 1  n) =
0 )M(1  2...i  −P0  η0)·
· δ4(P0 + Pi+1 + Pn)δ8(|P0⟩η0 + |i + 1⟩ηi+1 + |n⟩ηn)
⟨P0i + 1⟩⟨i + 1n⟩⟨nP0⟩
0 )M(1  2...i  −P0  η0)
δ4(P0 + Pi+1 + Pn)
⟨P0i + 1⟩⟨i + 1n⟩⟨nP0⟩·
· ⟨P0i + 1⟩4δ4(η0 − ⟨i + 1n⟩
⟨P0i + 1⟩ηn)δ4(ηi+1 −
⟨nP0⟩
⟨P0i + 1⟩ηn) =
d4P0δ(P 2
0 )M(1  2... ⟨i + 1n⟩
⟨P0i + 1⟩ηn  −P0)
⟨P0i + 1⟩3
⟨i + 1n⟩⟨nP0⟩·
· δ4(ηi+1 −
⟨P0i + 1⟩ηn)δ4(P0 + Pi+1 + Pn)
(.2)
From the delta-function δ4(P0 + Pi+1 + Pn) it follows that −P0 = Pi+1 + Pn
− |P0⟩⟨P0| = |i + 1⟩⟨i + 1| + |n⟩⟨n|
(.3)
10
The 3-particle special kinematics [20] yields [P0| ∼ [i + 1| ∼ [n| and thus
[i + 1| ∼ [n| ⇒ [i + 1| = [i + 11]
[n| ⇒
−|P0⟩⟨P0| = (|n⟩ + [i + 11]
|i + 1⟩)⟨n|
Using the analytical continuation of Weyl spinors | − P0] = −|P0] and | − P0⟩ = |P0⟩
one may rewrite −|P0⟩⟨P0| as | − P0⟩⟨−P0|. Since [P0| ∼ [n| than from little group
scaling [20] it follows  that one may assume in Eq. (.4) −|P0] = | − P0] = |n] and
|P0⟩ = | − P0⟩ = |n⟩ + [i+11]
[n1] |i + 1⟩.
Integration over the variable P0 is very simple  since it enters the integrand through
the delta function  which imposes the restriction (.3).
Therefore  we exclude P0
everywhere in the integral. Let us start with the expression
⟨P0i+1⟩. For this purpose
we multiply (.3) from the left by ⟨P0|  and from the right by |1]  then we obtain
⟨P0i + 1⟩ = [i + 11]
(.5)
Now we express
⟨i+1n⟩
⟨P0i+1⟩ multiplying (.3) from the left by ⟨i + 1|  and from the right by
|1] which yields ⟨i + 1n⟩ = ⟨P0i + 1⟩ since [P0| = −[n|. The delta function δ(P 2
0 )  given
that δ4(P0 + Pi+1 + Pn)  equals
δ((Pi+1 + Pn)2) = δ(2Pi+1 · Pn) = δ(⟨i + 1n⟩[ni + 1]) = δ([ni + 1])
⟨i + 1n⟩
(.6)
⟨i + 1n⟩2⟨nP0⟩ = ⟨i + 1n⟩
(.7)
where ⟨i + 1n⟩ = ⟨P0i + 1⟩ is used. Let us calculate it  multiplying (.3) from the left by
⟨n|  we get
− ⟨nP0⟩[P0| = ⟨ni + 1⟩[i + 1| = ⟨ni + 1⟩[i + 11]
[n|
(.8)
Since −[P0| = [n|  then
[i + 11]
(.9)
Finally  performing the integration over P0 in the last line of Eq. (.2)  we arrive at
δ((Pi+1 + Pn)2)M(1  2... ⟨i + 1n⟩
⟨P0i + 1⟩ηn  | − P0⟩  | − P0])
= δ([i + 1n])
[i + 11]M(1  2...λn + [i + 11]
λi+1  �λn  ηn)δ4(ηi+1 − [i + 11]
ηn)
(.10)
11
amplitudes in N = 4 SYM via integrability  Fortsch. Phys. 62 (2014) 556 [1403.3382].
amplitudes in N = 4 SYM  JHEP 04 (2017) 019 [1607.02320]."
R006,1,CVPR,"This research introduces MLB-YouTube  a new and complex dataset created for
nuanced activity recognition in baseball videos. This dataset is structured to
support two types of analysis: one for classifying activities in segmented videos
and another for detecting activities in unsegmented  continuous video streams. This
study evaluates several","Action recognition  a significant problem in computer vision  finds extensive use in sports. Profes-
sional sporting events are extensively recorded for entertainment  and these recordings are invaluable
for subsequent analysis by coaches  scouts  and media analysts. While numerous game statistics
are currently gathered manually  the potential exists for these to be replaced by computer vision
systems. Systems like PITCHf/x and Statcast have been employed by Major League Baseball (MLB)
to automatically record pitch speed and movement  utilizing a network of high-speed cameras and
radar to collect detailed data on each player. Access to much of this data is restricted from the public
domain.
This paper introduces MLB-YouTube  a novel dataset that includes densely annotated frames of activi-
ties extracted from broadcast baseball videos. Unlike many current datasets for activity recognition or
detection  our dataset emphasizes fine-grained activity recognition. The differences between activities
are often minimal  primarily involving the movement of a single individual  with a consistent scene
structure across activities. The determination of activity is based on a single camera perspective. This
study compares various methods for temporal feature aggregation  both for classifying activities in
segmented videos and for detecting them in continuous video streams.
2 Related Work
The field of activity recognition has garnered substantial attention in computer vision research. Initial
successes were achieved with hand-engineered features such as dense trajectories. The focus of more
recent studies has shifted towards the application of Convolutional Neural Networks (CNNs) for
activity recognition. Two-stream CNN architectures utilize both spatial RGB frames and optical
flow frames. To capture spatio-temporal characteristics  3D XYT convolutional models have been
developed. The development of these advanced CNN models has been supported by large datasets
such as Kinetics  THUMOS  and ActivityNet.
Several studies have investigated the aggregation of temporal features for the purpose of activity
recognition. Research has compared several pooling techniques and determined that both Long Short-
.
Term Memory networks (LSTMs) and max-pooling across entire videos yielded the best outcomes. It
has been discovered that pooling intervals from varying locations and durations is advantageous for
activity recognition. It was demonstrated that identifying and classifying key sub-event intervals can
lead to better performance.
Recently  segment-based 3D CNNs have been employed to capture spatio-temporal data concurrently
for activity detection. These methods depend on the 3D CNN to capture temporal dynamics  which
typically span only 16 frames. Although longer-term temporal structures have been explored  this was
usually accomplished with temporal pooling of localized features or (spatio-)temporal convolutions
with extended fixed intervals. Recurrent Neural Networks (RNNs) have also been applied to represent
transitions in activity between frames.
3 MLB-YouTube Dataset
We have compiled an extensive dataset from 20 baseball games of the 2017 MLB postseason  available
on YouTube  totaling over 42 hours of video. Our dataset includes two main parts: segmented videos
intended for activity recognition and continuous videos designed for activity classification. The
dataset’s complexity is amplified by the fact that it originates from televised baseball games  where a
single camera perspective is shared among various activities. Additionally  there is minimal variance
in motion and appearance among different activities  such as swinging a bat versus bunting. In
contrast to datasets like THUMOS and ActivityNet  which encompass a broad spectrum of activities
with diverse settings  scales  and camera angles  our dataset features activities where a single frame
might not be adequate to determine the activity.
The minor differences between a ball and a strike are illustrated in Figure 3. Differentiating between
these actions requires identifying whether the batter swings or not  detecting the umpire’s signal
(Figure 4) for a strike  or noting the absence of a signal for a ball. This is further complicated because
the batter or catcher can obstruct the umpire  and each umpire has their unique style of signaling a
strike.
Our dataset for segmented video analysis comprises 4 290 clips. Each clip is annotated for multiple
baseball actions  including swing  hit  ball  strike  and foul. Given that a single clip may contain
several activities  this is considered a multi-label classification task. Table 1 presents the complete
list of activities and their respective counts within the dataset. Additionally  clips featuring a pitch
were annotated with the type of pitch (e.g.  fastball  curveball  slider) and its speed. Furthermore  a
collection of 2 983 hard negative examples  where no action is present  was gathered. These instances
include views of the crowd  the field  or players standing idly before or after a pitch. Examples of
activities and hard negatives are depicted in Figure 2.
Our continuous video dataset includes 2 128 clips  each lasting between 1 and 2 minutes. Every
frame in these videos is annotated with the baseball activities that occur. On average  each continuous
clip contains 7.2 activities  amounting to over 15 000 activity instances in total.
Table 1: Activity classes and their instance counts in the segmented MLB-YouTube dataset.
Activity Count
No Activity 2983
Ball 1434
Strike 1799
Swing 2506
Hit 1391
Foul 718
In Play 679
Bunt 24
Hit by Pitch 14
2
4 Segmented Video Recognition Approach
We investigate different techniques for aggregating temporal features in segmented video activity
recognition. In segmented videos  the classification task is simpler because each frame corresponds to
an activity  eliminating the need for the model to identify the start and end of activities. Our methods
are based on a CNN that generates a per-frame or per-segment representation  derived from standard
two-stream CNNs using deep CNNs like I3D or InceptionV3.
Given video features vof dimensions T×D  where Trepresents the video’s temporal length and D
is the feature’s dimensionality  the usual approach for feature pooling involves max- or mean-pooling
across the temporal dimension  followed by a fully-connected layer for video clip classification  as
depicted in Fig. 5(a). This approach  however  yields a single representation for the entire video
losing temporal information. An alternative is to employ a fixed temporal pyramid with various
lengths  as shown in Fig 5(b)  dividing the video into intervals of lengths 1/2  1/4  and 1/8  and
max-pooling each. The pooled features are concatenated  creating a K×Drepresentation  where K
is the number of intervals in the temporal pyramid  and a fully-connected layer classifies the clip.
We also explore learning temporal convolution filters to aggregate local temporal structures. A kernel
of size L×1is applied to each frame  enabling each timestep representation to incorporate information
from adjacent frames. After applying max-pooling to the output of the temporal convolution  a fully-
connected layer is used for classification  as illustrated in Fig. 5(c).
While temporal pyramid pooling retains some structure  the intervals are fixed and predetermined.
Previous studies have shown that learning the sub-interval to pool is beneficial for activity recognition.
These learned intervals are defined by three parameters: a center g  a width σ  and a stride δ
parameterizing NGaussians. Given the video length T  the positions of the strided Gaussians are
first calculated as:
gn= 0.5−T−(gn+ 1)
N−1forn = 0 1  . . .   N −1
pt n=gn+ (t−0.5T+ 0.5)1
δfort = 0 1  . . .   T −1
The filters are then generated as:
Fm[i  t] =1
Zmexp
−(t−µi m)2
2σ2m
i∈ {0 1  . . .   N −1}  t∈ {0 1  . . .   T −1}
where Zmis a normalization constant.
We apply these filters Fto the T×Dvideo representation through matrix multiplication  yielding an
N×Drepresentation that serves as input to a fully-connected layer for classification. This method
is shown in Fig 5(d).
Additionally  we compare a bi-directional LSTM with 512 hidden units  using the final hidden state
as input to a fully-connected layer for classification. We frame our tasks as multi-label classification
and train these models to minimize binary cross-entropy:
L(v) =X
czclog(p(c|G(v))) + (1 −zc) log(1 −p(c|G(v)))
where G(v)is the function that pools the temporal information  and zcis the ground truth label for
class c.
5 Activity Detection in Continuous Videos
Detecting activities in continuous videos poses a greater challenge. The goal here is to classify each
frame according to the activities occurring. Unlike segmented videos  continuous videos feature
multiple sequential activities  often interspersed with frames of inactivity. This necessitates that
the model learn to identify the start and end points of activities. As a baseline  we train a single
fully-connected layer to serve as a per-frame classifier  which does not utilize temporal information
beyond that contained in the features.
3
We adapt the methods developed for segmented video classification to continuous videos by imple-
menting a temporal sliding window approach. We select a fixed window duration of Lfeatures  apply
max-pooling to each window (similar to Fig. 5(a))  and classify each pooled segment. This approach
is extended to temporal pyramid pooling by dividing the window of length Linto segments of lengths
L/2 L/4  and L/8  resulting in 14 segments per window. Max-pooling is applied to each segment
and the pooled features are concatenated  yielding a 14×D-dimensional representation for each
window  which is then used as input to the classifier.
For temporal convolutional models in continuous videos  we modify the segmented video approach by
learning a temporal convolutional kernel of length Land convolving it with the input video features.
This operation transforms input of size T×Dinto output of size T×D  followed by a per-frame
classifier. This enables the model to aggregate local temporal information.
To extend the sub-event model to continuous videos  we follow a similar approach but set T=Lin
Eq. 1  resulting in filters of length L. The T×Dvideo representation is convolved with the sub-event
filters F  producing an N×D×T-dimensional representation used as input to a fully-connected
layer for frame classification.
The model is trained to minimize per-frame binary classification:
t czt clog(p(c|H(vt))) + (1 −zt c) log(1 −p(c|H(vt)))
where vtis the per-frame or per-segment feature at time t H(vt)is the sliding window application of
one of the feature pooling methods  and zt cis the ground truth class at time t.
A method to learn ’super-events’ (i.e.  global video context) has been introduced and shown to be
effective for activity detection in continuous videos. This approach involves learning a set of temporal
structure filters modeled as NCauchy distributions. Each distribution is defined by a center xnand a
width γn. Given the video length T  the filters are constructed by:
xn=(T−1)(tanh( x′
n) + 1)
fn(t) =1
Znγn
π((t−xn)2+γ2n)exp(1−2|tanh( γ′
n)|)
where Znis a normalization constant  t∈ {1 2  . . .   T }  and n∈ {1 2  . . .   N }.
The filters are combined with learned per-class soft-attention weights A  and the super-event repre-
sentation is computed as:
Sc=X
nAc nX
tfn(t)·vt
where vis the T×Dvideo representation. These filters enable the model to focus on relevant
intervals for temporal context. The super-event representation is concatenated to each timestep and
used for classification. We also experiment with combining the super- and sub-event representations
to form a three-level hierarchy for event representation.
6 Experiments
6.1 Implementation Details
For our base per-segment CNN  we utilize the I3D network  pre-trained on the ImageNet and Kinetics
datasets. I3D has achieved state-of-the-art performance on segmented video tasks  providing a reliable
feature representation. We also employ a two-stream version of InceptionV3  pre-trained on Imagenet
and Kinetics  as our base per-frame CNN for comparison. InceptionV3 was chosen for its depth
compared to previous two-stream CNNs. Frames were extracted at 25 fps  and TVL1 optical flow
was computed and clipped to [−20 20]. For InceptionV3  features were computed every 3 frames
(8 fps)  while for I3D  every frame was used  with I3D having a temporal stride of 8  resulting in
3 features per second (3 fps). Models were implemented in PyTorch and trained using the Adam
optimizer with a learning rate of 0.01  decayed by a factor of 0.1 every 10 epochs  for a total of 50
epochs.
4
6.2 Segmented Video Activity Recognition
We initially conducted binary pitch/non-pitch classification for each video segment. This task is
relatively straightforward due to the distinct differences between pitch and non-pitch frames. The","capture the temporal organization of activities in videos. This evaluation starts
with categorizing segmented videos and progresses to applying these methods
to continuous video feeds. Additionally  this paper assesses the effectiveness of
different models in the challenging task of forecasting pitch velocity and type
using baseball broadcast videos. The findings indicate that incorporating temporal
dynamics into models is beneficial for detailed activity recognition.
1","Table 2: Performance on segmented videos for binary pitch/non-pitch classification.
Model RGB Flow Two-stream
InceptionV3 97.46 98.44 98.67
InceptionV3 + sub-events 98.67 98.73 99.36
I3D 98.64 98.88 98.70
I3D + sub-events 98.42 98.35 98.65
6.2.1 Multi-label Classification
We assessed various temporal feature aggregation methods by calculating the mean average precision
(mAP) for each video clip  a standard metric for multi-label classification. Table 4 compares the
performance of these methods. All methods surpass mean/max-pooling  highlighting the importance
of preserving temporal structure for activity recognition. Fixed temporal pyramid pooling and LSTMs
show some improvement. Temporal convolution offers a more significant performance boost but
requires substantially more parameters (see Table 3). Learning sub-events  as per previous research
yields the best results. While LSTMs and temporal convolutions have been used before  they need
more parameters and perform less effectively  likely due to overfitting. Moreover  LSTMs necessitate
sequential processing of video features  whereas other methods can be fully parallelized.
Table 3: Additional parameters required for models when added to the base model (e.g.  I3D or
Inception V3).
Model # Parameters
Max/Mean Pooling 16K
Pyramid Pooling 115K
LSTM 10.5M
Temporal Conv 31.5M
Sub-events 36K
Table 4: Mean Average Precision (mAP) results on segmented videos for multi-label classification.
Learning sub-intervals for pooling is found to be crucial for activity recognition.
Method RGB Flow Two-stream
Random 16.3 16.3 16.3
InceptionV3 + mean-pool 35.6 47.2 45.3
InceptionV3 + max-pool 47.9 48.6 54.4
InceptionV3 + pyramid 49.7 53.2 55.3
InceptionV3 + LSTM 47.6 55.6 57.7
InceptionV3 + temporal conv 47.2 55.2 56.1
InceptionV3 + sub-events 56.2 62.5 62.6
I3D + mean-pool 42.4 47.6 52.7
I3D + max-pool 48.3 53.4 57.2
I3D + pyramid 53.2 56.7 58.7
I3D + LSTM 48.2 53.1 53.1
I3D + temporal conv 52.8 57.1 58.4
I3D + sub-events 55.5 61.2 61.3
Table 5 shows the average precision for each activity class. Learning temporal structure is particularly
beneficial for frame-based features (e.g.  InceptionV3)  which capture less temporal information
5
compared to segment-based features (e.g.  I3D). Sub-event learning significantly aids in detecting
strikes  hits  foul balls  and hit-by-pitch events  which exhibit changes in video features post-event.
For instance  after a hit  the camera often tracks the ball’s trajectory  while after a hit-by-pitch  it
follows the player to first base  as illustrated in Fig. 6 and Fig. 7.
Table 5: Per-class average precision for segmented videos using two-stream features in multi-
label activity classification. Utilizing sub-events to discern temporal intervals of interest proves
advantageous for activity recognition.
Method Ball Strike Swing Hit Foul In Play Bunt Hit by Pitch
Random 21.8 28.6 37.4 20.9 11.4 10.3 1.1 4.5
InceptionV3 + max-pool 60.2 84.7 85.9 80.8 40.3 74.2 10.2 15.7
InceptionV3 + sub-events 66.9 93.9 90.3 90.9 60.7 89.7 12.4 29.2
I3D + max-pool 59.4 90.3 87.7 85.9 48.1 76.1 14.3 18.2
I3D + sub-events 62.5 91.3 88.5 86.5 47.3 75.9 16.2 21.0
6.2.2 Pitch Speed Regression
Estimating pitch speed from video frames is an exceptionally difficult problem  as it requires the
network to pinpoint the pitch’s start and end  and derive the speed from a minimal signal. The baseball
often obscured by the pitcher  travels at speeds over 100mph and covers 60.5 feet in approximately 0.5
seconds. Initially  with frame rates of 8fps and 3fps  only 1-2 features captured the pitch in mid-air
proving insufficient for speed determination. Utilizing the 60fps rate available in YouTube videos  we
recalculated optical flow and extracted RGB frames at this higher rate. Employing a fully-connected
layer with a single output for pitch speed prediction and minimizing the L1 loss between predicted
and actual speeds  we achieved an average error of 3.6mph. Table 6 compares different models  and
Fig. 8 illustrates the sub-events learned for various speeds.
Table 6: Results for pitch speed regression on segmented videos  reporting root-mean-squared errors.
Method Two-stream
I3D 4.3 mph
I3D + LSTM 4.1 mph
I3D + sub-events 3.9 mph
InceptionV3 5.3 mph
InceptionV3 + LSTM 4.5 mph
InceptionV3 + sub-events 3.6 mph
6.2.3 Pitch Type Classification
We conducted experiments to determine the feasibility of predicting pitch types from video  a task
made challenging by pitchers’ efforts to disguise their pitches from batters and the subtle differences
between pitches  such as grip and rotation. We incorporated pose data extracted using OpenPose
utilizing heatmaps of joint and body part locations as input to a newly trained InceptionV3 CNN.
Pose features were considered due to variations in body mechanics between different pitches. Our
dataset includes six pitch types  with results presented in Table 7. LSTMs performed worse than the
baseline  likely due to overfitting  whereas learning sub-events proved beneficial. Fastballs were the
easiest to detect (68% accuracy)  followed by sliders (45%)  while sinkers were the most difficult
(12%).
6.3 Continuous Video Activity Detection
We evaluate models extended for continuous videos using per-frame mean average precision (mAP)
with results shown in Table 8. This setting is more challenging than segmented videos  requiring
the model to identify activity start and end times and handle ambiguous negative examples. All
models improve upon the baseline per-frame classification  confirming the importance of temporal
information. Fixed temporal pyramid pooling outperforms max-pooling  while LSTM and temporal
6
Table 7: Accuracy of pitch type classification using I3D for video inputs and InceptionV3 for pose
heatmaps.
Method Accuracy
Random 17.0%
I3D 25.8%
I3D + LSTM 18.5%
I3D + sub-events 34.5%
Pose 28.4%
Pose + LSTM 27.6%
Pose + sub-events 36.4%
convolution appear to overfit. Convolutional sub-events  especially when combined with super-event
representation  significantly enhance performance  particularly for frame-based features.
Table 8: Performance on continuous videos for multi-label activity classification (per-frame mAP).
Random 13.4 13.4 13.4
I3D 33.8 35.1 34.2
I3D + max-pooling 34.9 36.4 36.8
I3D + pyramid 36.8 37.5 39.7
I3D + LSTM 36.2 37.3 39.4
I3D + temporal conv 35.2 38.1 39.2
I3D + sub-events 35.5 37.5 38.5
I3D + super-events 38.7 38.6 39.1
I3D + sub+super-events 38.2 39.4 40.4
InceptionV3 31.2 31.8 31.9
InceptionV3 + max-pooling 31.8 34.1 35.2
InceptionV3 + pyramid 32.2 35.1 36.8
InceptionV3 + LSTM 32.1 33.5 34.1
InceptionV3 + temporal conv 28.4 34.4 33.4
InceptionV3 + sub-events 32.1 35.8 37.3
InceptionV3 + super-events 31.5 36.2 39.6
InceptionV3 + sub+super-events 34.2 40.2 40.9
7","This paper introduces MLB-YouTube  a novel and challenging dataset designed for detailed activity
recognition in videos. We conduct a comparative analysis of various recognition techniques that
employ temporal feature pooling for both segmented and continuous videos. Our findings reveal that
learning sub-events to pinpoint temporal regions of interest significantly enhances performance in
segmented video classification. In the context of activity detection in continuous videos  we establish
that incorporating convolutional sub-events with a super-event representation  creating a three-level
activity hierarchy  yields the most favorable outcomes.
7
ThisresearchintroducesMLB-YouTube anewandcomplexdatasetcreatedfor
supporttwotypesofanalysis: oneforclassifyingactivitiesinsegmentedvideos
andanotherfordetectingactivitiesinunsegmented continuousvideostreams. This
studyevaluatesseveralmethodsforrecognizingactivities focusingonhowthey
tocontinuousvideofeeds. Additionally thispaperassessestheeffectivenessof
usingbaseballbroadcastvideos. Thefindingsindicatethatincorporatingtemporal
dynamicsintomodelsisbeneficialfordetailedactivityrecognition.
Actionrecognition asignificantproblemincomputervision findsextensiveuseinsports. Profes-
sionalsportingeventsareextensivelyrecordedforentertainment andtheserecordingsareinvaluable
forsubsequentanalysisbycoaches  scouts  andmediaanalysts. Whilenumerousgamestatistics
systems. SystemslikePITCHf/xandStatcasthavebeenemployedbyMajorLeagueBaseball(MLB)
toautomaticallyrecordpitchspeedandmovement utilizinganetworkofhigh-speedcamerasand
radartocollectdetaileddataoneachplayer. Accesstomuchofthisdataisrestrictedfromthepublic
ThispaperintroducesMLB-YouTube anoveldatasetthatincludesdenselyannotatedframesofactivi-
tiesextractedfrombroadcastbaseballvideos. Unlikemanycurrentdatasetsforactivityrecognitionor
detection ourdatasetemphasizesfine-grainedactivityrecognition. Thedifferencesbetweenactivities
areoftenminimal primarilyinvolvingthemovementofasingleindividual withaconsistentscene
structureacrossactivities. Thedeterminationofactivityisbasedonasinglecameraperspective. This
studycomparesvariousmethodsfortemporalfeatureaggregation bothforclassifyingactivitiesin
segmentedvideosandfordetectingthemincontinuousvideostreams.
2 RelatedWork
Thefieldofactivityrecognitionhasgarneredsubstantialattentionincomputervisionresearch. Initial
successeswereachievedwithhand-engineeredfeaturessuchasdensetrajectories. Thefocusofmore
recentstudieshasshiftedtowardstheapplicationofConvolutionalNeuralNetworks(CNNs)for
activityrecognition. Two-streamCNNarchitecturesutilizebothspatialRGBframesandoptical
flowframes. Tocapturespatio-temporalcharacteristics 3DXYTconvolutionalmodelshavebeen
developed. ThedevelopmentoftheseadvancedCNNmodelshasbeensupportedbylargedatasets
suchasKinetics THUMOS andActivityNet.
Severalstudieshaveinvestigatedtheaggregationoftemporalfeaturesforthepurposeofactivity
recognition. ResearchhascomparedseveralpoolingtechniquesanddeterminedthatbothLongShort-
TermMemorynetworks(LSTMs)andmax-poolingacrossentirevideosyieldedthebestoutcomes. It
hasbeendiscoveredthatpoolingintervalsfromvaryinglocationsanddurationsisadvantageousfor
activityrecognition. Itwasdemonstratedthatidentifyingandclassifyingkeysub-eventintervalscan
leadtobetterperformance.
Recently segment-based3DCNNshavebeenemployedtocapturespatio-temporaldataconcurrently
foractivitydetection. Thesemethodsdependonthe3DCNNtocapturetemporaldynamics which
typicallyspanonly16frames. Althoughlonger-termtemporalstructureshavebeenexplored thiswas
usuallyaccomplishedwithtemporalpoolingoflocalizedfeaturesor(spatio-)temporalconvolutions
withextendedfixedintervals. RecurrentNeuralNetworks(RNNs)havealsobeenappliedtorepresent
transitionsinactivitybetweenframes.
3 MLB-YouTubeDataset
Wehavecompiledanextensivedatasetfrom20baseballgamesofthe2017MLBpostseason available
onYouTube totalingover42hoursofvideo. Ourdatasetincludestwomainparts: segmentedvideos
dataset’scomplexityisamplifiedbythefactthatitoriginatesfromtelevisedbaseballgames wherea
singlecameraperspectiveissharedamongvariousactivities. Additionally thereisminimalvariance
contrasttodatasetslikeTHUMOSandActivityNet whichencompassabroadspectrumofactivities
withdiversesettings scales andcameraangles ourdatasetfeaturesactivitieswhereasingleframe
mightnotbeadequatetodeterminetheactivity.
TheminordifferencesbetweenaballandastrikeareillustratedinFigure3. Differentiatingbetween
theseactionsrequiresidentifyingwhetherthebatterswingsornot  detectingtheumpire’ssignal
(Figure4)forastrike ornotingtheabsenceofasignalforaball. Thisisfurthercomplicatedbecause
thebatterorcatchercanobstructtheumpire andeachumpirehastheiruniquestyleofsignalinga
Ourdatasetforsegmentedvideoanalysiscomprises4 290clips. Eachclipisannotatedformultiple
baseballactions includingswing hit ball  strike andfoul. Giventhatasingleclipmaycontain
severalactivities thisisconsideredamulti-labelclassificationtask. Table1presentsthecomplete
listofactivitiesandtheirrespectivecountswithinthedataset. Additionally clipsfeaturingapitch
wereannotatedwiththetypeofpitch(e.g. fastball curveball slider)anditsspeed. Furthermore a
collectionof2 983hardnegativeexamples wherenoactionispresent wasgathered. Theseinstances
includeviewsofthecrowd thefield orplayersstandingidlybeforeorafterapitch. Examplesof
activitiesandhardnegativesaredepictedinFigure2.
Ourcontinuousvideodatasetincludes2 128clips  eachlastingbetween1and2minutes. Every
frameinthesevideosisannotatedwiththebaseballactivitiesthatoccur. Onaverage eachcontinuous
clipcontains7.2activities amountingtoover15 000activityinstancesintotal.
Table1: ActivityclassesandtheirinstancecountsinthesegmentedMLB-YouTubedataset.
NoActivity 2983
InPlay 679
HitbyPitch 14
4 SegmentedVideoRecognitionApproach
Weinvestigatedifferenttechniquesforaggregatingtemporalfeaturesinsegmentedvideoactivity
recognition. Insegmentedvideos theclassificationtaskissimplerbecauseeachframecorrespondsto
anactivity eliminatingtheneedforthemodeltoidentifythestartandendofactivities. Ourmethods
arebasedonaCNNthatgeneratesaper-frameorper-segmentrepresentation derivedfromstandard
two-streamCNNsusingdeepCNNslikeI3DorInceptionV3.
GivenvideofeaturesvofdimensionsT ×D whereT representsthevideo’stemporallengthandD
isthefeature’sdimensionality theusualapproachforfeaturepoolinginvolvesmax-ormean-pooling
acrossthetemporaldimension followedbyafully-connectedlayerforvideoclipclassification as
depictedinFig. 5(a). Thisapproach however yieldsasinglerepresentationfortheentirevideo
max-poolingeach. Thepooledfeaturesareconcatenated creatingaK×Drepresentation whereK
isthenumberofintervalsinthetemporalpyramid andafully-connectedlayerclassifiestheclip.
Wealsoexplorelearningtemporalconvolutionfilterstoaggregatelocaltemporalstructures. Akernel
ofsizeL×1isappliedtoeachframe enablingeachtimesteprepresentationtoincorporateinformation
fromadjacentframes. Afterapplyingmax-poolingtotheoutputofthetemporalconvolution afully-
connectedlayerisusedforclassification asillustratedinFig. 5(c).
Whiletemporalpyramidpoolingretainssomestructure theintervalsarefixedandpredetermined.
Previousstudieshaveshownthatlearningthesub-intervaltopoolisbeneficialforactivityrecognition.
parameterizingN Gaussians. GiventhevideolengthT thepositionsofthestridedGaussiansare
firstcalculatedas:
T −(g +1)
g =0.5− n forn=0 1 ... N −1
n N −1
1
p =g +(t−0.5T +0.5) fort=0 1 ... T −1
t n n δ
Thefiltersarethengeneratedas:
1 (cid:18) (t−µ )2(cid:19)
F [i t]= exp − i m i∈{0 1 ... N −1} t∈{0 1 ... T −1}
m Z 2σ2
m m
whereZ isanormalizationconstant.
m
WeapplythesefiltersF totheT ×Dvideorepresentationthroughmatrixmultiplication yieldingan
N ×Drepresentationthatservesasinputtoafully-connectedlayerforclassification. Thismethod
isshowninFig5(d).
Additionally wecompareabi-directionalLSTMwith512hiddenunits usingthefinalhiddenstate
asinputtoafully-connectedlayerforclassification. Weframeourtasksasmulti-labelclassification
andtrainthesemodelstominimizebinarycross-entropy:
(cid:88)
L(v)= z log(p(c|G(v)))+(1−z )log(1−p(c|G(v)))
c c
c
whereG(v)isthefunctionthatpoolsthetemporalinformation andz isthegroundtruthlabelfor
classc.
5 ActivityDetectioninContinuousVideos
Detectingactivitiesincontinuousvideosposesagreaterchallenge. Thegoalhereistoclassifyeach
frameaccordingtotheactivitiesoccurring. Unlikesegmentedvideos  continuousvideosfeature
the model learn toidentify the start and end points of activities. As a baseline  wetrain a single
fully-connectedlayertoserveasaper-frameclassifier whichdoesnotutilizetemporalinformation
beyondthatcontainedinthefeatures.
Weadaptthemethodsdevelopedforsegmentedvideoclassificationtocontinuousvideosbyimple-
mentingatemporalslidingwindowapproach. WeselectafixedwindowdurationofLfeatures apply
max-poolingtoeachwindow(similartoFig. 5(a)) andclassifyeachpooledsegment. Thisapproach
isextendedtotemporalpyramidpoolingbydividingthewindowoflengthLintosegmentsoflengths
L/2 L/4 andL/8 resultingin14segmentsperwindow. Max-poolingisappliedtoeachsegment
andthepooledfeaturesareconcatenated yieldinga14×D-dimensionalrepresentationforeach
window whichisthenusedasinputtotheclassifier.
Fortemporalconvolutionalmodelsincontinuousvideos wemodifythesegmentedvideoapproachby
learningatemporalconvolutionalkerneloflengthLandconvolvingitwiththeinputvideofeatures.
ThisoperationtransformsinputofsizeT ×DintooutputofsizeT ×D followedbyaper-frame
classifier. Thisenablesthemodeltoaggregatelocaltemporalinformation.
Toextendthesub-eventmodeltocontinuousvideos wefollowasimilarapproachbutsetT =Lin
Eq. 1 resultinginfiltersoflengthL. TheT×Dvideorepresentationisconvolvedwiththesub-event
filtersF producinganN ×D×T-dimensionalrepresentationusedasinputtoafully-connected
layerforframeclassification.
Themodelistrainedtominimizeper-framebinaryclassification:
L(v)= z log(p(c|H(v )))+(1−z )log(1−p(c|H(v )))
t c t t c t
t c
wherev istheper-frameorper-segmentfeatureattimet H(v )istheslidingwindowapplicationof
t t
oneofthefeaturepoolingmethods andz isthegroundtruthclassattimet.
Amethodtolearn’super-events’(i.e. globalvideocontext)hasbeenintroducedandshowntobe
effectiveforactivitydetectionincontinuousvideos. Thisapproachinvolveslearningasetoftemporal
structurefiltersmodeledasN Cauchydistributions. Eachdistributionisdefinedbyacenterx anda
n
widthγ . GiventhevideolengthT thefiltersareconstructedby:
(T −1)(tanh(x′ )+1)
x = n
n 2
1 γ
f (t)= n exp(1−2|tanh(γ′)|)
n Z π((t−x )2+γ2) n
n n n
whereZ isanormalizationconstant t∈{1 2 ... T} andn∈{1 2 ... N}.
Thefiltersarecombinedwithlearnedper-classsoft-attentionweightsA andthesuper-eventrepre-
sentationiscomputedas:
(cid:88) (cid:88)
S = A f (t)·v
c c n n t
n t
where v is the T ×D video representation. These filters enable the model to focus on relevant
intervalsfortemporalcontext. Thesuper-eventrepresentationisconcatenatedtoeachtimestepand
usedforclassification. Wealsoexperimentwithcombiningthesuper-andsub-eventrepresentations
toformathree-levelhierarchyforeventrepresentation.
6.1 ImplementationDetails
Forourbaseper-segmentCNN weutilizetheI3Dnetwork pre-trainedontheImageNetandKinetics
datasets.I3Dhasachievedstate-of-the-artperformanceonsegmentedvideotasks providingareliable
featurerepresentation. Wealsoemployatwo-streamversionofInceptionV3 pre-trainedonImagenet
andKinetics  asourbaseper-frameCNNforcomparison. InceptionV3waschosenforitsdepth
comparedtoprevioustwo-streamCNNs. Frameswereextractedat25fps andTVL1opticalflow
wascomputedandclippedto[−20 20]. ForInceptionV3 featureswerecomputedevery3frames
(8fps) whileforI3D everyframewasused withI3Dhavingatemporalstrideof8 resultingin
3featurespersecond(3fps). ModelswereimplementedinPyTorchandtrainedusingtheAdam
optimizerwithalearningrateof0.01 decayedbyafactorof0.1every10epochs foratotalof50
6.2 SegmentedVideoActivityRecognition
relativelystraightforwardduetothedistinctdifferencesbetweenpitchandnon-pitchframes. The
results detailedinTable2 revealminimalvariationacrossdifferentfeaturesormodels.
Table2: Performanceonsegmentedvideosforbinarypitch/non-pitchclassification.
InceptionV3+sub-events 98.67 98.73 99.36
I3D+sub-events 98.42 98.35 98.65
6.2.1 Multi-labelClassification
Weassessedvarioustemporalfeatureaggregationmethodsbycalculatingthemeanaverageprecision
(mAP)foreachvideoclip astandardmetricformulti-labelclassification. Table4comparesthe
performanceofthesemethods. Allmethodssurpassmean/max-pooling highlightingtheimportance
ofpreservingtemporalstructureforactivityrecognition.FixedtemporalpyramidpoolingandLSTMs
showsomeimprovement. Temporalconvolutionoffersamoresignificantperformanceboostbut
requiressubstantiallymoreparameters(seeTable3). Learningsub-events asperpreviousresearch
yieldsthebestresults. WhileLSTMsandtemporalconvolutionshavebeenusedbefore theyneed
moreparametersandperformlesseffectively likelyduetooverfitting. Moreover LSTMsnecessitate
sequentialprocessingofvideofeatures whereasothermethodscanbefullyparallelized.
InceptionV3).
Model #Parameters
Max/MeanPooling 16K
PyramidPooling 115K
TemporalConv 31.5M
Table4: MeanAveragePrecision(mAP)resultsonsegmentedvideosformulti-labelclassification.
Learningsub-intervalsforpoolingisfoundtobecrucialforactivityrecognition.
InceptionV3+mean-pool 35.6 47.2 45.3
InceptionV3+max-pool 47.9 48.6 54.4
InceptionV3+pyramid 49.7 53.2 55.3
InceptionV3+LSTM 47.6 55.6 57.7
InceptionV3+temporalconv 47.2 55.2 56.1
InceptionV3+sub-events 56.2 62.5 62.6
I3D+mean-pool 42.4 47.6 52.7
I3D+max-pool 48.3 53.4 57.2
I3D+pyramid 53.2 56.7 58.7
I3D+LSTM 48.2 53.1 53.1
I3D+temporalconv 52.8 57.1 58.4
I3D+sub-events 55.5 61.2 61.3
Table5showstheaverageprecisionforeachactivityclass. Learningtemporalstructureisparticularly
comparedtosegment-basedfeatures(e.g. I3D).Sub-eventlearningsignificantlyaidsindetecting
strikes hits foulballs andhit-by-pitchevents whichexhibitchangesinvideofeaturespost-event.
Forinstance afterahit thecameraoftentrackstheball’strajectory whileafterahit-by-pitch it
followstheplayertofirstbase asillustratedinFig. 6andFig. 7.
advantageousforactivityrecognition.
Method Ball Strike Swing Hit Foul InPlay Bunt HitbyPitch
InceptionV3+max-pool 60.2 84.7 85.9 80.8 40.3 74.2 10.2 15.7
InceptionV3+sub-events 66.9 93.9 90.3 90.9 60.7 89.7 12.4 29.2
I3D+max-pool 59.4 90.3 87.7 85.9 48.1 76.1 14.3 18.2
I3D+sub-events 62.5 91.3 88.5 86.5 47.3 75.9 16.2 21.0
6.2.2 PitchSpeedRegression
Estimatingpitchspeedfromvideoframesisanexceptionallydifficultproblem asitrequiresthe
networktopinpointthepitch’sstartandend andderivethespeedfromaminimalsignal.Thebaseball
oftenobscuredbythepitcher travelsatspeedsover100mphandcovers60.5feetinapproximately0.5
seconds. Initially withframeratesof8fpsand3fps only1-2featurescapturedthepitchinmid-air
provinginsufficientforspeeddetermination. Utilizingthe60fpsrateavailableinYouTubevideos we
recalculatedopticalflowandextractedRGBframesatthishigherrate. Employingafully-connected
layerwithasingleoutputforpitchspeedpredictionandminimizingtheL1lossbetweenpredicted
andactualspeeds weachievedanaverageerrorof3.6mph. Table6comparesdifferentmodels and
Fig. 8illustratesthesub-eventslearnedforvariousspeeds.
Table6: Resultsforpitchspeedregressiononsegmentedvideos reportingroot-mean-squarederrors.
I3D 4.3mph
I3D+LSTM 4.1mph
I3D+sub-events 3.9mph
InceptionV3 5.3mph
InceptionV3+LSTM 4.5mph
InceptionV3+sub-events 3.6mph
6.2.3 PitchTypeClassification
Weconductedexperimentstodeterminethefeasibilityofpredictingpitchtypesfromvideo atask
madechallengingbypitchers’effortstodisguisetheirpitchesfrombattersandthesubtledifferences
betweenpitches suchasgripandrotation. WeincorporatedposedataextractedusingOpenPose
utilizingheatmapsofjointandbodypartlocationsasinputtoanewlytrainedInceptionV3CNN.
Posefeatureswereconsideredduetovariationsinbodymechanicsbetweendifferentpitches. Our
datasetincludessixpitchtypes withresultspresentedinTable7. LSTMsperformedworsethanthe
baseline likelyduetooverfitting whereaslearningsub-eventsprovedbeneficial. Fastballswerethe
easiesttodetect(68%accuracy) followedbysliders(45%) whilesinkerswerethemostdifficult
6.3 ContinuousVideoActivityDetection
Weevaluatemodelsextendedforcontinuousvideosusingper-framemeanaverageprecision(mAP)
withresultsshowninTable8. Thissettingismorechallengingthansegmentedvideos requiring
modelsimproveuponthebaselineper-frameclassification confirmingtheimportanceoftemporal
information. Fixedtemporalpyramidpoolingoutperformsmax-pooling whileLSTMandtemporal
Table7: AccuracyofpitchtypeclassificationusingI3DforvideoinputsandInceptionV3forpose
I3D+LSTM 18.5%
I3D+sub-events 34.5%
Pose+LSTM 27.6%
Pose+sub-events 36.4%
convolutionappeartooverfit. Convolutionalsub-events especiallywhencombinedwithsuper-event
representation significantlyenhanceperformance particularlyforframe-basedfeatures.
Table8: Performanceoncontinuousvideosformulti-labelactivityclassification(per-framemAP).
I3D+max-pooling 34.9 36.4 36.8
I3D+pyramid 36.8 37.5 39.7
I3D+LSTM 36.2 37.3 39.4
I3D+temporalconv 35.2 38.1 39.2
I3D+sub-events 35.5 37.5 38.5
I3D+super-events 38.7 38.6 39.1
I3D+sub+super-events 38.2 39.4 40.4
InceptionV3+max-pooling 31.8 34.1 35.2
InceptionV3+pyramid 32.2 35.1 36.8
InceptionV3+LSTM 32.1 33.5 34.1
InceptionV3+temporalconv 28.4 34.4 33.4
InceptionV3+sub-events 32.1 35.8 37.3
InceptionV3+super-events 31.5 36.2 39.6
InceptionV3+sub+super-events 34.2 40.2 40.9
ThispaperintroducesMLB-YouTube anovelandchallengingdatasetdesignedfordetailedactivity
recognitioninvideos. Weconductacomparativeanalysisofvariousrecognitiontechniquesthat
employtemporalfeaturepoolingforbothsegmentedandcontinuousvideos. Ourfindingsrevealthat
learningsub-eventstopinpointtemporalregionsofinterestsignificantlyenhancesperformancein
segmentedvideoclassification. Inthecontextofactivitydetectionincontinuousvideos weestablish
thatincorporatingconvolutionalsub-eventswithasuper-eventrepresentation creatingathree-level
activityhierarchy yieldsthemostfavorableoutcomes.
Introduction
Related Work
MLB-YouTube Dataset
Activity
Count
No Activity
2983
Ball
1434
Strike
1799
Swing
2506
Hit
1391
Foul
718
In Play
679
Bunt
24
Hit by Pitch
14
Segmented Video Recognition Approach
Given video features v of dimensions T × D  where T represents the video’s temporal length and D
max-pooling each. The pooled features are concatenated  creating a K × D representation  where K
of size L×1 is applied to each frame  enabling each timestep representation to incorporate information
parameterizing N Gaussians. Given the video length T  the positions of the strided Gaussians are
gn = 0.5 − T − (gn + 1)
N − 1
forn = 0  1  . . .   N − 1
pt n = gn + (t − 0.5T + 0.5)1
δ
fort = 0  1  . . .   T − 1
Fm[i  t] =
Zm
exp
�
−(t − µi m)2
2σ2m
i ∈ {0  1  . . .   N − 1}  t ∈ {0  1  . . .   T − 1}
where Zm is a normalization constant.
We apply these filters F to the T × D video representation through matrix multiplication  yielding an
N × D representation that serves as input to a fully-connected layer for classification. This method
L(v) =
zc log(p(c|G(v))) + (1 − zc) log(1 − p(c|G(v)))
where G(v) is the function that pools the temporal information  and zc is the ground truth label for
Activity Detection in Continuous Videos
menting a temporal sliding window approach. We select a fixed window duration of L features  apply
is extended to temporal pyramid pooling by dividing the window of length L into segments of lengths
L/2  L/4  and L/8  resulting in 14 segments per window. Max-pooling is applied to each segment
and the pooled features are concatenated  yielding a 14 × D-dimensional representation for each
learning a temporal convolutional kernel of length L and convolving it with the input video features.
This operation transforms input of size T × D into output of size T × D  followed by a per-frame
To extend the sub-event model to continuous videos  we follow a similar approach but set T = L in
Eq. 1  resulting in filters of length L. The T ×D video representation is convolved with the sub-event
filters F  producing an N × D × T-dimensional representation used as input to a fully-connected
zt c log(p(c|H(vt))) + (1 − zt c) log(1 − p(c|H(vt)))
where vt is the per-frame or per-segment feature at time t  H(vt) is the sliding window application of
one of the feature pooling methods  and zt c is the ground truth class at time t.
structure filters modeled as N Cauchy distributions. Each distribution is defined by a center xn and a
xn = (T − 1)(tanh(x′
fn(t) = 1
Zn
γn
π((t − xn)2 + γ2n) exp(1 − 2| tanh(γ′
where Zn is a normalization constant  t ∈ {1  2  . . .   T}  and n ∈ {1  2  . . .   N}.
Sc =
Ac n
t
fn(t) · vt
where v is the T × D video representation. These filters enable the model to focus on relevant
Experiments
6.1
Implementation Details
was computed and clipped to [−20  20]. For InceptionV3  features were computed every 3 frames
6.2
Segmented Video Activity Recognition
Model
RGB
Flow
Two-stream
InceptionV3
97.46
98.44
98.67
InceptionV3 + sub-events
98.73
99.36
I3D
98.64
98.88
98.70
I3D + sub-events
98.42
98.35
98.65
6.2.1
Multi-label Classification
# Parameters
Max/Mean Pooling
16K
Pyramid Pooling
115K
LSTM
10.5M
Temporal Conv
31.5M
Sub-events
36K
Method
Random
16.3
InceptionV3 + mean-pool
35.6
47.2
45.3
InceptionV3 + max-pool
47.9
48.6
54.4
InceptionV3 + pyramid
49.7
53.2
55.3
InceptionV3 + LSTM
47.6
55.6
57.7
InceptionV3 + temporal conv
55.2
56.1
56.2
62.5
62.6
I3D + mean-pool
42.4
52.7
I3D + max-pool
48.3
53.4
57.2
I3D + pyramid
56.7
58.7
I3D + LSTM
48.2
53.1
I3D + temporal conv
52.8
57.1
58.4
55.5
61.2
61.3
21.8
28.6
37.4
20.9
11.4
10.3
1.1
4.5
60.2
84.7
85.9
80.8
40.3
74.2
10.2
15.7
66.9
93.9
90.3
90.9
60.7
89.7
12.4
29.2
59.4
87.7
48.1
76.1
14.3
18.2
91.3
88.5
86.5
47.3
75.9
16.2
21.0
6.2.2
Pitch Speed Regression
4.3 mph
4.1 mph
3.9 mph
5.3 mph
4.5 mph
3.6 mph
6.2.3
Pitch Type Classification
6.3
Continuous Video Activity Detection
Accuracy
17.0%
25.8%
18.5%
34.5%
Pose
28.4%
Pose + LSTM
27.6%
Pose + sub-events
36.4%
13.4
33.8
35.1
34.2
I3D + max-pooling
34.9
36.4
36.8
37.5
39.7
36.2
37.3
39.4
35.2
38.1
39.2
35.5
38.5
I3D + super-events
38.7
38.6
39.1
I3D + sub+super-events
38.2
40.4
31.2
31.8
31.9
InceptionV3 + max-pooling
34.1
32.2
32.1
33.5
28.4
34.4
33.4
35.8
InceptionV3 + super-events
31.5
39.6
InceptionV3 + sub+super-events
40.2
40.9
Conclusion"
R007,1,CVPR,"The growing focus on leveraging computer vision for dietary oversight and nutri-
tion tracking has spurred the creation of sophisticated 3D reconstruction methods
for food. The lack of comprehensive  high-fidelity data  coupled with limited
collaborative efforts between academic and industrial sectors  has significantly
hindered advancements in this domain. This study addresses these obstacles by
introducing the MetaFood Challenge  aimed at generating precise  volumetrically
accurate 3D food models from 2D images  utilizing a checkerboard for size cal-
ibration. The challenge was structured around 20 food items across three levels
of complexity: easy (200 images)  medium (30 images)  and hard (1 image). A
total of 16 teams participated in the final assessment phase. The methodologies
developed during this challenge have yielded highly encouraging outcomes in
3D food reconstruction  showing great promise for refining portion estimation in
dietary evaluations and nutritional tracking. Further information on this workshop
challenge and the dataset is accessible via the provided URL.
1","The convergence of computer vision technologies with culinary practices has pioneered innovative
approaches to dietary monitoring and nutritional assessment. The MetaFood Workshop Challenge
represents a landmark initiative in this emerging field  responding to the pressing demand for precise
and scalable techniques for estimating food portions and monitoring nutritional consumption. Such
technologies are vital for fostering healthier eating behaviors and addressing health issues linked to
diet.
By concentrating on the development of accurate 3D models of food derived from various visual
inputs  including multiple views and single perspectives  this challenge endeavors to bridge the
disparity between current methodologies and practical needs. It promotes the creation of unique
solutions capable of managing the intricacies of food morphology  texture  and illumination  while also
meeting the real-world demands of dietary evaluation. This initiative gathers experts from computer
vision  machine learning  and nutrition science to propel 3D food reconstruction technologies forward.
These advancements have the potential to substantially enhance the precision and utility of food
portion estimation across diverse applications  from individual health tracking to extensive nutritional
investigations.
Conventional methods for assessing diet  like 24-Hour Recall or Food Frequency Questionnaires
(FFQs)  are frequently reliant on manual data entry  which is prone to inaccuracies and can be
burdensome. The lack of 3D data in 2D RGB food images further complicates the use of regression-
based methods for estimating food portions directly from images of eating occasions. By enhancing
3D reconstruction for food  the aim is to provide more accurate and intuitive nutritional assessment
tools. This technology could revolutionize the sharing of culinary experiences and significantly
impact nutrition science and public health.
Participants were tasked with creating 3D models of 20 distinct food items from 2D images  mim-
icking scenarios where mobile devices equipped with depth-sensing cameras are used for dietary
.
recording and nutritional tracking. The challenge was segmented into three tiers of difficulty based
on the number of images provided: approximately 200 images for easy  30 for medium  and a single
top-view image for hard. This design aimed to rigorously test the adaptability and resilience of
proposed solutions under various realistic conditions. A notable feature of this challenge was the use
of a visible checkerboard for physical referencing and the provision of depth images for each frame
ensuring the 3D models maintained accurate real-world measurements for portion size estimation.
This initiative not only expands the frontiers of 3D reconstruction technology but also sets the stage
for more reliable and user-friendly real-world applications  including image-based dietary assessment.
The resulting solutions hold the potential to profoundly influence nutritional intake monitoring and
comprehension  supporting broader health and wellness objectives. As progress continues  innovative
applications are anticipated to transform personal health management  nutritional research  and the
wider food industry. The remainder of this report is structured as follows: Section 2 delves into the
existing literature on food portion size estimation  Section 3 describes the dataset and evaluation
framework used in the challenge  and Sections 4  5  and 6 discuss the methodologies and findings of
the top three teams (V olETA  ININ-VIAUN  and FoodRiddle)  respectively.
2 Related Work
Estimating food portions is a crucial part of image-based dietary assessment  aiming to determine the
volume  energy content  or macronutrients directly from images of meals. Unlike the well-studied
task of food recognition  estimating food portions is particularly challenging due to the lack of 3D
information and physical size references necessary for accurately judging the actual size of food
portions. Accurate portion size estimation requires understanding the volume and density of food
elements that are hard to deduce from a 2D image  underscoring the need for sophisticated techniques
to tackle this problem. Current methods for estimating food portions are grouped into four categories.
Stereo-Based Approaches use multiple images to reconstruct the 3D structure of food. Some methods
estimate food volume using multi-view stereo reconstruction based on epipolar geometry  while
others perform two-view dense reconstruction. Simultaneous Localization and Mapping (SLAM) has
also been used for continuous  real-time food volume estimation. However  these methods are limited
by their need for multiple images  which is not always practical.
Model-Based Approaches use predefined shapes and templates to estimate volume. For instance
certain templates are assigned to foods from a library and transformed based on physical references to
estimate the size and location of the food. Template matching approaches estimate food volume from
a single image  but they struggle with variations in food shapes that differ from predefined templates.
Recent work has used 3D food meshes as templates to align camera and object poses for portion size
estimation.
Depth Camera-Based Approaches use depth cameras to create depth maps  capturing the distance from
the camera to the food. These depth maps form a voxel representation used for volume estimation.
The main drawback is the need for high-quality depth maps and the extra processing required for
consumer-grade depth sensors.
Deep Learning Approaches utilize neural networks trained on large image datasets for portion
estimation. Regression networks estimate the energy value of food from single images or from an
'Energy Distribution Map' that maps input images to energy distributions. Some networks use both
images and depth maps to estimate energy  mass  and macronutrient content. However  deep learning
methods require extensive data for training and are not always interpretable  with performance
degrading when test images significantly differ from training data.
While these methods have advanced food portion estimation  they face limitations that hinder their
widespread use and accuracy. Stereo-based methods are impractical for single images  model-based
approaches struggle with diverse food shapes  depth camera methods need specialized hardware
and deep learning approaches lack interpretability and struggle with out-of-distribution samples. 3D
reconstruction offers a promising solution by providing comprehensive spatial information  adapting
to various shapes  potentially working with single images  offering visually interpretable","The team’s research employs multi-view reconstruction to generate detailed food meshes and calculate
precise food volumes.
4.1.1 Overview
The team’s method integrates computer vision and deep learning to accurately estimate food volume
from RGBD images and masks. Keyframe selection ensures data quality  supported by perceptual
hashing and blur detection. Camera pose estimation and object segmentation pave the way for neural
surface reconstruction  creating detailed meshes for volume estimation. Refinement steps  including
isolated piece removal and scaling factor adjustments  enhance accuracy. This approach provides a
thorough solution for accurate food volume assessment  with potential uses in nutrition analysis.
4.1.2 The Team’s Proposal: VolETA
The team starts by acquiring input data  specifically RGBD images and corresponding food object
masks. The RGBD images  denoted as ID={IDi}n
i=1  where nis the total number of frames
provide depth information alongside RGB images. The food object masks  {Mf
i}n
i=1  help identify
regions of interest within these images.
Next  the team selects keyframes. From the set {IDi}n
i=1  keyframes {IK
j}k
j=1⊆ {IDi}n
i=1are
chosen. A method is implemented to detect and remove duplicate and blurry images  ensuring
high-quality frames. This involves applying a Gaussian blurring kernel followed by the fast Fourier
transform method. Near-Image Similarity uses perceptual hashing and Hamming distance threshold-
ing to detect similar images and retain overlapping ones. Duplicates and blurry images are excluded
to maintain data integrity and accuracy.
Using the selected keyframes {IK
j=1  the team estimates camera poses through a method called
PixSfM  which involves extracting features using SuperPoint  matching them with SuperGlue  and
refining them. The outputs are the camera poses {Cj}k
j=1  crucial for understanding the scene’s
spatial layout.
4
In parallel  the team uses a tool called SAM for reference object segmentation. SAM segments
the reference object with a user-provided prompt  producing a reference object mask MRfor each
keyframe. This mask helps track the reference object across all frames. The XMem++ method
extends the reference object mask MRto all frames  creating a comprehensive set of reference object
masks {MR
i=1. This ensures consistent reference object identification throughout the dataset.
To create RGBA images  the team combines RGB images  reference object masks {MR
i=1  and
food object masks {MF
i=1. This step  denoted as {IR
i=1  integrates various data sources into a
unified format for further processing.
The team converts the RGBA images {IR
i=1and camera poses {Cj}k
j=1into meaningful metadata
and modeled data Dm. This transformation facilitates accurate scene reconstruction.
The modeled data Dmis input into NeuS2 for mesh reconstruction. NeuS2 generates colorful meshes
{Rf  Rr}for the reference and food objects  providing detailed 3D representations. The team uses the
'Remove Isolated Pieces' technique to refine the meshes. Given that the scenes contain only one food
item  the diameter threshold is set to 5% of the mesh size. This method deletes isolated connected
components with diameters less than or equal to 5%  resulting in a cleaned mesh {RCf  RCr}. This
step ensures that only significant parts of the mesh are retained.
The team manually identifies an initial scaling factor Susing the reference mesh via MeshLab. This
factor is fine-tuned to Sfusing depth information and food and reference masks  ensuring accurate
scaling relative to real-world dimensions. Finally  the fine-tuned scaling factor Sfis applied to the
cleaned food mesh RCf  producing the final scaled food mesh RFf. This step culminates in an
accurately scaled 3D representation of the food object  enabling precise volume estimation.
4.1.3 Detecting the scaling factor
Generally  3D reconstruction methods produce unitless meshes by default. To address this  the team
manually determines the scaling factor by measuring the distance for each block of the reference
object mesh. The average of all block lengths lavgis calculated  while the actual real-world length is
constant at lreal= 0.012meters. The scaling factor S=lreal/lavgis applied to the clean food mesh
RCf  resulting in the final scaled food mesh RFfin meters.
The team uses depth information along with food and reference object masks to validate the scaling
factors. The method for assessing food size involves using overhead RGB images for each scene.
Initially  the pixel-per-unit (PPU) ratio (in meters) is determined using the reference object. Subse-
quently  the food width ( fw) and length ( fl) are extracted using a food object mask. To determine the
food height ( fh)  a two-step process is followed. First  binary image segmentation is performed using
the overhead depth and reference images  yielding a segmented depth image for the reference object.
The average depth is then calculated using the segmented reference object depth ( dr). Similarly
employing binary image segmentation with an overhead food object mask and depth image  the
average depth for the segmented food depth image ( df) is computed. The estimated food height fhis
the absolute difference between dranddf. To assess the accuracy of the scaling factor S  the food
bounding box volume (fw×fl×fh)×PPU is computed. The team evaluates if the scaling factor
Sgenerates a food volume close to this potential volume  resulting in Sfine. Table 2 lists the scaling
factors  PPU  2D reference object dimensions  3D food object dimensions  and potential volume.
For one-shot 3D reconstruction  the team uses One-2-3-45 to reconstruct a 3D model from a single
RGBA view input after applying binary image segmentation to both food RGB and mask images.
Isolated pieces are removed from the generated mesh  and the scaling factor S  which is closer to the
potential volume of the clean mesh  is reused.
4.2 Experimental Results
4.2.1 Implementation settings
Experiments were conducted using two GPUs: GeForce GTX 1080 Ti/12G and RTX 3060/6G. The
Hamming distance for near image similarity was set to 12. For Gaussian kernel radius  even numbers
in the range [0...30] were used for detecting blurry images. The diameter for removing isolated pieces
was set to 5%. NeuS2 was run for 15 000 iterations with a mesh resolution of 512x512  a unit cube
'aabb scale' of 1  'scale' of 0.15  and 'offset' of [0.5  0.5  0.5] for each food scene.
5
4.2.2 VolETA Results
The team extensively validated their approach on the challenge dataset and compared their results
with ground truth meshes using MAPE and Chamfer distance metrics. The team’s approach was
applied separately to each food scene. A one-shot food volume estimation approach was used if
the number of keyframes kequaled 1; otherwise  a few-shot food volume estimation was applied.
Notably  the keyframe selection process chose 34.8% of the total frames for the rest of the pipeline
showing the minimum frames with the highest information.
Table 2: List of Extracted Information Using RGBD and Masks
Level Id Label Sf PPU Rw×Rl (fw×fl×fh)
1 Strawberry 0.08955223881 0.01786 320×360 (238 ×257×2.353)
2 Cinnamon bun 0.1043478261 0.02347 236×274 (363 ×419×2.353)
3 Pork rib 0.1043478261 0.02381 246×270 (435 ×778×1.176)
Easy 4 Corn 0.08823529412 0.01897 291×339 (262 ×976×2.353)
5 French toast 0.1034482759 0.02202 266×292 (530 ×581×2.53)
6 Sandwich 0.1276595745 0.02426 230×265 (294 ×431×2.353)
7 Burger 0.1043478261 0.02435 208×264 (378 ×400×2.353)
8 Cake 0.1276595745 0.02143 256×300 (298 ×310×4.706)
9 Blueberry muffin 0.08759124088 0.01801 291×357 (441 ×443×2.353)
10 Banana 0.08759124088 0.01705 315×377 (446 ×857×1.176)
Medium 11 Salmon 0.1043478261 0.02390 242×269 (201 ×303×1.176)
13 Burrito 0.1034482759 0.02372 244×271 (251 ×917×2.353)
14 Frankfurt sandwich 0.1034482759 0.02115 266×304 (400 ×1022×2.353)
16 Everything bagel 0.08759124088 0.01747 306×368 (458 ×134×1.176)
Hard 17 Croissant 0.1276595745 0.01751 319×367 (395 ×695×2.176)
18 Shrimp 0.08759124088 0.02021 249×318 (186 ×95×0.987)
19 Waffle 0.01034482759 0.01902 294×338 (465 ×537×0.8)
20 Pizza 0.01034482759 0.01913 292×336 (442 ×651×1.176)
After finding keyframes  PixSfM estimated the poses and point cloud. After generating scaled meshes
the team calculated volumes and Chamfer distance with and without transformation metrics. Meshes
were registered with ground truth meshes using ICP to obtain transformation metrics.
Table 3 presents quantitative comparisons of the team’s volumes and Chamfer distance with and
without estimated transformation metrics from ICP. For overall method performance  Table 4 shows
the MAPE and Chamfer distance with and without transformation metrics.
Additionally  qualitative results on one- and few-shot 3D reconstruction from the challenge dataset
are shown. The model excels in texture details  artifact correction  missing data handling  and color
adjustment across different scene parts.
Limitations: Despite promising results  several limitations need to be addressed in future work:
•Manual processes: The current pipeline includes manual steps like providing segmentation
prompts and identifying scaling factors  which should be automated to enhance efficiency.
•Input requirements: The method requires extensive input information  including food
masks and depth data. Streamlining these inputs would simplify the process and increase
applicability.
•Complex backgrounds and objects: The method has not been tested in environments with
complex backgrounds or highly intricate food objects.
•Capturing complexities: The method has not been evaluated under different capturing
complexities  such as varying distances and camera speeds.
•Pipeline complexity: For one-shot neural rendering  the team currently uses One-2-3-45.
They aim to use only the 2D diffusion model  Zero123  to reduce complexity and improve
efficiency.
6
Table 3: Quantitative Comparison with Ground Truth Using Chamfer Distance
L Id Team’s V ol. GT V ol. Ch. w/ t.m Ch. w/o t.m
1 40.06 38.53 1.63 85.40
2 216.9 280.36 7.12 111.47
3 278.86 249.67 13.69 172.88
E 4 279.02 295.13 2.03 61.30
5 395.76 392.58 13.67 102.14
6 205.17 218.44 6.68 150.78
7 372.93 368.77 4.70 66.91
8 186.62 173.13 2.98 152.34
9 224.08 232.74 3.91 160.07
10 153.76 163.09 2.67 138.45
M 11 80.4 85.18 3.37 151.14
13 363.99 308.28 5.18 147.53
14 535.44 589.83 4.31 89.66
16 163.13 262.15 18.06 28.33
H 17 224.08 181.36 9.44 28.94
18 25.4 20.58 4.28 12.84
19 110.05 108.35 11.34 23.98
20 130.96 119.83 15.59 31.05
Table 4: Quantitative Comparison with Ground Truth Using MAPE and Chamfer Distance
MAPE Ch. w/ t.m Ch. w/o t.m
(%) sum mean sum mean
10.973 0.130 0.007 1.715 0.095
5 Second Place Team - ININ-VIAUN
5.1 Methodology
This section details the team’s proposed network  illustrating the step-by-step process from original
images to final mesh models.
5.1.1 Scale factor estimation
The procedure for estimating the scale factor at the coordinate level is illustrated in Figure 9. The
team adheres to a method involving corner projection matching. Specifically  utilizing the COLMAP
dense model  the team acquires the pose of each image along with dense point cloud data. For any
given image imgkand its extrinsic parameters [R|t]k  the team initially performs threshold-based
corner detection  setting the threshold at 240. This step allows them to obtain the pixel coordinates
of all detected corners. Subsequently  using the intrinsic parameters kand the extrinsic parameters
[R|t]k  the point cloud is projected onto the image plane. Based on the pixel coordinates of the
corners  the team can identify the closest point coordinates Pk
ifor each corner  where irepresents the
index of the corner. Thus  they can calculate the distance between any two corners as follows:
Dk
ij= (Pk
i−Pk
j)2∀i̸=j (3)
To determine the final computed length of each checkerboard square in image k  the team takes the
minimum value of each row of the matrix Dk(excluding the diagonal) to form the vector dk. The
median of this vector is then used. The final scale calculation formula is given by Equation 4  where
0.012 represents the known length of each square (1.2 cm):
scale =0.012Pn
i=1med(dk)(4)
7
5.1.2 3D Reconstruction
The 3D reconstruction process  depicted in Figure 10  involves two different pipelines to accommodate
variations in input viewpoints. The first fifteen objects are processed using one pipeline  while the
last five single-view objects are processed using another.
For the initial fifteen objects  the team uses COLMAP to estimate poses and segment the food using
the provided segment masks. Advanced multi-view 3D reconstruction methods are then applied to
reconstruct the segmented food. The team employs three different reconstruction methods: COLMAP
DiffusioNeRF  and NeRF2Mesh. They select the best reconstruction results from these methods and
extract the mesh. The extracted mesh is scaled using the estimated scale factor  and optimization
techniques are applied to obtain a refined mesh.
For the last five single-view objects  the team experiments with several single-view reconstruction
methods  including Zero123  Zero123++  One2345  ZeroNVS  and DreamGaussian. They choose
ZeroNVS to obtain a 3D food model consistent with the distribution of the input image. The
intrinsic camera parameters from the fifteenth object are used  and an optimization method based
on reprojection error refines the extrinsic parameters of the single camera. Due to limitations in
single-view reconstruction  depth information from the dataset and the checkerboard in the monocular
image are used to determine the size of the extracted mesh. Finally  optimization techniques are
applied to obtain a refined mesh.
5.1.3 Mesh refinement
During the 3D Reconstruction phase  it was observed that the model’s results often suffered from low
quality due to holes on the object’s surface and substantial noise  as shown in Figure 11.
To address the holes  MeshFix  an optimization method based on computational geometry  is em-
ployed. For surface noise  Laplacian Smoothing is used for mesh smoothing operations. The
Laplacian Smoothing method adjusts the position of each vertex to the average of its neighboring
vertices:
V(new)
i =V(old)
i+λ
1
|N(i)|X
j∈N(i)V(old)
j−V(old)
i
 (5)
In their implementation  the smoothing factor λis set to 0.2  and 10 iterations are performed.
5.2 Experimental Results
5.2.1 Estimated scale factor
The scale factors estimated using the described method are shown in Table 5. Each image and the
corresponding reconstructed 3D model yield a scale factor  and the table presents the average scale
factor for each object.
5.2.2 Reconstructed meshes
The refined meshes obtained using the described methods are shown in Figure 12. The predicted
model volumes  ground truth model volumes  and the percentage errors between them are presented
in Table 6.
5.2.3 Alignment
The team designs a multi-stage alignment method for evaluating reconstruction quality. Figure 13
illustrates the alignment process for Object 14. First  the central points of both the predicted and
ground truth models are calculated  and the predicted model is moved to align with the central point
of the ground truth model. Next  ICP registration is performed for further alignment  significantly
reducing the Chamfer distance. Finally  gradient descent is used for additional fine-tuning to obtain
the final transformation matrix.
The total Chamfer distance between all 18 predicted models and the ground truths is 0.069441169.
8
Table 5: Estimated Scale Factors
Object Index Food Item Scale Factor
1 Strawberry 0.060058
2 Cinnamon bun 0.081829
3 Pork rib 0.073861
4 Corn 0.083594
5 French toast 0.078632
6 Sandwich 0.088368
7 Burger 0.103124
8 Cake 0.068496
9 Blueberry muffin 0.059292
10 Banana 0.058236
11 Salmon 0.083821
13 Burrito 0.069663
14 Hotdog 0.073766
Table 6: Metric of V olume
Object Index Predicted V olume Ground Truth Error Percentage
1 44.51 38.53 15.52
2 321.26 280.36 14.59
3 336.11 249.67 34.62
4 347.54 295.13 17.76
5 389.28 392.58 0.84
6 197.82 218.44 9.44
7 412.52 368.77 11.86
8 181.21 173.13 4.67
9 233.79 232.74 0.45
10 160.06 163.09 1.86
11 86.0 85.18 0.96
13 334.7 308.28 8.57
14 517.75 589.83 12.22
16 176.24 262.15 32.77
17 180.68 181.36 0.37
18 13.58 20.58 34.01
19 117.72 108.35 8.64
20 117.43 119.83 20.03
6 Best 3D Mesh Reconstruction Team - FoodRiddle
6.1 Methodology
To achieve high-fidelity food mesh reconstruction  the team developed two procedural pipelines as
depicted in Figure 14. For simple and medium complexity cases  they employed a structure-from-
motion strategy to ascertain the pose of each image  followed by mesh reconstruction. Subsequently
a sequence of post-processing steps was implemented to recalibrate the scale and improve mesh
quality. For cases involving only a single image  the team utilized image generation techniques to
facilitate model generation.
6.1.1 Multi-View Reconstruction
For Structure from Motion (SfM)  the team enhanced the advanced COLMAP method by integrating
SuperPoint and SuperGlue techniques. This integration significantly addressed the issue of limited
keypoints in scenes with minimal texture  as illustrated in Figure 15.
In the mesh reconstruction phase  the team’s approach builds upon 2D Gaussian Splatting  which
employs a differentiable 2D Gaussian renderer and includes regularization terms for depth distortion
9
and normal consistency. The Truncated Signed Distance Function (TSDF) results are utilized to
produce a dense point cloud.
During post-processing  the team applied filtering and outlier removal methods  identified the outline
of the supporting surface  and projected the lower mesh vertices onto this surface. They utilized
the reconstructed checkerboard to correct the model’s scale and employed Poisson reconstruction to
create a complete  watertight mesh of the subject.
6.1.2 Single-View Reconstruction
For 3D reconstruction from a single image  the team utilized advanced methods such as LGM  Instant
Mesh  and One-2-3-45 to generate an initial mesh. This initial mesh was then refined in conjunction
with depth structure information.
To adjust the scale  the team estimated the object’s length using the checkerboard as a reference
assuming that the object and the checkerboard are on the same plane. They then projected the 3D
object back onto the original 2D image to obtain a more precise scale for the object.
6.2 Experimental Results
Through a process of nonlinear optimization  the team sought to identify a transformation that
minimizes the Chamfer distance between their mesh and the ground truth mesh. This optimization
aimed to align the two meshes as closely as possible in three-dimensional space. Upon completion
of this process  the average Chamfer dis- tance across the final reconstructions of the 20 objects
amounted to 0.0032175 meters. As shown in Table 7  Team FoodRiddle achieved the best scores for
both multi- view and single-view reconstructions  outperforming other teams in the competition.
Table 7: Total Errors for Different Teams on Multi-view and Single-view Data
Team Multi-view (1-14) Single-view (16-20)
FoodRiddle 0.036362 0.019232
ININ-VIAUN 0.041552 0.027889
V olETA 0.071921 0.058726","and enabling a standardized approach to food portion estimation. These benefits motivated the
organization of the 3D Food Reconstruction challenge  aiming to overcome existing limitations and
2
develop more accurate  user-friendly  and widely applicable food portion estimation techniques
impacting nutritional assessment and dietary monitoring.
3 Datasets and Evaluation Pipeline
3.1 Dataset Description
The dataset for the MetaFood Challenge features 20 carefully chosen food items from the MetaFood3D
dataset  each scanned in 3D and accompanied by video recordings. To ensure precise size accuracy
in the reconstructed 3D models  each food item was captured alongside a checkerboard and pattern
mat  serving as physical scaling references. The challenge is divided into three levels of difficulty
determined by the quantity of 2D images provided for reconstruction:
• Easy: Around 200 images taken from video.
• Medium: 30 images.
• Hard: A single image from a top-down perspective.
Table 1 details the food items included in the dataset.
Table 1: MetaFood Challenge Data Details
Object Index Food Item Difficulty Level Number of Frames
1 Strawberry Easy 199
2 Cinnamon bun Easy 200
3 Pork rib Easy 200
4 Corn Easy 200
5 French toast Easy 200
6 Sandwich Easy 200
7 Burger Easy 200
8 Cake Easy 200
9 Blueberry muffin Medium 30
10 Banana Medium 30
11 Salmon Medium 30
12 Steak Medium 30
13 Burrito Medium 30
14 Hotdog Medium 30
15 Chicken nugget Medium 30
16 Everything bagel Hard 1
17 Croissant Hard 1
18 Shrimp Hard 1
19 Waffle Hard 1
20 Pizza Hard 1
3.2 Evaluation Pipeline
The evaluation process is split into two phases  focusing on the accuracy of the reconstructed 3D
models in terms of shape (3D structure) and portion size (volume).
3.2.1 Phase-I: Volume Accuracy
In the first phase  the Mean Absolute Percentage Error (MAPE) is used to evaluate portion size
accuracy  calculated as follows:
MAPE =1
nnX
i=1Ai−Fi
Ai×100% (1)
3
where Aiis the actual volume (in ml) of the i-th food item obtained from the scanned 3D food mesh
andFiis the volume calculated from the reconstructed 3D mesh.
3.2.2 Phase-II: Shape Accuracy
Teams that perform well in Phase-I are asked to submit complete 3D mesh files for each food item.
This phase involves several steps to ensure precision and fairness:
•Model Verification: Submitted models are checked against the final Phase-I submissions for
consistency  and visual inspections are conducted to prevent rule violations.
•Model Alignment: Participants receive ground truth 3D models and a script to compute the
final Chamfer distance. They must align their models with the ground truth and prepare a
transformation matrix for each submitted object. The final Chamfer distance is calculated
using these models and matrices.
•Chamfer Distance Calculation: Shape accuracy is assessed using the Chamfer distance
metric. Given two point sets XandY  the Chamfer distance is defined as:
dCD(X  Y ) =1
|X|X
x∈Xmin
y∈Y∥x−y∥2
2+1
|Y|X
y∈Ymin
x∈X∥x−y∥2
2 (2)
This metric offers a comprehensive measure of similarity between the reconstructed 3D models and
the ground truth. The final ranking is determined by combining scores from both Phase-I (volume
accuracy) and Phase-II (shape accuracy). Note that after the Phase-I evaluation  quality issues were
found with the data for object 12 (steak) and object 15 (chicken nugget)  so these items were excluded
from the final overall evaluation.
4 First Place Team - VolETA
4.1","This report examines and compiles the techniques and findings from the MetaFood Workshop
challenge on 3D Food Reconstruction. The challenge sought to enhance 3D reconstruction methods
by concentrating on food items  tackling the distinct difficulties presented by varied textures  reflective
surfaces  and intricate geometries common in culinary subjects.
The competition involved 20 diverse food items  captured under various conditions and with differing
numbers of input images  specifically designed to challenge participants in creating robust reconstruc-
tion models. The evaluation was based on a two-phase process  assessing both portion size accuracy
through Mean Absolute Percentage Error (MAPE) and shape accuracy using the Chamfer distance
metric.
Of all participating teams  three reached the final submission stage  presenting a range of innovative
solutions. Team V olETA secured first place with the best overall performance in both Phase-I and
Phase-II  followed by team ININ-VIAUN in second place. Additionally  the FoodRiddle team
exhibited superior performance in Phase-II  highlighting a competitive and high-caliber field of
entries for 3D mesh reconstruction. The challenge has successfully advanced the field of 3D food
reconstruction  demonstrating the potential for accurate volume estimation and shape reconstruction
in nutritional analysis and food presentation applications. The novel methods developed by the
participating teams establish a strong foundation for future research in this area  potentially leading
to more precise and user-friendly approaches for dietary assessment and monitoring.
10
Thegrowingfocusonleveragingcomputervisionfordietaryoversightandnutri-
tiontrackinghasspurredthecreationofsophisticated3Dreconstructionmethods
hinderedadvancementsinthisdomain. Thisstudyaddressestheseobstaclesby
introducingtheMetaFoodChallenge aimedatgeneratingprecise volumetrically
accurate3Dfoodmodelsfrom2Dimages utilizingacheckerboardforsizecal-
ibration. Thechallengewasstructuredaround20fooditemsacrossthreelevels
ofcomplexity: easy(200images) medium(30images) andhard(1image). A
totalof16teamsparticipatedinthefinalassessmentphase. Themethodologies
3Dfoodreconstruction showinggreatpromiseforrefiningportionestimationin
dietaryevaluationsandnutritionaltracking. Furtherinformationonthisworkshop
challengeandthedatasetisaccessibleviatheprovidedURL.
Theconvergenceofcomputervisiontechnologieswithculinarypracticeshaspioneeredinnovative
approachestodietarymonitoringandnutritionalassessment. TheMetaFoodWorkshopChallenge
representsalandmarkinitiativeinthisemergingfield respondingtothepressingdemandforprecise
andscalabletechniquesforestimatingfoodportionsandmonitoringnutritionalconsumption. Such
technologiesarevitalforfosteringhealthiereatingbehaviorsandaddressinghealthissueslinkedto
Byconcentratingonthedevelopmentofaccurate3Dmodelsoffoodderivedfromvariousvisual
disparitybetweencurrentmethodologiesandpracticalneeds. Itpromotesthecreationofunique
solutionscapableofmanagingtheintricaciesoffoodmorphology texture andillumination whilealso
meetingthereal-worlddemandsofdietaryevaluation. Thisinitiativegathersexpertsfromcomputer
vision machinelearning andnutritionsciencetopropel3Dfoodreconstructiontechnologiesforward.
portionestimationacrossdiverseapplications fromindividualhealthtrackingtoextensivenutritional
Conventionalmethodsforassessingdiet like24-HourRecallorFoodFrequencyQuestionnaires
burdensome. Thelackof3Ddatain2DRGBfoodimagesfurthercomplicatestheuseofregression-
basedmethodsforestimatingfoodportionsdirectlyfromimagesofeatingoccasions. Byenhancing
3Dreconstructionforfood theaimistoprovidemoreaccurateandintuitivenutritionalassessment
impactnutritionscienceandpublichealth.
Participantsweretaskedwithcreating3Dmodelsof20distinctfooditemsfrom2Dimages mim-
ickingscenarioswheremobiledevicesequippedwithdepth-sensingcamerasareusedfordietary
recordingandnutritionaltracking. Thechallengewassegmentedintothreetiersofdifficultybased
onthenumberofimagesprovided: approximately200imagesforeasy 30formedium andasingle
proposedsolutionsundervariousrealisticconditions. Anotablefeatureofthischallengewastheuse
ofavisiblecheckerboardforphysicalreferencingandtheprovisionofdepthimagesforeachframe
ensuringthe3Dmodelsmaintainedaccuratereal-worldmeasurementsforportionsizeestimation.
Thisinitiativenotonlyexpandsthefrontiersof3Dreconstructiontechnologybutalsosetsthestage
formorereliableanduser-friendlyreal-worldapplications includingimage-baseddietaryassessment.
Theresultingsolutionsholdthepotentialtoprofoundlyinfluencenutritionalintakemonitoringand
comprehension supportingbroaderhealthandwellnessobjectives. Asprogresscontinues innovative
applicationsareanticipatedtotransformpersonalhealthmanagement nutritionalresearch andthe
widerfoodindustry. Theremainderofthisreportisstructuredasfollows: Section2delvesintothe
existingliteratureonfoodportionsizeestimation  Section3describesthedatasetandevaluation
frameworkusedinthechallenge andSections4 5 and6discussthemethodologiesandfindingsof
thetopthreeteams(VolETA ININ-VIAUN andFoodRiddle) respectively.
2 RelatedWork
Estimatingfoodportionsisacrucialpartofimage-baseddietaryassessment aimingtodeterminethe
volume energycontent ormacronutrientsdirectlyfromimagesofmeals. Unlikethewell-studied
taskoffoodrecognition estimatingfoodportionsisparticularlychallengingduetothelackof3D
informationandphysicalsizereferencesnecessaryforaccuratelyjudgingtheactualsizeoffood
portions. Accurateportionsizeestimationrequiresunderstandingthevolumeanddensityoffood
elementsthatarehardtodeducefroma2Dimage underscoringtheneedforsophisticatedtechniques
totacklethisproblem. Currentmethodsforestimatingfoodportionsaregroupedintofourcategories.
Stereo-BasedApproachesusemultipleimagestoreconstructthe3Dstructureoffood. Somemethods
othersperformtwo-viewdensereconstruction. SimultaneousLocalizationandMapping(SLAM)has
alsobeenusedforcontinuous real-timefoodvolumeestimation. However thesemethodsarelimited
bytheirneedformultipleimages whichisnotalwayspractical.
Model-BasedApproachesusepredefinedshapesandtemplatestoestimatevolume. Forinstance
certaintemplatesareassignedtofoodsfromalibraryandtransformedbasedonphysicalreferencesto
estimatethesizeandlocationofthefood. Templatematchingapproachesestimatefoodvolumefrom
asingleimage buttheystrugglewithvariationsinfoodshapesthatdifferfrompredefinedtemplates.
Recentworkhasused3Dfoodmeshesastemplatestoaligncameraandobjectposesforportionsize
DepthCamera-BasedApproachesusedepthcamerastocreatedepthmaps capturingthedistancefrom
thecameratothefood. Thesedepthmapsformavoxelrepresentationusedforvolumeestimation.
Themaindrawbackistheneedforhigh-qualitydepthmapsandtheextraprocessingrequiredfor
consumer-gradedepthsensors.
estimation. Regressionnetworksestimatetheenergyvalueoffoodfromsingleimagesorfroman
'EnergyDistributionMap'thatmapsinputimagestoenergydistributions. Somenetworksuseboth
imagesanddepthmapstoestimateenergy mass andmacronutrientcontent. However deeplearning
degradingwhentestimagessignificantlydifferfromtrainingdata.
Whilethesemethodshaveadvancedfoodportionestimation theyfacelimitationsthathindertheir
widespreaduseandaccuracy. Stereo-basedmethodsareimpracticalforsingleimages model-based
approachesstrugglewithdiversefoodshapes  depthcameramethodsneedspecializedhardware
anddeeplearningapproacheslackinterpretabilityandstrugglewithout-of-distributionsamples. 3D
reconstructionoffersapromisingsolutionbyprovidingcomprehensivespatialinformation adapting
organizationofthe3DFoodReconstructionchallenge aimingtoovercomeexistinglimitationsand
impactingnutritionalassessmentanddietarymonitoring.
3 DatasetsandEvaluationPipeline
3.1 DatasetDescription
ThedatasetfortheMetaFoodChallengefeatures20carefullychosenfooditemsfromtheMetaFood3D
dataset eachscannedin3Dandaccompaniedbyvideorecordings. Toensureprecisesizeaccuracy
inthereconstructed3Dmodels eachfooditemwascapturedalongsideacheckerboardandpattern
mat servingasphysicalscalingreferences. Thechallengeisdividedintothreelevelsofdifficulty
determinedbythequantityof2Dimagesprovidedforreconstruction:
• Easy: Around200imagestakenfromvideo.
• Medium: 30images.
• Hard: Asingleimagefromatop-downperspective.
Table1detailsthefooditemsincludedinthedataset.
Table1: MetaFoodChallengeDataDetails
ObjectIndex FoodItem DifficultyLevel NumberofFrames
2 Cinnamonbun Easy 200
3 Porkrib Easy 200
5 Frenchtoast Easy 200
9 Blueberrymuffin Medium 30
15 Chickennugget Medium 30
16 Everythingbagel Hard 1
3.2 EvaluationPipeline
Theevaluationprocessissplitintotwophases focusingontheaccuracyofthereconstructed3D
modelsintermsofshape(3Dstructure)andportionsize(volume).
3.2.1 Phase-I:VolumeAccuracy
accuracy calculatedasfollows:
n (cid:12) (cid:12)
MAPE= 1 (cid:88)(cid:12)(cid:12)Ai−Fi(cid:12)(cid:12)×100% (1)
n (cid:12) A (cid:12)
i
i=1
whereA istheactualvolume(inml)ofthei-thfooditemobtainedfromthescanned3Dfoodmesh
andF isthevolumecalculatedfromthereconstructed3Dmesh.
3.2.2 Phase-II:ShapeAccuracy
TeamsthatperformwellinPhase-Iareaskedtosubmitcomplete3Dmeshfilesforeachfooditem.
Thisphaseinvolvesseveralstepstoensureprecisionandfairness:
• ModelVerification: SubmittedmodelsarecheckedagainstthefinalPhase-Isubmissionsfor
consistency andvisualinspectionsareconductedtopreventruleviolations.
• ModelAlignment: Participantsreceivegroundtruth3Dmodelsandascripttocomputethe
finalChamferdistance. Theymustaligntheirmodelswiththegroundtruthandpreparea
transformationmatrixforeachsubmittedobject. ThefinalChamferdistanceiscalculated
usingthesemodelsandmatrices.
• Chamfer Distance Calculation: Shape accuracy is assessed using the Chamfer distance
metric. GiventwopointsetsX andY theChamferdistanceisdefinedas:
1 (cid:88) 1 (cid:88)
d (X Y)= min∥x−y∥2+ min∥x−y∥2 (2)
CD |X| y∈Y 2 |Y| x∈X 2
x∈X y∈Y
Thismetricoffersacomprehensivemeasureofsimilaritybetweenthereconstructed3Dmodelsand
thegroundtruth. ThefinalrankingisdeterminedbycombiningscoresfrombothPhase-I(volume
accuracy)andPhase-II(shapeaccuracy). NotethatafterthePhase-Ievaluation qualityissueswere
foundwiththedataforobject12(steak)andobject15(chickennugget) sotheseitemswereexcluded
fromthefinaloverallevaluation.
4 FirstPlaceTeam-VolETA
Theteam’sresearchemploysmulti-viewreconstructiontogeneratedetailedfoodmeshesandcalculate
precisefoodvolumes.
Theteam’smethodintegratescomputervisionanddeeplearningtoaccuratelyestimatefoodvolume
fromRGBDimagesandmasks. Keyframeselectionensuresdataquality supportedbyperceptual
hashingandblurdetection. Cameraposeestimationandobjectsegmentationpavethewayforneural
surfacereconstruction creatingdetailedmeshesforvolumeestimation. Refinementsteps including
isolatedpieceremovalandscalingfactoradjustments enhanceaccuracy. Thisapproachprovidesa
thoroughsolutionforaccuratefoodvolumeassessment withpotentialusesinnutritionanalysis.
4.1.2 TheTeam’sProposal: VolETA
Theteamstartsbyacquiringinputdata specificallyRGBDimagesandcorrespondingfoodobject
masks. The RGBD images  denoted as I = {I }n   where n is the total number of frames
D Di i=1
providedepthinformationalongsideRGBimages. Thefoodobjectmasks {Mf}n  helpidentify
i i=1
regionsofinterestwithintheseimages.
Next  the team selects keyframes. From the set {I }n   keyframes {IK}k ⊆ {I }n are
Di i=1 j j=1 Di i=1
high-qualityframes. ThisinvolvesapplyingaGaussianblurringkernelfollowedbythefastFourier
transformmethod. Near-ImageSimilarityusesperceptualhashingandHammingdistancethreshold-
ingtodetectsimilarimagesandretainoverlappingones. Duplicatesandblurryimagesareexcluded
tomaintaindataintegrityandaccuracy.
Usingtheselectedkeyframes{IK}k  theteamestimatescameraposesthroughamethodcalled
j j=1
PixSfM whichinvolvesextractingfeaturesusingSuperPoint matchingthemwithSuperGlue and
refining them. The outputs are the camera poses {C }k   crucial for understanding the scene’s
spatiallayout.
thereferenceobjectwithauser-providedprompt producingareferenceobjectmaskMR foreach
extendsthereferenceobjectmaskMRtoallframes creatingacomprehensivesetofreferenceobject
masks{MR}n . Thisensuresconsistentreferenceobjectidentificationthroughoutthedataset.
TocreateRGBAimages theteamcombinesRGBimages referenceobjectmasks{MR}n  and
foodobjectmasks{MF}n . Thisstep denotedas{IR}n  integratesvariousdatasourcesintoa
i i=1 i i=1
unifiedformatforfurtherprocessing.
TheteamconvertstheRGBAimages{IR}n andcameraposes{C }k intomeaningfulmetadata
i i=1 j j=1
andmodeleddataD . Thistransformationfacilitatesaccuratescenereconstruction.
m
ThemodeleddataD isinputintoNeuS2formeshreconstruction. NeuS2generatescolorfulmeshes
{Rf Rr}forthereferenceandfoodobjects providingdetailed3Drepresentations.Theteamusesthe
'RemoveIsolatedPieces'techniquetorefinethemeshes. Giventhatthescenescontainonlyonefood
item thediameterthresholdissetto5%ofthemeshsize. Thismethoddeletesisolatedconnected
componentswithdiameterslessthanorequalto5% resultinginacleanedmesh{RCf RCr}. This
stepensuresthatonlysignificantpartsofthemeshareretained.
TheteammanuallyidentifiesaninitialscalingfactorS usingthereferencemeshviaMeshLab. This
factorisfine-tunedtoS usingdepthinformationandfoodandreferencemasks ensuringaccurate
f
scalingrelativetoreal-worlddimensions. Finally thefine-tunedscalingfactorS isappliedtothe
cleanedfoodmeshRCf  producingthefinalscaledfoodmeshRFf. Thisstepculminatesinan
accuratelyscaled3Drepresentationofthefoodobject enablingprecisevolumeestimation.
4.1.3 Detectingthescalingfactor
Generally 3Dreconstructionmethodsproduceunitlessmeshesbydefault. Toaddressthis theteam
manuallydeterminesthescalingfactorbymeasuringthedistanceforeachblockofthereference
objectmesh. Theaverageofallblocklengthsl iscalculated whiletheactualreal-worldlengthis
avg
constantatl =0.012meters. ThescalingfactorS =l /l isappliedtothecleanfoodmesh
real real avg
RCf resultinginthefinalscaledfoodmeshRFf inmeters.
Theteamusesdepthinformationalongwithfoodandreferenceobjectmaskstovalidatethescaling
factors. ThemethodforassessingfoodsizeinvolvesusingoverheadRGBimagesforeachscene.
Initially thepixel-per-unit(PPU)ratio(inmeters)isdeterminedusingthereferenceobject. Subse-
quently thefoodwidth(f )andlength(f )areextractedusingafoodobjectmask. Todeterminethe
w l
foodheight(f ) atwo-stepprocessisfollowed. First binaryimagesegmentationisperformedusing
h
theoverheaddepthandreferenceimages yieldingasegmenteddepthimageforthereferenceobject.
The average depth is then calculated using the segmented reference object depth (d ). Similarly
r
averagedepthforthesegmentedfooddepthimage(d )iscomputed. Theestimatedfoodheightf is
f h
theabsolutedifferencebetweend andd . ToassesstheaccuracyofthescalingfactorS thefood
r f
boundingboxvolume(f ×f ×f )×PPU iscomputed. Theteamevaluatesifthescalingfactor
w l h
S generatesafoodvolumeclosetothispotentialvolume resultinginS . Table2liststhescaling
fine
factors PPU 2Dreferenceobjectdimensions 3Dfoodobjectdimensions andpotentialvolume.
Forone-shot3Dreconstruction theteamusesOne-2-3-45toreconstructa3Dmodelfromasingle
RGBAviewinputafterapplyingbinaryimagesegmentationtobothfoodRGBandmaskimages.
Isolatedpiecesareremovedfromthegeneratedmesh andthescalingfactorS whichisclosertothe
potentialvolumeofthecleanmesh isreused.
4.2 ExperimentalResults
4.2.1 Implementationsettings
ExperimentswereconductedusingtwoGPUs: GeForceGTX1080Ti/12GandRTX3060/6G.The
Hammingdistancefornearimagesimilaritywassetto12. ForGaussiankernelradius evennumbers
intherange[0...30]wereusedfordetectingblurryimages. Thediameterforremovingisolatedpieces
wassetto5%. NeuS2wasrunfor15 000iterationswithameshresolutionof512x512 aunitcube
'aabbscale'of1 'scale'of0.15 and'offset'of[0.5 0.5 0.5]foreachfoodscene.
4.2.2 VolETAResults
Theteamextensivelyvalidatedtheirapproachonthechallengedatasetandcomparedtheirresults
withgroundtruthmeshesusingMAPEandChamferdistancemetrics. Theteam’sapproachwas
appliedseparatelytoeachfoodscene. Aone-shotfoodvolumeestimationapproachwasusedif
thenumberofkeyframeskequaled1;otherwise afew-shotfoodvolumeestimationwasapplied.
Notably thekeyframeselectionprocesschose34.8%ofthetotalframesfortherestofthepipeline
showingtheminimumframeswiththehighestinformation.
Table2: ListofExtractedInformationUsingRGBDandMasks
Level Id Label S PPU R ×R (f ×f ×f )
f w l w l h
1 Strawberry 0.08955223881 0.01786 320×360 (238×257×2.353)
2 Cinnamonbun 0.1043478261 0.02347 236×274 (363×419×2.353)
3 Porkrib 0.1043478261 0.02381 246×270 (435×778×1.176)
Easy 4 Corn 0.08823529412 0.01897 291×339 (262×976×2.353)
5 Frenchtoast 0.1034482759 0.02202 266×292 (530×581×2.53)
6 Sandwich 0.1276595745 0.02426 230×265 (294×431×2.353)
7 Burger 0.1043478261 0.02435 208×264 (378×400×2.353)
8 Cake 0.1276595745 0.02143 256×300 (298×310×4.706)
9 Blueberrymuffin 0.08759124088 0.01801 291×357 (441×443×2.353)
10 Banana 0.08759124088 0.01705 315×377 (446×857×1.176)
Medium 11 Salmon 0.1043478261 0.02390 242×269 (201×303×1.176)
13 Burrito 0.1034482759 0.02372 244×271 (251×917×2.353)
14 Frankfurtsandwich 0.1034482759 0.02115 266×304 (400×1022×2.353)
16 Everythingbagel 0.08759124088 0.01747 306×368 (458×134×1.176)
Hard 17 Croissant 0.1276595745 0.01751 319×367 (395×695×2.176)
18 Shrimp 0.08759124088 0.02021 249×318 (186×95×0.987)
19 Waffle 0.01034482759 0.01902 294×338 (465×537×0.8)
20 Pizza 0.01034482759 0.01913 292×336 (442×651×1.176)
Afterfindingkeyframes PixSfMestimatedtheposesandpointcloud.Aftergeneratingscaledmeshes
theteamcalculatedvolumesandChamferdistancewithandwithouttransformationmetrics. Meshes
wereregisteredwithgroundtruthmeshesusingICPtoobtaintransformationmetrics.
Table3presentsquantitativecomparisonsoftheteam’svolumesandChamferdistancewithand
withoutestimatedtransformationmetricsfromICP.Foroverallmethodperformance Table4shows
theMAPEandChamferdistancewithandwithouttransformationmetrics.
Additionally qualitativeresultsonone-andfew-shot3Dreconstructionfromthechallengedataset
areshown. Themodelexcelsintexturedetails artifactcorrection missingdatahandling andcolor
adjustmentacrossdifferentsceneparts.
Limitations: Despitepromisingresults severallimitationsneedtobeaddressedinfuturework:
• Manualprocesses: Thecurrentpipelineincludesmanualstepslikeprovidingsegmentation
promptsandidentifyingscalingfactors whichshouldbeautomatedtoenhanceefficiency.
• Input requirements: The method requires extensive input information  including food
masksanddepthdata. Streamliningtheseinputswouldsimplifytheprocessandincrease
• Complexbackgroundsandobjects: Themethodhasnotbeentestedinenvironmentswith
complexbackgroundsorhighlyintricatefoodobjects.
• Capturing complexities: The method has not been evaluated under different capturing
complexities suchasvaryingdistancesandcameraspeeds.
• Pipelinecomplexity: Forone-shotneuralrendering theteamcurrentlyusesOne-2-3-45.
Theyaimtouseonlythe2Ddiffusionmodel Zero123 toreducecomplexityandimprove
Table3: QuantitativeComparisonwithGroundTruthUsingChamferDistance
L Id Team’sVol. GTVol. Ch. w/t.m Ch. w/ot.m
Table4: QuantitativeComparisonwithGroundTruthUsingMAPEandChamferDistance
MAPE Ch. w/t.m Ch. w/ot.m
5 SecondPlaceTeam-ININ-VIAUN
Thissectiondetailstheteam’sproposednetwork illustratingthestep-by-stepprocessfromoriginal
imagestofinalmeshmodels.
5.1.1 Scalefactorestimation
TheprocedureforestimatingthescalefactoratthecoordinatelevelisillustratedinFigure9. The
teamadherestoamethodinvolvingcornerprojectionmatching. Specifically utilizingtheCOLMAP
densemodel theteamacquirestheposeofeachimagealongwithdensepointclouddata. Forany
givenimageimg anditsextrinsicparameters[R|t]  theteaminitiallyperformsthreshold-based
k k
cornerdetection settingthethresholdat240. Thisstepallowsthemtoobtainthepixelcoordinates
ofalldetectedcorners. Subsequently usingtheintrinsicparameterskandtheextrinsicparameters
[R|t]   the point cloud is projected onto the image plane. Based on the pixel coordinates of the
k
corners theteamcanidentifytheclosestpointcoordinatesPk foreachcorner whereirepresentsthe
indexofthecorner. Thus theycancalculatethedistancebetweenanytwocornersasfollows:
Dk =(Pk−Pk)2 ∀i̸=j (3)
ij i j
Todeterminethefinalcomputedlengthofeachcheckerboardsquareinimagek theteamtakesthe
minimumvalueofeachrowofthematrixDk (excludingthediagonal)toformthevectordk. The
medianofthisvectoristhenused. ThefinalscalecalculationformulaisgivenbyEquation4 where
0.012representstheknownlengthofeachsquare(1.2cm):
0.012
scale= (4)
(cid:80)n med(dk)
5.1.2 3DReconstruction
The3Dreconstructionprocess depictedinFigure10 involvestwodifferentpipelinestoaccommodate
variationsininputviewpoints. Thefirstfifteenobjectsareprocessedusingonepipeline whilethe
lastfivesingle-viewobjectsareprocessedusinganother.
Fortheinitialfifteenobjects theteamusesCOLMAPtoestimateposesandsegmentthefoodusing
theprovidedsegmentmasks. Advancedmulti-view3Dreconstructionmethodsarethenappliedto
reconstructthesegmentedfood. Theteamemploysthreedifferentreconstructionmethods:COLMAP
DiffusioNeRF andNeRF2Mesh. Theyselectthebestreconstructionresultsfromthesemethodsand
extractthemesh. Theextractedmeshisscaledusingtheestimatedscalefactor andoptimization
techniquesareappliedtoobtainarefinedmesh.
Forthelastfivesingle-viewobjects theteamexperimentswithseveralsingle-viewreconstruction
methods includingZero123 Zero123++ One2345 ZeroNVS andDreamGaussian. Theychoose
intrinsiccameraparametersfromthefifteenthobjectareused andanoptimizationmethodbased
single-viewreconstruction depthinformationfromthedatasetandthecheckerboardinthemonocular
imageareusedtodeterminethesizeoftheextractedmesh. Finally  optimizationtechniquesare
appliedtoobtainarefinedmesh.
5.1.3 Meshrefinement
Duringthe3DReconstructionphase itwasobservedthatthemodel’sresultsoftensufferedfromlow
qualityduetoholesontheobject’ssurfaceandsubstantialnoise asshowninFigure11.
Toaddresstheholes MeshFix anoptimizationmethodbasedoncomputationalgeometry isem-
LaplacianSmoothingmethodadjuststhepositionofeachvertextotheaverageofitsneighboring
 
1 (cid:88)
V(new) =V(old)+λ V(old)−V(old) (5)
i i |N(i)| j i
j∈N(i)
Intheirimplementation thesmoothingfactorλissetto0.2 and10iterationsareperformed.
5.2 ExperimentalResults
5.2.1 Estimatedscalefactor
ThescalefactorsestimatedusingthedescribedmethodareshowninTable5. Eachimageandthe
correspondingreconstructed3Dmodelyieldascalefactor andthetablepresentstheaveragescale
factorforeachobject.
5.2.2 Reconstructedmeshes
TherefinedmeshesobtainedusingthedescribedmethodsareshowninFigure12. Thepredicted
modelvolumes groundtruthmodelvolumes andthepercentageerrorsbetweenthemarepresented
inTable6.
Theteamdesignsamulti-stagealignmentmethodforevaluatingreconstructionquality. Figure13
illustratesthealignmentprocessforObject14. First thecentralpointsofboththepredictedand
groundtruthmodelsarecalculated andthepredictedmodelismovedtoalignwiththecentralpoint
ofthegroundtruthmodel. Next ICPregistrationisperformedforfurtheralignment significantly
reducingtheChamferdistance. Finally gradientdescentisusedforadditionalfine-tuningtoobtain
thefinaltransformationmatrix.
ThetotalChamferdistancebetweenall18predictedmodelsandthegroundtruthsis0.069441169.
Table5: EstimatedScaleFactors
ObjectIndex FoodItem ScaleFactor
2 Cinnamonbun 0.081829
3 Porkrib 0.073861
5 Frenchtoast 0.078632
9 Blueberrymuffin 0.059292
Table6: MetricofVolume
ObjectIndex PredictedVolume GroundTruth ErrorPercentage
6 Best3DMeshReconstructionTeam-FoodRiddle
Toachievehigh-fidelityfoodmeshreconstruction theteamdevelopedtwoproceduralpipelinesas
depictedinFigure14. Forsimpleandmediumcomplexitycases theyemployedastructure-from-
motionstrategytoascertaintheposeofeachimage followedbymeshreconstruction. Subsequently
quality. Forcasesinvolvingonlyasingleimage theteamutilizedimagegenerationtechniquesto
facilitatemodelgeneration.
6.1.1 Multi-ViewReconstruction
ForStructurefromMotion(SfM) theteamenhancedtheadvancedCOLMAPmethodbyintegrating
SuperPointandSuperGluetechniques. Thisintegrationsignificantlyaddressedtheissueoflimited
keypointsinsceneswithminimaltexture asillustratedinFigure15.
Inthemeshreconstructionphase theteam’sapproachbuildsupon2DGaussianSplatting which
employsadifferentiable2DGaussianrendererandincludesregularizationtermsfordepthdistortion
produceadensepointcloud.
Duringpost-processing theteamappliedfilteringandoutlierremovalmethods identifiedtheoutline
ofthesupportingsurface  andprojectedthelowermeshverticesontothissurface. Theyutilized
thereconstructedcheckerboardtocorrectthemodel’sscaleandemployedPoissonreconstructionto
createacomplete watertightmeshofthesubject.
6.1.2 Single-ViewReconstruction
For3Dreconstructionfromasingleimage theteamutilizedadvancedmethodssuchasLGM Instant
Mesh andOne-2-3-45togenerateaninitialmesh. Thisinitialmeshwasthenrefinedinconjunction
withdepthstructureinformation.
Toadjustthescale  theteamestimatedtheobject’slengthusingthecheckerboardasareference
assumingthattheobjectandthecheckerboardareonthesameplane. Theythenprojectedthe3D
objectbackontotheoriginal2Dimagetoobtainamoreprecisescalefortheobject.
6.2 ExperimentalResults
minimizestheChamferdistancebetweentheirmeshandthegroundtruthmesh. Thisoptimization
aimedtoalignthetwomeshesascloselyaspossibleinthree-dimensionalspace. Uponcompletion
amountedto0.0032175meters. AsshowninTable7 TeamFoodRiddleachievedthebestscoresfor
bothmulti-viewandsingle-viewreconstructions outperformingotherteamsinthecompetition.
Table7: TotalErrorsforDifferentTeamsonMulti-viewandSingle-viewData
Team Multi-view(1-14) Single-view(16-20)
VolETA 0.071921 0.058726
challengeon3DFoodReconstruction. Thechallengesoughttoenhance3Dreconstructionmethods
byconcentratingonfooditems tacklingthedistinctdifficultiespresentedbyvariedtextures reflective
surfaces andintricategeometriescommoninculinarysubjects.
Thecompetitioninvolved20diversefooditems capturedundervariousconditionsandwithdiffering
numbersofinputimages specificallydesignedtochallengeparticipantsincreatingrobustreconstruc-
tionmodels. Theevaluationwasbasedonatwo-phaseprocess assessingbothportionsizeaccuracy
throughMeanAbsolutePercentageError(MAPE)andshapeaccuracyusingtheChamferdistance
Ofallparticipatingteams threereachedthefinalsubmissionstage presentingarangeofinnovative
solutions. TeamVolETAsecuredfirstplacewiththebestoverallperformanceinbothPhase-Iand
entriesfor3Dmeshreconstruction. Thechallengehassuccessfullyadvancedthefieldof3Dfood
reconstruction demonstratingthepotentialforaccuratevolumeestimationandshapereconstruction
participatingteamsestablishastrongfoundationforfutureresearchinthisarea potentiallyleading
tomorepreciseanduser-friendlyapproachesfordietaryassessmentandmonitoring.
1
Introduction
the top three teams (VolETA  ININ-VIAUN  and FoodRiddle)  respectively.
Related Work
Datasets and Evaluation Pipeline
3.1
Dataset Description
Object Index
Food Item
Difficulty Level
Number of Frames
Strawberry
Easy
199
Cinnamon bun
200
Pork rib
Corn
French toast
Sandwich
Burger
Cake
Blueberry muffin
Medium
30
Banana
11
Salmon
12
Steak
13
Burrito
14
Hotdog
15
Chicken nugget
16
Everything bagel
Hard
17
Croissant
18
Shrimp
19
Waffle
20
Pizza
3.2
Evaluation Pipeline
3.2.1
Phase-I: Volume Accuracy
MAPE = 1
n
�
����
Ai − Fi
Ai
���� × 100%
(1)
where Ai is the actual volume (in ml) of the i-th food item obtained from the scanned 3D food mesh
and Fi is the volume calculated from the reconstructed 3D mesh.
3.2.2
Phase-II: Shape Accuracy
• Model Verification: Submitted models are checked against the final Phase-I submissions for
• Model Alignment: Participants receive ground truth 3D models and a script to compute the
metric. Given two point sets X and Y   the Chamfer distance is defined as:
dCD(X  Y ) =
|X|
x∈X
min
y∈Y ∥x − y∥2
2 + 1
|Y |
y∈Y
x∈X ∥x − y∥2
(2)
First Place Team - VolETA
4.1
Methodology
4.1.1
Overview
4.1.2
The Team’s Proposal: VolETA
masks. The RGBD images  denoted as ID = {IDi}n
i=1  where n is the total number of frames
provide depth information alongside RGB images. The food object masks  {M f
i }n
j }k
j=1 ⊆ {IDi}n
i=1 are
the reference object with a user-provided prompt  producing a reference object mask M R for each
extends the reference object mask M R to all frames  creating a comprehensive set of reference object
masks {M R
To create RGBA images  the team combines RGB images  reference object masks {M R
food object masks {M F
i=1 and camera poses {Cj}k
j=1 into meaningful metadata
The modeled data Dm is input into NeuS2 for mesh reconstruction. NeuS2 generates colorful meshes
{Rf  Rr} for the reference and food objects  providing detailed 3D representations. The team uses the
The team manually identifies an initial scaling factor S using the reference mesh via MeshLab. This
factor is fine-tuned to Sf using depth information and food and reference masks  ensuring accurate
scaling relative to real-world dimensions. Finally  the fine-tuned scaling factor Sf is applied to the
cleaned food mesh RCf  producing the final scaled food mesh RF f. This step culminates in an
4.1.3
Detecting the scaling factor
object mesh. The average of all block lengths lavg is calculated  while the actual real-world length is
constant at lreal = 0.012 meters. The scaling factor S = lreal/lavg is applied to the clean food mesh
RCf  resulting in the final scaled food mesh RF f in meters.
quently  the food width (fw) and length (fl) are extracted using a food object mask. To determine the
food height (fh)  a two-step process is followed. First  binary image segmentation is performed using
The average depth is then calculated using the segmented reference object depth (dr). Similarly
average depth for the segmented food depth image (df) is computed. The estimated food height fh is
the absolute difference between dr and df. To assess the accuracy of the scaling factor S  the food
bounding box volume (fw × fl × fh) × PPU is computed. The team evaluates if the scaling factor
S generates a food volume close to this potential volume  resulting in Sfine. Table 2 lists the scaling
4.2
Experimental Results
4.2.1
Implementation settings
4.2.2
VolETA Results
the number of keyframes k equaled 1; otherwise  a few-shot food volume estimation was applied.
Level
Id
Label
Sf
PPU
Rw × Rl
(fw × fl × fh)
0.08955223881
0.01786
320 × 360
(238 × 257 × 2.353)
0.1043478261
0.02347
236 × 274
(363 × 419 × 2.353)
0.02381
246 × 270
(435 × 778 × 1.176)
0.08823529412
0.01897
291 × 339
(262 × 976 × 2.353)
0.1034482759
0.02202
266 × 292
(530 × 581 × 2.53)
0.1276595745
0.02426
230 × 265
(294 × 431 × 2.353)
0.02435
208 × 264
(378 × 400 × 2.353)
0.02143
256 × 300
(298 × 310 × 4.706)
0.08759124088
0.01801
291 × 357
(441 × 443 × 2.353)
0.01705
315 × 377
(446 × 857 × 1.176)
0.02390
242 × 269
(201 × 303 × 1.176)
0.02372
244 × 271
(251 × 917 × 2.353)
Frankfurt sandwich
0.02115
266 × 304
(400 × 1022 × 2.353)
0.01747
306 × 368
(458 × 134 × 1.176)
0.01751
319 × 367
(395 × 695 × 2.176)
0.02021
249 × 318
(186 × 95 × 0.987)
0.01034482759
0.01902
294 × 338
(465 × 537 × 0.8)
0.01913
292 × 336
(442 × 651 × 1.176)
• Manual processes: The current pipeline includes manual steps like providing segmentation
• Complex backgrounds and objects: The method has not been tested in environments with
• Pipeline complexity: For one-shot neural rendering  the team currently uses One-2-3-45.
L
Team’s Vol.
GT Vol.
Ch. w/ t.m
Ch. w/o t.m
40.06
38.53
1.63
85.40
216.9
280.36
7.12
111.47
278.86
249.67
13.69
172.88
E
279.02
295.13
2.03
61.30
395.76
392.58
13.67
102.14
205.17
218.44
6.68
150.78
372.93
368.77
4.70
66.91
186.62
173.13
2.98
152.34
224.08
232.74
3.91
160.07
153.76
163.09
2.67
138.45
M
80.4
85.18
3.37
151.14
363.99
308.28
5.18
147.53
535.44
589.83
4.31
89.66
163.13
262.15
18.06
28.33
H
181.36
9.44
28.94
25.4
20.58
4.28
12.84
110.05
108.35
11.34
23.98
130.96
119.83
15.59
31.05
MAPE
(%)
sum
mean
10.973
0.130
0.007
1.715
0.095
Second Place Team - ININ-VIAUN
5.1
5.1.1
Scale factor estimation
given image imgk and its extrinsic parameters [R|t]k  the team initially performs threshold-based
of all detected corners. Subsequently  using the intrinsic parameters k and the extrinsic parameters
corners  the team can identify the closest point coordinates P k
i for each corner  where i represents the
ij = (P k
i − P k
j )2
∀i ̸= j
(3)
minimum value of each row of the matrix Dk (excluding the diagonal) to form the vector dk. The
scale =
�n
i=1 med(dk)
(4)
5.1.2
3D Reconstruction
5.1.3
Mesh refinement
V (new)
= V (old)
+ λ


|N(i)|
V (old)
j
− V (old)


(5)
In their implementation  the smoothing factor λ is set to 0.2  and 10 iterations are performed.
5.2
5.2.1
Estimated scale factor
5.2.2
Reconstructed meshes
5.2.3
Alignment
Scale Factor
0.060058
0.081829
0.073861
0.083594
0.078632
0.088368
0.103124
0.068496
0.059292
0.058236
0.083821
0.069663
0.073766
Table 6: Metric of Volume
Predicted Volume
Ground Truth
Error Percentage
44.51
15.52
321.26
14.59
336.11
34.62
347.54
17.76
389.28
0.84
197.82
412.52
11.86
181.21
4.67
233.79
0.45
160.06
1.86
86.0
0.96
334.7
8.57
517.75
12.22
176.24
32.77
180.68
0.37
13.58
34.01
117.72
8.64
117.43
20.03
Best 3D Mesh Reconstruction Team - FoodRiddle
6.1
6.1.1
Multi-View Reconstruction
6.1.2
Single-View Reconstruction
6.2
Team
Multi-view (1-14)
Single-view (16-20)
FoodRiddle
0.036362
0.019232
ININ-VIAUN
0.041552
0.027889
VolETA
0.071921
0.058726
Conclusion
solutions. Team VolETA secured first place with the best overall performance in both Phase-I and"
R008,1,EMNLP,"This study examines the effectiveness of transfer learning and multi-task learning
in the context of a complex semantic classification problem: understanding the
meaning of noun-noun compounds. Through a series of detailed experiments and
an in-depth analysis of errors  we demonstrate that employing transfer learning by
initializing parameters and multi-task learning through parameter sharing enables a
neural classification model to better generalize across a dataset characterized by a
highly uneven distribution of semantic relationships. Furthermore  we illustrate
how utilizing dual annotations  which involve two distinct sets of relations applied
to the same compounds  can enhance the overall precision of a neural classifier and
improve its F1 scores for less common yet more challenging semantic relations.
1","Noun-noun compound interpretation involves determining the semantic connection between two
nouns (or noun phrases in multi-word compounds). For instance  in the compound 'street protest '
the task is to identify the semantic relationship between 'street' and 'protest ' which is a locative
relation in this example. Given the prevalence of noun-noun compounds in natural language and its
significance to other natural language processing (NLP) tasks like question answering and information
retrieval  understanding noun-noun compounds has been extensively studied in theoretical linguistics
psycholinguistics  and computational linguistics.
In computational linguistics  noun-noun compound interpretation is typically treated as an automatic
classification task. Various machine learning (ML) algorithms and models  such as Maximum
Entropy  Support Vector Machines  and Neural Networks  have been employed to decipher the
semantics of nominal compounds. These models utilize information from lexical semantics  like
WordNet-based features  and distributional semantics  such as word embeddings. However  noun-
noun compound interpretation remains a challenging NLP problem due to the high productivity
of noun-noun compounding as a linguistic structure and the difficulty in deriving the semantics of
noun-noun compounds from their constituents. Our research contributes to advancing NLP research
on noun-noun compound interpretation through the application of transfer and multi-task learning.
The application of transfer learning (TL) and multi-task learning (MTL) in NLP has gained significant
attention in recent years  yielding varying outcomes based on the specific tasks  model architectures
and datasets involved. These varying","our attention on methods that frame the interpretation problem as a classification task involving a
fixed  predetermined set of relations. Various machine learning models have been applied to this
task  including nearest neighbor classifiers that use semantic similarity based on lexical resources
kernel-based methods like SVMs that utilize lexical and relational features  Maximum Entropy
models that incorporate a wide range of lexical and surface form features  and neural networks that
rely on word embeddings or combine word embeddings with path embeddings. Among these studies
some have utilized the same dataset. To our knowledge  TL and MTL have not been previously
applied to compound interpretation. Therefore  we review prior research on TL and MTL in other
NLP tasks.
Several recent studies have conducted extensive experiments on the application of TL and MTL to a
variety of NLP tasks  such as named entity recognition  semantic labeling  sentence-level sentiment
classification  super-tagging  chunking  and semantic dependency parsing. The consensus among
these studies is that the advantages of TL and MTL are largely contingent on the characteristics of the
tasks involved  including the unevenness of the data distribution  the semantic relatedness between
the source and target tasks  the learning trajectory of the auxiliary and main tasks (where target tasks
that quickly reach a plateau benefit most from non-plateauing auxiliary tasks)  and the structural
similarity between the tasks. Besides differing in the NLP tasks they investigate  the aforementioned
studies employ slightly varied definitions of TL and MTL. Our research aligns with certain studies in
that we apply TL and MTL to learn different semantic annotations of noun-noun compounds using
the same dataset. However  our experimental design is more akin to other work in that we experiment
with initializing parameters across all layers of the neural network and concurrently train a single
MTL model on two sets of relations.
3 Task Definition and Dataset
The objective of this task is to train a model to categorize the semantic relationships between pairs
of nouns in a labeled dataset  where each pair forms a noun-noun compound. The complexity of
this task is influenced by factors such as the label set used and its distribution. For the experiments
detailed in this paper  we utilize a noun-noun compounds dataset that features compounds annotated
with two distinct taxonomies of relations. This means that each noun-noun compound is associated
with two different relations  each based on different linguistic theories. This dataset is derived from
established linguistic resources  including NomBank and the Prague Czech-English Dependency
Treebank 2.0 (PCEDT). We chose this dataset for two primary reasons: firstly  the dual annotation of
relations on the same set of compounds is ideal for exploring TL and MTL approaches; secondly
aligning two different annotation frameworks on the same data allows for a comparative analysis
across these frameworks.
Specifically  we use a portion of the dataset  focusing on type-based instances of two-word compounds.
The original dataset also encompasses multi-word compounds (those made up of more than two
nouns) and multiple instances per compound type. We further divide the dataset into three parts:
training  development  and test sets. Table 1 details the number of compound types and the vocabulary
size for each set  including a breakdown of words appearing in the right-most (right constituents)
and left-most (left constituents) positions. The two label sets consist of 35 PCEDT functors and 18
2
NomBank argument and adjunct relations. As discussed in Section 7.1  these label sets have a highly
uneven distribution.
Table 1: Characteristics of the noun-noun compound dataset used in our experiments. The numbers
in this table correspond to a subset of the dataset  see Section 3.
Train Dev Test
Compounds 6932 920 1759
V ocab size 4102 1163 1772
Right constituents 2304 624 969
Left constituents 2405 618 985
Many relations in PCEDT and NomBank conceptually describe similar semantic ideas  as they are
used to annotate the semantics of the same text. For instance  the temporal and locative relations in
NomBank (ARGM-TMP and ARGM-LOC  respectively) and their PCEDT counterparts (TWHEN
and LOC) exhibit relatively consistent behavior across frameworks  as they annotate many of the
same compounds. However  some relations that are theoretically similar do not align well in practice.
For example  the functor AIM in PCEDT and the modifier argument ARGM-PNC in NomBank
express a somewhat related semantic concept (purpose)  but there is minimal overlap between the
sets of compounds they annotate. Nevertheless  it is reasonable to assume that the semantic similarity
in the label sets  where it exists  can be leveraged through transfer and multi-task learning  especially
since the overall distribution of relations differs between the two frameworks.
4 Transfer vs. Multi-Task Learning
In this section  we employ the terminology and definitions established by Pan and Yang (2010) to
articulate our framework for transfer and multi-task learning. Our classification task can be described
in terms of all training pairs (X  Y) and a probability distribution P(X)  where X represents the input
feature space  Y denotes the set of all labels  and N is the training data size. The domain of a task is
defined by X  P(X). Our goal is to learn a function f(X) that predicts Y based on the input features X.
Considering two ML tasks  Ta and Tb  we would train two distinct models to learn separate functions
fa and fb for predicting Ya and Yb in a single-task learning scenario. However  if Ta and Tb are
related  either explicitly or implicitly  TL and MTL can enhance the generalization of either or both
tasks. Two tasks are deemed related when their domains are similar but their label sets differ  or when
their domains are dissimilar but their label sets are identical. Consequently  noun-noun compound
interpretation using the dataset is well-suited for TL and MTL  as the training examples are identical
but the label sets are distinct.
For clarity  we differentiate between transfer learning and multi-task learning in this paper  despite
these terms sometimes being used interchangeably in the literature. We define TL as the utilization of
parameters from a model trained on Ta to initialize another model for Tb. In contrast  MTL involves
training parts of the same model to learn both Ta and Tb  essentially learning one set of parameters
for both tasks. The concept is to train a single model simultaneously on both tasks  where one task
introduces an inductive bias that aids the model in generalizing over the main task. It is important to
note that this does not necessarily imply that we aim to use a single model to predict both label sets
in practice.
5 Neural Classification Models
This section introduces the neural classification models utilized in our experiments. To discern the
impact of TL and MTL  we initially present a single-task learning model  which acts as our baseline.
Subsequently  we employ this same model to implement TL and MTL.
5.1 Single-Task Learning Model
In our single-task learning (STL) configuration  we train and fine-tune a feed-forward neural network
inspired by the neural classifier proposed by Dima and Hinrichs (2015). This network comprises four
layers: 1) an input layer  2) an embedding layer  3) a hidden layer  and 4) an output layer. The input
3
layer consists of two integers that indicate the indices of a compound’s constituents in the embedding
layer  where the word embedding vectors are stored. These selected vectors are then passed to a fully
connected hidden layer  the size of which matches the dimensionality of the word embedding vectors.
Finally  a softmax function is applied to the output layer to select the most probable relation.
The compound’s constituents are represented using a 300-dimensional word embedding model trained
on an English Wikipedia dump and the English Gigaword Fifth Edition. The embedding model was
trained by Fares et al. (2017). If a word is not found during lookup in the embedding model  we
check if the word is uppercased and attempt to find the lowercase version. For hyphenated words
not found in the embedding vocabulary  we split the word at the hyphen and average the vectors of
its parts  if they are present in the vocabulary. If the word remains unrepresented after these steps  a
designated vector for unknown words is employed.
5.1.1 Architecture and Hyperparameters
Our selection of hyperparameters is informed by multiple rounds of experimentation with the single-
task learning model  as well as the choices made by prior work. The weights of the embedding layer
are updated during the training of all models. We utilize the Adaptive Moment Estimation (Adam)
optimization function across all models  with a learning rate set to 0.001. The loss function employed
is the negative-log likelihood. A Sigmoid activation function is used for the units in the hidden layer.
All models are trained with mini-batches of size five. The maximum number of epochs is capped
at 50  but an early stopping criterion based on the model’s accuracy on the validation split is also
implemented. This means that training is halted if the validation accuracy does not improve over five
consecutive epochs. All models are implemented in Keras  using TensorFlow as the backend. The TL
and MTL models are trained using the same hyperparameters as the STL model.
5.2 Transfer Learning Models
In our experiments  transfer learning involves training an STL model on PCEDT relations and then
using some of its weights to initialize another model for NomBank relations. Given the neural
classifier architecture detailed in Section 5.1  we identify three ways to implement TL: 1) TLE:
Transferring the embedding layer weights  2) TLH: Transferring the hidden layer weights  and 3)
TLEH: Transferring both the embedding and hidden layer weights. Furthermore  we differentiate
between transfer learning from PCEDT to NomBank and vice versa. This results in six setups
as shown in Table 2. We do not apply TL (or MTL) to the output layer because it is task- or
dataset-specific.
5.3 Multi-Task Learning Models
In MTL  we train a single model to simultaneously learn both PCEDT and NomBank relations
meaning all MTL models have two objective functions and two output layers. We implement two
MTL setups: MTLE  which features a shared embedding layer but two task-specific hidden layers
and MTLF  which has no task-specific layers aside from the output layer (i.e.  both the embedding
and hidden layers are shared). We distinguish between the auxiliary and main tasks based on which
validation accuracy (NomBank’s or PCEDT’s) is monitored by the early stopping criterion. This
leads to a total of four MTL models  as shown in Table 3.
6 Experimental Results
Tables 2 and 3 display the accuracies of the various TL and MTL models on the development and test
splits for NomBank and PCEDT. The top row in both tables indicates the accuracy of the STL model.
All models were trained solely on the training split. Several insights can be gleaned from these
tables. Firstly  the accuracy of the STL models decreases when evaluated on the test split for both
NomBank and PCEDT. Secondly  all TL models achieve improved accuracy on the NomBank test
split  although transfer learning does not significantly enhance accuracy on the development split of
the same dataset. The MTL models  especially MTLF  have a detrimental effect on the development
accuracy of NomBank  yet we observe a similar improvement  as with TL  on the test split. Thirdly
both TL and MTL models demonstrate less consistent effects on PCEDT (on both development and
test splits) compared to NomBank. For instance  all TL models yield an absolute improvement of
4
about 1.25 points in accuracy on NomBank  whereas in PCEDT  TLE clearly outperforms the other
two TL models (TLE improves over the STL accuracy by 1.37 points).
Table 2: Accuracy (%) of the transfer learning models.
Model NomBank PCEDT
Dev Test Dev Test
STL 78.15 76.75 58.80 56.05
TLE 78.37 78.05 59.57 57.42
TLH 78.15 78.00 59.24 56.51
TLEH 78.48 78.00 59.89 56.68
Table 3: Accuracy (%) of the MTL models.
MTLE 77.93 78.45 59.89 56.96
MTLF 76.74 78.51 58.91 56.00
Overall  the STL models’ accuracy declines when tested on the NomBank and PCEDT test splits
compared to their performance on the development split. This could suggest overfitting  especially
since our stopping criterion selects the model with the best performance on the development split.
Conversely  TL and MTL enhance accuracy on the test splits  despite using the same stopping criterion
as STL. We interpret this as an improvement in the models’ ability to generalize. However  since
these improvements are relatively minor  we further analyze the results to understand if and how TL
and MTL are beneficial.
7 Results Analysis
This section provides a detailed analysis of the models’ performance  drawing on insights from the
dataset and the classification errors made by the models. The discussion in the following sections is
primarily based on the results from the test split  as it is larger than the development split.
7.1 Relation Distribution
To illustrate the complexity of the task  we depict the distribution of the most frequent relations in
NomBank and PCEDT across the three data splits in Figure 1. Notably  approximately 71.18% of the
relations in the NomBank training split are of type ARG1 (prototypical patient)  while 52.20% of the
PCEDT relations are of type RSTR (an underspecified adnominal modifier). Such a highly skewed
distribution makes learning some of the other relations more challenging  if not impossible in certain
cases. In fact  out of the 15 NomBank relations observed in the test split  five are never predicted
by any of the STL  TL  or MTL models. Similarly  of the 26 PCEDT relations in the test split  only
six are predicted. However  the unpredicted relations are extremely rare in the training split (e.g.  23
PCEDT functors appear less than 20 times)  making it doubtful whether any ML model could learn
them under any circumstances.
Given this imbalanced distribution  it is evident that accuracy alone is insufficient to determine the
best-performing model. Therefore  in the subsequent section  we report and analyze the F1 scores of
the predicted NomBank and PCEDT relations across all STL  TL  and MTL models.
7.2 Per-Relation F1 Scores
Tables 4 and 5 present the per-relation F1 scores for NomBank and PCEDT  respectively. We only
include results for relations that are actually predicted by at least one of the models.
5
Table 4: Per-label F1 score on the NomBank test split.
A0 A1 A2 A3 LOC MNR TMP
Count 132 1282 153 75 25 25 27
STL 49.82 87.54 45.78 60.81 28.57 29.41 66.67
TLE 55.02 87.98 41.61 60.14 27.91 33.33 63.83
TLH 54.81 87.93 42.51 60.00 25.00 35.29 65.31
TLEH 53.62 87.95 42.70 61.11 29.27 33.33 65.22
MTLE 54.07 88.34 42.86 61.97 30.00 28.57 66.67
MTLF 53.09 88.41 38.14 62.69 00.00 00.00 52.17
Table 5: Per-label F1 score on the PCEDT test split.
ACT TWHEN APP PAT REG RSTR
Count 89 14 118 326 216 900
STL 43.90 42.11 22.78 42.83 20.51 68.81
TLE 49.37 70.97 27.67 41.60 30.77 69.67
TLH 53.99 62.07 25.00 43.01 26.09 68.99
TLEH 49.08 64.52 28.57 42.91 28.57 69.08
MTLE 54.09 66.67 24.05 42.03 27.21 69.31
MTLF 47.80 42.11 25.64 40.73 19.22 68.89
Several noteworthy patterns emerge from Tables 4 and 5. Firstly  the MTLF model appears to be
detrimental to both datasets  leading to significantly degraded F1 scores for four NomBank relations
including the locative modifier ARGM-LOC and the manner modifier ARGM-MNR (abbreviated as
LOC and MNR in Table 4)  which the model fails to predict altogether. This same model exhibits
the lowest F1 score compared to all other models for two PCEDT relations: REG (expressing a
circumstance) and PAT (patient). Considering that the MTLF model achieves the highest accuracy
on the NomBank test split (as shown in Table 3)  it becomes even more apparent that relying solely
on accuracy scores is inadequate for evaluating the effectiveness of TL and MTL for this task and
dataset.
Secondly  with the exception of the MTLF model  all TL and MTL models consistently improve
the F1 score for all PCEDT relations except PAT. Notably  the F1 scores for the relations TWHEN
and ACT show a substantial increase compared to other PCEDT relations when only the embedding
layer’s weights are shared (MTLE) or transferred (TLE). This outcome can be partially understood
by examining the correspondence matrices between NomBank arguments and PCEDT functors
presented in Tables 7 and 6. These tables illustrate how PCEDT functors map to NomBank arguments
in the training split (Table 6) and vice versa (Table 7). Table 6 reveals that 80% of the compounds
annotated as TWHEN in PCEDT were annotated as ARGM-TMP in NomBank. Additionally  47% of
ACT (Actor) relations map to ARG0 (Proto-Agent) in NomBank. While this mapping is not as distinct
as one might hope  it is still relatively high when compared to how other PCEDT relations map to
ARG0. The correspondence matrices also demonstrate that the presumed theoretical similarities
between NomBank and PCEDT relations do not always hold in practice. Nevertheless  even such
imperfect correspondences can provide a training signal that assists the TL and MTL models in
learning relations like TWHEN and ACT.
Since the TLE model outperforms STL in predicting REG by ten absolute points  we examined
all REG compounds correctly classified by TLE but misclassified by STL. We found that STL
misclassified them as RSTR  indicating that TL from NomBank helps TLE recover from STL’s
overgeneralization in RSTR prediction.
The two NomBank relations that receive the highest boost in F1 score (about five absolute points)
are ARG0 and ARGM-MNR  but the improvement in the latter corresponds to only one additional
compound  which might be a chance occurrence. Overall  TL and MTL from NomBank to PCEDT
are more helpful than the reverse. One explanation is that five PCEDT relations (including the four
most frequent ones) map to ARG1 in NomBank in more than 60% of cases for each relation  as seen
in the first rows of Tables 6 and 7. This suggests that the weights learned to predict PCEDT relations
6
Table 6: Correspondence matrix between PCEDT functors and NomBank arguments. Slots with ’-’
indicate zero  0.00 represents a very small number but not zero.
A1 A2 A0 A3 LOC TMP MNR
RSTR 0.70 0.11 0.06 0.06 0.02 0.01 0.02
PAT 0.90 0.05 0.01 0.02 0.01 - 0.00
REG 0.78 0.10 0.04 0.06 0.00 0.00 0.00
APP 0.62 0.21 0.13 0.02 0.01 0.00 -
ACT 0.47 0.03 0.47 0.01 0.01 - 0.01
AIM 0.65 0.12 0.07 0.06 0.01 - -
TWHEN 0.10 0.03 - - - 0.80 -
Count 3617 1312 777 499 273 116 59
Table 7: Correspondence matrix between NomBank arguments and PCEDT functors.
RSTR PAT REG APP ACT AIM TWHEN
A1 0.51 0.54 0.12 0.06 0.03 0.02 0.00
A2 0.47 0.09 0.11 0.14 0.01 0.02 0.00
A0 0.63 0.03 0.07 0.13 0.26 0.02 -
A3 0.66 0.08 0.13 0.03 0.01 0.02 -
LOC 0.36 0.07 0.02 0.05 0.03 0.01 -
TMP 0.78 - 0.01 0.01 - - 0.01
MNR 0.24 0.05 0.01 - 0.03 - -
Count 4932 715 495 358 119 103 79
offer little to no inductive bias for NomBank relations. Conversely  the mapping from NomBank to
PCEDT shows that although many NomBank arguments map to RSTR in PCEDT  the percentages
are lower  making the mapping more diverse and discriminative  which seems to aid TL and MTL
models in learning less frequent PCEDT relations.
To understand why the PCEDT functor AIM is never predicted despite being more frequent than
TWHEN  we found that AIM is almost always misclassified as RSTR by all models. Furthermore
AIM and RSTR have the highest lexical overlap in the training set among all PCEDT relation pairs:
78.35% of left constituents and 73.26% of right constituents of compounds annotated as AIM occur
in other compounds annotated as RSTR. This explains the models’ inability to learn AIM but raises
questions about their ability to learn relational representations  which we explore further in Section
7.3.
Table 8: Macro-average F1 score on the test split.
STL 52.66 40.15
TLE 52.83 48.34
TLH 52.98 46.52
TLEH 53.31 47.12
MTLE 53.21 47.23
MTLF 42.07 40.73
Finally  to demonstrate the benefits of TL and MTL for NomBank and PCEDT  we report the F1
macro-average scores in Table 8. This is arguably the appropriate evaluation measure for imbalanced
classification problems. Note that relations not predicted by any model are excluded from the macro-
average calculation. Table 8 clearly shows that TL and MTL on the embedding layer yield significant
improvements for PCEDT  with about a 7-8 point increase in macro-average F1  compared to just
0.65 in the best case for NomBank.
7
7.3 Generalization on Unseen Compounds
We now analyze the models’ ability to generalize to compounds not seen during training. Recent
research suggests that gains in noun-noun compound interpretation using word embeddings and
similar neural classification models might be due to lexical memorization. In other words  the models
learn that specific nouns are strong indicators of specific relations. To assess the role of lexical
memorization in our models  we quantify the number of unseen compounds that the STL  TL  and
MTL models predict correctly.
We differentiate between ’partly’ and ’completely’ unseen compounds. A compound is ’partly’
unseen if one of its constituents (left or right) is not present in the training data. A ’completely’
unseen compound is one where neither the left nor the right constituent appears in the training data.
Overall  nearly 20% of the compounds in the test split have an unseen left constituent  about 16%
have an unseen right constituent  and 4% are completely unseen. Table 9 compares the performance
of the different models on these three groups in terms of the proportion of compounds misclassified
in each group.
Table 9: Generalization error on the subset of unseen compounds in the test split. L: Left constituent.
R: Right constituent. L&R: Completely unseen.
NomBank PCEDT
Model L R L&R L R L&R
Count 351 286 72 351 286 72
STL 27.92 39.51 50.00 45.01 47.55 41.67
TLE 25.93 36.71 48.61 43.87 47.55 41.67
TLH 26.21 38.11 50.00 46.15 49.30 47.22
TLEH 26.50 38.81 52.78 45.87 47.55 43.06
MTLE 24.50 33.22 38.89 44.44 47.20 43.06
MTLF 22.79 34.27 40.28 44.16 47.90 38.89
Table 9 shows that Transfer Learning (TL) and Multi-Task Learning (MTL) approaches reduce
generalization error in NomBank across all scenarios  with the exception of TLH and TLEH for
completely unseen compounds  where error increases. The greatest error reductions are achieved
by MTL models across all three types of unseen compounds. Specifically  MTLE reduces the error
by approximately six points for compounds with unseen right constituents and by eleven points for
fully unseen compounds. Moreover  MTLF reduces the error by five points when the left constituent
is unseen. It’s important to interpret these results in conjunction with the Count row in Table 9 for
a comprehensive view. For example  the eleven-point error decrease in fully unseen compounds
represents eight compounds. In PCEDT  the largest error reduction is on unseen left constituents
which is about 1.14 points  corresponding to four compounds; it’s 0.35 on unseen right constituents
(one compound) and 2.7 on fully unseen compounds  or two compounds.
Upon manual inspection of compounds that led to substantial reductions in the generalization error
specifically within NomBank  we examined the distribution of relations within correctly predicted
unseen compound sets. Compared to the STL model  MTLE reduces generalization error for
completely unseen compounds by a total of eight compounds  of which seven are annotated with the
relation ARG1  which is the most common in NomBank. Regarding the unseen right constituents
MTLE’s 24 improved compounds consist of 18 ARG1  5 ARG0  and 1 ARG2 compounds. A
similar pattern arises when examining TLE model improvements  where most gains come from better
predictions of ARG1 and ARG0 relations.
A large portion of unseen compounds  whether partly or entirely unseen  that were misclassified by
every model  were not of type ARG1 in NomBank  or RSTR in PCEDT. This pattern  along with
correctly predicted unseen compounds primarily annotated with the most common relations  suggests
that classification models rely on lexical memorization to learn the compound relation interpretation.
To better comprehend lexical memorization’s impact  we present the ratio of relation-specific con-
stituents in both NomBank and PCEDT  as depicted in Figure 2. We define a relation-specific
constituent as a left or right constituent that appears with only one specific relation within the training
data. Its ratio is calculated as its proportion in the full set of left or right constituents for each
8
relation. Analyzing Figure 2 reveals that NomBank relations possess higher ratios of relation-specific
constituents compared to PCEDT. This potentially makes learning the former easier if the model
solely relies on lexical memorization. Additionally  ARGM-TMP in NomBank and TWHEN in
PCEDT have distinctly high ratios compared to other relations in Figure 2. These relations also
have the second-highest F1 score in their datasets—except for STL on PCEDT (see Tables 4 and
5). Lexical memorization is therefore a likely cause of these high F1 scores. We also observed that
lower ratios of relation-specific constituents correlate with lower F1 scores  such as APP and REG in
PCEDT. Based on these insights  we can’t dismiss the possibility that our models show some degree
of lexical memorization  despite manual analysis also presenting cases where models demonstrate
generalization and correct predictions in situations where lexical memorization is impossible.","been previously applied to noun-noun compound interpretation  motivate our thorough empirical
investigation into the use of TL and MTL for this task. Our aim is not only to add to the existing
research on the effectiveness of TL and MTL for semantic NLP tasks generally but also to ascertain
their specific advantages for compound interpretation.
A key reason for utilizing multi-task learning is to enhance generalization by making use of the
domain-specific details present in the training data of related tasks. In this study  we demonstrate that
TL and MTL can serve as a form of regularization  enabling the prediction of infrequent relations
within a dataset marked by a highly skewed distribution of relations. This dataset is particularly
well-suited for TL and MTL experimentation  as elaborated in Section 3.
Our contributions are summarized as follows:
1. Through meticulous analysis of results  we discover that TL and MTL  especially when applied
to the embedding layer  enhance overall accuracy and F1 scores for less frequent relations in a
highly skewed dataset  compared to a robust single-task learning baseline. 2. Although our research
concentrates on TL and MTL  we present  to our knowledge  the first experimental results on the
relatively recent dataset from Fares (2016).
2 Related Work
Approaches to interpreting noun-noun compounds differ based on the classification of compound
relations  as well as the machine learning models and features employed to learn these relations. For
instance  some define a broad set of relations  while others employ a more detailed classification.
Some researchers challenge the idea that noun-noun compounds can be interpreted using a fixed
predetermined set of relations  proposing alternative","The application of transfer and multi-task learning in natural language processing has gained sig-
nificant traction  yet considerable ambiguity persists regarding the effectiveness of particular task
characteristics and experimental setups. This research endeavors to clarify the benefits of TL and
MTL in the context of semantic interpretation of noun-noun compounds. By executing a sequence of
minimally contrasting experiments and conducting thorough analysis of results and prediction errors
we demonstrate how both TL and MTL can mitigate the effects of class imbalance and drastically
enhance predictions for low-frequency relations. Overall  our TL  and particularly our MTL models
are better at making predictions both quantitatively and qualitatively. Notably  the improvements are
observed on the ’most challenging’ inputs that include at least one constituent that was not present in
the training data. However  clear indications of ’lexical memorization’ effects are evident in our error
analysis of unseen compounds.
Typically  the transfer of representations or sharing between tasks is more effective at the embedding
layers  which represent the model’s internal representation of the compound constituents. Furthermore
in multi-task learning  the complete sharing of model architecture across tasks degrades its capacity
to generalize when it comes to less frequent relations.
The dataset provided by Fares (2016) is an appealing resource for new neural approaches to compound
interpretation because it links this sub-problem with broad-coverage semantic role labeling or
semantic dependency parsing in PCEDT and NomBank. Future research will focus on incorporating
additional natural language processing tasks defined using these frameworks to understand noun-noun
compound interpretation using TL and MTL.
9
Thisstudyexaminestheeffectivenessoftransferlearningandmulti-tasklearning
inthecontextofacomplexsemanticclassificationproblem: understandingthe
meaningofnoun-nouncompounds. Throughaseriesofdetailedexperimentsand
anin-depthanalysisoferrors wedemonstratethatemployingtransferlearningby
initializingparametersandmulti-tasklearningthroughparametersharingenablesa
neuralclassificationmodeltobettergeneralizeacrossadatasetcharacterizedbya
highlyunevendistributionofsemanticrelationships. Furthermore weillustrate
howutilizingdualannotations whichinvolvetwodistinctsetsofrelationsapplied
tothesamecompounds canenhancetheoverallprecisionofaneuralclassifierand
improveitsF1scoresforlesscommonyetmorechallengingsemanticrelations.
nouns(ornounphrasesinmulti-wordcompounds). Forinstance inthecompound'streetprotest '
thetaskistoidentifythesemanticrelationshipbetween'street'and'protest 'whichisalocative
relationinthisexample. Giventheprevalenceofnoun-nouncompoundsinnaturallanguageandits
significancetoothernaturallanguageprocessing(NLP)taskslikequestionansweringandinformation
retrieval understandingnoun-nouncompoundshasbeenextensivelystudiedintheoreticallinguistics
psycholinguistics andcomputationallinguistics.
Incomputationallinguistics noun-nouncompoundinterpretationistypicallytreatedasanautomatic
semanticsofnominalcompounds. Thesemodelsutilizeinformationfromlexicalsemantics like
WordNet-basedfeatures anddistributionalsemantics suchaswordembeddings. However noun-
ofnoun-nouncompoundingasalinguisticstructureandthedifficultyinderivingthesemanticsof
noun-nouncompoundsfromtheirconstituents. OurresearchcontributestoadvancingNLPresearch
onnoun-nouncompoundinterpretationthroughtheapplicationoftransferandmulti-tasklearning.
Theapplicationoftransferlearning(TL)andmulti-tasklearning(MTL)inNLPhasgainedsignificant
attentioninrecentyears yieldingvaryingoutcomesbasedonthespecifictasks modelarchitectures
anddatasetsinvolved. Thesevaryingresults combinedwiththefactthatneitherTLnorMTLhas
beenpreviouslyappliedtonoun-nouncompoundinterpretation motivateourthoroughempirical
investigationintotheuseofTLandMTLforthistask. Ouraimisnotonlytoaddtotheexisting
researchontheeffectivenessofTLandMTLforsemanticNLPtasksgenerallybutalsotoascertain
theirspecificadvantagesforcompoundinterpretation.
domain-specificdetailspresentinthetrainingdataofrelatedtasks. Inthisstudy wedemonstratethat
TLandMTLcanserveasaformofregularization enablingthepredictionofinfrequentrelations
well-suitedforTLandMTLexperimentation aselaboratedinSection3.
Ourcontributionsaresummarizedasfollows:
1. Throughmeticulousanalysisofresults wediscoverthatTLandMTL especiallywhenapplied
highlyskeweddataset comparedtoarobustsingle-tasklearningbaseline. 2. Althoughourresearch
concentratesonTLandMTL wepresent toourknowledge thefirstexperimentalresultsonthe
relativelyrecentdatasetfromFares(2016).
2 RelatedWork
Approachestointerpretingnoun-nouncompoundsdifferbasedontheclassificationofcompound
relations aswellasthemachinelearningmodelsandfeaturesemployedtolearntheserelations. For
instance somedefineabroadsetofrelations whileothersemployamoredetailedclassification.
Someresearcherschallengetheideathatnoun-nouncompoundscanbeinterpretedusingafixed
ourattentiononmethodsthatframetheinterpretationproblemasaclassificationtaskinvolvinga
fixed predeterminedsetofrelations. Variousmachinelearningmodelshavebeenappliedtothis
task includingnearestneighborclassifiersthatusesemanticsimilaritybasedonlexicalresources
modelsthatincorporateawiderangeoflexicalandsurfaceformfeatures andneuralnetworksthat
relyonwordembeddingsorcombinewordembeddingswithpathembeddings. Amongthesestudies
appliedtocompoundinterpretation. Therefore wereviewpriorresearchonTLandMTLinother
NLPtasks.
SeveralrecentstudieshaveconductedextensiveexperimentsontheapplicationofTLandMTLtoa
varietyofNLPtasks suchasnamedentityrecognition semanticlabeling sentence-levelsentiment
classification super-tagging chunking andsemanticdependencyparsing. Theconsensusamong
thesestudiesisthattheadvantagesofTLandMTLarelargelycontingentonthecharacteristicsofthe
tasksinvolved includingtheunevennessofthedatadistribution thesemanticrelatednessbetween
thesourceandtargettasks thelearningtrajectoryoftheauxiliaryandmaintasks(wheretargettasks
thatquicklyreachaplateaubenefitmostfromnon-plateauingauxiliarytasks)  andthestructural
similaritybetweenthetasks. BesidesdifferingintheNLPtaskstheyinvestigate theaforementioned
studiesemployslightlyvarieddefinitionsofTLandMTL.Ourresearchalignswithcertainstudiesin
thatweapplyTLandMTLtolearndifferentsemanticannotationsofnoun-nouncompoundsusing
thesamedataset. However ourexperimentaldesignismoreakintootherworkinthatweexperiment
withinitializingparametersacrossalllayersoftheneuralnetworkandconcurrentlytrainasingle
MTLmodelontwosetsofrelations.
3 TaskDefinitionandDataset
Theobjectiveofthistaskistotrainamodeltocategorizethesemanticrelationshipsbetweenpairs
ofnounsinalabeleddataset whereeachpairformsanoun-nouncompound. Thecomplexityof
thistaskisinfluencedbyfactorssuchasthelabelsetusedanditsdistribution. Fortheexperiments
detailedinthispaper weutilizeanoun-nouncompoundsdatasetthatfeaturescompoundsannotated
withtwodistincttaxonomiesofrelations. Thismeansthateachnoun-nouncompoundisassociated
withtwodifferentrelations eachbasedondifferentlinguistictheories. Thisdatasetisderivedfrom
establishedlinguisticresources  includingNomBankandthePragueCzech-EnglishDependency
Treebank2.0(PCEDT).Wechosethisdatasetfortwoprimaryreasons: firstly thedualannotationof
relationsonthesamesetofcompoundsisidealforexploringTLandMTLapproaches;secondly
aligningtwodifferentannotationframeworksonthesamedataallowsforacomparativeanalysis
acrosstheseframeworks.
Specifically weuseaportionofthedataset focusingontype-basedinstancesoftwo-wordcompounds.
nouns)andmultipleinstancespercompoundtype. Wefurtherdividethedatasetintothreeparts:
training development andtestsets.Table1detailsthenumberofcompoundtypesandthevocabulary
sizeforeachset includingabreakdownofwordsappearingintheright-most(rightconstituents)
andleft-most(leftconstituents)positions. Thetwolabelsetsconsistof35PCEDTfunctorsand18
NomBankargumentandadjunctrelations. AsdiscussedinSection7.1 theselabelsetshaveahighly
unevendistribution.
Table1: Characteristicsofthenoun-nouncompounddatasetusedinourexperiments. Thenumbers
inthistablecorrespondtoasubsetofthedataset seeSection3.
Vocabsize 4102 1163 1772
Rightconstituents 2304 624 969
Leftconstituents 2405 618 985
ManyrelationsinPCEDTandNomBankconceptuallydescribesimilarsemanticideas astheyare
usedtoannotatethesemanticsofthesametext. Forinstance thetemporalandlocativerelationsin
NomBank(ARGM-TMPandARGM-LOC respectively)andtheirPCEDTcounterparts(TWHEN
andLOC)exhibitrelativelyconsistentbehavioracrossframeworks astheyannotatemanyofthe
samecompounds. However somerelationsthataretheoreticallysimilardonotalignwellinpractice.
expressasomewhatrelatedsemanticconcept(purpose) butthereisminimaloverlapbetweenthe
setsofcompoundstheyannotate. Nevertheless itisreasonabletoassumethatthesemanticsimilarity
inthelabelsets whereitexists canbeleveragedthroughtransferandmulti-tasklearning especially
sincetheoveralldistributionofrelationsdiffersbetweenthetwoframeworks.
4 Transfervs. Multi-TaskLearning
Inthissection weemploytheterminologyanddefinitionsestablishedbyPanandYang(2010)to
articulateourframeworkfortransferandmulti-tasklearning. Ourclassificationtaskcanbedescribed
intermsofalltrainingpairs(X Y)andaprobabilitydistributionP(X) whereXrepresentstheinput
featurespace Ydenotesthesetofalllabels andNisthetrainingdatasize. Thedomainofataskis
definedbyX P(X).Ourgoalistolearnafunctionf(X)thatpredictsYbasedontheinputfeaturesX.
ConsideringtwoMLtasks TaandTb wewouldtraintwodistinctmodelstolearnseparatefunctions
faandfbforpredictingYaandYbinasingle-tasklearningscenario. However  ifTaandTbare
related eitherexplicitlyorimplicitly TLandMTLcanenhancethegeneralizationofeitherorboth
tasks. Twotasksaredeemedrelatedwhentheirdomainsaresimilarbuttheirlabelsetsdiffer orwhen
theirdomainsaredissimilarbuttheirlabelsetsareidentical. Consequently noun-nouncompound
interpretationusingthedatasetiswell-suitedforTLandMTL asthetrainingexamplesareidentical
butthelabelsetsaredistinct.
Forclarity wedifferentiatebetweentransferlearningandmulti-tasklearninginthispaper despite
thesetermssometimesbeingusedinterchangeablyintheliterature. WedefineTLastheutilizationof
parametersfromamodeltrainedonTatoinitializeanothermodelforTb. Incontrast MTLinvolves
trainingpartsofthesamemodeltolearnbothTaandTb essentiallylearningonesetofparameters
forbothtasks. Theconceptistotrainasinglemodelsimultaneouslyonbothtasks whereonetask
introducesaninductivebiasthataidsthemodelingeneralizingoverthemaintask. Itisimportantto
notethatthisdoesnotnecessarilyimplythatweaimtouseasinglemodeltopredictbothlabelsets
inpractice.
5 NeuralClassificationModels
Thissectionintroducestheneuralclassificationmodelsutilizedinourexperiments. Todiscernthe
impactofTLandMTL weinitiallypresentasingle-tasklearningmodel whichactsasourbaseline.
Subsequently weemploythissamemodeltoimplementTLandMTL.
5.1 Single-TaskLearningModel
Inoursingle-tasklearning(STL)configuration wetrainandfine-tuneafeed-forwardneuralnetwork
inspiredbytheneuralclassifierproposedbyDimaandHinrichs(2015). Thisnetworkcomprisesfour
layers: 1)aninputlayer 2)anembeddinglayer 3)ahiddenlayer and4)anoutputlayer. Theinput
layerconsistsoftwointegersthatindicatetheindicesofacompound’sconstituentsintheembedding
layer wherethewordembeddingvectorsarestored. Theseselectedvectorsarethenpassedtoafully
connectedhiddenlayer thesizeofwhichmatchesthedimensionalityofthewordembeddingvectors.
Finally asoftmaxfunctionisappliedtotheoutputlayertoselectthemostprobablerelation.
Thecompound’sconstituentsarerepresentedusinga300-dimensionalwordembeddingmodeltrained
onanEnglishWikipediadumpandtheEnglishGigawordFifthEdition. Theembeddingmodelwas
trainedbyFaresetal. (2017). Ifawordisnotfoundduringlookupintheembeddingmodel we
checkifthewordisuppercasedandattempttofindthelowercaseversion. Forhyphenatedwords
notfoundintheembeddingvocabulary wesplitthewordatthehyphenandaveragethevectorsof
itsparts iftheyarepresentinthevocabulary. Ifthewordremainsunrepresentedafterthesesteps a
designatedvectorforunknownwordsisemployed.
5.1.1 ArchitectureandHyperparameters
Ourselectionofhyperparametersisinformedbymultipleroundsofexperimentationwiththesingle-
tasklearningmodel aswellasthechoicesmadebypriorwork. Theweightsoftheembeddinglayer
areupdatedduringthetrainingofallmodels. WeutilizetheAdaptiveMomentEstimation(Adam)
optimizationfunctionacrossallmodels withalearningratesetto0.001. Thelossfunctionemployed
isthenegative-loglikelihood. ASigmoidactivationfunctionisusedfortheunitsinthehiddenlayer.
Allmodelsaretrainedwithmini-batchesofsizefive. Themaximumnumberofepochsiscapped
at50 butanearlystoppingcriterionbasedonthemodel’saccuracyonthevalidationsplitisalso
implemented. Thismeansthattrainingishaltedifthevalidationaccuracydoesnotimproveoverfive
consecutiveepochs. AllmodelsareimplementedinKeras usingTensorFlowasthebackend. TheTL
andMTLmodelsaretrainedusingthesamehyperparametersastheSTLmodel.
5.2 TransferLearningModels
Inourexperiments transferlearninginvolvestraininganSTLmodelonPCEDTrelationsandthen
Transferringtheembeddinglayerweights 2)TLH:Transferringthehiddenlayerweights and3)
TLEH:Transferringboththeembeddingandhiddenlayerweights. Furthermore wedifferentiate
5.3 Multi-TaskLearningModels
meaningallMTLmodelshavetwoobjectivefunctionsandtwooutputlayers. Weimplementtwo
MTLsetups: MTLE whichfeaturesasharedembeddinglayerbuttwotask-specifichiddenlayers
andMTLF whichhasnotask-specificlayersasidefromtheoutputlayer(i.e. boththeembedding
andhiddenlayersareshared). Wedistinguishbetweentheauxiliaryandmaintasksbasedonwhich
validationaccuracy(NomBank’sorPCEDT’s)ismonitoredbytheearlystoppingcriterion. This
leadstoatotaloffourMTLmodels asshowninTable3.
6 ExperimentalResults
Tables2and3displaytheaccuraciesofthevariousTLandMTLmodelsonthedevelopmentandtest
splitsforNomBankandPCEDT.ThetoprowinbothtablesindicatestheaccuracyoftheSTLmodel.
tables. Firstly theaccuracyoftheSTLmodelsdecreaseswhenevaluatedonthetestsplitforboth
NomBankandPCEDT.Secondly allTLmodelsachieveimprovedaccuracyontheNomBanktest
split althoughtransferlearningdoesnotsignificantlyenhanceaccuracyonthedevelopmentsplitof
thesamedataset. TheMTLmodels especiallyMTLF haveadetrimentaleffectonthedevelopment
accuracyofNomBank yetweobserveasimilarimprovement aswithTL onthetestsplit. Thirdly
bothTLandMTLmodelsdemonstratelessconsistenteffectsonPCEDT(onbothdevelopmentand
testsplits)comparedtoNomBank. Forinstance allTLmodelsyieldanabsoluteimprovementof
about1.25pointsinaccuracyonNomBank whereasinPCEDT TLEclearlyoutperformstheother
twoTLmodels(TLEimprovesovertheSTLaccuracyby1.37points).
Table2: Accuracy(%)ofthetransferlearningmodels.
Table3: Accuracy(%)oftheMTLmodels.
Overall theSTLmodels’accuracydeclineswhentestedontheNomBankandPCEDTtestsplits
comparedtotheirperformanceonthedevelopmentsplit. Thiscouldsuggestoverfitting especially
sinceourstoppingcriterionselectsthemodelwiththebestperformanceonthedevelopmentsplit.
Conversely TLandMTLenhanceaccuracyonthetestsplits despiteusingthesamestoppingcriterion
asSTL.Weinterpretthisasanimprovementinthemodels’abilitytogeneralize. However since
theseimprovementsarerelativelyminor wefurtheranalyzetheresultstounderstandifandhowTL
andMTLarebeneficial.
7 ResultsAnalysis
Thissectionprovidesadetailedanalysisofthemodels’performance drawingoninsightsfromthe
datasetandtheclassificationerrorsmadebythemodels. Thediscussioninthefollowingsectionsis
primarilybasedontheresultsfromthetestsplit asitislargerthanthedevelopmentsplit.
7.1 RelationDistribution
Toillustratethecomplexityofthetask wedepictthedistributionofthemostfrequentrelationsin
NomBankandPCEDTacrossthethreedatasplitsinFigure1. Notably approximately71.18%ofthe
relationsintheNomBanktrainingsplitareoftypeARG1(prototypicalpatient) while52.20%ofthe
PCEDTrelationsareoftypeRSTR(anunderspecifiedadnominalmodifier). Suchahighlyskewed
distributionmakeslearningsomeoftheotherrelationsmorechallenging ifnotimpossibleincertain
cases. Infact outofthe15NomBankrelationsobservedinthetestsplit fiveareneverpredicted
byanyoftheSTL TL orMTLmodels. Similarly ofthe26PCEDTrelationsinthetestsplit only
sixarepredicted. However theunpredictedrelationsareextremelyrareinthetrainingsplit(e.g. 23
PCEDTfunctorsappearlessthan20times) makingitdoubtfulwhetheranyMLmodelcouldlearn
themunderanycircumstances.
Giventhisimbalanceddistribution itisevidentthataccuracyaloneisinsufficienttodeterminethe
best-performingmodel. Therefore inthesubsequentsection wereportandanalyzetheF1scoresof
thepredictedNomBankandPCEDTrelationsacrossallSTL TL andMTLmodels.
7.2 Per-RelationF1Scores
Tables4and5presenttheper-relationF1scoresforNomBankandPCEDT respectively. Weonly
includeresultsforrelationsthatareactuallypredictedbyatleastoneofthemodels.
Table4: Per-labelF1scoreontheNomBanktestsplit.
Table5: Per-labelF1scoreonthePCEDTtestsplit.
SeveralnoteworthypatternsemergefromTables4and5. Firstly theMTLFmodelappearstobe
detrimentaltobothdatasets leadingtosignificantlydegradedF1scoresforfourNomBankrelations
includingthelocativemodifierARGM-LOCandthemannermodifierARGM-MNR(abbreviatedas
LOCandMNRinTable4) whichthemodelfailstopredictaltogether. Thissamemodelexhibits
circumstance)andPAT(patient). ConsideringthattheMTLFmodelachievesthehighestaccuracy
ontheNomBanktestsplit(asshowninTable3) itbecomesevenmoreapparentthatrelyingsolely
onaccuracyscoresisinadequateforevaluatingtheeffectivenessofTLandMTLforthistaskand
Secondly withtheexceptionoftheMTLFmodel allTLandMTLmodelsconsistentlyimprove
theF1scoreforallPCEDTrelationsexceptPAT.Notably theF1scoresfortherelationsTWHEN
andACTshowasubstantialincreasecomparedtootherPCEDTrelationswhenonlytheembedding
layer’sweightsareshared(MTLE)ortransferred(TLE).Thisoutcomecanbepartiallyunderstood
presentedinTables7and6. ThesetablesillustratehowPCEDTfunctorsmaptoNomBankarguments
inthetrainingsplit(Table6)andviceversa(Table7). Table6revealsthat80%ofthecompounds
annotatedasTWHENinPCEDTwereannotatedasARGM-TMPinNomBank. Additionally 47%of
ACT(Actor)relationsmaptoARG0(Proto-Agent)inNomBank.Whilethismappingisnotasdistinct
asonemighthope itisstillrelativelyhighwhencomparedtohowotherPCEDTrelationsmapto
betweenNomBankandPCEDTrelationsdonotalwaysholdinpractice. Nevertheless evensuch
learningrelationslikeTWHENandACT.
overgeneralizationinRSTRprediction.
ThetwoNomBankrelationsthatreceivethehighestboostinF1score(aboutfiveabsolutepoints)
areARG0andARGM-MNR buttheimprovementinthelattercorrespondstoonlyoneadditional
compound whichmightbeachanceoccurrence. Overall TLandMTLfromNomBanktoPCEDT
aremorehelpfulthanthereverse. OneexplanationisthatfivePCEDTrelations(includingthefour
mostfrequentones)maptoARG1inNomBankinmorethan60%ofcasesforeachrelation asseen
inthefirstrowsofTables6and7. ThissuggeststhattheweightslearnedtopredictPCEDTrelations
Table6: CorrespondencematrixbetweenPCEDTfunctorsandNomBankarguments. Slotswith’-’
indicatezero 0.00representsaverysmallnumberbutnotzero.
Table7: CorrespondencematrixbetweenNomBankargumentsandPCEDTfunctors.
offerlittletonoinductivebiasforNomBankrelations. Conversely themappingfromNomBankto
PCEDTshowsthatalthoughmanyNomBankargumentsmaptoRSTRinPCEDT thepercentages
arelower makingthemappingmorediverseanddiscriminative whichseemstoaidTLandMTL
modelsinlearninglessfrequentPCEDTrelations.
TounderstandwhythePCEDTfunctorAIMisneverpredicteddespitebeingmorefrequentthan
TWHEN wefoundthatAIMisalmostalwaysmisclassifiedasRSTRbyallmodels. Furthermore
AIMandRSTRhavethehighestlexicaloverlapinthetrainingsetamongallPCEDTrelationpairs:
78.35%ofleftconstituentsand73.26%ofrightconstituentsofcompoundsannotatedasAIMoccur
inothercompoundsannotatedasRSTR.Thisexplainsthemodels’inabilitytolearnAIMbutraises
questionsabouttheirabilitytolearnrelationalrepresentations whichweexplorefurtherinSection
Table8: Macro-averageF1scoreonthetestsplit.
Finally todemonstratethebenefitsofTLandMTLforNomBankandPCEDT wereporttheF1
macro-averagescoresinTable8. Thisisarguablytheappropriateevaluationmeasureforimbalanced
classificationproblems. Notethatrelationsnotpredictedbyanymodelareexcludedfromthemacro-
averagecalculation. Table8clearlyshowsthatTLandMTLontheembeddinglayeryieldsignificant
improvementsforPCEDT withabouta7-8pointincreaseinmacro-averageF1 comparedtojust
0.65inthebestcaseforNomBank.
7.3 GeneralizationonUnseenCompounds
Wenowanalyzethemodels’abilitytogeneralizetocompoundsnotseenduringtraining. Recent
similarneuralclassificationmodelsmightbeduetolexicalmemorization. Inotherwords themodels
memorizationinourmodels wequantifythenumberofunseencompoundsthattheSTL TL and
MTLmodelspredictcorrectly.
unseencompoundisonewhereneithertheleftnortherightconstituentappearsinthetrainingdata.
Overall nearly20%ofthecompoundsinthetestsplithaveanunseenleftconstituent about16%
haveanunseenrightconstituent and4%arecompletelyunseen. Table9comparestheperformance
ofthedifferentmodelsonthesethreegroupsintermsoftheproportionofcompoundsmisclassified
ineachgroup.
Table9: Generalizationerroronthesubsetofunseencompoundsinthetestsplit. L:Leftconstituent.
R:Rightconstituent. L&R:Completelyunseen.
completelyunseencompounds whereerrorincreases. Thegreatesterrorreductionsareachieved
byMTLmodelsacrossallthreetypesofunseencompounds. Specifically MTLEreducestheerror
byapproximatelysixpointsforcompoundswithunseenrightconstituentsandbyelevenpointsfor
fullyunseencompounds. Moreover MTLFreducestheerrorbyfivepointswhentheleftconstituent
isunseen. It’simportanttointerprettheseresultsinconjunctionwiththeCountrowinTable9for
representseightcompounds. InPCEDT thelargesterrorreductionisonunseenleftconstituents
whichisabout1.14points correspondingtofourcompounds;it’s0.35onunseenrightconstituents
(onecompound)and2.7onfullyunseencompounds ortwocompounds.
Uponmanualinspectionofcompoundsthatledtosubstantialreductionsinthegeneralizationerror
specificallywithinNomBank weexaminedthedistributionofrelationswithincorrectlypredicted
completelyunseencompoundsbyatotalofeightcompounds ofwhichsevenareannotatedwiththe
relationARG1 whichisthemostcommoninNomBank. Regardingtheunseenrightconstituents
similarpatternariseswhenexaminingTLEmodelimprovements wheremostgainscomefrombetter
predictionsofARG1andARG0relations.
Alargeportionofunseencompounds whetherpartlyorentirelyunseen thatweremisclassifiedby
everymodel werenotoftypeARG1inNomBank orRSTRinPCEDT.Thispattern alongwith
correctlypredictedunseencompoundsprimarilyannotatedwiththemostcommonrelations suggests
thatclassificationmodelsrelyonlexicalmemorizationtolearnthecompoundrelationinterpretation.
Tobettercomprehendlexicalmemorization’simpact wepresenttheratioofrelation-specificcon-
constituentasaleftorrightconstituentthatappearswithonlyonespecificrelationwithinthetraining
relation. AnalyzingFigure2revealsthatNomBankrelationspossesshigherratiosofrelation-specific
constituentscomparedtoPCEDT.Thispotentiallymakeslearningtheformereasierifthemodel
havethesecond-highestF1scoreintheirdatasets—exceptforSTLonPCEDT(seeTables4and
5). LexicalmemorizationisthereforealikelycauseofthesehighF1scores. Wealsoobservedthat
lowerratiosofrelation-specificconstituentscorrelatewithlowerF1scores suchasAPPandREGin
PCEDT.Basedontheseinsights wecan’tdismissthepossibilitythatourmodelsshowsomedegree
oflexicalmemorization despitemanualanalysisalsopresentingcaseswheremodelsdemonstrate
generalizationandcorrectpredictionsinsituationswherelexicalmemorizationisimpossible.
Theapplicationoftransferandmulti-tasklearninginnaturallanguageprocessinghasgainedsig-
nificanttraction yetconsiderableambiguitypersistsregardingtheeffectivenessofparticulartask
characteristicsandexperimentalsetups. ThisresearchendeavorstoclarifythebenefitsofTLand
MTLinthecontextofsemanticinterpretationofnoun-nouncompounds. Byexecutingasequenceof
minimallycontrastingexperimentsandconductingthoroughanalysisofresultsandpredictionerrors
wedemonstratehowbothTLandMTLcanmitigatetheeffectsofclassimbalanceanddrastically
enhancepredictionsforlow-frequencyrelations. Overall ourTL andparticularlyourMTLmodels
arebetteratmakingpredictionsbothquantitativelyandqualitatively. Notably theimprovementsare
observedonthe’mostchallenging’inputsthatincludeatleastoneconstituentthatwasnotpresentin
thetrainingdata. However clearindicationsof’lexicalmemorization’effectsareevidentinourerror
analysisofunseencompounds.
Typically thetransferofrepresentationsorsharingbetweentasksismoreeffectiveattheembedding
layers whichrepresentthemodel’sinternalrepresentationofthecompoundconstituents.Furthermore
inmulti-tasklearning thecompletesharingofmodelarchitectureacrosstasksdegradesitscapacity
togeneralizewhenitcomestolessfrequentrelations.
ThedatasetprovidedbyFares(2016)isanappealingresourcefornewneuralapproachestocompound
semanticdependencyparsinginPCEDTandNomBank. Futureresearchwillfocusonincorporating
additionalnaturallanguageprocessingtasksdefinedusingtheseframeworkstounderstandnoun-noun
compoundinterpretationusingTLandMTL.
1
Introduction
Related Work
Task Definition and Dataset
Train
Dev
Test
Compounds
6932
920
1759
Vocab size
4102
1163
1772
Right constituents
2304
624
969
Left constituents
2405
618
985
Transfer vs. Multi-Task Learning
Neural Classification Models
5.1
Single-Task Learning Model
5.1.1
Architecture and Hyperparameters
5.2
Transfer Learning Models
5.3
Multi-Task Learning Models
Experimental Results
Model
NomBank
PCEDT
STL
78.15
76.75
58.80
56.05
TLE
78.37
78.05
59.57
57.42
TLH
78.00
59.24
56.51
TLEH
78.48
59.89
56.68
MTLE
77.93
78.45
56.96
MTLF
76.74
78.51
58.91
56.00
Results Analysis
7.1
Relation Distribution
7.2
Per-Relation F1 Scores
A0
A1
A2
A3
LOC
MNR
TMP
Count
132
1282
153
75
25
27
49.82
87.54
45.78
60.81
28.57
29.41
66.67
55.02
87.98
41.61
60.14
27.91
33.33
63.83
54.81
87.93
42.51
60.00
25.00
35.29
65.31
53.62
87.95
42.70
61.11
29.27
65.22
54.07
88.34
42.86
61.97
30.00
53.09
88.41
38.14
62.69
00.00
52.17
ACT
TWHEN
APP
PAT
REG
RSTR
89
14
118
326
216
900
43.90
42.11
22.78
42.83
20.51
68.81
49.37
70.97
27.67
41.60
30.77
69.67
53.99
62.07
43.01
26.09
68.99
49.08
64.52
42.91
69.08
54.09
24.05
42.03
27.21
69.31
47.80
25.64
40.73
19.22
68.89
0.70
0.11
0.06
0.02
0.01
0.90
0.05
-
0.00
0.78
0.10
0.04
0.62
0.21
0.13
0.47
0.03
AIM
0.65
0.12
0.07
0.80
3617
1312
777
499
273
116
59
0.51
0.54
0.09
0.14
0.63
0.26
0.66
0.08
0.36
0.24
4932
715
495
358
119
103
79
52.66
40.15
52.83
48.34
52.98
46.52
53.31
47.12
53.21
47.23
42.07
7.3
Generalization on Unseen Compounds
L
R
L&R
351
286
72
27.92
39.51
50.00
45.01
47.55
41.67
25.93
36.71
48.61
43.87
26.21
38.11
46.15
49.30
47.22
26.50
38.81
52.78
45.87
43.06
24.50
33.22
38.89
44.44
47.20
22.79
34.27
40.28
44.16
47.90
Conclusion"
R009,1,EMNLP,"This study demonstrates that incorporating the written explanations provided by
individuals when making predictions enhances the accuracy of aggregated crowd-
sourced forecasts. The research shows that while majority and weighted vote","The concept of the 'wisdom of the crowd' posits that combining information from numerous non-
expert individuals can produce answers that are as accurate as  or even more accurate than  those
provided by a single expert. A classic example of this concept is the observation that the median
estimate of an ox’s weight from a large group of fair attendees was remarkably close to the actual
weight. While generally supported  the idea is not without its limitations. Historical examples
demonstrate instances where crowds behaved irrationally  and even a world chess champion was able
to defeat the combined moves of a crowd.
In the current era  the advantages of collective intelligence are widely utilized. For example  Wikipedia
relies on the contributions of volunteers  and community-driven question-answering platforms have
garnered significant attention from the research community. When compiling information from
large groups  it is important to determine whether the individual inputs were made independently. If
not  factors like group psychology and the influence of persuasive arguments can skew individual
judgments  thus negating the positive effects of crowd wisdom.
This paper focuses on forecasts concerning questions spanning political  economic  and social
domains. Each forecast includes a prediction  estimating the probability of a particular event  and
a written justification that explains the reasoning behind the prediction. Forecasts with identical
predictions can have justifications of varying strength  which  in turn  affects the perceived reliability
of the predictions. For instance  a justification that simply refers to an external source without
explanation may appear to rely heavily on the prevailing opinion of the crowd and might be considered
weaker than a justification that presents specific  verifiable facts from external resources.
To clarify the terminology used: a 'question' is defined as a statement that seeks information (e.g.
'Will new legislation be implemented before a certain date?'). Questions have a defined start and
end date  and the period between these dates constitutes the 'life' of the question. 'Forecasters'
are individuals who provide a 'forecast ' which consists of a 'prediction' and a 'justification.' The
prediction is a numerical representation of the likelihood of an event occurring. The justification
is the text provided by the forecaster to support their prediction. The central problem addressed in
this work is termed 'calling a question ' which refers to the process of determining a final prediction
by aggregating individual forecasts. Two strategies are employed for calling questions each day
throughout their life: considering forecasts submitted on the given day ('daily') and considering the
last forecast submitted by each forecaster ('active').
Inspired by prior research on recognizing and fostering skilled forecasters  and analyzing written
justifications to assess the quality of individual or collective forecasts  this paper investigates the
automated calling of questions throughout their duration based on the forecasts available each day.
The primary contributions are empirical findings that address the following research questions:
* When making a prediction on a specific day  is it advantageous to include forecasts from previous
days? (Yes) * Does the accuracy of the prediction improve when considering the question itself
and the written justifications provided with the forecasts? (Yes) * Is it easier to make an accurate
prediction toward the end of a question’s duration? (Yes) * Are written justifications more valuable
when the crowd’s predictions are less accurate? (Yes)
In addition  this research presents an examination of the justifications associated with both accurate
and inaccurate forecasts. This analysis aims to identify the features that contribute to a justification
being more or less credible.
2 Related Work
The language employed by individuals is indicative of various characteristics. Prior research includes
both predictive models (using language samples to predict attributes about the author) and models
that provide valuable insights (using language samples and author attributes to identify differentiating
linguistic features). Previous studies have examined factors such as gender and age  political ideology
health outcomes  and personality traits. In this paper  models are constructed to predict outcomes
based on crowd-sourced forecasts without knowledge of individual forecasters’ identities.
Previous research has also explored how language use varies depending on the relationships between
individuals. For instance  studies have analyzed language patterns in social networks  online commu-
nities  and corporate emails to understand how individuals in positions of authority communicate.
Similarly  researchers have examined how language provides insights into interpersonal interactions
and relationships. In terms of language form and function  prior research has investigated politeness
empathy  advice  condolences  usefulness  and deception. Related to the current study’s focus
researchers have examined the influence of Wikipedia editors and studied influence levels within
online communities. Persuasion has also been analyzed from a computational perspective  including
within the context of dialogue systems. The work presented here complements these previous studies.
The goal is to identify credible justifications to improve the aggregation of crowdsourced forecasts
without explicitly targeting any of the aforementioned characteristics.
Within the field of computational linguistics  the task most closely related to this research is argumen-
tation. A strong justification for a forecast can be considered a well-reasoned supporting argument.
Previous work in this area includes identifying argument components such as claims  premises
backing  rebuttals  and refutations  as well as mining arguments that support or oppose a particular
claim. Despite these efforts  it was found that crowdsourced justifications rarely adhere to these
established argumentation frameworks  even though such justifications are valuable for aggregating
forecasts.
Finally  several studies have focused on forecasting using datasets similar or identical to the one used
in this research. From a psychological perspective  researchers have explored strategies for enhancing
forecasting accuracy  such as utilizing top-performing forecasters (often called 'superforecasters')
and have analyzed the traits that contribute to their success. These studies aim to identify and cultivate
superforecasters but do not incorporate the written justifications accompanying forecasts. In contrast
the present research develops models to call questions without using any information about the
forecasters themselves. Within the field of computational linguistics  researchers have evaluated the
language used in high-quality justifications  focusing on aspects like rating  benefit  and influence.
Other researchers have developed models to predict forecaster skill using the textual justifications
from specific datasets  such as the Good Judgment Open data  and have also applied these models
to predict the accuracy of individual forecasts in other contexts  such as company earnings reports.
However  none of these prior works have specifically aimed to call questions throughout their entire
duration.
2
3 Dataset
The research utilizes data from the Good Judgment Open  a platform where questions are posted  and
individuals submit their forecasts. The questions primarily revolve around geopolitics  encompassing
areas such as domestic and international politics  the economy  and social matters. For this study  all
binary questions were collected  along with their associated forecasts  each comprising a prediction
and a justification. In total  the dataset contains 441 questions and 96 664 forecasts submitted
over 32 708 days. This dataset significantly expands upon previous research  nearly doubling the
number of forecasts analyzed. Since the objective is to accurately call questions throughout their
entire duration  all forecasts with written justifications are included  regardless of factors such as
justification length or the number of forecasts submitted by a single forecaster. Additionally  this
approach prioritizes privacy  as no information about the individual forecasters is utilized.
Table 1: Analysis of the questions from our dataset. Most questions are relatively long  contain two
or more named entities  and are open for over one month.
Metric Min Q1 Q2 (Median) Q3 Max Mean
# tokens 8 16 20 28 48 21.94
# entities 0 2 3 5 11 3.47
# verbs 0 2 2 3 6 2.26
# days open 2 24 59 98 475 74.16
Table 1 provides a basic analysis of the questions in the dataset. The majority of questions are
relatively lengthy  containing more than 16 tokens and multiple named entities  with geopolitical
person  and date entities being the most frequent. In terms of duration  half of the questions remain
open for nearly two months  and 75% are open for more than three weeks.
An examination of the topics covered by the questions using Latent Dirichlet Allocation (LDA)
reveals three primary themes: elections (including terms like 'voting ' 'winners ' and 'candidate')
government actions (including terms like 'negotiations ' 'announcements ' 'meetings ' and 'passing
(a law)')  and wars and violent crimes (including terms like 'groups ' 'killing ' 'civilian (casualties) '
and 'arms'). Although not explicitly represented in the LDA topics  the questions address both
domestic and international events within these broad themes.
Table 2: Analysis of the 96 664 written justifications submitted by forecasters in our dataset. The
readability scores indicate that most justifications are easily understood by high school students (11th
or 12th grade)  although a substantial amount (>25%) require a college education (Flesch under 50 or
Dale-Chall over 9.0).
Min Q1 Q2 Q3 Max
#sentences 1 1 1 3 56
#tokens 1 10 23 47 1295
#entities 0 0 2 4 154
#verbs 0 1 3 6 174
#adverbs 0 0 1 3 63
#adjectives 0 0 2 4 91
#negation 0 0 1 3 69
Sentiment -2.54 0 0 0.20 6.50
Readability
Flesch -49.68 50.33 65.76 80.62 121.22
Dale-Chall 0.05 6.72 7.95 9.20 19.77
Table 2 presents a fundamental analysis of the 96 664 forecast justifications in the dataset. The median
length is relatively short  consisting of one sentence and 23 tokens. Justifications mention named
entities less frequently than the questions themselves. Interestingly  half of the justifications contain
at least one negation  and 25% include three or more. This suggests that forecasters sometimes base
their predictions on events that might not occur or have not yet occurred. The sentiment polarity of
3
the justifications is generally neutral. In terms of readability  both the Flesch and Dale-Chall scores
suggest that approximately a quarter of the justifications require a college-level education for full
comprehension.
Regarding verbs and nouns  an analysis using WordNet lexical files reveals that the most common
verb classes are 'change' (e.g.  'happen ' 'remain ' 'increase')  'social' (e.g.  'vote ' 'support '
'help')  'cognition' (e.g.  'think ' 'believe ' 'know')  and 'motion' (e.g.  'go ' 'come ' 'leave').
The most frequent noun classes are 'act' (e.g.  'election ' 'support ' 'deal')  'communication' (e.g.
'questions ' 'forecast ' 'news')  'cognition' (e.g.  'point ' 'issue ' 'possibility')  and 'group' (e.g.
'government ' 'people ' 'party').
4 Experiments and","accuracy throughout most of a question’s duration  with the exception of its final
phase. Furthermore  the study analyzes the attributes that differentiate reliable and
unreliable justifications.
1","Experiments are conducted to address the challenge of accurately calling a question throughout
its duration. The input consists of the question itself and the associated forecasts (predictions and
justifications)  while the output is an aggregated answer to the question derived from all forecasts.
The number of instances corresponds to the total number of days all questions were open. Both
simple baselines and a neural network are employed  considering both (a) daily forecasts and (b)
active forecasts submitted up to ten days prior.
The questions are divided into training  validation  and test subsets. Subsequently  all forecasts
submitted throughout the duration of each question are assigned to their respective subsets. It’s
important to note that randomly splitting the forecasts would be an inappropriate approach. This is
because forecasts for the same question submitted on different days would be distributed across the
training  validation  and test subsets  leading to data leakage and inaccurate performance evaluation.
4.1 Baselines
Two unsupervised baselines are considered. The 'majority vote' baseline determines the answer to a
question based on the most frequent prediction among the forecasts. The 'weighted vote' baseline
on the other hand  assigns weights to the probabilities in the predictions and then aggregates them.
4.2 Neural Network Architecture
A neural network architecture is employed  which consists of three main components: one to generate
a representation of the question  another to generate a representation of each forecast  and an LSTM
to process the sequence of forecasts and ultimately call the question.
The representation of a question is obtained using BERT  followed by a fully connected layer with 256
neurons  ReLU activation  and dropout. The representation of a forecast is created by concatenating
three elements: (a) a binary flag indicating whether the forecast was submitted on the day the question
is being called or on a previous day  (b) the prediction itself (a numerical value between 0.0 and 1.0)
and (c) a representation of the justification. The representation of the justification is also obtained
using BERT  followed by a fully connected layer with 256 neurons  ReLU activation  and dropout.
The LSTM has a hidden state with a dimensionality of 256 and processes the sequence of forecasts
as its input. During the tuning process  it was discovered that providing the representation of the
question alongside each forecast is more effective than processing forecasts independently of the
question. Consequently  the representation of the question is concatenated with the representation of
each forecast before being fed into the LSTM. Finally  the last hidden state of the LSTM is connected
to a fully connected layer with a single neuron and sigmoid activation to produce the final prediction
for the question.
4.3 Architecture Ablation
Experiments are carried out with the complete neural architecture  as described above  as well as
with variations where certain components are disabled. Specifically  the representation of a forecast
is manipulated by incorporating different combinations of information:
4
* Only the prediction. * The prediction and the representation of the question. * The prediction and
the representation of the justification. * The prediction  the representation of the question  and the
representation of the justification.
4.4 Quantitative Results
The evaluation metric used is accuracy  which represents the average percentage of days a model
correctly calls a question throughout its duration. Results are reported for all days combined  as well
as for each of the four quartiles of the question’s duration.
Table 3: Results with the test questions (Accuracy: average percentage of days a model predicts a
question correctly). Results are provided for all days a question was open and for four quartiles (Q1:
first 25% of days  Q2: 25-50%  Q3: 50-75%  and Q4: last 25% of days).
Days When the Question Was Open
Model All Days Q1 Q2 Q3 Q4
Using Daily Forecasts Only
Baselines
Majority V ote (predictions) 71.89 64.59 66.59 73.26 82.22
Weighted V ote (predictions) 73.79 67.79 68.71 74.16 83.61
Neural Network Variants
Predictions Only 77.96 77.62 77.93 78.23 78.61
Predictions + Question 77.61 75.44 76.77 78.05 81.56
Predictions + Justifications 80.23 77.87 78.65 79.26 84.67
Predictions + Question + Justifications 79.96 78.65 78.11 80.29 83.28
Using Active Forecasts
Majority V ote (predictions) 77.27 68.83 73.92 77.98 87.44
Weighted V ote (predictions) 77.97 72.04 72.17 78.53 88.22
Predictions Only 78.81 77.31 78.04 78.53 81.11
Predictions + Question 79.35 76.05 78.53 79.56 82.94
Predictions + Justifications 80.84 77.86 79.07 79.74 86.17
Predictions + Question + Justifications 81.27 78.71 79.81 81.56 84.67
Despite their relative simplicity  the baseline methods achieve commendable results  demonstrating
that aggregating forecaster predictions without considering the question or justifications is a viable
strategy. However  the full neural network achieves significantly improved results.
**Using Daily or Active Forecasts** Incorporating active forecasts  rather than solely relying on
forecasts submitted on the day the question is called  proves advantageous for both baselines and all
neural network configurations  except for the one using only predictions and justifications.
**Encoding Questions and Justifications** The neural network that only utilizes the prediction
to represent a forecast surpasses both baseline methods. Notably  integrating the question  the
justification  or both into the forecast representation yields further improvements. These results
indicate that incorporating the question and forecaster-provided justifications into the model enhances
the accuracy of question calling.
**Calling Questions Throughout Their Life** When examining the results across the four quartiles of
a question’s duration  it’s observed that while using active forecasts is beneficial across all quartiles
for both baselines and all network configurations  the neural networks surprisingly outperform the
baselines only in the first three quartiles. In the last quartile  the neural networks perform significantly
worse than the baselines. This suggests that while modeling questions and justifications is generally
helpful  it becomes detrimental toward the end of a question’s life. This phenomenon can be attributed
to the increasing wisdom of the crowd as more evidence becomes available and more forecasters
contribute  making their aggregated predictions more accurate.
5
Table 4: Results with the test questions  categorized by question difficulty as determined by the best
baseline model. The table presents the accuracy (average percentage of days a question is predicted
correctly) for all questions and for each quartile of difficulty: Q1 (easiest 25%)  Q2 (25-50%)  Q3
(50-75%)  and Q4 (hardest 25%).
Question Difficulty (Based on Best Baseline)
All Q1 Q2 Q3 Q4
Weighted V ote Baseline (Predictions) 77.97 99.40 99.55 86.01 29.30
Neural Network with Components...
Predictions + Question 79.35 94.58 88.01 78.04 58.73
Predictions + Justifications 80.84 95.71 93.18 79.99 57.05
Predictions + Question + Justifications 81.27 94.17 90.11 78.67 64.41
**Calling Questions Based on Their Difficulty** The analysis is further refined by examining
results based on question difficulty  determined by the number of days the best-performing baseline
incorrectly calls the question. This helps to understand which questions benefit most from the neural
networks that incorporate questions and justifications. However  it’s important to note that calculating
question difficulty during the question’s active period is not feasible  making these experiments
unrealistic before the question closes and the correct answer is revealed.
Table 4 presents the results for selected models based on question difficulty. The weighted vote
baseline demonstrates superior performance for 75
5 Qualitative Analysis
This section provides insights into the factors that make questions more difficult to forecast and
examines the characteristics of justifications associated with incorrect and correct predictions.
**Questions** An analysis of the 88 questions in the test set revealed that questions called incorrectly
on at least one day by the best model tend to have a shorter duration (69.4 days vs. 81.7 days) and a
higher number of active forecasts per day (31.0 vs. 26.7). This suggests that the model’s errors align
with the questions that forecasters also find challenging.
**Justifications** A manual review of 400 justifications (200 associated with incorrect predictions
and 200 with correct predictions) was conducted  focusing on those submitted on days when the best
model made an incorrect prediction. The following observations were made:
* A higher percentage of incorrect predictions (78%) were accompanied by short justifications
(fewer than 20 tokens)  compared to 65% for correct predictions. This supports the idea that longer
user-generated text often indicates higher quality. * References to previous forecasts (either by the
same or other forecasters  or the current crowd’s forecast) were more common in justifications for
incorrect predictions (31.5%) than for correct predictions (16%). * A lack of a logical argument
was prevalent in the justifications  regardless of the prediction’s accuracy. However  it was more
frequent in justifications for incorrect predictions (62.5%) than for correct predictions (47.5%). *
Surprisingly  justifications with generic arguments did not clearly differentiate between incorrect and
correct predictions (16.0% vs. 14.5%). * Poor grammar and spelling or the use of non-English were
infrequent but more common in justifications for incorrect predictions (24.5%) compared to correct
predictions (14.5%).
6","Forecasting involves predicting future events  a capability highly valued by both governments and
industries as it enables them to anticipate and address potential challenges. This study focuses on
questions spanning the political  economic  and social domains  utilizing forecasts submitted by a
crowd of individuals without specialized training. Each forecast comprises a prediction and a natural
language justification.
6
The research demonstrates that aggregating the weighted predictions of forecasters is a solid baseline
for calling a question throughout its duration. However  models that incorporate both the question
and the justifications achieve significantly better results  particularly during the first three quartiles of
a question’s life. Importantly  the models developed in this study do not profile individual forecasters
or utilize any information about their identities. This work lays the groundwork for evaluating the
credibility of anonymous forecasts  enabling the development of robust aggregation strategies that do
not require tracking individual forecasters.
7
Thisstudydemonstratesthatincorporatingthewrittenexplanationsprovidedby
individualswhenmakingpredictionsenhancestheaccuracyofaggregatedcrowd-
accuracythroughoutmostofaquestion’sduration withtheexceptionofitsfinal
phase. Furthermore thestudyanalyzestheattributesthatdifferentiatereliableand
unreliablejustifications.
Theconceptofthe'wisdomofthecrowd'positsthatcombininginformationfromnumerousnon-
expertindividualscanproduceanswersthatareasaccurateas orevenmoreaccuratethan those
providedbyasingleexpert. Aclassicexampleofthisconceptistheobservationthatthemedian
estimateofanox’sweightfromalargegroupoffairattendeeswasremarkablyclosetotheactual
demonstrateinstanceswherecrowdsbehavedirrationally andevenaworldchesschampionwasable
todefeatthecombinedmovesofacrowd.
Inthecurrentera theadvantagesofcollectiveintelligencearewidelyutilized.Forexample Wikipedia
reliesonthecontributionsofvolunteers andcommunity-drivenquestion-answeringplatformshave
largegroups itisimportanttodeterminewhethertheindividualinputsweremadeindependently. If
not factorslikegrouppsychologyandtheinfluenceofpersuasiveargumentscanskewindividual
judgments thusnegatingthepositiveeffectsofcrowdwisdom.
domains. Eachforecastincludesaprediction estimatingtheprobabilityofaparticularevent and
predictionscanhavejustificationsofvaryingstrength which inturn affectstheperceivedreliability
explanationmayappeartorelyheavilyontheprevailingopinionofthecrowdandmightbeconsidered
weakerthanajustificationthatpresentsspecific verifiablefactsfromexternalresources.
Toclarifytheterminologyused: a'question'isdefinedasastatementthatseeksinformation(e.g.
'Willnewlegislationbeimplementedbeforeacertaindate?'). Questionshaveadefinedstartand
enddate  andtheperiodbetweenthesedatesconstitutesthe'life'ofthequestion. 'Forecasters'
areindividualswhoprovidea'forecast 'whichconsistsofa'prediction'anda'justification.'The
predictionisanumericalrepresentationofthelikelihoodofaneventoccurring. Thejustification
isthetextprovidedbytheforecastertosupporttheirprediction. Thecentralproblemaddressedin
thisworkistermed'callingaquestion 'whichreferstotheprocessofdeterminingafinalprediction
throughouttheirlife: consideringforecastssubmittedonthegivenday('daily')andconsideringthe
lastforecastsubmittedbyeachforecaster('active').
Inspiredbypriorresearchonrecognizingandfosteringskilledforecasters andanalyzingwritten
justificationstoassessthequalityofindividualorcollectiveforecasts  thispaperinvestigatesthe
automatedcallingofquestionsthroughouttheirdurationbasedontheforecastsavailableeachday.
Theprimarycontributionsareempiricalfindingsthataddressthefollowingresearchquestions:
*Whenmakingapredictiononaspecificday isitadvantageoustoincludeforecastsfromprevious
andthewrittenjustificationsprovidedwiththeforecasts? (Yes)*Isiteasiertomakeanaccurate
predictiontowardtheendofaquestion’sduration? (Yes)*Arewrittenjustificationsmorevaluable
whenthecrowd’spredictionsarelessaccurate? (Yes)
Inaddition thisresearchpresentsanexaminationofthejustificationsassociatedwithbothaccurate
andinaccurateforecasts. Thisanalysisaimstoidentifythefeaturesthatcontributetoajustification
beingmoreorlesscredible.
2 RelatedWork
Thelanguageemployedbyindividualsisindicativeofvariouscharacteristics. Priorresearchincludes
bothpredictivemodels(usinglanguagesamplestopredictattributesabouttheauthor)andmodels
thatprovidevaluableinsights(usinglanguagesamplesandauthorattributestoidentifydifferentiating
linguisticfeatures). Previousstudieshaveexaminedfactorssuchasgenderandage politicalideology
healthoutcomes andpersonalitytraits. Inthispaper modelsareconstructedtopredictoutcomes
basedoncrowd-sourcedforecastswithoutknowledgeofindividualforecasters’identities.
Previousresearchhasalsoexploredhowlanguageusevariesdependingontherelationshipsbetween
individuals. Forinstance studieshaveanalyzedlanguagepatternsinsocialnetworks onlinecommu-
nities andcorporateemailstounderstandhowindividualsinpositionsofauthoritycommunicate.
Similarly researchershaveexaminedhowlanguageprovidesinsightsintointerpersonalinteractions
andrelationships. Intermsoflanguageformandfunction priorresearchhasinvestigatedpoliteness
researchershaveexaminedtheinfluenceofWikipediaeditorsandstudiedinfluencelevelswithin
onlinecommunities. Persuasionhasalsobeenanalyzedfromacomputationalperspective including
withinthecontextofdialoguesystems. Theworkpresentedherecomplementsthesepreviousstudies.
Thegoalistoidentifycrediblejustificationstoimprovetheaggregationofcrowdsourcedforecasts
withoutexplicitlytargetinganyoftheaforementionedcharacteristics.
Withinthefieldofcomputationallinguistics thetaskmostcloselyrelatedtothisresearchisargumen-
tation. Astrongjustificationforaforecastcanbeconsideredawell-reasonedsupportingargument.
backing rebuttals andrefutations aswellasminingargumentsthatsupportoropposeaparticular
establishedargumentationframeworks eventhoughsuchjustificationsarevaluableforaggregating
Finally severalstudieshavefocusedonforecastingusingdatasetssimilaroridenticaltotheoneused
inthisresearch. Fromapsychologicalperspective researchershaveexploredstrategiesforenhancing
forecastingaccuracy suchasutilizingtop-performingforecasters(oftencalled'superforecasters')
andhaveanalyzedthetraitsthatcontributetotheirsuccess.Thesestudiesaimtoidentifyandcultivate
superforecastersbutdonotincorporatethewrittenjustificationsaccompanyingforecasts. Incontrast
forecastersthemselves. Withinthefieldofcomputationallinguistics researchershaveevaluatedthe
languageusedinhigh-qualityjustifications focusingonaspectslikerating benefit andinfluence.
Otherresearchershavedevelopedmodelstopredictforecasterskillusingthetextualjustifications
fromspecificdatasets suchastheGoodJudgmentOpendata andhavealsoappliedthesemodels
topredicttheaccuracyofindividualforecastsinothercontexts suchascompanyearningsreports.
However noneofthesepriorworkshavespecificallyaimedtocallquestionsthroughouttheirentire
TheresearchutilizesdatafromtheGoodJudgmentOpen aplatformwherequestionsareposted and
individualssubmittheirforecasts. Thequestionsprimarilyrevolvearoundgeopolitics encompassing
areassuchasdomesticandinternationalpolitics theeconomy andsocialmatters. Forthisstudy all
binaryquestionswerecollected alongwiththeirassociatedforecasts eachcomprisingaprediction
over32 708days. Thisdatasetsignificantlyexpandsuponpreviousresearch nearlydoublingthe
numberofforecastsanalyzed. Sincetheobjectiveistoaccuratelycallquestionsthroughouttheir
entireduration allforecastswithwrittenjustificationsareincluded regardlessoffactorssuchas
justificationlengthorthenumberofforecastssubmittedbyasingleforecaster. Additionally this
approachprioritizesprivacy asnoinformationabouttheindividualforecastersisutilized.
Table1: Analysisofthequestionsfromourdataset. Mostquestionsarerelativelylong containtwo
ormorenamedentities andareopenforoveronemonth.
Metric Min Q1 Q2(Median) Q3 Max Mean
#tokens 8 16 20 28 48 21.94
#entities 0 2 3 5 11 3.47
#verbs 0 2 2 3 6 2.26
#daysopen 2 24 59 98 475 74.16
relativelylengthy containingmorethan16tokensandmultiplenamedentities withgeopolitical
person anddateentitiesbeingthemostfrequent. Intermsofduration halfofthequestionsremain
openfornearlytwomonths and75%areopenformorethanthreeweeks.
revealsthreeprimarythemes: elections(includingtermslike'voting ''winners 'and'candidate')
governmentactions(includingtermslike'negotiations ''announcements ''meetings 'and'passing
(alaw)') andwarsandviolentcrimes(includingtermslike'groups ''killing ''civilian(casualties) '
domesticandinternationaleventswithinthesebroadthemes.
Table2: Analysisofthe96 664writtenjustificationssubmittedbyforecastersinourdataset. The
readabilityscoresindicatethatmostjustificationsareeasilyunderstoodbyhighschoolstudents(11th
or12thgrade) althoughasubstantialamount(>25%)requireacollegeeducation(Fleschunder50or
Dale-Challover9.0).
Table2presentsafundamentalanalysisofthe96 664forecastjustificationsinthedataset.Themedian
lengthisrelativelyshort consistingofonesentenceand23tokens. Justificationsmentionnamed
entitieslessfrequentlythanthequestionsthemselves. Interestingly halfofthejustificationscontain
atleastonenegation and25%includethreeormore. Thissuggeststhatforecasterssometimesbase
theirpredictionsoneventsthatmightnotoccurorhavenotyetoccurred. Thesentimentpolarityof
thejustificationsisgenerallyneutral. Intermsofreadability boththeFleschandDale-Challscores
suggestthatapproximatelyaquarterofthejustificationsrequireacollege-leveleducationforfull
Regardingverbsandnouns ananalysisusingWordNetlexicalfilesrevealsthatthemostcommon
verbclassesare'change'(e.g. 'happen ''remain ''increase') 'social'(e.g. 'vote ''support '
'help') 'cognition'(e.g. 'think ''believe ''know') and'motion'(e.g. 'go ''come ''leave').
Themostfrequentnounclassesare'act'(e.g. 'election ''support ''deal') 'communication'(e.g.
'questions ''forecast ''news') 'cognition'(e.g. 'point ''issue ''possibility') and'group'(e.g.
'government ''people ''party').
4 ExperimentsandResults
itsduration. Theinputconsistsofthequestionitselfandtheassociatedforecasts(predictionsand
justifications) whiletheoutputisanaggregatedanswertothequestionderivedfromallforecasts.
simplebaselinesandaneuralnetworkareemployed consideringboth(a)dailyforecastsand(b)
activeforecastssubmitteduptotendaysprior.
importanttonotethatrandomlysplittingtheforecastswouldbeaninappropriateapproach. Thisis
becauseforecastsforthesamequestionsubmittedondifferentdayswouldbedistributedacrossthe
training validation andtestsubsets leadingtodataleakageandinaccurateperformanceevaluation.
Twounsupervisedbaselinesareconsidered. The'majorityvote'baselinedeterminestheanswertoa
questionbasedonthemostfrequentpredictionamongtheforecasts. The'weightedvote'baseline
ontheotherhand assignsweightstotheprobabilitiesinthepredictionsandthenaggregatesthem.
4.2 NeuralNetworkArchitecture
Aneuralnetworkarchitectureisemployed whichconsistsofthreemaincomponents:onetogenerate
arepresentationofthequestion anothertogeneratearepresentationofeachforecast andanLSTM
toprocessthesequenceofforecastsandultimatelycallthequestion.
TherepresentationofaquestionisobtainedusingBERT followedbyafullyconnectedlayerwith256
neurons ReLUactivation anddropout. Therepresentationofaforecastiscreatedbyconcatenating
threeelements:(a)abinaryflagindicatingwhethertheforecastwassubmittedonthedaythequestion
isbeingcalledoronapreviousday (b)thepredictionitself(anumericalvaluebetween0.0and1.0)
and(c)arepresentationofthejustification. Therepresentationofthejustificationisalsoobtained
usingBERT followedbyafullyconnectedlayerwith256neurons ReLUactivation anddropout.
TheLSTMhasahiddenstatewithadimensionalityof256andprocessesthesequenceofforecasts
asitsinput. Duringthetuningprocess itwasdiscoveredthatprovidingtherepresentationofthe
questionalongsideeachforecastismoreeffectivethanprocessingforecastsindependentlyofthe
question. Consequently therepresentationofthequestionisconcatenatedwiththerepresentationof
eachforecastbeforebeingfedintotheLSTM.Finally thelasthiddenstateoftheLSTMisconnected
toafullyconnectedlayerwithasingleneuronandsigmoidactivationtoproducethefinalprediction
forthequestion.
4.3 ArchitectureAblation
Experimentsarecarriedoutwiththecompleteneuralarchitecture asdescribedabove aswellas
withvariationswherecertaincomponentsaredisabled. Specifically therepresentationofaforecast
ismanipulatedbyincorporatingdifferentcombinationsofinformation:
*Onlytheprediction. *Thepredictionandtherepresentationofthequestion. *Thepredictionand
therepresentationofthejustification. *Theprediction therepresentationofthequestion andthe
representationofthejustification.
4.4 QuantitativeResults
Theevaluationmetricusedisaccuracy whichrepresentstheaveragepercentageofdaysamodel
correctlycallsaquestionthroughoutitsduration. Resultsarereportedforalldayscombined aswell
asforeachofthefourquartilesofthequestion’sduration.
Table3: Resultswiththetestquestions(Accuracy: averagepercentageofdaysamodelpredictsa
questioncorrectly). Resultsareprovidedforalldaysaquestionwasopenandforfourquartiles(Q1:
first25%ofdays Q2: 25-50% Q3: 50-75% andQ4: last25%ofdays).
DaysWhentheQuestionWasOpen
Model AllDays Q1 Q2 Q3 Q4
UsingDailyForecastsOnly
MajorityVote(predictions) 71.89 64.59 66.59 73.26 82.22
WeightedVote(predictions) 73.79 67.79 68.71 74.16 83.61
NeuralNetworkVariants
PredictionsOnly 77.96 77.62 77.93 78.23 78.61
Predictions+Question 77.61 75.44 76.77 78.05 81.56
Predictions+Justifications 80.23 77.87 78.65 79.26 84.67
Predictions+Question+Justifications 79.96 78.65 78.11 80.29 83.28
UsingActiveForecasts
MajorityVote(predictions) 77.27 68.83 73.92 77.98 87.44
WeightedVote(predictions) 77.97 72.04 72.17 78.53 88.22
PredictionsOnly 78.81 77.31 78.04 78.53 81.11
Predictions+Question 79.35 76.05 78.53 79.56 82.94
Predictions+Justifications 80.84 77.86 79.07 79.74 86.17
Predictions+Question+Justifications 81.27 78.71 79.81 81.56 84.67
Despitetheirrelativesimplicity thebaselinemethodsachievecommendableresults demonstrating
thataggregatingforecasterpredictionswithoutconsideringthequestionorjustificationsisaviable
strategy. However thefullneuralnetworkachievessignificantlyimprovedresults.
**UsingDailyorActiveForecasts**Incorporatingactiveforecasts ratherthansolelyrelyingon
forecastssubmittedonthedaythequestioniscalled provesadvantageousforbothbaselinesandall
neuralnetworkconfigurations exceptfortheoneusingonlypredictionsandjustifications.
indicatethatincorporatingthequestionandforecaster-providedjustificationsintothemodelenhances
theaccuracyofquestioncalling.
**CallingQuestionsThroughoutTheirLife**Whenexaminingtheresultsacrossthefourquartilesof
aquestion’sduration it’sobservedthatwhileusingactiveforecastsisbeneficialacrossallquartiles
forbothbaselinesandallnetworkconfigurations theneuralnetworkssurprisinglyoutperformthe
baselinesonlyinthefirstthreequartiles. Inthelastquartile theneuralnetworksperformsignificantly
worsethanthebaselines. Thissuggeststhatwhilemodelingquestionsandjustificationsisgenerally
helpful itbecomesdetrimentaltowardtheendofaquestion’slife.Thisphenomenoncanbeattributed
totheincreasingwisdomofthecrowdasmoreevidencebecomesavailableandmoreforecasters
contribute makingtheiraggregatedpredictionsmoreaccurate.
Table4: Resultswiththetestquestions categorizedbyquestiondifficultyasdeterminedbythebest
baselinemodel. Thetablepresentstheaccuracy(averagepercentageofdaysaquestionispredicted
correctly)forallquestionsandforeachquartileofdifficulty: Q1(easiest25%) Q2(25-50%) Q3
(50-75%) andQ4(hardest25%).
QuestionDifficulty(BasedonBestBaseline)
WeightedVoteBaseline(Predictions) 77.97 99.40 99.55 86.01 29.30
NeuralNetworkwithComponents...
Predictions+Question 79.35 94.58 88.01 78.04 58.73
Predictions+Justifications 80.84 95.71 93.18 79.99 57.05
Predictions+Question+Justifications 81.27 94.17 90.11 78.67 64.41
resultsbasedonquestiondifficulty determinedbythenumberofdaysthebest-performingbaseline
incorrectlycallsthequestion. Thishelpstounderstandwhichquestionsbenefitmostfromtheneural
networksthatincorporatequestionsandjustifications. However it’simportanttonotethatcalculating
unrealisticbeforethequestionclosesandthecorrectanswerisrevealed.
baselinedemonstratessuperiorperformancefor75
5 QualitativeAnalysis
examinesthecharacteristicsofjustificationsassociatedwithincorrectandcorrectpredictions.
**Questions**Ananalysisofthe88questionsinthetestsetrevealedthatquestionscalledincorrectly
onatleastonedaybythebestmodeltendtohaveashorterduration(69.4daysvs. 81.7days)anda
highernumberofactiveforecastsperday(31.0vs. 26.7). Thissuggeststhatthemodel’serrorsalign
withthequestionsthatforecastersalsofindchallenging.
**Justifications**Amanualreviewof400justifications(200associatedwithincorrectpredictions
and200withcorrectpredictions)wasconducted focusingonthosesubmittedondayswhenthebest
modelmadeanincorrectprediction. Thefollowingobservationsweremade:
(fewerthan20tokens) comparedto65%forcorrectpredictions. Thissupportstheideathatlonger
user-generatedtextoftenindicateshigherquality. *Referencestopreviousforecasts(eitherbythe
sameorotherforecasters orthecurrentcrowd’sforecast)weremorecommoninjustificationsfor
incorrectpredictions(31.5%)thanforcorrectpredictions(16%). *Alackofalogicalargument
wasprevalentinthejustifications  regardlessoftheprediction’saccuracy. However  itwasmore
frequentinjustificationsforincorrectpredictions(62.5%)thanforcorrectpredictions(47.5%). *
Surprisingly justificationswithgenericargumentsdidnotclearlydifferentiatebetweenincorrectand
correctpredictions(16.0%vs. 14.5%). *Poorgrammarandspellingortheuseofnon-Englishwere
infrequentbutmorecommoninjustificationsforincorrectpredictions(24.5%)comparedtocorrect
predictions(14.5%).
Forecastinginvolvespredictingfutureevents acapabilityhighlyvaluedbybothgovernmentsand
industriesasitenablesthemtoanticipateandaddresspotentialchallenges. Thisstudyfocuseson
questionsspanningthepolitical economic andsocialdomains utilizingforecastssubmittedbya
crowdofindividualswithoutspecializedtraining. Eachforecastcomprisesapredictionandanatural
languagejustification.
Theresearchdemonstratesthataggregatingtheweightedpredictionsofforecastersisasolidbaseline
forcallingaquestionthroughoutitsduration. However modelsthatincorporateboththequestion
andthejustificationsachievesignificantlybetterresults particularlyduringthefirstthreequartilesof
aquestion’slife. Importantly themodelsdevelopedinthisstudydonotprofileindividualforecasters
orutilizeanyinformationabouttheiridentities. Thisworklaysthegroundworkforevaluatingthe
credibilityofanonymousforecasts enablingthedevelopmentofrobustaggregationstrategiesthatdo
notrequiretrackingindividualforecasters.
1
Introduction
Related Work
Dataset
Metric
Min
Q1
Q2 (Median)
Q3
Max
Mean
# tokens
8
16
20
28
48
21.94
# entities
0
11
3.47
# verbs
2.26
# days open
24
59
98
475
74.16
Q2
#sentences
56
#tokens
10
23
47
1295
#entities
154
#verbs
174
#adverbs
63
#adjectives
91
#negation
69
Sentiment
-2.54
0.20
6.50
Flesch
-49.68
50.33
65.76
80.62
121.22
Dale-Chall
0.05
6.72
7.95
9.20
19.77
Experiments and Results
4.1
4.2
Neural Network Architecture
4.3
Architecture Ablation
4.4
Quantitative Results
Model
All Days
Q4
Majority Vote (predictions)
71.89
64.59
66.59
73.26
82.22
Weighted Vote (predictions)
73.79
67.79
68.71
83.61
Predictions Only
77.96
77.62
77.93
78.23
78.61
Predictions + Question
77.61
75.44
76.77
78.05
81.56
Predictions + Justifications
80.23
77.87
78.65
79.26
84.67
Predictions + Question + Justifications
79.96
78.11
80.29
83.28
77.27
68.83
73.92
77.98
87.44
77.97
72.04
72.17
78.53
88.22
78.81
77.31
78.04
81.11
79.35
76.05
79.56
82.94
80.84
77.86
79.07
79.74
86.17
81.27
78.71
79.81
All
Weighted Vote Baseline (Predictions)
99.40
99.55
86.01
29.30
94.58
88.01
58.73
95.71
93.18
79.99
57.05
94.17
90.11
78.67
64.41
Qualitative Analysis
Conclusions"
R026,0,,"is a nitary factor of an i.i.d. process. With an additional assumption on the geometry of the graph
namely that no two balls with dierent centers are identical  we further show that the i.i.d. process
may be taken to have entropy arbitrarily close to that of the nitely dependent process. As an
application  we give an armative answer to a question of Holroyd [9].
1.Introduction
Consider a random process X= (Xv)v2Vliving on the vertex set Vof an innite graph G. The
processXis said to be nitely dependent if its restrictions to sets which are suciently separated (at
least some xed distance apart) are independent. A trivial example of a nitely dependent process
is a process Y= (Yv)v2Vin which all random variables are independent. A natural question is then
how close is a nitely dependent process to such an independent process? Before addressing this
question  we rst observe that \\local functions' of an independent process Yare always nitely
dependent. That is  if Xis obtained from Yby computing each Xvas a function only of the random
variablesYufor whichuis at a uniformly bounded distance from v  thenXis nitely dependent.
Suppose now that Gis transitive and henceforth restrict attention to processes Xwhich are
invariant under all automorphisms of G(or under a transitive subgroup of automorphisms). In
particular  the independent process Yconsidered above must now be an i.i.d. process { that is  in
addition to being independent  the fYvgvare also identically distributed. If Xis obtained from Yby
applying the same local function at each vertex v(i.e.  the function applied at uis the composition
of the function applied at vwith any automorphism taking utov)  thenXis said to be a block
factor ofY. Thus  block factors of i.i.d. processes provide a recipe for constructing invariant nitely
dependent processes.
It was a long-standing open problem [15 16] to determine whether block factors of i.i.d. processes
are the only (invariant) nitely dependent processes on Z  until nally an example was given by
Burton{Goulet{Meester [5] of a 1-dependent process which is not a block factor of any i.i.d. process.
Recently  Holroyd and Liggett [13] showed that proper colorings distinguish between block factors
of i.i.d. processes and nitely dependent processes { no proper coloring of Zis a block factor of an
i.i.d. process  but nitely dependent proper colorings exist.
Thus  it is not true that every nitely dependent process is a block factor of an i.i.d. process.
In other words  given a nitely dependent process X  one cannot in general hope to nd an i.i.d.
processYand an invariant rule for computing XfromY  which allows to determine the value of Xv
by looking at Yon a xed-size window around v. The goal of this paper is to show the \\next best
thing' { namely  that it is possible to determine Xvby looking at Yon a variable-sized window
aroundv  where the size of the window  though always nite  may vary according to the input Y.
We say that Xis anitary factor ofYif there is an invariant rule which allows to compute the
value ofXat any vertex vby only looking at variables Yufor whichuis within a certain nite
but random  distance from v(formal denitions are given below). Thus  a block factor is a nitary
factor in which the required distance is not only nite  but is determistically bounded by some
constant. The main contribution of this paper is to prove that every nitely dependent process is a
Date : January 22  2020.
1arXiv:1901.00123v2  [math.PR]  19 Jan 2020
2 YINON SPINKA
nitary factor of an i.i.d. process. This result holds on any amenable graph G. When it is further
assumed that no two balls in Gwith dierent centers are identical  it is also possible to control the
entropy of the i.i.d. process involved  and the result becomes that every nitely dependent process
is a nitary factor of an i.i.d. process with only slightly larger entropy.
1.1.Denitions and main result. LetVbe a countable set  let Gbe a graph on vertex set V
and let   be a group acting on V. A random eld (or random process) onGis a collection of
random variables X= (Xv)v2Vindexed by the vertices of Gand dened on a common probability
space. We say that Xis  -invariant if its distribution is not aected by the action of    i.e.  if
(Xv)v2Vhas the same distribution as Xfor any2 . We say that Xisk-dependent if (Xu)u2U
and (Xv)v2Vare independent for any two sets U;VVsuch that dist( u;v)>kfor allu2Uand
v2V. We say that Xisnitely dependent if it isk-dependent for some nite k.
Suppose now that Gis a transitive locally nite graph and that   is a subgroup of the automor-
phism group of G. LetSandTbe two measurable spaces  and let X= (Xv)v2VandY= (Yv)v2V
beS-valued and T-valued  -invariant random elds. A coding fromYtoXis a measurable func-
tion':TV!SVthat is   -equivariant   i.e.  commutes with the action of every element of    and
satises that '(Y) andXare identical in distribution. Such a coding is also called a factor map
fromYtoX  and when such a coding exists  we say that Xis a   -factor ofY.
Suppose now that SandTare countable. Let 02Vbe a distinguished vertex. The coding radius
of'at a pointy2TV  denoted by R(y)  is the minimal integer r0 such that '(y0)0='(y)0
for ally02TVwhich coincide with yon the ball of radius raround 0in the graph-distance  i.e.
y0
v=yvfor allv2Vsuch that dist( v;0)r. It may happen that no such rexists  in which case
R(y) =1. Thus  associated to a coding is a random variable R=R(Y) which describes the coding
radius. While Swill always be at most countable  we will allow Tto be a larger space  in which
case the coding radius may be similarly dened1. A coding is called nitary ifRis almost surely
nite. When there exists a nitary coding from YtoX  we say that Xis anitary  -factor ofY.
A graph is said to be amenable if infj@Vj=jVj= 0  where the inmum is over all nite non-empty
subsetsVofV  and where @Vdenotes the edge-boundary ofV.
Theorem 1.1. LetGbe a transitive amenable graph and let  be a transitive group of automor-
phisms ofG. Then any nitely dependent  -invariant random eld on Gis a nitary  -factor of
an i.i.d. process.
With a minor additional constraint on the geometry of the graph G  we can further control the
entropy of the i.i.d. process used in the coding (see Section 3.1 for the denition of entropy). The
condition we require is that
r(u)6= r(v) for any distinct u;v2Vandr0; (1)
where r(u) is the ball of radius raroundu.
Theorem 1.2. LetGbe a transitive amenable graph satisfying (1)  let  be a transitive group of
automorphisms of G  and letXbe a nite-valued nitely dependent  -invariant random eld on
G. Then for any >0there exists an i.i.d. process Ywith entropy h(Y)<h(X) +such thatX
is a nitary  -factor ofY.
Let us make some remarks about how the two theorems compare to one another. In Theorem 1.2
Sis nite (note the assumption that Xis nite-valued) and  in particular  Xhas nite entropy
while in Theorem 1.1  Smay be countable and Xmay have innite entropy. In Theorem 1.2  Y
has nite entropy so that Tis countable  whereas Theorem 1.1 may require a larger space Tfor
1We will only be concerned with spaces Twhich are nite  countable or of the form T1T2  for nite sets
(Ti). In the latter case  the coding radius is the smallest rfor which there exists nsuch that '(y0)0='(y)0for all
y0having the property that y0
v;i=yv;ifor all ( v; i) such that dist( v;0)rand 1 in.
FINITELY DEPENDENT PROCESSES ARE FINITARY 3
the",,"Section 2 for an outline.
The question of whether there exists a stationary nitely dependent process which is not a block
factor of any i.i.d. process was raised by Ibragimov and Linnik [15  16] in 1965. Some progress on
this question was made [1  2] until it was nally resolved in 1993 by Burton{Goulet{Meester [5]
who gave the rst example of a stationary nitely dependent process which is not a block factor
of an i.i.d. process. In fact  they showed such an example in which the nitely dependent process
is a 1-dependent hidden-Markov process with nite energy. Some history about nitely dependent
processes that cannot be written as block factors is given in [13].
Holroyd and Liggett [13] constructed a stationary 1-dependent 4-coloring and a stationary 2-
dependent 3-coloring of Z  neither of which is a block factor of an i.i.d. process (indeed  no coloring
is such [13]). Holroyd [9] subsequently showed that the 1-dependent 4-coloring is a nitary factor
of an i.i.d. process. Regarding the analogous statement for the 2-dependent 3-coloring  Holroyd
writes in [9] that \\one may attempt to apply our method to the 2-dependent 3-coloring  but we will
see that it meets a fundamental obstacle in this case'. Our result shows that either coloring is a
nitary factor of an i.i.d. process (with slightly larger entropy)  answering armatively question (iii)
in [9  Open problems]. In fact  the two colorings are also reection invariant  and hence the nitary
factors may also be taken to commute with reections. Related aspects of these two colorings were
studied in [11].
In a subsequent paper [12]  Holroyd and Liggett constructed  for any q4  a 1-dependent
q-coloring of Zwhich is invariant under translations and reections and is also symmetric under
4 YINON SPINKA
permutations of the colors. It was shown in [10] that each of these colorings is a nitary factor
of an i.i.d. process (with exponential tail on the coding radius). Our result shows that each of
these colorings is a nitary factor of an i.i.d. process  where the factor map commutes with all
automorphisms of Zand the i.i.d. process has entropy only slightly larger than the coloring.
In [13]  Holroyd and Liggett also constructed stationary nitely dependent colorings of Zdwith
d2. More specically  they constructed a stationary 1-dependent 4d-coloring of Zdand a sta-
tionary nitely dependent 4-coloring of Zd. However  unlike the above one-dimensional colorings
these colorings are only translation-invariant and not automorphism-invariant. In fact  it is still
unknown whether there exists a nitely dependent coloring of Zd(d2) which is invariant under
all automorphisms of Zd.
In the same paper [13]  Holroyd and Liggett also investigated the existence of stationary nitely
dependent processes on Zwhich are supported on a given shift of nite type. They showed that
for any reasonably non-degenerate (namely  nonlattice) shift of nite type SonZ  there exists a
stationary nitely dependent process which almost surely belongs to S. It was later shown [10]
that there exists such a process which is also a nitary factor of an i.i.d. process (with exponential
tail on the coding radius).
A block factor is precisely a nitary factor with bounded coding radius. Given a nitary factor
which is not a block factor  it is natural to wonder about the typical value of the coding radius.
As we have mentioned  the 1-dependent 4-coloring of Zfrom [13]  which is not a block factor of
any i.i.d. process  was shown in [9] to be a nitary factor of an i.i.d. process. This nitary factor
was shown to have (at least) power-law tail on the coding radius  thus yielding a perhaps innite
expected coding radius. Holroyd{Hutchcroft{Levy [10] showed that there exist nitely dependent
colorings of Zwhich are nitary factors of i.i.d. processes with exponential tail on the coding radius.
Indeed  they showed that such a k-dependent q-coloring exists when ( k;q) is either (1 ;5)  (2;4) or
(3;3). On the other hand  it is believed [9  10] that when ( k;q) is (1;4) or (2;3)  nok-dependent
q-coloring is a nitary factor of an i.i.d. process with nite expected coding radius. We mention that
optimal tails for the coding radius of colorings of Zd(which are not necessarily nitely dependent)
and shifts of nite type on Zhave been studied in [14].
Our main theorem gives no information about the coding radius beyond its almost-sure nite-
ness. Indeed  in light of the above discussion  it would seem that for an arbitrary nitely dependent
process on Z  there is not much hope to obtain a nitary factor with nite expected coding ra-
dius. Still  some information about the coding radius may be extracted from the proof given here
(see Remark 3). For example  for 1-dependent processes on Z  the nitary factor provided by
Theorem 1.1 has a coding radius Rsatisfying that P(R>r )8=rfor allr. In the particular case
of the 1-dependent 4-coloring of [13]  this improves the power in the power-law bound shown in [9]
(to an optimal power if the prediction above is indeed correct).
To the best of our knowledge  beyond Smorodinsky's result on Z  there do not exist any general","and Theorem 1.2 are new for any amenable graph Gother than Z  and also for G=Zin the
case when   is the full automorphism group of Z. Finally  we mention that the situation for non-
amenable graphs is still poorly understood { for example  on a regular tree (of degree at least
three)  it is not even known whether every automorphism-invariant nitely dependent process is a
(non-nitary) factor of an i.i.d. process [21].
1.3.Acknowledgments. I would like to thank Omer Angel  Nishant Chandgotia  Tom Meyerovitch
and Mathav Murugan for useful discussions. I am especially grateful to Nishant Chandgotia for
suggesting to extend the result from Zdto transitive amenable graphs  and to Omer Angel for
jointly proving Lemma 4.2 with me. I would also like to thank the referees for useful comments.
FINITELY DEPENDENT PROCESSES ARE FINITARY 5
1.4.Notation. Throughout the paper  Gis always assumed to be an innite  transitive  lo-
cally nite  connected graph on a countable vertex set V  and   is a subgroup of the auto-
morphism group of Gthat acts transitively on V. The full automorphism group of Gis de-
noted by Aut( G). The graph distance in Gis denoted by dist( ;). For sets U;VV  we
write dist(U;V) := min u2U;v2Vdist(u;v) and dist( u;V) := dist(fug;V). Forr0  denote
V+r:=fu2V: dist(u;V)rgandV r:=fu2V: dist(u;Vc)> rg. The ball of radius r
aroundvis denoted by  r(v) :=fvg+r. The neighborhood ofVisN(V) :=V+1nVand the
edge-boundary ofVis@V:=ffu;vg2E(G) :v2V;u =2Vg.
All logarithms are taken to be in base 2 and we use the convention that 0 log 0 is 0.
2.Outline of proof
Our goal is to express X  a nitely dependent invariant process  as a nitary factor of an i.i.d.
processY. The construction of the nitary coding involves the use of three sources of randomness:
a random number of random bits located at each vertex  a so-called cell process  and a random
total order on V. The rst of these three will simply be given by an i.i.d. process  denoted Ybits
while the latter two will be obtained as nitary factors of dierent i.i.d. processes  denoted Ycelland
Yord. In turn Ywill be a triplet Y= (Ybits;Ycell;Yord) consisting of three mutually independent
i.i.d. processes.
To illuminate the main ideas behind our construction  we provide a sketch of the proof below
explaining separately three ingredients:
(1)Constructing a nitary coding: The basic and most essential part of the construction
is how to obtain Xas a nitary factor of YwhenYis allowed to have innite entropy (this
is the setting of Theorem 1.1). In this case  Ybits
vandYord
vmay be taken to be uniform
random variables in [0 ;1]  and the total order may be taken to be the one induced by the
usual order on Yord
v.
(2)Controlling the entropy: The second part is how to control the entropy of the Ybits
process  requiring only slightly more entropy than that of X. To postpone dealing with the
issue of controlling the entropy of Yord  we shall assume in this part of the proof outline
that the vertices of Gcan be deterministically ordered in a  -invariant manner so that Yord
may be disregarded entirely (e.g.  it can be taken to be a constant process). For example
the lexicographical order is such an ordering when Gis the graph Zdand   is the group of
translations.
(3)Constructing a random order: The third part is how to allow for graphs Gand groups  
which do not admit such a deterministic order. This is of course the case for general
graphs  but it may also be the case for simpler graphs  such as ZorZd  when   is the
full automorphism group of G. In these cases  we are led to consider random orders with
suitable properties.
Already the rst part above relies on the aforementioned cell process. Before introducing this
process  it is convenient to observe that it suces to prove Theorem 1.1 and Theorem 1.2 for
1-dependent random elds. To see this  let G
kdenote the graph on vertex set Vin which two
vertices are adjacent if their distance in Gis at mostk. It is immediate from the denitions that
Xisk-dependent as a random eld on Gif and only if it is 1-dependent as a random eld on G
k.
Since any automorphism of Gis also an automorphism of G
k  and since G
kis amenable and
satises (1) whenever Gis such  we see that we may indeed assume that Xis 1-dependent. This
assumption  though not at all essential  is convenient as it obviates the need to work with a dierent
connectivity than the usual connectivity in G.
Acell process is a random sequence A= (A1;A2;:::) of subsets of Vsatisfying the following
properties almost surely:
A1A2A3 .
6 YINON SPINKA
A1[A2[ =V.
For eachn1  all connected components of Anare nite.
We will obtain a cell process Aas a nitary factor of an i.i.d. process Ycellwith arbitrarily small
entropy. We do not explain here how this is done and refer the reader to Section 4 for more details
and to Figure 1 for an illustration of the construction.
(1) Constructing a nitary coding: We construct a realization of Xas a nitary factor of Y
in innitely many steps with the idea that at the end of step n2f1;2;:::g  we will have dened
Xon the region An. In the rst step  we sample Xon the setA1{ this is particularly simple as
the cells in A1are at pairwise distance at least 2  and so  due to the 1-dependence assumption on
X  each cell in A1can be sampled independently. Next  at each step n2f2;3;:::g  we sample X
on the region AnnAn 1  conditioned on the value of XonAn 1  which has already been sampled
in the previous steps { the key observation here is that  due again to the 1-dependence assumption
onX  the values of Xon dierent cells in Anare conditionally independent { indeed  if fVigiare
at pairwise distance at least 2 from one another  then for any sets UiVi  givenfXUigi  one has
thatfXVigiare mutually conditionally independent. Since all the cells of every Anare nite  the
above steps can be carried out in a nitary manner { that is  the conditional distribution of Xon a
given cell depends only on the previously sampled values within that cell. Since Anincreases to V
the value at every given vertex will eventually be sampled  thus producing a realization of Xfrom
Yin a nitary and  -equivariant manner.
Let us be slightly more specic about the way in which we \\sample Xon a cell'. In each cell
inA1  we distinguish a vertex by choosing the smallest element in the cell according to the order
given byYord. We call these distinguished vertices level 1 agents . Similarly  for each n2 and
each cell in Anthat is not contained in An 1  we select a levelnagent in the cell by choosing the
smallest element in the cell which is not in An 1. Note that the level nagents are obtained as a
nitary factor of ( Ycell;Yord). With the notion of agents  we may now say more precisely that  in
stepnabove  ifCis a cell of Anthat is not contained in An 1  then we sample Xon the region
CnAn 1(conditionally on the previously sampled values of XonC\\An 1) by accessing a sample
of the desired distribution from the random variable Ybits
u  whereuis the unique level nagent inC
using also the order induced by YordonCnAn 1to break any symmetries which may be present
in the graph structure of this region (for example  if G=Zand   includes reections  then when
CnAn 1is a symmetric interval around u  the `left' and `right' sides of ucannot be dierentiated
in a  -equivariant way without some additional information; ordering all elements in the set is a
simple way to get rid of such problems). In this interpretation  we regard Ybits
uas consisting of
independent samples of P(XU2jXV=) for all nite U;VZdand2SV  most of which are
never used in practice.
(2) Controlling the entropy: It is clear from the last observation above that there is plenty of
waste in the above construction (in terms of the process Ybits). The problem is that we do not
know ahead of time which samples of which distributions we will need access to. The basic solution
to this is to place an innite sequence of random bits at each site  i.e.  Ybits
v2f0;1gN  from which
we may easily construct samples of any desired distributions (hence the name of the process Ybits).
Of course  this idea alone still does not provide any control on the entropy of Ybits. For this  we
must place a nite (perhaps random) number of random bits at each site  and somehow still be sure
that we are able to construct the required samples. By a random number of random bits  we mean
a random variable Wtaking values inf0;1g  the set of nite words over f0;1g  and having the
property that  conditioned on the length jWjof the word  Wis uniformly distributed on f0;1gjWj.
Suppose now that there exists a deterministic total order onVthat is  -invariant in the sense
thatuvimplies that uvfor anyu;v2Vand2 . For instance  the lexicographical order
is such an order when G=Zdand   is the translation group (but there is clearly no such order
when   is the full automorphism group of Zd). For the purpose of this part of the proof outline
FINITELY DEPENDENT PROCESSES ARE FINITARY 7
it is convenient to further suppose that every v2Vhas a-successor  which we denote by v+ 1
as is the case for the lexicographical order on Zd(in which case v+ 1 is simply v+ (1;0;:::; 0)).
The existence of such a deterministic order renders Yordunneeded  allowing us to focus now only
on the task of controlling the entropy of Ybits.
The idea is to associate to each possible distribution we might require  a \\simulation' which
outputs a sample of the distribution in question from an input of unbiased random bits. The
simulation is fed independent unbiased bits one-by-one  until at some point (a stopping time) it
halts and outputs the sample. Such simulations may be done eciently: the expected number of
input bits read by the simulation is bounded by the entropy of the target distribution  up to an
additive universal constant.
We shall use such simulations whenever we \\sample Xon a cell'. If the cell Cis large  then the
entropy ofXonCis also large  and thus the above additive error is negligible. When the boundary
of the cell is small in comparison to the size of the cell  the average entropy of XonCper site will
also be close to h(X)  the entropy of Xitself. Thus  it will be important that the cells in A1are
typically large with small boundary.
This already shows that  in some sense  the average number of random bits needed to generate
the samples required throughout the construction is very close to h(X). However  we must place
a nite number of bits at each site (more precisely  we need that h(Ybits)< h(X) +)  and even
if we have more than h(X) such bits at every site  it still may happen at some point during the
construction that a simulation carried out by an agent urequires access to many more input bits
than are available in Ybits
u. To solve this  we must allow to \\transfer' bits from one location to
another. This aspect of our construction is inspired by the algorithms in [4 8 25]. The idea is that
whenever an agent urequires access to an additional bit (beyond those available in Ybits
u)  it may
look for an \\unused' bit at u+ 1 (the-successor of u). If there are no available unused bits at
u+ 1 at that time  it may then proceed to look at u+ 2  and so on.
One consequence of the above description is that the steps of the construction cannot be directly
related to the levels of the cell process. That is  it will no longer be the case that after step nof the
construction  we will have dened Xon the region An. Instead  at any step of the construction
dierent regions of Gwill be at dierent levels of the cell process. We will continue to use nto
denote the levels of the cell process  and will use tto denote the step of the construction (which
we henceforth also refer to as time).
The way this is done is as follows. Initially  at time t= 0  all level 1 agents are deemed active. An
active level 1 agent attempts to collect unused bits until its associated simulation halts  at which
point in time the agent becomes inactive and is said to have completed level 1. Once all level 1
agents contained in some level 2 cell Chave completed  the level 2 agent associated to Cbecomes
active. An active level 2 agent proceeds in the same manner as an active level 1 agent  attempting
to read bits in order to complete its associated simulation. In general  a level nagent becomes
active once all level n 1 agents contained in its associated cell have completed.
In our actual construction  it is more convenient to employ the following policy which makes the
details simpler to write down: at time t  an agentumay read at most one bit  and this bit may
only be read from site u+t. This has the advantage that it ensures that no two agents ever try to
read bits from the same location simultaneously.
(3) Constructing a random order: For a general graph Gand group    there need not be a
deterministic total order of Vthat is  -invariant. Instead  we construct a random total order 
onVwhose distribution is  -invariant. Moreover  we construct as a nitary factor of an i.i.d.
processYordwith arbitrarily small entropy. Here nitary means that the order induced on any
nite set of vertices is determined by a nite (random) subset of fYord
vgv2V.
In addition  the constructed order will have the property that its order type is almost surely
the same as that of Z. That is  almost surely  every element vhas a successor v+1 and a predecessor
v 1  andfv+ngn2Z=V. Though it does not follow from the above denition of nitary  it will
8 YINON SPINKA
turn out to be the case that determining whether some vertex is the successor of some other vertex
is also a nitary property (i.e.  it is almost surely determined by a nite subset of fYord
vgv2V). Once
such an order is at hand  the proof continues as outlined above.
Organization. In Section 3  we introduce some preliminaries. In Section 4  we prove the existence
of a nitary cell process. In Section 5  we prove the existence of a nitary random total order
having the order type of Z. In Section 6  we give the construction of the nitary coding for the
nitely dependent process X. Finally  we end in Section 7 with some remarks and open problems.
3.Preliminaries
Recall that Gis always assumed to be an innite  transitive  locally nite  connected graph on
a countable vertex set V  and that   is assumed to be a subgroup of the automorphism group of G
that acts transitively on V.
3.1.Entropy. The Shannon entropy of a discrete random variable Zis
H(Z) := X
zP(Z=z) logP(Z=z);
where the sum is taken over zin the support of Z  or alternatively  we interpret 0 log 0 to be 0.
The measure-theoretic entropy (or Kolmogorov{Sinai entropy) of a  -invariant random eld Xon
an amenable graph Gis
h(X) := inf
VVnite
and non-emptyH(XV)
jVj:
AFlner sequence inGis a sequence ( Fn)1
n=1of non-empty nite subsets of Vsuch that
lim
n!1j@Fnj
jFnj= 0:
It is well-known that the entropy of Xmay be computed along any Flner sequence:
h(X) = lim
n!1H(XFn)
jFnjfor any Flner sequence ( Fn)1
n=1inG:
It follows that for any >0 there exists >0 such that
H(XF)
jFjh(X) + wheneverFVis non-empty and nite and j@FjjFj: (2)
Of course  since entropy is maximized by the uniform distribution  we also have that
H(XFjE)
jFjlogjSjwheneverFVis non-empty and nite
andEis an event with positive probability; (3)
whereSis the nite set in which Xtakes values. We note that if Yis an i.i.d. process  then its
entropyh(Y) is equal to the entropy of its single-site distribution H(Y0).
3.2.The mass-transport principle. Foru;v2V  denote
 u;v:=f2  :u=vg:
Note that   u;uis the stabilizer ofu. We say that   is unimodular if
j u;uvj=j v;vuj for allu;v2V:
It is well-known (see  e.g.  [22  Chapter 8]) that   is unimodular if and only if the following mass-
transport principle holds:
X
uf(u;0) =X
vf(0;v) for any diagonally  -invariant function f:V2![0;1]: (4)
FINITELY DEPENDENT PROCESSES ARE FINITARY 9
By diagonally  -invariant  we mean that f(u;v ) =f(u;v) for allu;v2Vand2 . We note the
well-known fact that  when Gis amenable  any transitive group of automorphisms   is unimodular.
3.3.Simulating distributions from random bits. We shall use a result about the simulation
of a given distribution from unbiased random bits. Let be a distribution on a countable set
.
Asimulation ofis a pair S= (Stime;Sout) of measurable functions Stime:f0;1gN!N[f1g and
Sout:f0;1gN!
with the properties:
If!is a sequence of independent unbiased bits  then Sout(!) has distribution .
IfStime(x) =nfor somex2f0;1gNandn2N  then Stime(x0) =nandSout(x0) =Sout(x)
for anyx02f0;1gNwhich coincides with xonf1;:::;ng.
The rst property says that we can use Sto simulate the desired distribution from random bits.
The second property may be interpreted as saying that Stimeis a stopping time and that Soutis
adapted to the -algebra generated by f!i: 1iStimeg{ that is  the simulation reads one input
bit at a time  and once the stopping time is reached  the output is determined only by the bits that
have already been read.
The following theorem follows from a result of Knuth and Yao [19] (see Theorems 2.1 and 2.2
there and the corollary just after).
Theorem 3.1. LetZbe a discrete random variable. There exists a simulation SofZfrom inde-
pendent unbiased bits !satisfying that Stime(!)<1almost surely and EStime(!)H(Z) + 2 .
Knuth and Yao show that the above is in fact optimal in a strong sense (they provide a simulation
whose stopping time is stochastically dominated by that of any other simulation). A version of this
theorem was proved in [8  Theorem 3] via a more concrete construction. The results in [8 19] also
provide explicit exponential bounds on the probability that the simulation uses more than nbits
but we shall not need this.
4.The cell process
Recall from Section 2 that a cell process is a random sequence A= (A1;A2;:::) of subsets
increasing to Vand satisfying that the connected components of each Anare nite. We may
identify a cell process Awith the N-valued random eld (min fn1 :v2Ang)v2V. In particular
when we say that Ais  -invariant or a nitary factor  we mean that this latter process is such.
In this section  we show that nitary cell processes with arbitrarily small entropy exist on any
transitive amenable graph.
Proposition 4.1. LetGbe a transitive amenable graph and let  > 0. There exists an i.i.d.
processYof entropy at most and a cell process Awhich is a nitary Aut (G)-factor ofY.
Let us mention that in the special case of G=Zd  several parts of the argument below can be
skipped or simplied  thereby leading to a shorter proof. In the general case  certain technicalities
arise which make the proof somewhat longer. The reader may therefore wish to have in mind the
case ofG=Zdon a rst reading.
Before giving the proof  let us explain the idea behind it; see Figure 1 for an illustration. The
main idea of the construction is to use the points of a low-density Bernoulli process to construct
Voronoi cells (determined from the Bernoulli process in a nitary manner)  which are then used as
the cells of A1(after slightly decreasing the Voronoi cells to ensure that they are well-separated).
Using another Bernoulli process of even lower density  we again construct Voronoi cells  which are
then used to obtain A2fromA1by \\lling in' some of the empty space between the cells of A1
taking care not to connect cells of A1which are not in the same Voronoi cell (thus ensuring that
an innite cluster is not created). Repeating in this manner  we obtain an increasing sequence
A1A2 of sets (each having only nite cells) as a nitary factor of a small entropy i.i.d.
10 YINON SPINKA
(a)A1
(b)Going from A1toA2
(c)A2
Figure 1. Constructing the cell process. The cells of A1are simply the Voronoi
cells of a Bernoulli process. To get from A1toA2  we consider the Voronoi cells of a
lower-density Bernoulli process  and \\merge' cells of A1which are entirely contained
in any such Voronoi cell. Repeating this procedure produces the cell process. The
green shade (light and dark) depicts regions belonging to the cell process. The dark
green depicts cells of A2which are not cells of A1.
process. It will then only remain to show that Anincreases to V. This is where amenability comes
into play.
To ensure that Anincreases to V  we must be careful in how we dene the Voronoi cells. When
the graphGhas a Flner sequence consisting of balls (as is the case when Ghas subexponential
growth)  the Voronoi cells may be taken with respect to the graph distance in G. However  for the
general case considered here  we must adapt the usual Voronoi cells to a certain \\metric' (which is
not necessarily a true metric) given by a suitable Flner sequence. Since we will need this metric
to be diagonally  -invariant  we need to choose a Flner sequence ( Fn)nhaving the property that
eachFnis invariant under the stabilizer of some xed vertex 02V  i.e. Fn=   0;0Fnfor alln.
It is a simple observation that any graph Gof subexponential growth has such a sequence for any
  (since there is a Flner sequence consisting of balls)  and that the Cayley graph Gof a nitely
generated amenable group   also has such a sequence (since the stabilizers are trivial). As it turns
out  such a Flner sequence always exists in an amenable (not necessarily transitive) graph  even
when   is its full automorphism group.
The following lemma (which appears to be new) shows that every amenable graph admits a
Flner sequence consisting of sets which are invariant under the stabilizer of some vertex. The
lemma was obtained jointly with Omer Angel.
Lemma 4.2. LetGbe an amenable graph  let  be the full automorphism group of Gand let 02V.
There exists a Flner sequence (Fn)ninGsuch thatFn=   0;0Fnfor alln.
Lemma 4.2 easily follows by applying the following lemma to each set of some Flner sequence
taking   0to be the stabilizer   0;0of0.
Lemma 4.3. LetGbe any graph and let  0be a group of automorphisms of Gunder which all
orbits of Vare nite. For any nite non-empty set FV  there exists a nite non-empty set
EVsuch that
j@Ej
jEjj@Fj
jFjandE=   0E:
Proof. We show the existence of the desired Evia a probabilistic method { that is  we choose a
random subset EofVand show that it satises the desired properties with positive probability.
FINITELY DEPENDENT PROCESSES ARE FINITARY 11
LetfVigibe the orbits of Vunder the action of   0. Thus fVigiis a partition of Vsuch that each
Viis nite (by assumption) and satises Vi=   0Vi.
LetUbe a uniform random variable in [0 ;1] and dene
E:=[
i:UpiVi; wherepi:=jF\\Vij
jVij:
Thus  each orbit Viis included in Ewith probability pi  where the choices for dierent iare
positively correlated through the use of the common variable U. Then
EjEj=X
iP(Upi)jVij=X
ipijVij=X
ijF\\Vij=jFj;
and
Ej@Ej=X
i;jP(pj<Upi)j@(Vi;Vj)j=X
i;jmaxfpi pj;0gj@(Vi;Vj)j;
where@(U;V) denotes the set of edges between two disjoint sets UandV. Let us show that each
term in the second sum is at most j@(F\\Vi;Fc\\Vj)j  i.e.
pi pjj@(F\\Vi;Fc\\Vj)j
j@(Vi;Vj)jwheneverpi>pjand@(Vi;Vj)6=;:
To see this  note that the right-hand side is the probability that an edge e=fu;vgthat is uniformly
chosen from @(Vi;Vj) belongs to @(F\\Vi;Fc\\Vj)  or equivalently  with the convention that u2Vi
andv2Vj  thatu2Fandv =2F. Since this probability is at least P(u2F) P(v2F)  it suces
to show that uandvare uniformly distributed in ViandVj  respectively. This follows from the
observation that the bipartite graph ( Vi[Vj;@(Vi;Vj)) is biregular { each vertex in Viis adjacent to
the same number of vertices in Vj  and similarly  each vertex in Vjis adjacent to the same number
of vertices in Vi. Indeed  for any u;v2Viand2 0such thatu=v  the mapping w7!w
denes a bijection between N(u)\\VjandN(v)\\Vj.
We thus conclude that
Ej@EjX
i;jj@(F\\Vi;Fc\\Vj)j=j@Fj=EjEj;
where:=j@Fj=jFj. Thus jEj j@Ejis a random variable with non-negative expectation. Since
jEj j@Ejis zero when Eis empty  conditioned on E6=;  we still have that jEj j@Ejhas
non-negative expectation. In particular  there is positive probability that E6=;andjEjj@Ej.
Finally  since Eis nite and satises E=   0Ealmost surely  we see that Esatises the desired
properties with positive probability. 
Suppose that ( Fn)nis a Flner sequence guaranteed by Lemma 4.2  i.e.  Fn=   0;0Fnfor alln
and further suppose that F1(F2(andF1[F2[ =V(there is clearly no loss in generality
in doing so). Recall the denition of   u;vfrom Section 3.2. For u;v2V  dene
(u;v) := min
n1 :v2 0;uFn
:
Using that   0;u= 0;ufor any2   one easily checks that is diagonally  -invariant  i.e.
(u;v ) =(u;v) for all2 . We stress that is not necessarily symmetric in that (u;v) may
not equal(v;u). In particular  we do not claim that is a metric. Nevertheless  we still think of
(u;v) as a measure of distance from vtou. One nice property of that is easily veriable and
which will be important is that  for any sequence of pairs of vertices ( ui;vi)1
i=1  we have
(ui;vi)!1 asi!1 if and only if dist( ui;vi)!1 asi!1: (5)
The fact that may not be symmetric presents a certain challenge in the proof  for which we
require the following lemma to address. We note that is indeed symmetric when the Flner
sequence (Fn)nconsists of balls  and that it is nearly symmetric when Gis a Cayley graph of  
12 YINON SPINKA
in which case switching the roles of uandvin(u;v) has the same eect as replacing each Fn
withF 1
n. Accordingly  in these cases  it is immediate that the two \\ -balls' of radius naround 0
fv:(0;v)ngandfu:(u;0)ng  have the same size  namely jFnj. The following lemma
shows that this is in fact always true in our setting.
Lemma 4.4. LetGbe a transitive amenable graph and let  be the full automorphism group of G.
LetFVbe invariant under the stabilizer of some vertex 0  i.e.   0;0F=F. Then
jfu2V:02 0;uFgj=jFj:
Proof. Denef:V2![0;1] by
f(u;v) :=1fv2 0;uFg:
Since   0;u= 0;ufor any2   it follows that fis diagonally  -invariant. Thus  by the
mass-transport principle (4)
jfu2V:02 0;uFgj=X
vf(0;v) =j 0;0Fj=jFj: 
We are now ready to give the proof of Proposition 4.1.
Proof of Proposition 4.1. Let (n) be a sequence to be chosen later which satises that 0 n
2 n. We shall construct a cell process Aas a nitary factor of the i.i.d. process Y= (Yv)v2Vin
whichYv= (Yv;n)n1are independent random variables with Yv;nBer(n). The entropy of Y
can be made arbitrary small  since
h(Y) =H(Yv) =1X
n=1H(Yv;n) =1X
n=1
nlog1
n+ (1 n) log1
1 n
10log1
:
As explained  the idea of the construction is to use the points in
Un:=fv2V:Yv;n= 1g;
for any given n  to construct Voronoi cells  which are then used to dene the cells of An. Precisely
we dene the Voronoi cells of a non-empty set UVby
CU(u) :=n
v2V:(u;v)<(u0;v) for allu02Unfugo
; u2U:
Thus  the Voronoi cell CU(u) associated to uconsists of all vertices v2Vwhich are closer (in the
distance measured by ) touthan to any other u02U. In particular  Voronoi cells associated to
dierent vertices in Uare disjoint  but they are not necessarily separated (they could be adjacent
to one another). We therefore dene modied Voronoi cells by slightly shrinking the sets CU(u).
Precisely  we dene
CU(u) := ( CU(u)) 1:
Using that the Voronoi cells are disjoint  it is straightforward to check that dist( CU(u);CU(u0))>1
for distinct u;u02U. We note thatCU(u) (in fact  already CU(u)) may be empty and need not be
connected.
Before proceeding with the construction of the cell process  let us rst show that the Voronoi
cells ofUare almost surely nite whenever Uis the set of points of an i.i.d. Bernoulli process. To
this end  it suces to show that the probability that CU(0) intersects FnnFn 1is summable over n.
Indeed  since a xed vertex v2FnnFn 1belongs to CU(0) only ifUnf0gcontains no element of
fu:(u;v)ng  it follows from Lemma 4.4 that the probability of this is at most pjFnj 1  wherep
is the density of the Bernoulli process. Since jFnjn  we see thatjFnjpjFnj 1is summable  and
hence that the CU(0) is almost surely nite.
FINITELY DEPENDENT PROCESSES ARE FINITARY 13
We now turn to the construction of the cell process A. The rst level set in the cell process is
simply taken to be the vertices in a modied Voronoi cell of U1  i.e.
A1:=[
u2U1CU1(u):
Since the Voronio cells of U1are almost surely nite  we see that CU1(u) is almost surely nite
for allu2U1. Since dist(CU1(u);CU1(u0))>1 for distinct u;u02U1  it follows that all connected
components of A1are almost surely nite.
Suppose now that  for some n1 Anhas been dened in such a way that all connected
components of Anare almost surely nite  and let us now dene An+1. LetA0
n+1be the union of
the modied Voronoi cells of Un+1  i.e.
A0
n+1:=[
u2Un+1CUn+1(u);
and recall that (as for A1) all connected components of A0
n+1are almost surely nite. Intuitively  we
would like to obtain An+1fromAnby addingA0
n+1. However  this might create innite clusters
and we must take care to avoid this by instead only adding a suitable subset of A0
n+1. It will
suce to slightly increase the \\forbidden region' ( A0
n+1)cas follows: let Dn+1denote the union
of the connected components of Anthat intersect ( A0
n+1)c  and addD+1
n+1to the forbidden region.
An+1:=An[(A0
n+1nD+1
n+1):
It is straightforward to check that all connected components of An+1are almost surely nite.
Assuming that A1[A2[ =Valmost surely  it is also easy to check using (5) that Ais a nitary
Aut(G)-factor ofY  which would complete the proof of the proposition.
It remains only to show that A1[A2[ =Valmost surely. By Aut( G)-invariance  this is
equivalent to the fact that P(02An)!1 asn!1 . Forn1  letBndenote the connected
component of 0inAn[f0g. Forn2  dene the event
En:=n
Bn 1CUn(u) for someu2Uno
Note that
f02AnnAn 1g=En\\f0=2An 1g:
Thus  it suces to show that
P(Enj0=2An 1)c for somec>0 and alln2:
As we now show  this holds when nis suitably chosen. Since Bn 1is almost surely nite  there
exists a suciently large rnso that
P(Bn 1rn 1j0=2An 1)1
2;
where r:= r(0). Since (Fs)sis a Flner sequence  there exists snsuciently large so that
jFsnj2n
andj@Fsnj
jFsnj1
2jrnj: (6)
Setn:=jFsnj 1. Then  noting that An 1(and thus also Bn 1) is independent of Un
P(Enj0=2An 1)P 
Bn 1rn 1and rn 1CUn(u) for someu2Unj0=2An 1
=P 
Bn 1rn 1j0=2An 1
P 
rn 1CUn(u) for someu2Un
Since the rst term on the right-hand side is at least1
2by the choice of rn  it remains to show that
for some constant c>0 which does not depend on n  we have
P 
c:
14 YINON SPINKA
Let us rst see how to show this when Fsnis a ball  say  `. In this case  it is not hard to see that
the event in question occurs when Unhas a unique point in  ` rnand no other point in  `+rn  so
that
P 
jUn\\` rnj=jUn\\`+rnj= 1
Ber(j` rnj;n) = 1
Ber(j`+rnn` rnj;n) = 0
We now handle the general case in more detail. Set U:=Un. Our goal is to bound from below
the probability that  rn 1CU(u) for someu2U. To this end  we rst nd a simple condition
that implies the occurrence of this event. For a set FV  denote
M(F) :=fu2V:02 0;uFg:
Setr:=rnands:=sn. Let us show that
jU\\M(F r
s)j=jU\\M(F+r
s)j= 1 =) r 1CU(u) for someu2U: (7)
Suppose that the left-hand side holds. Let us show that  r 1CU(u)  whereuis the unique
element in U\\M(F r
s). By the denition of CU(u)  this is equivalent to  rCU(u). Recalling
the denition of CU(u)  we see that we must show that (u;w)<(u0;w) for allu02Unfugand
w2r. Letu02Unfugandw2r. It suces to show that (u;w)sand(u0;w)>s.
Towards showing this  we rst note that ( V)+1=(V+1) and (V) 1=(V 1) for any2 
andVV  due to the fact that acts by an automorphism of G. In particular  (  0;uV)+r=
 0;u(V+r) and (  0;uV) r=   0;u(V r)  and we may drop the parenthesis when writing such terms.
Let us now show that (u;w)s. Sinceu2M(F r
s)  we have that 02 0;uF r
s  or equivalently
r 0;uFs. Sincew2r  it follows that (u;w)s. Next  we show that (u0;w)>s. Note that
u0=2M(F+r
s) sinceu2M(F r
s)M(F+r
s). Thus  0=2 0;u0F+r
s  or equivalently   r\\ 0;u0Fs=;.
Thus w =2 0;u0Fsand  using that F1;F2;:::;Fs 1Fs  it follows that (u0;w)>s.
Using (7)  we obtain
jUn\\M(F rnsn)j=jUn\\M(F+rnsn)j= 1
Ber(jM(F rnsn)j;n) = 1
Ber(jM(F+rnsn)j jM(F rnsn)j;n) = 0
By Lemma 4.4  we have that
jM(F rnsn)j=jF rnsnj andjM(F+rnsn)j=jF+rnsnj:
Finally  by (6)
jF rnsnjjFsnj j@Fsnjjrnj1
2jFsnj andjF+rnsnjjFsnj+j@Fsnjjrnj3
2jFsnj;
so that  by standard estimates for Bernoulli random variables  both probabilities in question are
bounded below by a positive constant. 
The above proposition established the existence of a nitary cell process A. In particular  Anis
an invariant set which has high density when nis large. The following proposition shows that the
clusters of a dense invariant set typically have relatively small boundary.
Lemma 4.5. LetGbe a transitive graph of degree dand let  be a transitive unimodular group of
automorphisms of G. LetBVbe a random set with no innite clusters and whose distribution
is -invariant. LetCvdenote the cluster of vinB. Then  for any >0
j@C0jjC0j
(d
+ 1)P(0=2B):
FINITELY DEPENDENT PROCESSES ARE FINITARY 15
Proof. The proof uses the mass-transport principle. Dene  :V2![0;1] by
(u;v) :=(jN(u)\\Cvj
jCvjifu =2B; v2B
0 otherwise:
Note that  almost surely
u (u;0) =102B
jC0jX
u=2BjN(u)\\C0j=j@C0j
jC0j102B
andX
v (0;v) =10=2BX
v2BjN(0)\\Cvj
jCvj=jN(0)\\Bj10=2Bd10=2B:
The  -invariance of Bimplies that f(u;v) :=E (u;v) is diagonally  -invariant. Thus  the mass-
transport principle (4) yields that
Eh
j@C0j
jC0j102Bi
dP(0=2B):
The proposition now follows from Markov's inequality. 
Remark 1. A result of H aggstr om [6  Theorem 1.6] states that any automorphism-invariant edge
percolation on a d-regular tree ( d3) with edge-density at least 2 =dhas an innite cluster with
positive probability. In particular  automorphism-invariant cell processes do not exist on such a
tree. Moreover  by Lemma 4.5 (see also the closely related [3  Theorem 1.2])  we see that  -invariant
cell processes do not exist on any transitive unimodular non-amenable graph. In fact  it is shown
in [3  Theorem 5.1] that a closed subgroup   of Aut( G) is amenable if and only if there is a  -
invariant site percolation on Gwith with no innite clusters and density arbitrarily close to 1. It
follows that a  -invariant cell process on Gexists if and only if   is amenable. The site percolation
constructed in [3] is a factor of an i.i.d. process  though it is not nitary.
5.Random total orders
In this section  we construct a random total order on Vthat has the following properties:
It is a nitary factor of an i.i.d. process with arbitrarily small entropy.
It is supported on total orders having the same order type as Z.
The successor/predecessor of any vertex can be found in a nitary manner.
Let us explain these properties. A random total order on V  or more generally  a random binary
relation on V  may be regarded as a random element in f0;1gV2. With this viewpoint  the notion
of nitary factor easily applies to such relations. Namely  such a relation is a  -factor of Yif it
has the same distribution as '(Y) for some measurable function ':TV!f0;1gV2satisfying that
'(y)(u;v)='(y)(u;v )for all2  u;v2Vandy2TV. Such a factor is nitary if for every
u;v2Vthere almost surely exists a nite (random) set WVsuch that'(Y)(u;v)is determined
by (Yw)w2W  in the sense that '(y)(u;v)='(Y)(u;v)for anyy2TVwhich coincides with YonW.
A total orderonVhas the same order type as Zif there is an order preserving bijection
between the two ordered spaces  i.e.  a bijection f:V!Zsuch thatf(u)f(v) if and only if
uv. This may be equivalently formulated as saying that has no minimum or maximum and
that there are nitely many elements between any two elements  i.e.  every interval of the form
fw2V:uwvgis nite. In particular  in such an order  every vertex vhas a successor (an
elementwvsuch thatuwfor alluv) and a predecessor (an element wvsuch thatuw
for alluv).
Given a factor from Yto a random total order onV  we say that successors (predecessors)
can be found in a nitary manner if for every u;v2Vthere almost surely exists a nite (random)
setWVsuch that the event that uis the-successor (-predecessor) of vis determined by
16 YINON SPINKA
(Yw)w2W. We note that  in general  there is no direct relation to the notion of nitary factor: it
may be that such a factor is nitary though successors/predecessors cannot be found in a nitary
manner  or it may that successors/predecessors can be found in a nitary manner though the factor
is not nitary. On the other hand  for a total order having the order type of Zalmost surely  the
second implication is easily seen to hold { if successors/predecessors can be found in a nitary
manner  then the factor is necessarily nitary.
A total order which is a nitary factor of an i.i.d. process with innite entropy is easily obtained
from the order induced by uniform random variables in [0 ;1] assigned to each vertex. It is easy
to see that this order almost surely has the same order type as Q. A total order (also with the
order type of Q) which is a nitary factor (with exponential tails on the coding radius) of an
i.i.d. process with nite entropy was constructed in [7] for any quasi-transitive graph satisfying a
geometric condition similar to (1). The application in [7] did not require the i.i.d. process to have
arbitrarily small entropy and so this was not stated there  though it easily follows from the proof
there that this is possible. Since the proof is short  we give it here. The following is essentially a
reformulation of [7  Lemma 18] for our situation.
Lemma 5.1. LetGbe a transitive non-empty graph satisfying (1). For any 0<1
2there exists
a total order on Vwhich is a nitary Aut (G)-factor of an i.i.d. Bernoulli process with density .
Proof. Let= (v)v2Vbe an i.i.d. Bernoulli process with density . For any v2V  dene
Zv= (Zv;n)n02f0;1;:::gf0;1;:::gby
Zv;n:=X
u2V:dist(u;v)=nu:
Dene a relationonVin whichuvif and only if ZuZv  wheredenotes the lexicographical
order onf0;1;:::gf0;1;:::g. Thenis clearly a Aut( G)-factor of.
It remains to show that is almost surely a total order on Vand that the factor is nitary. Since
is clearly a preorder  to show that it is a total order  it suces to show that P(Zu=Zv) = 0 for
distinctu;v2V. It then follows from the denition of the lexicographical order that the factor is
nitary.
Fixu;v2Vdistinct and consider the event
En:=n\\
i=1fZu;i=Zv;ig:
SinceP(En)!P(Zu=Zv) asn!1   it suces to show that P(EnjEn 1)1 for alln1.
By (1) and the assumption that the graph is non-empty  we have  n(u)nn 1(u)6n(v)  as
otherwise  3n(u)3n(v)  which in turn implies that  3n(u) =  3n(v) by transitivity. Thus
there exists some wn2n(u)n(n 1(u)[n(v)). Then
EnjVnfwng
max
k2ZP(wn=k) = maxf;1 g= 1 :
SinceEn 1is measurable with respect to Vnfwng  it follows that P(EnjEn 1)1 . 
Using Lemma 5.1 and the cell processes constructed in the previous section  we are able to
construct a total order satisfying all three properties described above.
Lemma 5.2. LetGbe a transitive amenable non-empty graph satisfying (1)and let>0. Then
there exists an i.i.d. process Ywith entropy at most   and a random total order onVwhich
almost surely has the same order type as Z  such thatis a nitary Aut (G)-factor ofYfor which
successors/predecessors can be found in a nitary manner.
Proof. By Lemma 5.1  there exists a total order onVthat is a nitary factor of an i.i.d. process Y
having entropy at most . By Proposition 4.1  there exist a cell process Athat is a nitary factor of
FINITELY DEPENDENT PROCESSES ARE FINITARY 17
an i.i.d. process Y0(which we take to be independent of Y) having entropy at most . We construct
the required total order onVas a nitary factor of ( Y;Y0).
The idea is to use to order the sites within the cells given by A. More precisely  we will dene
an increasing sequence of partial orders 1 2:::such that eachninduces a total order on
every cell of An. SinceS
nAn=V  this will produce a total order given by the unionS
nn
which we will show has the desired properties.
Precisely  we dene 1to be the relation in which u1vwheneveruandvbelong to the same
cell ofA1and satisfy that uv. Then1is clearly a partial order that induces a total order
on any cell of A1. In fact  it is the union of these total orders on the cells of A1(that is  it only
compares vertices that belong to the same cell).
Next  suppose we have dened the partial order n 1so that it is a union of total orders on the
cells onAn 1  and let us dene n. Consider a cell CofAnand letD:=C\\An 1=D1[[Dk
be the union of the cells D1;:::;DnofAn 1that are contained in C. We denenin such a
way thatDnCnDby requiring that unvwheneveru2Dandv2CnD. To obtain a
total order on C  it remains to order the vertices in Dand the vertices in CnD. The latter is
ordered by dening unvwheneveru;v2CnDanduv. The former is ordered by giving
an order to the cells D1;:::;Dkand using then 1order within each cell { that is  we require
thatncoincides withn 1on each cell Di  and that either DinDjorDjnDifor any
two cellsDiandDj. Finally  the order of the cells is determined by requiring that DinDj
whenever min DiminDj. That is  the i-th cell precedes the j-cell innif and only if the
-minimal element in the i-th cell is-smaller than the -minimal element in the j-th cell. It is
straightforward that nextendsn 1and thatnis a union of total orders on the cells of An.
We have thus obtained partial orders 1;2;:::such that  for each n nextendsn 1and is a
union of total orders on the cells of An. To show thathas the order type of Z  it remains to show
that  almost surely  every -interval is nite and there is no -minimum and no -maximum. It
follows from the construction that if uis then-successor of v  then it is also its n+1-successor.
Thus  to conclude that every -interval is nite  it suces to show that  for every u;v2Vhaving
uv  there exists nsuch thatunvand the interval [ u;v]nis nite. Indeed  since nonly
compares vertices within the same cell of Anand since all such cells are nite  all n-intervals are
nite. Finally  no minimum or maximum can exist as this would contradict the invariance of .
We have thus established that almost surely has the same order type as Z. It is straightforward
from the fact that the n-successor of a vertex u(if it exists) is also the n+1-successor of u  that
the constructed factor from ( Y;Y0) tohas the property that successors/predecessors can be found
in a nitary manner. As mentioned in the beginning of the section  this implies that the factor is
also nitary. 
6.The finitary coding
In this section  we construct a nitary coding for nitely dependent processes. We present the
details of the proof of Theorem 1.2. Theorem 1.1 may be proved in a similar manner (see Remark 2
in Section 7).
LetXbe a  -invariant nitely dependent process taking values in a nite set S. Recall from
the proof outline in Section 2 that we shall construct a nitary factor from an i.i.d. process Y=
(Ybits;Ycell;Yord) toX. Recall also that Awill be a cell process that is a nitary factor of Ycell
and thatwill be a random total order on Vthat is a nitary factor of Yord  has the order type of
Z  and for which successors/predecessors can be found in a nitary manner. Given the cell process
Aand the total order   we will use the additional randomness in Ybitsto construct a realization
ofX.
18 YINON SPINKA
6.1.Choosing the parameters. Fix>0. We shall choose the i.i.d. processes Ybits;Ycell;Yord
to satisfyh(Ybits)<h(X) + 5 h(Ycell)andh(Yord)so thatYhas entropy
h(Y)<h(X) + 7:
We letYbitsbe any i.i.d. process in which Ybits
vis a random number of random bits satisfying
H(Ybits
v)<h(X) + 5 and EjYbits
vj>h(X) + 3: (8)
Recall that  by a random number of random bits  we mean a random variable Wtaking values
inf0;1g  the set of nite words over f0;1g  and having the property that  conditioned on the
lengthjWjof the word  Wis uniformly distributed on f0;1gjWj. The desired random word can
be obtained by taking Wto be the empty word with probability por a uniformly chosen sequence
inf0;1gmwith probability 1  p  for some suitably chosen m1 and 0p < 1. Indeed
in this case  H(jWj) = plogp (1 p) log(1 p) and EjWj=pmso thatH(jWj)!0 and
EjWj!h(X) + 4asm!1 whenp=1
m(h(X) + 4). Since the entropy and length of Ware
related via H(W) =EjWj+H(jWj)  we see that (8) holds when mis suciently large.
Let>0 be as in (2). By decreasing   we may additionally assume that
2<: (9)
Recall from Section 2 that we may assume without loss of generality that Xis 1-dependent. By
Proposition 4.1  there exists a cell process Aand an i.i.d. process Ycellof entropy at most such
thatAis a nitary factor of Ycell. Since P(02An)!1 asn!1   Lemma 4.5 implies that
P(j@An(0)jjAn(0)j)!0 asn!1   whereAn(0) is the cell of 0inAn. Thus  by replacing
(A1;A2;:::) with (Am;Am+1;:::) for some large m  we may assume that P(j@A1(0)jjA1(0)j)
is arbitrary small. Specically  we require that
P
j@A1(0)jjA1(0)j
<
logjSj+ 2: (10)
LetYordbe an i.i.d. process with entropy at most and letbe a total order on Vas guaranteed
by Lemma 5.2 (note that if the graph Gcontains no edges  then Xis already an i.i.d. process so
that there is nothing to prove).
6.2.The construction of the nitary coding. Recall that the n-th-successor of vis denoted
byv+nand itsn-th-predecessor by v n. In particular  vnare random elements of Vwhich
are determined from Yordis a nitary manner. Recall also that the cell process Ais a nitary
factor ofYcell. It may be helpful from this point onward to think of Aandas given  and that
our goal is to use them  together with the random bits of Ybits  to construct a realization of X.
We shall dene  for every time t0 and every vertex u2V  a random variable
Lt
u2f0;1g:
We shall dene these inductively with the t= 0 variables given by
L0
u:=1fuis a level 1 agent and jYbitsuj>0g: (11)
We think of Lt
uas indicating whether uread a bit at time t. In particular  if Lt
u= 1 for some t
andu  then necessarily uis an agent (of some level). Since at time 0 an agent looks for an available
bit at its own location  (11) says that any level 1 agent reads a bit at time 0 if such a bit is available.
Before giving the main denitions of the construction  we rst set up some auxiliary notation
and denitions. As we have already mentioned  our construction has the property that if an agent u
reads a bit at some time t  then the bit it read is located at u+t  i.e.  it is one of the bits of the
wordYbits
u+t. In particular  the total number of bits read from location vby timetis
Mt
v:=L0
v+L1
v 1++Lt
v t:
FINITELY DEPENDENT PROCESSES ARE FINITARY 19
The bits at any location vare read sequentially { the rst agent to read a bit at vwill readYbits
v(1)
the second will read Ybits
v(2) and so on. Precisely  the bit read by uat timetis
^Wt
u:=(
Ybits
u+t(Mt
u+t) ifLt
u= 1
; otherwise:
For this to be well-dened  we must make sure that udoes not try to read a non-existent bit { we
mention already here that this does not occur  i.e.  our denitions will ensure that
vjYbits
vj for allv2Vand allt0:
The word read by uby timetis then
Wt
u:=^W0
u^W1
u ^Wt
u;
wheredenotes concatenation. That is  Wt
uis the word obtained by concatenating the bits read
byuuntil timetin the order they were read. In particular  Wt
uis a word inf0;1gof length
jWt
uj=L0
u+L1
u++Lt
u. We emphasize that ( Mt
u;Wt
u)u2Vis well-dened once ( Li
u;Ni
u)u2V;0it
is dened  as the former are functions of the latter and of Ybits.
As explained in the proof outline  we use \\simulations' to obtain samples of distributions from
random bits. We rst equip ourselves with simulations of all the possible distributions we may
require throughout the construction of the nitary coding. The basic distributions we need are
those ofXVfor a nite set VV. As we aim to obtain a  -equivariant factor  we must take
care when dealing with random elements of SV  as these are indexed by subsets of vertices. It
would be more proper to view XVas a random element of SjVjby using the order . Precisely
we proceed as follows. Recall the denition of a simulation from Section 3.3. By Theorem 3.1  for
every ordered sequence v1;:::;vm2Vof distinct vertices  there exists a simulation S(v1;:::;vm)of
(Xv1;:::;Xvm)2Smsatisfying that
EStime
(v1;:::;vm)(!)H(XV) + 2:
Since the distribution of Xis  -invariant  we may suppose that S(v1;:::;vm)=S(v1;:::;v m)for all
2  (e.g.  by choosing a simulation for a single representative of each orbit  and then setting
S(v1;:::;vm)to equal the simulation of its representative). Now  for a nite set VV  we let
v1;:::;vmbe the vertices of V  ordered according to   and set SVto be the simulation S(v1;:::;vm)
where  for notational convenience  we interpret Sout
Vas an element of SV(indexed by V) through
the identication Sout
V(!)vi=Sout
(v1;:::;vm)(!)i. We stress that SVimplicitly depends on the order .
As we will also encounter situations in which regions of Xhave already been sampled  we will
also need simulations of the distribution of XVconditioned on XUfor some nite set UVwhich
is disjoint from V. Thus  for every such VandUand every2SU  we similarly let SV;U; be a
simulation of P(XV2jXU=) satisfying that
V;U;(!)H(XVjXU=) + 2: (12)
For ease of notation later on  we allow VandUto intersect and we allow to have any domain
containingU  by interpreting SV;U; asSVnU;U; Uin such a case. We also identify SVwith SV;;;;.
Recall that every cell CinAnthat is not contained in An 1has an associated level nagent  and
that this agent is \\responsible' for generating the output on CnAn 1. We denote by An(v) the
cell ofvinAn  whereAn(v) :=;ifv =2An  byUnthe set of level nagents and  for v2AnnAn 1
byUn(v) the levelnagent associated to the cell An(v).
With the above notation and denitions  we may now proceed to construct the nitary coding.
Our goal is to dene a random eld Zt= (Zt
v)v2V  which represents the output at time t. This
output will be a function of ( Li
u;Mi
u;Wi
u)u2V;0it(and of course of the cell process Aand the total
order). OnceZtis dened for some t  it will then only remain to inductively dene ( Lt+1
u)u2V  as
this then also denes ( Mi
u)u2V;0it+1through the denitions above. This will therefore dene
20 YINON SPINKA
Zt+1as well. We will then take a limit as t!1 in order to obtain the outputZ= (Zv)v2V  which
is the desired realization of X.
To facilitate the inductive denition of ( Lt+1
u)u2V  we require some more denitions. We now
regardt0 as xed and suppose that ( Li
u)u2V;0it  and hence also ( Mi
u)u2V;0it  are already
dened. We dene two notions for a level nagent: that of having reached level nat timet  and
that of having completed level nof the simulation by time t. We dene these notions inductively
onn. We thus begin with level 1 agents. Given a level 1 agent u2U1  we say that
ureached level 1 by time t(always  with no condition).
ucompleted level 1 by time tif the stopping time Stime
A1(u)has been reached on input Wt
u;1.
Once a level 1 agent has completed level 1 of the simulation  the output is known on the corre-
sponding level 1 cell of the agent. That is  if a level 1 agent ucompleted level 1 by time t  then we
will have
Zt
v=Sout
A1(u)(Wt
u;1)v for allv2A1(u):
To be more precise  let us dene Zt;1= (Zt;1
v)v2Vby
Zt;1
v:=(
Sout
u)vifv2A1(u) for someu2U1anducompleted level 1 by time t
We will soon also dene Zt;n= (Zt;n
v)v2Vforn2  with the idea that it represents the known
output on all cells of level at most nfor which the simulation has completed. In particular  if
Zt;n
v6=;for somevandn  then it will be the case that Zt;n+1
v =Zt;n
v. Now xn2 and suppose
that we have dened Zt;1;:::;Zt;n 1and the two notions (reached and completed) for levels less
thann  in such a way that the above property holds { namely  if a level m2f1;:::;n 1gagentu
completed level mby timet  then the output is known on Am(u) at timetin the sense that
Zt;n 1
v =Zt;m
v6=;for allv2Am(u). Then  for a level nagentu2Un  we say that
ureached levelnby timetif every level n 1 agentu02Un 1\\An(u) has completed level
n 1 by timet.
The idea here is that if ureached level nby timet  then the output is known on An(u)\\An 1
at timet  and we may use this information to start generating the output on the remaining part
of the cell  namely  on An(u)nAn 1. That is  we use the simulation SV;U; withV=An(u)
U=An(u)\\An 1and=Zt;n 1. We thus say that
ucompleted levelnby timetif it reached level nby timetand the stopping time
Stime
An(u);An(u)\\An 1;Zt;n 1
has been reached on input Wt
u.
Putting this together leads to dening Zt;n= (Zt;n
An(u);An(u)\\An 1;Zt;n 1(Wt
u)vifv2AnnAn 1andUn(v)=ufor someu
anducompleted level nby timet
The outputZt= (Zt
v)v2Vat timetis then dened by Zt
v:= limn!1Zt;n
v. That is  to determine Zt
v
we rst look at the level nat whichventers the cell process  and then consider the level nagentu
responsible for generating the output on the cell of vinAn. Ifuhas indeed completed level nby
timet  then we read the value of Zt
vfrom the output of the corresponding simulation.
Finally  we are ready to dene ( Lt+1
u)u2V. As mentioned  these numbers are always zero for
non-agents  i.e.  we set Lt+1
u:= 0 foru =2U1[U2[ . Suppose now that u2Unfor somen1.
We say that uisactive at time t+ 1 if it has reached  but has not completed  level nby timet.
Thus  ifuis active at time t+ 1  then ideally it would like to read a bit at that time  and indeed it
FINITELY DEPENDENT PROCESSES ARE FINITARY 21
may do so as long as there is an available bit at u+t+ 1 (recall that umay only read a bit from
locationu+t+ 1 at time t+ 1). This leads us to dene
Lt+1
u:=1 
uis active at time t+ 1 andMt
u+t+1<jYbits
u+t+1j
: (13)
This completes the inductive denition of Lt+1
ufor allt0 andu2V.
We note that  by construction  once the output at a vertex is determined at some time  it remains
unchanged at future times { that is  if Zt
v6=;for somevandt  thenZt+1
v=Zt
v. Denote by
Tv:= minft0 :Zt
v6=;g
the time at which the output at vis rst determined. The outputZ= (Zv)v2Vis then given by
Zv:= lim
t!1Zt
v=(
ZTvvifTv<1
; ifTv=1:
This completes the construction of the nitary factor.
We record for later use the following simple property of the construction.
Lemma 6.1. For anyt0  we have that (Wt
u;Zt
u)u2Vand(Lt+1
u)u2Vare measurable with respect
toYcell Yord (jYbits
vj)v2Vand(Ybits
v(i))v2V;1iMtv.
Proof. The proof by induction on tis straightforward from the denitions. 
6.3.Concluding Theorem 1.2. To conclude the proof of Theorem 1.2  we must establish two
properties of the above construction: that the output it produces has the desired distribution  and
that the output can be determined from ( Ybits;Ycell;Yord) in a nitary manner. The former is
stated in the following proposition whose proof is postponed to Section 6.4 below.
Proposition 6.2. The output Zhas the same distribution as X.
Proof of Theorem 1.2. The random eld Zis clearly a deterministic and  -equivariant function '
of (Ybits;Ycell;Yord). Thus  in light of Proposition 6.2  we must only show that 'is nitary. Since
Tvis almost surely nite (as Zv6=;almost surely by Proposition 6.2)  it suces to show that Zt
is nitary for every t0. This follows rather easily from the construction. To see this  we explain
how to determine the value of Zt
vin a nitary manner.
We begin by nding the level nin whichventers the cell process  i.e.  v2AnnAn 1  and then
nding the cell An(v) ofvinAn. SinceAis a nitary factor of Ycell  this may be done in a nitary
manner. Next  we nd the level nagentUn(v) associated to the cell An(v). Since this is just the
-minimal element in An(v)nAn 1  and since the order is a nitary factor of Yord  this may
also be done in a nitary manner.
Let us suppose by induction that all steps of the construction up to time t 1 are nitary. Thus
recalling the denition of active  we see that  for any vertex w  we may determine in a nitary
manner whether wis active at time t. Since successors/predecessors in may be found in a
nitary manner from Yord  it then also follows that Lt
wmay be determined in a nitary manner.
Using again that successors/predecessors may be found in a nitary manner  we conclude that Wt
w
may be found in a nitary manner.
We would now like to check whether ucompleted level nby timet  and if so  nd the output
value. To check this  we start at level 1 and work our way up to level n. Thus  we rst nd all
level 1 agents which are contained in An(v) (since the cell process and total order are nitary  this
can be done in a nitary manner). Next  for each such agent u  we check whether ucompleted
level 1 by time t. Recall that the simulation SA1(u)depends on the cell A1(u) and on the order
induced byonA1(u). Since the input word Wt
u  the cell process and the order are nitary  we
see that we may determine whether ucompleted level 1 by time tin a nitary manner  and if so
also determine the output Zt;1
wfor allw2A1(u) in a nitary manner.
22 YINON SPINKA
We now proceed to the next levels. Consider some level 2 mn. We again begin by nding
all levelmagents which are contained in An(v). For each such agent u  we check whether ureached
levelmby timet. For this we must check whether the level m 1 agents in Am(u) completed level
m 1 by timet  which  by induction  may be done in a nitary manner. If ureached level mby
timet  we then check whether ucompleted level mby timet. Similarly to before  the simulation
SAm(u);Am(u)\\Am 1;Zt;m 1depends on Am(u) Am(u)nAm 1  the order induced by onAm(u)  and
on (Zt;m 1
w )w2Am(u)\\Am 1. Since the input word Wt
see that we may determine whether ucompleted level mby timetin a nitary manner  and if so
also determine the output Zt;m
wfor allw2Am(u) in a nitary manner.
Continuing up to level m=nyields that Zt;n
vmay be determined in a nitary manner. Since n
is the level in which venters the cell process  we have by denition that Zt
v=Zt;n
v. Thus Zt
vmay
be determined in a nitary manner  as required. 
6.4.The output has the correct distribution. In this section  we prove Proposition 6.2. The
proof is split up into several steps. The rst step is the following lemma which formalizes the
intuition that the simulations used in the construction are `fed' independent unbiased bits.
Lemma 6.3. Let!2f0;1gNconsist of a sequence of independent unbiased bits. Let (!u)u2Vbe a
collection of i.i.d. copies of !  independent of (Ybits;Ycell;Yord). Then  for any t0  conditioned
on(Ycell;Yord)  the collection (Wt
u!u)u2Vhas the same distribution as (!u)u2V.
Proof. We prove the statement by induction on t  takingt= 1 as a trivial base case (where
W 1
u:=;for allu2V). Suppose now that we know it for some t 1 and let us show it for
t+1. Recall that Wt+1
u=Wt
u^Wt+1
u. Thus  we need to show that  conditioned on ( Ycell;Yord)  the
collection (Wt
u!u)u2Vhas the same distribution as ( !u)u2V. To this end  it suces to show
that  conditioned on ( Ycell;Yord)  the collections ( Wt
u)u2Vand ( ^Wt+1
u!u)u2Vare independent and
that the conditional distribution of the latter is that of ( !u)u2V. Indeed  the induction hypothesis
will then yield the desired result.
We may restate our goal as showing that  conditioned on ( Ycell;Yord) and (Wt
u)u2V  the collection
(^Wt+1
u!u)u2Vhas the same distribution as ( !u)u2V. LetFbe the-algebra generated by Ycell
Yord  (jYbits
vj)v2Vand (Ybits
v(i))v2V;1iMtv. By Lemma 6.1
(Wt
u)u2V andQ:=
u:^Wt+1
u6=;
=
u:Lt+1
areF-measurable. Since ( !u)u2Vis independent of ( Ybits;Ycell;Yord) and hence also of F  it suces
to show that  conditioned on F  the random variables ( ^Wt+1
u)u2Qare independent unbiased bits.
Note thatQis the set of vertices (agents) that read a bit at time t+ 1  that
Q0:=fv:Mt+1
v>Mt
vg
is the set of vertices from which a bit was read at time t+ 1  and that u7!u+t+ 1 denes a
F-measurable bijection from QtoQ0. Recall also that ^Wt+1
u=Ybits
u+t+1(Mt+1
u+t+1) foru2Q. Thus
it suces to show that  conditioned on F  the random variables ( Ybits
v(Mt+1
v))v2Q0are independent
unbiased bits.
Indeed  since Q0and (Mt+1
v)v2Q0areF-measurable by Lemma 6.1  since Mt+1
v> Mt
vfor all
v2Q0  and sinceYbitsis an i.i.d. process that is independent of ( Ycell;Yord)  we see that the random
variables (Ybits
v))v2Q0are conditionally independent given F  and that  for any v2Q0  the
conditional distribution of Ybits
v) is the same as the distribution of Ybits
v) givenjYbits
vj.
SinceYbits
vis a random number of random bits  the latter is the distribution of an unbiased bit
and the proof is complete. 
FINITELY DEPENDENT PROCESSES ARE FINITARY 23
We will use the above lemma for xed tand then let ttend to innity. In doing so  we will
encounter the limiting word W1
u:= limt!1Wt
u. SinceWt+1
uextendsWt
u  this limit is well-dened
and is a word inf0;1gorf0;1gN(we will see that it is in fact a nite word almost surely).
The next step towards proving Proposition 6.2 is to show that the output at every vertex vis
eventually determined  i.e.  that Zv6=;(equivalently  Tv<1) almost surely. For this  we rst
show that every vertex is eventually inactive.
Lemma 6.4. Every vertex is almost surely eventually inactive. That is  for any u2V  there
almost surely exists a nite t0such thatuis not active at any time tt0.
Proof. Dene
(u;v) :=1fv=u+tandLtu=1 for some t0g:
Note that (u;v) indicates whether uread a bit located at v. Since an agent may read at most one
bit from any location   (u;v) also represents the number of bits read by ufrom location v. Thus
recalling thatjWt
u  we have
v (u;v) =1X
t=0Lt
u=jW1
uj andX
u (u;v) =1X
v t= lim
t!1Mt
v=:M1
v:
The left-hand side describes the number of bits read bya given site u  while the right-hand side
describes the number of bits read from a given site v. The mass-transport principle (4) tells us
that these quantities are the same in expectation:
EjW1
uj=EM1
v: (14)
LetEube the event that uis active at innitely many times t. We wish to show that P(Eu) = 0.
Note that  by (13)  the event Euis contained in the event that for all but nitely many t0  all
bits at location u+thave been read by time t  i.e.
Eu
u+tjYbits
u+tjfor all suciently large t
M1
u+t=jYbits
;
where the equality follows from the fact that Mt
vM1
v jYbits
vjfor allv2Vandt0.
Suppose now that P(Eu)>0. Then by ergodicity  almost surely  Ewoccurs for some w2V
and in particular  there almost surely exists w2Vsuch thatM1
w+i=jYbits
w+ijfor alli0. Since
fw+igi2Z=Valmost surely  it follows by  -invariance that M1
v=jYbits
vjfor allv2Valmost
surely. Thus  by (14)
uj=EjYbits
vj: (15)
That is  the expected number of bits read by each site is precisely the expected number of available
bits per site. It remains to show that this is impossible.
Dene
(u;v) :=(jW1
uj
jAn(u)nAn 1jifv2AnnAn 1andUn(v) =u
Recall that Un(v) is the level nagent associated to the cell An(v). Since a level nagentuis
responsible for simulating the output on An(u)nAn 1and does so via the input word W1
u  we may
think of(u;v) as follows: every level nagentuequally divides a total `cost' of jW1
ujamong the
vertices it `serviced'. Observe that
v(u;v) =jW1
u(u;v) =jW1
UNv(v)j
jANv(v)nANv 1j;
24 YINON SPINKA
whereNvis the level at which ventered the cell process  i.e.  v2ANvnANv 1  and where we used
thatAn(v) =An(u) whenever Un(v) =u. Thus  by (15) and the mass-transport principle (4)
EjYbits
vj=E'jW1
jANv(v)nANv 1j#
: (16)
This relates the expected number of available bits per site to the length of the input words used by
the simulations. We would like to reach a contradiction to the fact that there are many available
bits and that the simulation is ecient.
Suppose that uis a levelnagent. It is straightforward from the denitions that the stopping
time Stime
An(u);An(u)\\An 1;Zt;n 1is not reached on any prex of Wt
uthat is notWt
uitself (it may or may
not be reached on the entire word Wt
u). It therefore follows from Lemma 6.3 that  conditioned on
(Ycell;Yord)
ujis stochastically dominated by Stime
An(u);An(u)\\An 1;Zt;n 1(!)1(ureached level nby timet);
where!2f0;1gNconsists of a sequence of independent unbiased bits  independent of Y. Note
that  ifureached level nby timet  thenZt;n 1coincides with ZonAn(u)\\An 1. Thus  taking
expectations and t!1   we obtain that
jW1
ujjYcell;Yordi
Eh
An(u);An(u)\\An 1;Z(!)jYcell;Yordi
1(ueventually reached level n):
Hence  by (12) and (3)  on the event that u2Un  we have
2 +(
HX(A1(u)) if n= 1
jAn(u)nAn 1jlogjSjifn2;
where we denote HX(V) :=H(XV) for a nite set VV. Therefore  by (2) and the choice of 
E'jW1
jANv(v)nANv 1jjYcell;Yord#
(h(X) ++ 2)1E+ (logjSj+ 2)1Ec;
whereEis the event that Nv= 1 andj@A1(v)jjA1(v)j. Thus  by (16)
vj(h(X) ++ 2)P(E) + (logjSj+ 2)P(Ec):
Using (9) and (10)  we see that EjYbits
vj<h(X) + 3  which contradicts (8). We therefore conclude
thatP(Eu) = 0 as required. 
We are now ready to show that the output at every vertex is eventually determined.
Lemma 6.5. For anyv2V  we have that Tv<1almost surely.
Proof. SinceAnalmost surely increases to V  it suces to show that P(Zv=;andv2An) = 0 for
alln1. We prove this by induction on n  takingn= 0 as a trivial base case by setting A0:=;.
Letn1 and suppose that P(Zv=;andv2An 1) = 0. By  -invariance  we actually have that
P(Zw=;for somew2An 1) = 0. We may thus assume that Zw6=;for allw2An 1. Suppose
now thatZv=;andv2An. Letube the level nagentUn(v) associated to the cell An(v).
Observe that  by the denition of ZvandZt;n
v  we have that  for all t0 udid not complete
levelnby timet. On the other hand  since Zw6=;for allw2An 1  there exists a nite t00
such thatuhas reached level nby timet0. It follows that uis active at time tfor everyt>t 0. By
Lemma 6.4  almost surely  no vertex is active at innitely many times  thus completing the proof
thatP(Zv=;andv2An) = 0. 
Now that we have established that the output at every vertex is eventually determined  it remains
to show that the distribution of the output is the correct one  namely  that of X. The following
immediately implies Proposition 6.2.
FINITELY DEPENDENT PROCESSES ARE FINITARY 25
Proposition 6.6. Conditioned on (Ycell;Yord) Zalmost surely has the same distribution as X
where we regard Xas independent of (Ycell;Yord).
Proof. Throughout the proof  we regard Xas independent of YcellandYord. We also condition
on (Ycell;Yord) throughout the entire proof  without explicitly mentioning this. In particular  any
statement about distributions or independence should be understood as conditional on ( Ycell;Yord).
Since every nite subset of Vis almost surely contained in some cell of the cell process  it suces
to show that  for any n1 and any cellCofAn ZChas the same distribution as XC. We prove
this by induction on n  takingn= 0 as a trivial base case (where A0:=;).
Suppose now that n1. LetCbe a cell of Anand denoteC0:=C\\An 1. We will show that
ZC0d=XC0 (17)
P(ZCnC02jZC0=) =P(XCnC02jXC0=) for any feasible 2SC0: (18)
By feasible   we mean that P(XC0=)>0. The desired equality in distribution ZCd=XCfollows
immediately from (17) and (18). Both parts require some type of independence  which we now
establish.
Let!2f0;1gNconsist of a sequence of independent unbiased bits. Let ( !u)u2Vbe a collection
of i.i.d. copies of !  independent of Ybits. By Lemma 6.3  for any t0  (Wt
u!u)uhas the same
distribution as ( !u)u. Taking the limit as t!1   we see that ( W1
u!u)ualso has the same
distribution as ( !u)u.
Observe that  by construction  if Cis some cell of the cell process  then Zt
Cis a function of
u)u2C. Taking the limit as t!1   it follows that ZCis a function of ( W1
u)u2C. It also follows
from the denition of Zand the fact that Tv<1for allv  thatZis unchanged by concatenating
any word to any W1
u. In particular  ZCis also a function of ( W1
u!u)u2C.
We now show (17). To this end  let C1;:::;Cmbe the cells in An 1that are contained in C
so thatC0=C1[[C m. By the induction hypothesis  ZCjd=XCjfor every 1jm. Since
Ais a cell process  we have that dist( Cj;Cj0)>1 for 1j < j0m. Hence  using that Xis
1-dependent  we see that fXCjg1jmare independent. Thus  it remains to show that fZCjg1jm
are also independent. Since ZCjis a function of ( W1
u!u)u2Cj  this follows from the fact that
fW1
u!ugu2Vare independent.
To complete the proof  it remains to show (18). Let wbe the agent associated to Cand recall
thatw2CnC0and thatC=An(w). Note that
ZCnC0=Sout
C;C0;Z(W1
w) =Sout
w!w):
Recall that SC;C0;Zis shorthand for SCnC0;C0;ZC0. SinceW1
w!wis independent of ( W1
u!u)u6=w
and hence also of ZC0  we conclude that the conditional distribution of ZCnC0given that ZC0=
is equal to the distribution of Sout
CnC0;C0;(W1
w!w). Thus  using that W1
w!whas the same
distribution as !  we see that the distribution in question is that of Sout
CnC0;C0;(!)  which is by
denition P(XCnC02jXC0=)  as required. 
7.Remarks and open problems
Remark 2. We have given the details of the proof of Theorem 1.2. Theorem 1.1 follows the same
lines of proof  with minor modications  all of which are in fact simplications.
To obtain a proof of Theorem 1.1 with the least modications to the existing proof  we may
replace the random total order constructed in Lemma 5.1 with the total order induced by an i.i.d.
process consisting of uniform [0 ;1] random variables. Using this order in the proof of Lemma 5.2
yields a random total order with the same properties as in Lemma 5.2 (except for the bound on
the entropy of the i.i.d. process). When Xis nite-valued  the proof then goes through with no
26 YINON SPINKA
further modications. Otherwise  we let Ybits
vconsist of innitely many independent random bits
and then the proof goes through after an additional modication to the proof of Lemma 6.4 (which
relied on the fact that the entropy of Xis nite to deduce a bound on the expected number of bits
used by the simulation; instead we only rely on the fact that  almost surely  the simulation uses
only nitely many bits; see Theorem 3.1).
It is instructive to note that a shorter and conceptually simpler proof exists when one does
not need to worry about the entropy of the i.i.d. process. This is essentially what is described in
\\constructing a nitary coding' in Section 2. One way to implement the described coding would
be to simply replace utwithueverywhere in the construction in Section 6. That is  instead
of having an agent utry to read an unused bit from location u+tat timet  it always reads bits
located atu. Since we may place an innite sequence of bits at every vertex  it will never run out of
available bits. In this way  there is no \\moving around' of bits from one location to another. This
could be made conceptually even simpler if instead of using simulations from random bits to obtain
samples of distributions as they are needed  from the start  each Ybits
vis a collection ( WV;U;)V;U;
of independent random variables having distribution P(XV2jXU=) for all nite U;VV
and2SU. Either way  a nice feature of this construction is that the coding radius depends only
on the cell process Aconstructed in Section 4. Namely  the coding radius for determining X0is at
most the maximum of min fr0 :A(0)r(0gg  whereA(0) is the cell of 0inAminfn1:02Ang
the coding radius for determining the cell A(0) and the coding radius for determining the cell
process onA(0)  i.e.  (An\\A(0))n1. We elaborate on this in the next remark.
Remark 3. Our main theorems give no information about the coding radius beyond its almost-sure
niteness. However  some information about the coding radius may be extracted from the proof
given here. Specically  Theorem 1.1 may be enhanced to give a universal bound on the tail of the
coding radius for any xed graph and nite-dependence range. More precisely  for any transitive
amenable graph Gand any integer k1  there exists a sequence ( cn)1
n=1tending to zero such that
anyk-dependent invariant random eld XonGis a nitary factor of an i.i.d. process with a coding
radiusRsatisfying that P(Rn)cnfor alln. The sequence ( cn) depends only on the graph G
and on the parameter k  and not on the group   nor on the random eld X. Indeed  the sequence
(cn) is governed by the properties of the cell process (see the last part of the previous remark). In
particular  for many concrete choices of G(andk)  an explicit sequence ( cn) may be found.
To illustrate this in a simple setting  let us show that for G=Zandk= 1  one may take
cn= 8=n. In this case  instead of using the construction given in Section 4  it is simpler to consider
the nitary cell process Agiven byAn:=B1[[Bn  where (Bn)n1are independent random
subsets of Z  each being an independent Bernoulli percolation with parameter 1 =2. Then the level
N:= minfn: 02Angat which 0 enters the cell process is a geometric random variable with
parameter 1 =2  conditioned on which  the lengths L:= minfm1 :m =2ANgof the cell
of 0 inANin the positive/negative directions are (independent) geometric random variables with
parameter 2 N  and the coding radius Rfor determining X0is bounded by max fL+;L g. Thus
P(R>r )2E
(1 2 N)r
21X
n=12 ne r2 n4
r1X
m= 12me 2m8
r;
where we used the substitution n=blog2rc m.
We remark that if one would like to simultaneously control also the entropy of the i.i.d. process
(as in Theorem 1.2)  then it is plausible that this can be done by allowing ( cn) to depend on the
entropy gap (and perhaps onjSj)  but we did not pursue this.
Remark 4. We do not know whether condition (1) is necessary as stated in Theorem 1.2  however
as we now explain  some condition of this form is needed (i.e.  the condition cannot be completely
dropped). Let Gbe an innite transitive graph on vertex set Vand letHbe a nite transitive
graph onm2 vertices. Let G0be the graph obtained by replacing each vertex of Gwith a copy
FINITELY DEPENDENT PROCESSES ARE FINITARY 27
ofH{ that is  the vertex set of G0isVf1;:::;mg  and (u;i) and (v;j) are adjacent in G0if and
only ifuandvare adjacent in G  oru=vandiandjare adjacent in H. Any graph G0obtained
in this manner is transitive  but fails to satisfy (1). Indeed  the balls of radius 2 (or even 1 when
His a complete graph) around ( v;i) and (v;j) coincide. A simple case to have in mind is when
G=ZandHconsists of an edge on two vertices  so that the vertices of G0areZf0;1gand there
is an edge between ( u;i) and (v;j) if and only ifju vj1.
LetG0be any graph as above and let ( Wv)v2Vbe independent uniform random variables on
f1;:::;mg. Consider the random eld XonG0dened byX(v;i):=1fWv=ig. It is clear that Xis 2-
dependent and Aut( G0)-invariant. We claim that Xis not a Aut( G0)-factor of any i.i.d. process Yon
G0whose single-site distribution has at least one atom (in particular  Ycannot have nite entropy).
Indeed  for any such process Y  the event Y(v;1)==Y(v;m)has positive probability  and on
this event there is no Aut( G0)-equivariant way to distinguish between ( v;1);:::; (v;m). That is
any Aut(G0)-equivariant function ':TVf1;:::;mg!f0;1gVf1;:::;mgmust satisfy '(y)(v;1)==
'(y)(v;m)whenevery2TVf1;:::;mgis such that y(v;1)==y(v;m). In particular  the event
'(Y)(v;1)=='(Y)(v;m)has positive probability  and hence  '(Y) cannot have the same
distribution as X.
We remark that there are transitive subgroups   of Aut( G0) for which the above obstruction
does not exist. For example  let   be the subgroup of Aut( G0) generated by Aut( G) and Aut(H)
both of which are naturally embedded in Aut( G0). Simple modications to the proofs of Lemma 5.1
and Lemma 5.2 yield a  -invariant random total order with the desired properties. The rest of the
proof then goes through unchanged showing that our main result holds in this case: any nitely
dependent  -invariant process on G0is a nitary  -factor of an i.i.d. process with slightly larger
entropy. We believe that our main result holds in many similar situations  where condition (1) is
replaced by a suitable condition on  . We did not pursue this direction.
Open problems.
(1) One may wonder about the situation on non-amenable graphs such as a regular tree (of
degree at least three). Namely  is every automorphism-invariant nitely dependent process
on a tree a nitary factor of an i.i.d. process? In fact  even the more fundamental question
of whether such a process is a factor of i.i.d. (without the nitary condition) is still open;
see [21  Question 2.2]. The same questions may be asked on any transitive non-amenable
graph.
(2) Does there exist a stationary nitely dependent process on Z(or  more generally  on some
transitive amenable graph) that cannot be expressed as a nitary factor of an i.i.d. pro-
cess with nite expected coding radius? As mentioned  the 1-dependent 4-coloring and
2-dependent 3-coloring of [13] are believed to be examples of such processes  but this is still
unproved.
(3) A nitary isomorphism is a nitary factor that is invertible and whose inverse is also nitary.
Somorodinky [24] showed that every stationary nitely dependent process on Zis nitarily
isomorphic to an i.i.d. process. Is this true in higher dimensions? Namely  is every stationary
nitely dependent process on Zdnitarily isomorphic to an i.i.d. process?
References
[1] Jon Aaronson  David Gilat  and Michael Keane  On the structure of 1-dependent Markov chains   Journal of
Theoretical Probability 5(1992)  no. 3  545{561.
[2] Jon Aaronson  David Gilat  Michael Keane  and Vincent de Valk  An algebraic construction of a class of one-
dependent processes   The annals of probability (1989)  128{143.
[3] Itai Benjamini  Russell Lyons  Yuval Peres  and Oded Schramm  Group-invariant percolation on graphs   Geo-
metric & Functional Analysis GAFA 9(1999)  no. 1  29{66.
[4] Jacob van den Berg and Jerey E Steif  On the existence and nonexistence of nitary codings for a class of
random elds   Annals of probability (1999)  1501{1522.
28 YINON SPINKA
[5] Robert M Burton  Marc Goulet  Ronald Meester  et al.  On 1-dependent processes and k-block factors   The
Annals of Probability 21(1993)  no. 4  2157{2168.
[6] Olle H aggstr om  Innite clusters in dependent automorphism invariant percolation on trees   The Annals of
Probability (1997)  1423{1436.
[7] Matan Harel and Yinon Spinka  Finitary codings for the random-cluster model and other innite-range monotone
models   arXiv preprint arXiv:1808.02333 (2018).
[8] Nate Harvey  Alexander E Holroyd  Yuval Peres  and Dan Romik  Universal nitary codes with exponential tails
Proceedings of the London Mathematical Society 94(2006)  no. 2  475{496.
[9] Alexander E Holroyd  One-dependent coloring by nitary factors   Annales de l'institut henri poincar e  probabilit es
et statistiques  2017  pp. 753{765.
[10] Alexander E Holroyd  Tom Hutchcroft  and Avi Levy  Mallows permutations and nite dependence   To appear
in Annals of Probability  arXiv:1706.09526 (2017).
[11] Alexander E Holroyd  Tom Hutchcroft  and Avi Levy  Finitely dependent cycle coloring   Electronic Communi-
cations in Probability 23(2018).
[12] Alexander E Holroyd and Thomas M Liggett  Symmetric 1-dependent colorings of the integers   Electronic Com-
munications in Probability 20(2015).
[13] Alexander E Holroyd and Thomas M Liggett  Finitely dependent coloring   Forum of mathematics  pi  2016.
[14] Alexander E Holroyd  Oded Schramm  and David B Wilson  Finitary coloring   The Annals of Probability 45
(2017)  no. 5  2867{2898.
[15] IA Ibragimov and YV Linnik  Independent and stationarily connected variables   Izdat. Nauka  Moscow (1965).
[16] IA Ibragimov and YV Linnik  Independent and stationary sequences of random variables   Wolters  Noordho
Pub. (1971).
[17] Michael Keane and Meir Smorodinsky  A class of nitary codes   Israel Journal of Mathematics 26(1977)  no. 3-4
352{371.
[18] Michael Keane and Meir Smorodinsky  Bernoulli schemes of the same entropy are nitarily isomorphic   Annals
of Mathematics 109(1979)  no. 2  397{406.
[19] Donald Knuth and Andrew Yao  The complexity of nonuniform random number generation   Algorithm and
Complexity  New Directions and Results (1976)  357{428.
[20] Nathan Linial  Distributive graph algorithms global solutions from local data   Foundations of computer science
1987.  28th annual symposium on  1987  pp. 331{335.
[21] Russell Lyons  Factors of IID on trees   Combinatorics  Probability and Computing 26(2017)  no. 2  285{300.
[22] Russell Lyons and Yuval Peres  Probability on trees and networks   Vol. 42  Cambridge University Press  2017.
[23] Moni Naor  A lower bound on probabilistic algorithms for distributive ring coloring   SIAM Journal on Discrete
Mathematics 4(1991)  no. 3  409{412.
[24] Meir Smorodinsky  Finitary isomorphism of m-dependent processes   Symbolic dynamics and its applications
(1992)  373{376.
[25] Yinon Spinka  Finitary coding for the sub-critical Ising model with nite expected coding volume   To appear in
Electronic Journal of Probability  arXiv:1801.02529 (2018).
University of British Columbia. Department of Mathematics. Vancouver  BC V6T 1Z2  Canada.
E-mail address :yinon@math.ubc.ca
Abstract. Weshowthatanyﬁnitelydependentinvariantprocessonatransitiveamenablegraph
isaﬁnitaryfactorofani.i.d.process. Withanadditionalassumptiononthegeometryofthegraph
0
namelythatnotwoballswithdiﬀerentcentersareidentical wefurthershowthatthei.i.d.process
2
may be taken to have entropy arbitrarily close to that of the ﬁnitely dependent process. As an
application  we give an aﬃrmative answer to a question of Holroyd [9].
n
a
J
1. Introduction
9
1
Consider a random process X = (Xv)v∈V living on the vertex set V of an inﬁnite graph G. The
] processX issaidtobeﬁnitelydependent ifitsrestrictionstosetswhicharesuﬃcientlyseparated(at
R
least some ﬁxed distance apart) are independent. A trivial example of a ﬁnitely dependent process
P
. is a process Y = (Yv)v∈V in which all random variables are independent. A natural question is then
h how close is a ﬁnitely dependent process to such an independent process? Before addressing this
t
a question  we ﬁrst observe that “local functions” of an independent process Y are always ﬁnitely
m
dependent. Thatis ifX isobtainedfromY bycomputingeachX asafunctiononlyoftherandom
[ variables Y for which u is at a uniformly bounded distance from v  then X is ﬁnitely dependent.
u
Suppose now that G is transitive and henceforth restrict attention to processes X which are
v invariant under all automorphisms of G (or under a transitive subgroup of automorphisms). In
3 particular  the independent process Y considered above must now be an i.i.d. process – that is  in
additiontobeingindependent the{Y } arealsoidenticallydistributed. IfX isobtainedfromY by
1 v v
0 applying the same local function at each vertex v (i.e.  the function applied at u is the composition
0 of the function applied at v with any automorphism taking u to v)  then X is said to be a block
1 factor ofY. Thus  blockfactorsofi.i.d.processesprovidearecipeforconstructinginvariantﬁnitely
0 dependent processes.
Itwasalong-standingopenproblem[15 16]todeterminewhetherblockfactorsofi.i.d.processes
: are the only (invariant) ﬁnitely dependent processes on Z  until ﬁnally an example was given by
Burton–Goulet–Meester[5]ofa1-dependentprocesswhichisnotablockfactorofanyi.i.d.process.
i
r of i.i.d. processes and ﬁnitely dependent processes – no proper coloring of Z is a block factor of an
i.i.d. process  but ﬁnitely dependent proper colorings exist.
Thus  it is not true that every ﬁnitely dependent process is a block factor of an i.i.d. process.
In other words  given a ﬁnitely dependent process X  one cannot in general hope to ﬁnd an i.i.d.
processY andaninvariantruleforcomputingX fromY  whichallowstodeterminethevalueofX
by looking at Y on a ﬁxed-size window around v. The goal of this paper is to show the “next best
thing” – namely  that it is possible to determine X by looking at Y on a variable-sized window
around v  where the size of the window  though always ﬁnite  may vary according to the input Y.
We say that X is a ﬁnitary factor of Y if there is an invariant rule which allows to compute the
value of X at any vertex v by only looking at variables Y for which u is within a certain ﬁnite
but random  distance from v (formal deﬁnitions are given below). Thus  a block factor is a ﬁnitary
factor in which the required distance is not only ﬁnite  but is determistically bounded by some
constant. The main contribution of this paper is to prove that every ﬁnitely dependent process is a
Date: January 22  2020.
2 YINONSPINKA
ﬁnitary factor of an i.i.d. process. This result holds on any amenable graph G. When it is further
assumed that no two balls in G with diﬀerent centers are identical  it is also possible to control the
entropy of the i.i.d. process involved  and the result becomes that every ﬁnitely dependent process
is a ﬁnitary factor of an i.i.d. process with only slightly larger entropy.
1.1. Deﬁnitions and main result. Let V be a countable set  let G be a graph on vertex set V
and let Γ be a group acting on V. A random ﬁeld (or random process) on G is a collection of
random variables X = (Xv)v∈V indexed by the vertices of G and deﬁned on a common probability
space. We say that X is Γ-invariant if its distribution is not aﬀected by the action of Γ  i.e.  if
(Xγv)v∈V has the same distribution as X for any γ ∈ Γ. We say that X is k-dependent if (Xu)u∈U
and (X ) are independent for any two sets U V ⊂ V such that dist(u v) > k for all u ∈ U and
v v∈V
v ∈ V. We say that X is ﬁnitely dependent if it is k-dependent for some ﬁnite k.
Suppose now that G is a transitive locally ﬁnite graph and that Γ is a subgroup of the automor-
phism group of G. Let S and T be two measurable spaces  and let X = (Xv)v∈V and Y = (Yv)v∈V
be S-valued and T-valued Γ-invariant random ﬁelds. A coding from Y to X is a measurable func-
V V
tion ϕ: T → S that is Γ-equivariant  i.e.  commutes with the action of every element of Γ  and
satisﬁes that ϕ(Y) and X are identical in distribution. Such a coding is also called a factor map
from Y to X  and when such a coding exists  we say that X is a Γ-factor of Y.
SupposenowthatS andT arecountable. Let0 ∈ Vbeadistinguishedvertex. Thecoding radius
of ϕ at a point y ∈ TV  denoted by R(y)  is the minimal integer r ≥ 0 such that ϕ(y(cid:48)) = ϕ(y)
0 0
for all y(cid:48) ∈ TV which coincide with y on the ball of radius r around 0 in the graph-distance  i.e.
y(cid:48) = y for all v ∈ V such that dist(v 0) ≤ r. It may happen that no such r exists  in which case
v v
R(y) = ∞. Thus  associatedtoacodingisarandomvariableR = R(Y)whichdescribesthecoding
radius. While S will always be at most countable  we will allow T to be a larger space  in which
case the coding radius may be similarly deﬁned1. A coding is called ﬁnitary if R is almost surely
ﬁnite. When there exists a ﬁnitary coding from Y to X  we say that X is a ﬁnitary Γ-factor of Y.
Agraphissaidtobeamenable ifinf|∂V|/|V| = 0  wheretheinﬁmumisoverallﬁnitenon-empty
subsets V of V  and where ∂V denotes the edge-boundary of V.
Theorem 1.1. Let G be a transitive amenable graph and let Γ be a transitive group of automor-
phisms of G. Then any ﬁnitely dependent Γ-invariant random ﬁeld on G is a ﬁnitary Γ-factor of
entropy of the i.i.d. process used in the coding (see Section 3.1 for the deﬁnition of entropy). The
Λ (u) (cid:54)= Λ (v) for any distinct u v ∈ V and r ≥ 0  (1)
r r
where Λ (u) is the ball of radius r around u.
r
Theorem 1.2. Let G be a transitive amenable graph satisfying (1)  let Γ be a transitive group of
automorphisms of G  and let X be a ﬁnite-valued ﬁnitely dependent Γ-invariant random ﬁeld on
G. Then for any (cid:15) > 0 there exists an i.i.d. process Y with entropy h(Y) < h(X)+(cid:15) such that X
is a ﬁnitary Γ-factor of Y.
Letusmakesomeremarksabouthowthetwotheoremscomparetooneanother. InTheorem1.2
S is ﬁnite (note the assumption that X is ﬁnite-valued) and  in particular  X has ﬁnite entropy
while in Theorem 1.1  S may be countable and X may have inﬁnite entropy. In Theorem 1.2  Y
has ﬁnite entropy so that T is countable  whereas Theorem 1.1 may require a larger space T for
1We will only be concerned with spaces T which are ﬁnite  countable or of the form T ×T ×··· for ﬁnite sets
1 2
(T ). In the latter case  the coding radius is the smallest r for which there exists n such that ϕ(y(cid:48)) =ϕ(y) for all
i 0 0
y(cid:48) having the property that y(cid:48) =y for all (v i) such that dist(v 0)≤r and 1≤i≤n.
v i v i
the conclusion to hold. In fact  in the absence of condition (1)  even when S is ﬁnite  it might not
be possible to have T countable (see Remark 4 in Section 7).
are isomorphic and ﬁnite dependence implies that the random ﬁeld is independent on diﬀerent
1.2. Discussion. Finite dependence and ﬁnitary factors have applications in computer science.
For example  if the graph G represents machines in a network and the random ﬁeld X represents a
commonplaninwhicheachmachinev isassignedaspeciﬁcroleX  thenﬁnitedependenceprovides
certain security beneﬁts in the face of an attacker (if someone gains access to some machines  they
learn nothing about the roles of far away machines  thereby conﬁning the security breach)  and
being a ﬁnitary factor of an i.i.d. process provides reliability (e.g.  no single point of failure) as
a common protocol  while using local randomness and communicating with ﬁnitely many other
The ﬁnitary coding properties of ﬁnitely dependent processes on G = Z  and in some cases on
G = Zd with d ≥ 2  have been studied in various contexts. We give a brief account of these works.
We are unaware of any works regarding ﬁnitary factors for ﬁnitely dependent processes on other
A result by Smorodinsky [24] shows that every stationary (i.e.  translation-invariant) ﬁnitely
dependent process on Z is ﬁnitarily isomorphic (a stronger notion than being a ﬁnitary factor) to
reﬂections)  but rather only for the group of translations. In this respect  Theorem 1.2 strengthens
thisresult(ifoneiscontentwithaﬁnitaryfactor ratherthanaﬁnitaryisomorphism) asitprovides
a ﬁnitary factor which is also reﬂection invariant whenever the ﬁnitely dependent process is such.
The proof in [24] is based on the so-called marker-ﬁller method of Keane and Smorodinsky [17 18].
as in [18] with some necessary modiﬁcations). Our proof is based on a diﬀerent approach; see
The question of whether there exists a stationary ﬁnitely dependent process which is not a block
factor of any i.i.d. process was raised by Ibragimov and Linnik [15 16] in 1965. Some progress on
this question was made [1 2] until it was ﬁnally resolved in 1993 by Burton–Goulet–Meester [5]
who gave the ﬁrst example of a stationary ﬁnitely dependent process which is not a block factor
of an i.i.d. process. In fact  they showed such an example in which the ﬁnitely dependent process
is a 1-dependent hidden-Markov process with ﬁnite energy. Some history about ﬁnitely dependent
is such [13]). Holroyd [9] subsequently showed that the 1-dependent 4-coloring is a ﬁnitary factor
writes in [9] that “one may attempt to apply our method to the 2-dependent 3-coloring  but we will
see that it meets a fundamental obstacle in this case”. Our result shows that either coloring is a
ﬁnitaryfactorofani.i.d.process(withslightlylargerentropy) answeringaﬃrmativelyquestion(iii)
in [9  Open problems]. In fact  the two colorings are also reﬂection invariant  and hence the ﬁnitary
factors may also be taken to commute with reﬂections. Related aspects of these two colorings were
In a subsequent paper [12]  Holroyd and Liggett constructed  for any q ≥ 4  a 1-dependent
q-coloring of Z which is invariant under translations and reﬂections and is also symmetric under
4 YINONSPINKA
permutations of the colors. It was shown in [10] that each of these colorings is a ﬁnitary factor
these colorings is a ﬁnitary factor of an i.i.d. process  where the factor map commutes with all
automorphisms of Z and the i.i.d. process has entropy only slightly larger than the coloring.
In [13]  Holroyd and Liggett also constructed stationary ﬁnitely dependent colorings of Zd with
d ≥ 2. More speciﬁcally  they constructed a stationary 1-dependent 4d-coloring of Zd and a sta-
tionary ﬁnitely dependent 4-coloring of Zd. However  unlike the above one-dimensional colorings
unknown whether there exists a ﬁnitely dependent coloring of Zd (d ≥ 2) which is invariant under
In the same paper [13]  Holroyd and Liggett also investigated the existence of stationary ﬁnitely
dependent processes on Z which are supported on a given shift of ﬁnite type. They showed that
for any reasonably non-degenerate (namely  nonlattice) shift of ﬁnite type S on Z  there exists a
stationary ﬁnitely dependent process which almost surely belongs to S. It was later shown [10]
that there exists such a process which is also a ﬁnitary factor of an i.i.d. process (with exponential
A block factor is precisely a ﬁnitary factor with bounded coding radius. Given a ﬁnitary factor
As we have mentioned  the 1-dependent 4-coloring of Z from [13]  which is not a block factor of
any i.i.d. process  was shown in [9] to be a ﬁnitary factor of an i.i.d. process. This ﬁnitary factor
was shown to have (at least) power-law tail on the coding radius  thus yielding a perhaps inﬁnite
expected coding radius. Holroyd–Hutchcroft–Levy [10] showed that there exist ﬁnitely dependent
coloringsofZwhichareﬁnitaryfactorsofi.i.d.processeswithexponentialtailonthecodingradius.
Indeed  they showed that such a k-dependent q-coloring exists when (k q) is either (1 5)  (2 4) or
(3 3). On the other hand  it is believed [9 10] that when (k q) is (1 4) or (2 3)  no k-dependent
q-coloringisaﬁnitaryfactorofani.i.d.processwithﬁniteexpectedcodingradius. Wementionthat
optimal tails for the coding radius of colorings of Zd (which are not necessarily ﬁnitely dependent)
and shifts of ﬁnite type on Z have been studied in [14].
Our main theorem gives no information about the coding radius beyond its almost-sure ﬁnite-
ness. Indeed  in light of the above discussion  it would seem that for an arbitrary ﬁnitely dependent
process on Z  there is not much hope to obtain a ﬁnitary factor with ﬁnite expected coding ra-
(see Remark 3). For example  for 1-dependent processes on Z  the ﬁnitary factor provided by
Theorem 1.1 has a coding radius R satisfying that P(R > r) ≤ 8/r for all r. In the particular case
To the best of our knowledge  beyond Smorodinsky’s result on Z  there do not exist any general
results on the ﬁnitary coding properties of ﬁnitely dependent processes. In particular  Theorem 1.1
and Theorem 1.2 are new for any amenable graph G other than Z  and also for G = Z in the
case when Γ is the full automorphism group of Z. Finally  we mention that the situation for non-
amenable graphs is still poorly understood – for example  on a regular tree (of degree at least
three)  it is not even known whether every automorphism-invariant ﬁnitely dependent process is a
(non-ﬁnitary) factor of an i.i.d. process [21].
1.3. Acknowledgments. IwouldliketothankOmerAngel NishantChandgotia TomMeyerovitch
suggesting to extend the result from Zd to transitive amenable graphs  and to Omer Angel for
1.4. Notation. Throughout the paper  G is always assumed to be an inﬁnite  transitive  lo-
cally ﬁnite  connected graph on a countable vertex set V  and Γ is a subgroup of the auto-
morphism group of G that acts transitively on V. The full automorphism group of G is de-
noted by Aut(G). The graph distance in G is denoted by dist(· ·). For sets U V ⊂ V  we
write dist(U V) := min dist(u v) and dist(u V) := dist({u} V). For r ≥ 0  denote
u∈U v∈V
V+r := {u ∈ V : dist(u V) ≤ r} and V−r := {u ∈ V : dist(u Vc) > r}. The ball of radius r
around v is denoted by Λ (v) := {v}+r. The neighborhood of V is N(V) := V+1 \\ V and the
edge-boundary of V is ∂V := {{u v} ∈ E(G) : v ∈ V u ∈/ V}.
All logarithms are taken to be in base 2 and we use the convention that 0log0 is 0.
2. Outline of proof
Our goal is to express X  a ﬁnitely dependent invariant process  as a ﬁnitary factor of an i.i.d.
process Y. The construction of the ﬁnitary coding involves the use of three sources of randomness:
total order on V. The ﬁrst of these three will simply be given by an i.i.d. process  denoted Ybits
whilethelattertwowillbeobtainedasﬁnitaryfactorsofdiﬀerenti.i.d.processes  denotedYcell and
Yord. In turn  Y will be a triplet Y = (Ybits Ycell Yord) consisting of three mutually independent
(1) Constructing a ﬁnitary coding: The basic and most essential part of the construction
is how to obtain X as a ﬁnitary factor of Y when Y is allowed to have inﬁnite entropy (this
is the setting of Theorem 1.1). In this case  Ybits and Yord may be taken to be uniform
random variables in [0 1]  and the total order may be taken to be the one induced by the
usual order on Yord.
(2) Controlling the entropy: The second part is how to control the entropy of the Ybits
that the vertices of G can be deterministically ordered in a Γ-invariant manner so that Yord
the lexicographical order is such an ordering when G is the graph Zd and Γ is the group of
(3) Constructing a random order: ThethirdpartishowtoallowforgraphsGandgroupsΓ
graphs  but it may also be the case for simpler graphs  such as Z or Zd  when Γ is the
Already the ﬁrst part above relies on the aforementioned cell process. Before introducing this
process  it is convenient to observe that it suﬃces to prove Theorem 1.1 and Theorem 1.2 for
1-dependent random ﬁelds. To see this  let G⊗k denote the graph on vertex set V in which two
vertices are adjacent if their distance in G is at most k. It is immediate from the deﬁnitions that
X is k-dependent as a random ﬁeld on G if and only if it is 1-dependent as a random ﬁeld on G⊗k.
Since any automorphism of G is also an automorphism of G⊗k  and since G⊗k is amenable and
satisﬁes (1) whenever G is such  we see that we may indeed assume that X is 1-dependent. This
assumption thoughnotatallessential isconvenientasitobviatestheneedtoworkwithadiﬀerent
A cell process is a random sequence A = (A  A  ...) of subsets of V satisfying the following
• A ⊂ A ⊂ A ⊂ ···.
1 2 3
6 YINONSPINKA
• A ∪A ∪··· = V.
• For each n ≥ 1  all connected components of A are ﬁnite.
We will obtain a cell process A as a ﬁnitary factor of an i.i.d. process Ycell with arbitrarily small
(1) Constructing a ﬁnitary coding: We construct a realization of X as a ﬁnitary factor of Y
in inﬁnitely many steps with the idea that at the end of step n ∈ {1 2 ...}  we will have deﬁned
X on the region A . In the ﬁrst step  we sample X on the set A – this is particularly simple as
n 1
the cells in A are at pairwise distance at least 2  and so  due to the 1-dependence assumption on
X  each cell in A can be sampled independently. Next  at each step n ∈ {2 3 ...}  we sample X
on the region A \\A   conditioned on the value of X on A   which has already been sampled
n n−1 n−1
in the previous steps – the key observation here is that  due again to the 1-dependence assumption
on X  the values of X on diﬀerent cells in A are conditionally independent – indeed  if {V } are
n i i
at pairwise distance at least 2 from one another  then for any sets U ⊂ V   given {X }   one has
i i Ui i
that {X } are mutually conditionally independent. Since all the cells of every A are ﬁnite  the
Vi i n
above steps can be carried out in a ﬁnitary manner – that is  the conditional distribution of X on a
given cell depends only on the previously sampled values within that cell. Since A increases to V
the value at every given vertex will eventually be sampled  thus producing a realization of X from
Y in a ﬁnitary and Γ-equivariant manner.
Let us be slightly more speciﬁc about the way in which we “sample X on a cell”. In each cell
in A   we distinguish a vertex by choosing the smallest element in the cell according to the order
given by Yord. We call these distinguished vertices level 1 agents. Similarly  for each n ≥ 2 and
each cell in A that is not contained in A   we select a level n agent in the cell by choosing the
n n−1
smallest element in the cell which is not in A . Note that the level n agents are obtained as a
n−1
ﬁnitary factor of (Ycell Yord). With the notion of agents  we may now say more precisely that  in
step n above  if C is a cell of A that is not contained in A   then we sample X on the region
C\\A (conditionally on the previously sampled values of X on C∩A ) by accessing a sample
n−1 n−1
of the desired distribution from the random variable Ybits  where u is the unique level n agent in C
using also the order induced by Yord on C \\A to break any symmetries which may be present
in the graph structure of this region (for example  if G = Z and Γ includes reﬂections  then when
C \\A is a symmetric interval around u  the ‘left’ and ‘right’ sides of u cannot be diﬀerentiated
in a Γ-equivariant way without some additional information; ordering all elements in the set is a
simple way to get rid of such problems). In this interpretation  we regard Ybits as consisting of
independent samples of P(X ∈ · | X = τ) for all ﬁnite U V ⊂ Zd and τ ∈ SV  most of which are
U V
to this is to place an inﬁnite sequence of random bits at each site  i.e.  Ybits ∈ {0 1}N  from which
mustplaceaﬁnite(perhapsrandom)numberofrandombitsateachsite  andsomehowstillbesure
a random variable W taking values in {0 1}∗  the set of ﬁnite words over {0 1}  and having the
property that  conditioned on the length |W| of the word  W is uniformly distributed on {0 1}|W|.
Suppose now that there exists a deterministic total order ≤ on V that is Γ-invariant in the sense
that u ≤ v implies that γu ≤ γv for any u v ∈ V and γ ∈ Γ. For instance  the lexicographical order
is such an order when G = Zd and Γ is the translation group (but there is clearly no such order
when Γ is the full automorphism group of Zd). For the purpose of this part of the proof outline
it is convenient to further suppose that every v ∈ V has a ≤-successor  which we denote by v+1
as is the case for the lexicographical order on Zd (in which case v +1 is simply v +(1 0 ... 0)).
The existence of such a deterministic order renders Yord unneeded  allowing us to focus now only
The idea is to associate to each possible distribution we might require  a “simulation” which
halts and outputs the sample. Such simulations may be done eﬃciently: the expected number of
We shall use such simulations whenever we “sample X on a cell”. If the cell C is large  then the
entropy of X on C is also large  and thus the above additive error is negligible. When the boundary
of the cell is small in comparison to the size of the cell  the average entropy of X on C per site will
also be close to h(X)  the entropy of X itself. Thus  it will be important that the cells in A are
a ﬁnite number of bits at each site (more precisely  we need that h(Ybits) < h(X)+(cid:15))  and even
construction that a simulation carried out by an agent u requires access to many more input bits
than are available in Ybits. To solve this  we must allow to “transfer” bits from one location to
whenever an agent u requires access to an additional bit (beyond those available in Ybits)  it may
look for an “unused” bit at u+1 (the ≤-successor of u). If there are no available unused bits at
u+1 at that time  it may then proceed to look at u+2  and so on.
related to the levels of the cell process. That is  it will no longer be the case that after step n of the
construction  we will have deﬁned X on the region A . Instead  at any step of the construction
diﬀerent regions of G will be at diﬀerent levels of the cell process. We will continue to use n to
denote the levels of the cell process  and will use t to denote the step of the construction (which
Thewaythisisdoneisasfollows. Initially  attimet = 0  alllevel1agentsaredeemedactive. An
agents contained in some level 2 cell C have completed  the level 2 agent associated to C becomes
to read bits in order to complete its associated simulation. In general  a level n agent becomes
active once all level n−1 agents contained in its associated cell have completed.
details simpler to write down: at time t  an agent u may read at most one bit  and this bit may
(3) Constructing a random order: For a general graph G and group Γ  there need not be a
deterministic total order of V that is Γ-invariant. Instead  we construct a random total order ≤
on V whose distribution is Γ-invariant. Moreover  we construct ≤ as a ﬁnitary factor of an i.i.d.
process Yord with arbitrarily small entropy. Here ﬁnitary means that the order induced on any
ﬁnite set of vertices is determined by a ﬁnite (random) subset of {Yvord}v∈V.
In addition  the constructed order ≤ will have the property that its order type is almost surely
thesameasthatofZ. Thatis almostsurely everyelementv hasasuccessorv+1andapredecessor
v−1  and {v+n}n∈Z = V. Though it does not follow from the above deﬁnition of ﬁnitary  it will
8 YINONSPINKA
isalsoaﬁnitaryproperty(i.e.  itisalmostsurelydeterminedbyaﬁnitesubsetof{Yvord}v∈V). Once
of a ﬁnitary cell process. In Section 5  we prove the existence of a ﬁnitary random total order
having the order type of Z. In Section 6  we give the construction of the ﬁnitary coding for the
ﬁnitely dependent process X. Finally  we end in Section 7 with some remarks and open problems.
3. Preliminaries
Recall that G is always assumed to be an inﬁnite  transitive  locally ﬁnite  connected graph on
a countable vertex set V  and that Γ is assumed to be a subgroup of the automorphism group of G
3.1. Entropy. The Shannon entropy of a discrete random variable Z is
(cid:88)
H(Z) := − P(Z = z)logP(Z = z)
z
where the sum is taken over z in the support of Z  or alternatively  we interpret 0log0 to be 0.
The measure-theoretic entropy (or Kolmogorov–Sinai entropy) of a Γ-invariant random ﬁeld X on
an amenable graph G is
H(X )
V
h(X) := inf .
V⊂Vﬁnite |V|
andnon-empty
A Følner sequence in G is a sequence (F )∞ of non-empty ﬁnite subsets of V such that
n n=1
|∂F |
lim = 0.
n→∞ |Fn|
It is well-known that the entropy of X may be computed along any Følner sequence:
h(X) = lim Fn for any Følner sequence (F )∞ in G.
n→∞ |Fn| n n=1
It follows that for any (cid:15) > 0 there exists δ > 0 such that
F ≤ h(X)+(cid:15) whenever F ⊂ V is non-empty and ﬁnite and |∂F| ≤ δ|F|. (2)
|F|
H(X | E)
F ≤ log|S| wheneverF⊂Visnon-emptyandﬁnite   (3)
|F| andE isaneventwithpositiveprobability
where S is the ﬁnite set in which X takes values. We note that if Y is an i.i.d. process  then its
entropy h(Y) is equal to the entropy of its single-site distribution H(Y ).
3.2. The mass-transport principle. For u v ∈ V  denote
Γ := {γ ∈ Γ : γu = v}.
u v
Note that Γ is the stabilizer of u. We say that Γ is unimodular if
u u
|Γ v| = |Γ u| for all u v ∈ V.
u u v v
It is well-known (see  e.g.  [22  Chapter 8]) that Γ is unimodular if and only if the following mass-
(cid:88) (cid:88)
f(u 0) = f(0 v) for any diagonally Γ-invariant function f: V2 → [0 ∞]. (4)
BydiagonallyΓ-invariant wemeanthatf(γu γv) = f(u v)forallu v ∈ Vandγ ∈ Γ. Wenotethe
well-known fact that  when G is amenable  any transitive group of automorphisms Γ is unimodular.
3.3. Simulating distributions from random bits. We shall use a result about the simulation
of a given distribution from unbiased random bits. Let µ be a distribution on a countable set Ω.
A simulation of µ is a pair S = (Stime Sout) of measurable functions Stime: {0 1}N → N∪{∞} and
Sout: {0 1}N → Ω with the properties:
• If ω is a sequence of independent unbiased bits  then Sout(ω) has distribution µ.
• If Stime(x) = n for some x ∈ {0 1}N and n ∈ N  then Stime(x(cid:48)) = n and Sout(x(cid:48)) = Sout(x)
for any x(cid:48) ∈ {0 1}N which coincides with x on {1 ... n}.
The ﬁrst property says that we can use S to simulate the desired distribution from random bits.
The second property may be interpreted as saying that Stime is a stopping time and that Sout is
adapted to the σ-algebra generated by {ω : 1 ≤ i ≤ Stime} – that is  the simulation reads one input
Theorem 3.1. Let Z be a discrete random variable. There exists a simulation S of Z from inde-
pendent unbiased bits ω satisfying that Stime(ω) < ∞ almost surely and EStime(ω) ≤ H(Z)+2.
KnuthandYaoshowthattheaboveisinfactoptimalinastrongsense(theyprovideasimulation
provide explicit exponential bounds on the probability that the simulation uses more than n bits
4. The cell process
Recall from Section 2 that a cell process is a random sequence A = (A  A  ...) of subsets
increasing to V and satisfying that the connected components of each A are ﬁnite. We may
identify a cell process A with the N-valued random ﬁeld (min{n ≥ 1 : v ∈ An})v∈V. In particular
when we say that A is Γ-invariant or a ﬁnitary factor  we mean that this latter process is such.
In this section  we show that ﬁnitary cell processes with arbitrarily small entropy exist on any
Proposition 4.1. Let G be a transitive amenable graph and let (cid:15) > 0. There exists an i.i.d.
process Y of entropy at most (cid:15) and a cell process A which is a ﬁnitary Aut(G)-factor of Y.
Let us mention that in the special case of G = Zd  several parts of the argument below can be
skipped or simpliﬁed  thereby leading to a shorter proof. In the general case  certain technicalities
case of G = Zd on a ﬁrst reading.
Voronoi cells (determined from the Bernoulli process in a ﬁnitary manner)  which are then used as
the cells of A (after slightly decreasing the Voronoi cells to ensure that they are well-separated).
then used to obtain A from A by “ﬁlling in” some of the empty space between the cells of A
2 1 1
taking care not to connect cells of A which are not in the same Voronoi cell (thus ensuring that
an inﬁnite cluster is not created). Repeating in this manner  we obtain an increasing sequence
A ⊂ A ⊂ ··· of sets (each having only ﬁnite cells) as a ﬁnitary factor of a small entropy i.i.d.
10 YINONSPINKA
(a) A (b) Going from A to A (c) A
1 1 2 2
Figure 1. Constructing the cell process. The cells of A are simply the Voronoi
cells of a Bernoulli process. To get from A to A   we consider the Voronoi cells of a
lower-densityBernoulliprocess and“merge”cellsofA whichareentirelycontained
green depicts cells of A which are not cells of A .
2 1
process. It will then only remain to show that A increases to V. This is where amenability comes
To ensure that A increases to V  we must be careful in how we deﬁne the Voronoi cells. When
the graph G has a Følner sequence consisting of balls (as is the case when G has subexponential
general case considered here  we must adapt the usual Voronoi cells to a certain “metric” (which is
not necessarily a true metric) given by a suitable Følner sequence. Since we will need this metric
to be diagonally Γ-invariant  we need to choose a Følner sequence (F ) having the property that
n n
each F is invariant under the stabilizer of some ﬁxed vertex 0 ∈ V  i.e.  F = Γ F for all n.
n n 0 0 n
It is a simple observation that any graph G of subexponential growth has such a sequence for any
Γ (since there is a Følner sequence consisting of balls)  and that the Cayley graph G of a ﬁnitely
generated amenable group Γ also has such a sequence (since the stabilizers are trivial). As it turns
out  such a Følner sequence always exists in an amenable (not necessarily transitive) graph  even
when Γ is its full automorphism group.
Følner sequence consisting of sets which are invariant under the stabilizer of some vertex. The
Lemma 4.2. Let G be an amenable graph  let Γ be the full automorphism group of G and let 0 ∈ V.
There exists a Følner sequence (F ) in G such that F = Γ F for all n.
n n n 0 0 n
Lemma 4.2 easily follows by applying the following lemma to each set of some Følner sequence
taking Γ to be the stabilizer Γ of 0.
0 0 0
Lemma 4.3. Let G be any graph and let Γ be a group of automorphisms of G under which all
orbits of V are ﬁnite. For any ﬁnite non-empty set F ⊂ V  there exists a ﬁnite non-empty set
E ⊂ V such that
|∂E| |∂F|
≤ and E = Γ E.
|E| |F|
Proof. We show the existence of the desired E via a probabilistic method – that is  we choose a
random subset E of V and show that it satisﬁes the desired properties with positive probability.
Let {V } be the orbits of V under the action of Γ . Thus  {V } is a partition of V such that each
i i 0 i i
V is ﬁnite (by assumption) and satisﬁes V = Γ V .
i i 0 i
Let U be a uniform random variable in [0 1] and deﬁne
(cid:91) |F ∩Vi|
E := V   where p := .
i i
|V |
i:U≤pi
Thus  each orbit V is included in E with probability p   where the choices for diﬀerent i are
(cid:88) (cid:88) (cid:88)
E|E| = P(U ≤ p )·|V | = p |V | = |F ∩V | = |F|
i i i i i
i i i
E|∂E| = P(p < U ≤ p )·|∂(V  V )| = max{p −p  0}·|∂(V  V )|
j i i j i j i j
i j i j
where ∂(U V) denotes the set of edges between two disjoint sets U and V. Let us show that each
term in the second sum is at most |∂(F ∩V  Fc∩V )|  i.e.
i j
|∂(F ∩V  Fc∩V )|
p −p ≤ whenever p > p and ∂(V  V ) (cid:54)= ∅.
i j i j i j
|∂(V  V )|
Toseethis  notethattheright-handsideistheprobabilitythatanedgee = {u v}thatisuniformly
chosen from ∂(V  V ) belongs to ∂(F ∩V  Fc∩V )  or equivalently  with the convention that u ∈ V
i j i j i
and v ∈ V   that u ∈ F and v ∈/ F. Since this probability is at least P(u ∈ F)−P(v ∈ F)  it suﬃces
j
to show that u and v are uniformly distributed in V and V   respectively. This follows from the
observation thatthebipartite graph(V ∪V  ∂(V  V ))is biregular–eachvertex inV isadjacentto
the same number of vertices in V   and similarly  each vertex in V is adjacent to the same number
j j
of vertices in V . Indeed  for any u v ∈ V and γ ∈ Γ such that γu = v  the mapping w (cid:55)→ γw
i i 0
deﬁnes a bijection between N(u)∩V and N(v)∩V .
E|∂E| ≤ |∂(F ∩V  Fc∩V )| = |∂F| = α·E|E|
where α := |∂F|/|F|. Thus  α|E|−|∂E| is a random variable with non-negative expectation. Since
α|E|−|∂E| is zero when E is empty  conditioned on E (cid:54)= ∅  we still have that α|E|−|∂E| has
non-negative expectation. In particular  there is positive probability that E (cid:54)= ∅ and α|E| ≥ |∂E|.
Finally  since E is ﬁnite and satisﬁes E = Γ E almost surely  we see that E satisﬁes the desired
properties with positive probability. (cid:3)
Suppose that (F ) is a Følner sequence guaranteed by Lemma 4.2  i.e.  F = Γ F for all n
and further suppose that F (cid:40) F (cid:40) ··· and F ∪F ∪··· = V (there is clearly no loss in generality
1 2 1 2
in doing so). Recall the deﬁnition of Γ from Section 3.2. For u v ∈ V  deﬁne
(cid:8) (cid:9)
ρ(u v) := min n ≥ 1 : v ∈ Γ F .
0 u n
Using that Γ = γΓ for any γ ∈ Γ  one easily checks that ρ is diagonally Γ-invariant  i.e.
0 γu 0 u
ρ(γu γv) = ρ(u v) for all γ ∈ Γ. We stress that ρ is not necessarily symmetric in that ρ(u v) may
not equal ρ(v u). In particular  we do not claim that ρ is a metric. Nevertheless  we still think of
ρ(u v) as a measure of distance from v to u. One nice property of ρ that is easily veriﬁable and
which will be important is that  for any sequence of pairs of vertices (u  v )∞   we have
i i i=1
ρ(u  v ) → ∞ as i → ∞ if and only if dist(u  v ) → ∞ as i → ∞. (5)
i i i i
The fact that ρ may not be symmetric presents a certain challenge in the proof  for which we
require the following lemma to address. We note that ρ is indeed symmetric when the Følner
sequence (F ) consists of balls  and that it is nearly symmetric when G is a Cayley graph of Γ
12 YINONSPINKA
in which case switching the roles of u and v in ρ(u v) has the same eﬀect as replacing each F
with F−1. Accordingly  in these cases  it is immediate that the two “ρ-balls” of radius n around 0
{v : ρ(0 v) ≤ n} and {u : ρ(u 0) ≤ n}  have the same size  namely |F |. The following lemma
Lemma 4.4. Let G be a transitive amenable graph and let Γ be the full automorphism group of G.
Let F ⊂ V be invariant under the stabilizer of some vertex 0  i.e.  Γ F = F. Then
|{u ∈ V : 0 ∈ Γ F}| = |F|.
0 u
Proof. Deﬁne f: V2 → [0 1] by
f(u v) := 1 .
{v∈Γ0 uF}
Since Γ = γΓ for any γ ∈ Γ  it follows that f is diagonally Γ-invariant. Thus  by the
|{u ∈ V : 0 ∈ Γ F}| = f(u 0) = f(0 v) = |Γ F| = |F|. (cid:3)
0 u 0 0
Proof of Proposition 4.1. Let ((cid:15) ) be a sequence to be chosen later which satisﬁes that 0 ≤ (cid:15) ≤
(cid:15)2−n. We shall construct a cell process A as a ﬁnitary factor of the i.i.d. process Y = (Yv)v∈V in
which Y = (Y ) are independent random variables with Y ∼ Ber((cid:15) ). The entropy of Y
v v n n≥1 v n n
∞ ∞
(cid:88) (cid:88)(cid:16) (cid:17)
h(Y) = H(Y ) = H(Y ) = (cid:15) log 1 +(1−(cid:15) )log 1 ≤ 10(cid:15)log 1.
v v n n (cid:15)n n 1−(cid:15)n (cid:15)
n=1 n=1
U := {v ∈ V : Y = 1}
n v n
for any given n  to construct Voronoi cells  which are then used to deﬁne the cells of A . Precisely
we deﬁne the Voronoi cells of a non-empty set U ⊂ V by
(cid:110) (cid:111)
C¯ (u) := v ∈ V : ρ(u v) < ρ(u(cid:48) v) for all u(cid:48) ∈ U \\{u}   u ∈ U.
U
Thus  the Voronoi cell C¯ (u) associated to u consists of all vertices v ∈ V which are closer (in the
distance measured by ρ) to u than to any other u(cid:48) ∈ U. In particular  Voronoi cells associated to
diﬀerent vertices in U are disjoint  but they are not necessarily separated (they could be adjacent
to one another). We therefore deﬁne modiﬁed Voronoi cells by slightly shrinking the sets C¯ (u).
Precisely  we deﬁne
C (u) := (C¯ (u))−1.
U U
Using that the Voronoi cells are disjoint  it is straightforward to check that dist(C (u) C (u(cid:48))) > 1
for distinct u u(cid:48) ∈ U. We note that C (u) (in fact  already C¯ (u)) may be empty and need not be
Before proceeding with the construction of the cell process  let us ﬁrst show that the Voronoi
cells of U are almost surely ﬁnite whenever U is the set of points of an i.i.d. Bernoulli process. To
this end  it suﬃces to show that the probability that C¯ (0) intersects F \\F is summable over n.
U n n−1
Indeed  since a ﬁxed vertex v ∈ F \\F belongs to C¯ (0) only if U \\{0} contains no element of
n n−1 U
{u : ρ(u v) ≤ n}  it follows from Lemma 4.4 that the probability of this is at most p|Fn|−1  where p
is the density of the Bernoulli process. Since |Fn| ≥ n  we see that |Fn|·p|Fn|−1 is summable  and
hence that the C¯ (0) is almost surely ﬁnite.
We now turn to the construction of the cell process A. The ﬁrst level set in the cell process is
simply taken to be the vertices in a modiﬁed Voronoi cell of U   i.e.
(cid:91)
A := C (u).
1 U1
u∈U1
Since the Voronio cells of U are almost surely ﬁnite  we see that C (u) is almost surely ﬁnite
for all u ∈ U . Since dist(C (u) C (u(cid:48))) > 1 for distinct u u(cid:48) ∈ U   it follows that all connected
1 U1 U1 1
components of A are almost surely ﬁnite.
Suppose now that  for some n ≥ 1  A has been deﬁned in such a way that all connected
components of A are almost surely ﬁnite  and let us now deﬁne A . Let A(cid:48) be the union of
n n+1 n+1
the modiﬁed Voronoi cells of U   i.e.
n+1
A(cid:48) := C (u)
n+1 Un+1
u∈Un+1
andrecallthat(asforA )allconnectedcomponentsofA(cid:48) arealmostsurelyﬁnite. Intuitively we
1 n+1
would like to obtain A from A by adding A(cid:48) . However  this might create inﬁnite clusters
n+1 n n+1
and we must take care to avoid this by instead only adding a suitable subset of A(cid:48) . It will
suﬃce to slightly increase the “forbidden region” (A(cid:48) )c as follows: let D denote the union
n+1 n+1
of the connected components of A that intersect (A(cid:48) )c  and add D+1 to the forbidden region.
A := A ∪(A(cid:48) \\D+1 ).
n+1 n n+1 n+1
It is straightforward to check that all connected components of A are almost surely ﬁnite.
Assuming that A ∪A ∪··· = V almost surely  it is also easy to check using (5) that A is a ﬁnitary
Aut(G)-factor of Y  which would complete the proof of the proposition.
It remains only to show that A ∪A ∪··· = V almost surely. By Aut(G)-invariance  this is
equivalent to the fact that P(0 ∈ A ) → 1 as n → ∞. For n ≥ 1  let B denote the connected
component of 0 in A ∪{0}. For n ≥ 2  deﬁne the event
E := B ⊂ C (u) for some u ∈ U .
n n−1 Un n
{0 ∈ A \\A } = E ∩{0 ∈/ A }.
n n−1 n n−1
Thus  it suﬃces to show that
P(E | 0 ∈/ A ) ≥ c for some c > 0 and all n ≥ 2.
As we now show  this holds when (cid:15) is suitably chosen. Since B is almost surely ﬁnite  there
exists a suﬃciently large r so that
P(B ⊂ Λ | 0 ∈/ A ) ≥ 1
n−1 rn−1 n−1 2
where Λ := Λ (0). Since (F ) is a Følner sequence  there exists s suﬃciently large so that
r r s s n
2n |∂F | 1
|F | ≥ and sn ≤ . (6)
sn (cid:15) |F | 2|Λ |
sn rn
Set (cid:15) := |F |−1. Then  noting that A (and thus also B ) is independent of U
n sn n−1 n−1 n
P(E | 0 ∈/ A ) ≥ P(cid:0)B ⊂ Λ and Λ ⊂ C (u) for some u ∈ U | 0 ∈/ A (cid:1)
n n−1 n−1 rn−1 rn−1 Un n n−1
= P(cid:0)B ⊂ Λ | 0 ∈/ A (cid:1)·P(cid:0)Λ ⊂ C (u) for some u ∈ U (cid:1).
n−1 rn−1 n−1 rn−1 Un n
Since the ﬁrst term on the right-hand side is at least 1 by the choice of r   it remains to show that
2 n
for some constant c > 0 which does not depend on n  we have
P(cid:0)Λ ⊂ C (u) for some u ∈ U (cid:1) ≥ c.
rn−1 Un n
14 YINONSPINKA
Let us ﬁrst see how to show this when F is a ball  say Λ . In this case  it is not hard to see that
sn (cid:96)
the event in question occurs when U has a unique point in Λ and no other point in Λ   so
n (cid:96)−rn (cid:96)+rn
P(cid:0)Λ ⊂ C (u) for some u ∈ U (cid:1)
≥ P(cid:0)|U ∩Λ | = |U ∩Λ | = 1(cid:1)
n (cid:96)−rn n (cid:96)+rn
= P(cid:0)Ber(|Λ | (cid:15) ) = 1(cid:1)·P(cid:0)Ber(|Λ \\Λ | (cid:15) ) = 0(cid:1) ≥ c.
(cid:96)−rn n (cid:96)+rn (cid:96)−rn n
We now handle the general case in more detail. Set U := U . Our goal is to bound from below
the probability that Λ ⊂ C (u) for some u ∈ U. To this end  we ﬁrst ﬁnd a simple condition
rn−1 U
that implies the occurrence of this event. For a set F ⊂ V  denote
M(F) := {u ∈ V : 0 ∈ Γ F}.
Set r := r and s := s . Let us show that
|U ∩M(F−r)| = |U ∩M(F+r)| = 1 =⇒ Λ ⊂ C (u) for some u ∈ U. (7)
s s r−1 U
Suppose that the left-hand side holds. Let us show that Λ ⊂ C (u)  where u is the unique
r−1 U
element in U ∩M(F−r). By the deﬁnition of C (u)  this is equivalent to Λ ⊂ C¯ (u). Recalling
s U r U
the deﬁnition of C¯ (u)  we see that we must show that ρ(u w) < ρ(u(cid:48) w) for all u(cid:48) ∈ U \\{u} and
w ∈ Λ . Let u(cid:48) ∈ U \\{u} and w ∈ Λ . It suﬃces to show that ρ(u w) ≤ s and ρ(u(cid:48) w) > s.
Towards showing this  we ﬁrst note that (γV)+1 = γ(V+1) and (γV)−1 = γ(V−1) for any γ ∈ Γ
and V ⊂ V  due to the fact that γ acts by an automorphism of G. In particular  (Γ V)+r =
Γ (V+r) and (Γ V)−r = Γ (V−r)  and we may drop the parenthesis when writing such terms.
0 u 0 u 0 u
Letusnowshowthatρ(u w) ≤ s. Sinceu ∈ M(F−r)  wehavethat0 ∈ Γ F−r  orequivalently
s 0 u s
Λ ⊂ Γ F . Since w ∈ Λ   it follows that ρ(u w) ≤ s. Next  we show that ρ(u(cid:48) w) > s. Note that
r 0 u s r
u(cid:48) ∈/ M(F+r) since u ∈ M(F−r) ⊂ M(F+r). Thus  0 ∈/ Γ F+r  or equivalently  Λ ∩Γ F = ∅.
s s s 0 u(cid:48) s r 0 u(cid:48) s
Thus  w ∈/ Γ F and  using that F  F  ... F ⊂ F   it follows that ρ(u(cid:48) w) > s.
0 u(cid:48) s 1 2 s−1 s
≥ P(cid:0)|U ∩M(F−rn)| = |U ∩M(F+rn)| = 1(cid:1)
n sn n sn
= P(cid:0)Ber(|M(F−rn)| (cid:15) ) = 1(cid:1)·P(cid:0)Ber(|M(F+rn)|−|M(F−rn)| (cid:15) ) = 0(cid:1).
sn n sn sn n
|M(F−rn)| = |F−rn| and |M(F+rn)| = |F+rn|.
sn sn sn sn
|F−rn| ≥ |F |−|∂F |·|Λ | ≥ 1|F | and |F+rn| ≤ |F |+|∂F |·|Λ | ≤ 3|F |
sn sn sn rn 2 sn sn sn sn rn 2 sn
bounded below by a positive constant. (cid:3)
The above proposition established the existence of a ﬁnitary cell process A. In particular  A is
an invariant set which has high density when n is large. The following proposition shows that the
Lemma 4.5. Let G be a transitive graph of degree d and let Γ be a transitive unimodular group of
automorphisms of G. Let B ⊂ V be a random set with no inﬁnite clusters and whose distribution
is Γ-invariant. Let C denote the cluster of v in B. Then  for any δ > 0
P(cid:0)|∂C | ≥ δ|C |(cid:1) ≤ (d +1)·P(0 ∈/ B).
0 0 δ
Proof. The proof uses the mass-transport principle. Deﬁne ψ: V2 → [0 1] by
(cid:40)
|N(u)∩Cv| if u ∈/ B  v ∈ B
ψ(u v) := |Cv| .
0 otherwise
(cid:88) 10∈B (cid:88) |∂C0|
ψ(u 0) = · |N(u)∩C | = ·1
0 0∈B
|C | |C |
u u∈/B
(cid:88) (cid:88) |N(0)∩Cv|
ψ(0 v) = 1 · = |N(0)∩B|·1 ≤ d·1 .
0∈/B |C | 0∈/B 0∈/B
v v∈B
The Γ-invariance of B implies that f(u v) := Eψ(u v) is diagonally Γ-invariant. Thus  the mass-
(cid:104) (cid:105)
E |∂C0| ·1 ≤ d·P(0 ∈/ B).
|C0| 0∈B
The proposition now follows from Markov’s inequality. (cid:3)
Remark 1. A result of H¨aggstr¨om [6  Theorem 1.6] states that any automorphism-invariant edge
percolation on a d-regular tree (d ≥ 3) with edge-density at least 2/d has an inﬁnite cluster with
tree. Moreover byLemma4.5(seealsothecloselyrelated[3 Theorem1.2]) weseethatΓ-invariant
in [3  Theorem 5.1] that a closed subgroup Γ of Aut(G) is amenable if and only if there is a Γ-
invariant site percolation on G with with no inﬁnite clusters and density arbitrarily close to 1. It
follows that a Γ-invariant cell process on G exists if and only if Γ is amenable. The site percolation
constructed in [3] is a factor of an i.i.d. process  though it is not ﬁnitary.
5. Random total orders
In this section  we construct a random total order on V that has the following properties:
• It is a ﬁnitary factor of an i.i.d. process with arbitrarily small entropy.
• It is supported on total orders having the same order type as Z.
• The successor/predecessor of any vertex can be found in a ﬁnitary manner.
relation on V  may be regarded as a random element in {0 1}V2. With this viewpoint  the notion
of ﬁnitary factor easily applies to such relations. Namely  such a relation is a Γ-factor of Y if it
has the same distribution as ϕ(Y) for some measurable function ϕ: TV → {0 1}V2 satisfying that
ϕ(y) = ϕ(γy) for all γ ∈ Γ  u v ∈ V and y ∈ TV. Such a factor is ﬁnitary if for every
(u v) (γu γv)
u v ∈ V there almost surely exists a ﬁnite (random) set W ⊂ V such that ϕ(Y) is determined
(u v)
by (Y )   in the sense that ϕ(y) = ϕ(Y) for any y ∈ T which coincides with Y on W.
w w∈W (u v) (u v)
A total order ≤ on V has the same order type as Z if there is an order preserving bijection
between the two ordered spaces  i.e.  a bijection f: V → Z such that f(u) ≤ f(v) if and only if
u ≤ v. This may be equivalently formulated as saying that ≤ has no minimum or maximum and
that there are ﬁnitely many elements between any two elements  i.e.  every interval of the form
{w ∈ V : u ≤ w ≤ v} is ﬁnite. In particular  in such an order  every vertex v has a successor (an
element w ≥ v such that u ≥ w for all u ≥ v) and a predecessor (an element w ≤ v such that u ≤ w
for all u ≤ v).
Given a factor from Y to a random total order ≤ on V  we say that successors (predecessors)
can be found in a ﬁnitary manner if for every u v ∈ V there almost surely exists a ﬁnite (random)
set W ⊂ V such that the event that u is the ≤-successor (≤-predecessor) of v is determined by
16 YINONSPINKA
(Y ) . We note that  in general  there is no direct relation to the notion of ﬁnitary factor: it
w w∈W
may be that such a factor is ﬁnitary though successors/predecessors cannot be found in a ﬁnitary
manner  oritmaythatsuccessors/predecessorscanbefoundinaﬁnitarymannerthoughthefactor
is not ﬁnitary. On the other hand  for a total order having the order type of Z almost surely  the
second implication is easily seen to hold – if successors/predecessors can be found in a ﬁnitary
manner  then the factor is necessarily ﬁnitary.
A total order which is a ﬁnitary factor of an i.i.d. process with inﬁnite entropy is easily obtained
from the order induced by uniform random variables in [0 1] assigned to each vertex. It is easy
order type of Q) which is a ﬁnitary factor (with exponential tails on the coding radius) of an
i.i.d. process with ﬁnite entropy was constructed in [7] for any quasi-transitive graph satisfying a
Lemma 5.1. Let G be a transitive non-empty graph satisfying (1). For any 0 < (cid:15) ≤ 1 there exists
a total order on V which is a ﬁnitary Aut(G)-factor of an i.i.d. Bernoulli process with density (cid:15).
Proof. Let η = (ηv)v∈V be an i.i.d. Bernoulli process with density (cid:15). For any v ∈ V  deﬁne
Z = (Z ) ∈ {0 1 ...}{0 1 ...} by
v v n n≥0
Z := η .
v n u
u∈V:dist(u v)=n
Deﬁnearelation≤onVinwhichu ≤ v ifandonlyifZ (cid:22) Z   where(cid:22)denotesthelexicographical
order on {0 1 ...}{0 1 ...}. Then ≤ is clearly a Aut(G)-factor of η.
Itremainstoshowthat≤isalmostsurelyatotalorderonVandthatthefactorisﬁnitary. Since
≤ is clearly a preorder  to show that it is a total order  it suﬃces to show that P(Z = Z ) = 0 for
distinct u v ∈ V. It then follows from the deﬁnition of the lexicographical order that the factor is
ﬁnitary.
Fix u v ∈ V distinct and consider the event
(cid:92)
E := {Z = Z }.
n u i v i
i=1
Since P(E ) → P(Z = Z ) as n → ∞  it suﬃces to show that P(E | E ) ≤ 1−(cid:15) for all n ≥ 1.
n u v n n−1
By (1) and the assumption that the graph is non-empty  we have Λ (u) \\ Λ (u) (cid:54)⊂ Λ (v)  as
n n−1 n
otherwise Λ (u) ⊂ Λ (v)  which in turn implies that Λ (u) = Λ (v) by transitivity. Thus
3n 3n 3n 3n
there exists some w ∈ Λ (u)\\(Λ (u)∪Λ (v)). Then
n n n−1 n
P(cid:0)E | η (cid:1) ≤ maxP(η = k) = max{(cid:15) 1−(cid:15)} = 1−(cid:15).
n V\\{wn} k∈Z wn
Since E is measurable with respect to η   it follows that P(E | E ) ≤ 1−(cid:15). (cid:3)
n−1 V\\{wn} n n−1
Lemma 5.2. Let G be a transitive amenable non-empty graph satisfying (1) and let (cid:15) > 0. Then
there exists an i.i.d. process Y with entropy at most (cid:15)  and a random total order ≤ on V which
almost surely has the same order type as Z  such that ≤ is a ﬁnitary Aut(G)-factor of Y for which
successors/predecessors can be found in a ﬁnitary manner.
Proof. ByLemma5.1  thereexistsatotalorder(cid:22)onVthatisaﬁnitaryfactorofani.i.d.processY
having entropy at most (cid:15). By Proposition 4.1  there exist a cell process A that is a ﬁnitary factor of
an i.i.d. process Y(cid:48) (which we take to be independent of Y) having entropy at most (cid:15). We construct
the required total order ≤ on V as a ﬁnitary factor of (Y Y(cid:48)).
The idea is to use (cid:22) to order the sites within the cells given by A. More precisely  we will deﬁne
an increasing sequence of partial orders ≤ ⊂≤ ⊂ ... such that each ≤ induces a total order on
1 2 n
every cell of A . Since (cid:83) A = V  this will produce a total order ≤ given by the union (cid:83) ≤
n n n n n
Precisely  we deﬁne ≤ to be the relation in which u ≤ v whenever u and v belong to the same
1 1
cell of A and satisfy that u (cid:22) v. Then ≤ is clearly a partial order that induces a total order
on any cell of A . In fact  it is the union of these total orders on the cells of A (that is  it only
Next  suppose we have deﬁned the partial order ≤ so that it is a union of total orders on the
cells on A   and let us deﬁne ≤ . Consider a cell C of A and let D := C∩A = D ∪···∪D
n−1 n n n−1 1 k
be the union of the cells D  ... D of A that are contained in C. We deﬁne ≤ in such a
1 n n−1 n
way that D ≤ C \\ D by requiring that u ≤ v whenever u ∈ D and v ∈ C \\ D. To obtain a
total order on C  it remains to order the vertices in D and the vertices in C \\D. The latter is
ordered by deﬁning u ≤ v whenever u v ∈ C \\D and u (cid:22) v. The former is ordered by giving
an order to the cells D  ... D and using the ≤ order within each cell – that is  we require
1 k n−1
that ≤ coincides with ≤ on each cell D   and that either D ≤ D or D ≤ D for any
n n−1 i i n j j n i
two cells D and D . Finally  the order of the cells is determined by requiring that D ≤ D
i j i n j
whenever min D (cid:22) min D . That is  the i-th cell precedes the j-cell in ≤ if and only if the
(cid:22) i (cid:22) j n
(cid:22)-minimal element in the i-th cell is (cid:22)-smaller than the (cid:22)-minimal element in the j-th cell. It is
straightforward that ≤ extends ≤ and that ≤ is a union of total orders on the cells of A .
n n−1 n n
We have thus obtained partial orders ≤  ≤  ... such that  for each n  ≤ extends ≤ and is a
1 2 n n−1
union of total orders on the cells of A . To show that ≤ has the order type of Z  it remains to show
that  almost surely  every ≤-interval is ﬁnite and there is no ≤-minimum and no ≤-maximum. It
follows from the construction that if u is the ≤ -successor of v  then it is also its ≤ -successor.
n n+1
Thus  to conclude that every ≤-interval is ﬁnite  it suﬃces to show that  for every u v ∈ V having
u ≤ v  there exists n such that u ≤ v and the interval [u v] is ﬁnite. Indeed  since ≤ only
n ≤n n
compares vertices within the same cell of A and since all such cells are ﬁnite  all ≤ -intervals are
ﬁnite. Finally  no minimum or maximum can exist as this would contradict the invariance of ≤.
Wehavethusestablishedthat≤almostsurelyhasthesameordertypeasZ. Itisstraightforward
from the fact that the ≤ -successor of a vertex u (if it exists) is also the ≤ -successor of u  that
theconstructedfactorfrom(Y Y(cid:48))to≤hasthepropertythatsuccessors/predecessorscanbefound
in a ﬁnitary manner. As mentioned in the beginning of the section  this implies that the factor is
also ﬁnitary. (cid:3)
6. The finitary coding
In this section  we construct a ﬁnitary coding for ﬁnitely dependent processes. We present the
Let X be a Γ-invariant ﬁnitely dependent process taking values in a ﬁnite set S. Recall from
the proof outline in Section 2 that we shall construct a ﬁnitary factor from an i.i.d. process Y =
(Ybits Ycell Yord) to X. Recall also that A will be a cell process that is a ﬁnitary factor of Ycell
and that ≤ will be a random total order on V that is a ﬁnitary factor of Yord  has the order type of
Z  and for which successors/predecessors can be found in a ﬁnitary manner. Given the cell process
A and the total order ≤  we will use the additional randomness in Ybits to construct a realization
of X.
18 YINONSPINKA
6.1. Choosing the parameters. Fix (cid:15) > 0. We shall choose the i.i.d. processes Ybits Ycell Yord
to satisfy h(Ybits) < h(X)+5(cid:15)  h(Ycell) ≤ (cid:15) and h(Yord) ≤ (cid:15) so that Y has entropy
h(Y) < h(X)+7(cid:15).
We let Ybits be any i.i.d. process in which Ybits is a random number of random bits satisfying
H(Ybits) < h(X)+5(cid:15) and E|Ybits| > h(X)+3(cid:15). (8)
Recall that  by a random number of random bits  we mean a random variable W taking values
in {0 1}∗  the set of ﬁnite words over {0 1}  and having the property that  conditioned on the
length |W| of the word  W is uniformly distributed on {0 1}|W|. The desired random word can
be obtained by taking W to be the empty word with probability p or a uniformly chosen sequence
in {0 1}m with probability 1 − p  for some suitably chosen m ≥ 1 and 0 ≤ p < 1. Indeed
in this case  H(|W|) = −plogp − (1 − p)log(1 − p) and E|W| = pm so that H(|W|) → 0 and
E|W| → h(X)+4(cid:15) as m → ∞ when p = 1(h(X)+4(cid:15)). Since the entropy and length of W are
related via H(W) = E|W|+H(|W|)  we see that (8) holds when m is suﬃciently large.
Let δ > 0 be as in (2). By decreasing δ  we may additionally assume that
2δ < (cid:15). (9)
Recall from Section 2 that we may assume without loss of generality that X is 1-dependent. By
Proposition 4.1  there exists a cell process A and an i.i.d. process Ycell of entropy at most (cid:15) such
that A is a ﬁnitary factor of Ycell. Since P(0 ∈ A ) → 1 as n → ∞  Lemma 4.5 implies that
P(|∂A (0)| ≥ δ|A (0)|) → 0 as n → ∞  where A (0) is the cell of 0 in A . Thus  by replacing
n n n n
(A  A  ...) with (A  A  ...) for some large m  we may assume that P(|∂A (0)| ≥ δ|A (0)|)
1 2 m m+1 1 1
is arbitrary small. Speciﬁcally  we require that
(cid:16) (cid:17) (cid:15)
P |∂A (0)| ≥ δ|A (0)| < . (10)
log|S|+2
LetYord beani.i.d.processwithentropyatmost(cid:15)andlet≤beatotalorderonVasguaranteed
by Lemma 5.2 (note that if the graph G contains no edges  then X is already an i.i.d. process so
6.2. The construction of the ﬁnitary coding. Recall that the n-th ≤-successor of v is denoted
by v+n and its n-th ≤-predecessor by v−n. In particular  v±n are random elements of V which
are determined from Yord is a ﬁnitary manner. Recall also that the cell process A is a ﬁnitary
factor of Ycell. It may be helpful from this point onward to think of A and ≤ as given  and that
We shall deﬁne  for every time t ≥ 0 and every vertex u ∈ V  a random variable
Lt ∈ {0 1}.
We shall deﬁne these inductively with the t = 0 variables given by
L0 := 1 . (11)
u {uisalevel1agentand|Ybits|>0}
We think of Lt as indicating whether u read a bit at time t. In particular  if Lt = 1 for some t
and u  then necessarily u is an agent (of some level). Since at time 0 an agent looks for an available
bitatitsownlocation  (11)saysthatanylevel1agentreadsabitattime0ifsuchabitisavailable.
Before giving the main deﬁnitions of the construction  we ﬁrst set up some auxiliary notation
anddeﬁnitions. Aswehavealreadymentioned  ourconstructionhasthepropertythatifanagentu
word Ybits. In particular  the total number of bits read from location v by time t is
u+t
Mt := L0 +L1 +···+Lt .
v v v−1 v−t
Thebitsatanylocationv arereadsequentially–theﬁrstagenttoreadabitatv willreadYbits(1)
the second will read Ybits(2) and so on. Precisely  the bit read by u at time t is
Ybits(Mt ) if Lt = 1
Wˆ t := u+t u+t u .
∅ otherwise
For this to be well-deﬁned  we must make sure that u does not try to read a non-existent bit – we
mention already here that this does not occur  i.e.  our deﬁnitions will ensure that
Mt ≤ |Ybits| for all v ∈ V and all t ≥ 0.
The word read by u by time t is then
Wt := Wˆ 0◦Wˆ 1◦···◦Wˆ t
u u u u
where ◦ denotes concatenation. That is  Wt is the word obtained by concatenating the bits read
by u until time t in the order they were read. In particular  Wt is a word in {0 1}∗ of length
|Wut| = L0u+L1u+···+Ltu. We emphasize that (Mut Wut)u∈V is well-deﬁned once (Liu Nui)u∈V 0≤i≤t
is deﬁned  as the former are functions of the latter and of Ybits.
As explained in the proof outline  we use “simulations” to obtain samples of distributions from
random bits. We ﬁrst equip ourselves with simulations of all the possible distributions we may
require throughout the construction of the ﬁnitary coding. The basic distributions we need are
those of X for a ﬁnite set V ⊂ V. As we aim to obtain a Γ-equivariant factor  we must take
would be more proper to view X as a random element of S|V| by using the order ≤. Precisely
we proceed as follows. Recall the deﬁnition of a simulation from Section 3.3. By Theorem 3.1  for
every ordered sequence v  ... v ∈ V of distinct vertices  there exists a simulation S of
1 m (v1 ... vm)
(X  ... X ) ∈ Sm satisfying that
v1 vm
EStime (ω) ≤ H(X )+2.
(v1 ... vm) V
Since the distribution of X is Γ-invariant  we may suppose that S = S for all
(v1 ... vm) (γv1 ... γvm)
γ ∈ Γ (e.g.  by choosing a simulation for a single representative of each orbit  and then setting
S to equal the simulation of its representative). Now  for a ﬁnite set V ⊂ V  we let
(v1 ... vm)
v  ... v be the vertices of V  ordered according to ≤  and set S to be the simulation S
1 m V (v1 ... vm)
where  for notational convenience  we interpret Sout as an element of SV (indexed by V) through
the identiﬁcation Sout(ω) = Sout (ω) . We stress that S implicitly depends on the order ≤.
V vi (v1 ... vm) i V
As we will also encounter situations in which regions of X have already been sampled  we will
also need simulations of the distribution of X conditioned on X for some ﬁnite set U ⊂ V which
V U
is disjoint from V. Thus  for every such V and U and every τ ∈ SU  we similarly let S be a
V U τ
simulation of P(X ∈ · | X = τ) satisfying that
EStime (ω) ≤ H(X | X = τ)+2. (12)
V U τ V U
For ease of notation later on  we allow V and U to intersect and we allow τ to have any domain
containing U  by interpreting S as S in such a case. We also identify S with S .
V U τ V\\U U τU V V ∅ ∅
Recall that every cell C in A that is not contained in A has an associated level n agent  and
that this agent is “responsible” for generating the output on C \\A . We denote by A (v) the
n−1 n
cell of v in A   where A (v) := ∅ if v ∈/ A   by U the set of level n agents and  for v ∈ A \\A
n n n n n n−1
by U (v) the level n agent associated to the cell A (v).
With the above notation and deﬁnitions  we may now proceed to construct the ﬁnitary coding.
Our goal is to deﬁne a random ﬁeld Zt = (Zvt)v∈V  which represents the output at time t. This
output will be a function of (Liu Mui Wui)u∈V 0≤i≤t (and of course of the cell process A and the total
order ≤). Once Zt is deﬁned for some t  it will then only remain to inductively deﬁne (Ltu+1)u∈V  as
this then also deﬁnes (Mui Wui)u∈V 0≤i≤t+1 through the deﬁnitions above. This will therefore deﬁne
20 YINONSPINKA
Zt+1 as well. We will then take a limit as t → ∞ in order to obtain the output Z = (Zv)v∈V  which
To facilitate the inductive deﬁnition of (Ltu+1)u∈V  we require some more deﬁnitions. We now
regardt ≥ 0asﬁxedandsupposethat(Liu)u∈V 0≤i≤t andhencealso(Mui Wui)u∈V 0≤i≤t arealready
deﬁned. We deﬁne two notions for a level n agent: that of having reached level n at time t  and
that of having completed level n of the simulation by time t. We deﬁne these notions inductively
on n. We thus begin with level 1 agents. Given a level 1 agent u ∈ U   we say that
• u reached level 1 by time t (always  with no condition).
• u completed level 1 by time t if the stopping time Stime has been reached on input Wt .
A1(u) u 1
sponding level 1 cell of the agent. That is  if a level 1 agent u completed level 1 by time t  then we
Zt = Sout (Wt ) for all v ∈ A (u).
v A1(u) u 1 v 1
To be more precise  let us deﬁne Zt 1 = (Zvt 1)v∈V by
Sout (Wt) if v ∈ A (u) for some u ∈ U and u completed level 1 by time t
Zt 1 := A1(u) u v 1 1 .
We will soon also deﬁne Zt n = (Zvt n)v∈V for n ≥ 2  with the idea that it represents the known
output on all cells of level at most n for which the simulation has completed. In particular  if
Zt n (cid:54)= ∅ for some v and n  then it will be the case that Zt n+1 = Zt n. Now ﬁx n ≥ 2 and suppose
v v v
that we have deﬁned Zt 1 ... Zt n−1 and the two notions (reached and completed) for levels less
than n  in such a way that the above property holds – namely  if a level m ∈ {1 ... n−1} agent u
completed level m by time t  then the output is known on A (u) at time t in the sense that
Zt n−1 = Zt m (cid:54)= ∅ for all v ∈ A (u). Then  for a level n agent u ∈ U   we say that
v v m n
• u reached level n by time t if every level n−1 agent u(cid:48) ∈ U ∩A (u) has completed level
n−1 by time t.
The idea here is that if u reached level n by time t  then the output is known on A (u)∩A
at time t  and we may use this information to start generating the output on the remaining part
of the cell  namely  on A (u) \\ A . That is  we use the simulation S with V = A (u)
n n−1 V U τ n
U = A (u)∩A and τ = Zt n−1. We thus say that
• u completed level n by time t if it reached level n by time t and the stopping time
An(u) An(u)∩An−1 Zt n−1
has been reached on input Wt.
Putting this together leads to deﬁning Zt n = (Zvt n)v∈V by
Sout (Wt) ifv∈An\\An−1 andUn(v)=uforsomeu
Zt n := An(u) An(u)∩An−1 Zt n−1 u v anducompletedlevelnbytimet .
The output Zt = (Zvt)v∈V at time t is then deﬁned by Zvt := limn→∞Zvt n. That is  to determine Zvt
we ﬁrst look at the level n at which v enters the cell process  and then consider the level n agent u
responsible for generating the output on the cell of v in A . If u has indeed completed level n by
time t  then we read the value of Zt from the output of the corresponding simulation.
Finally  we are ready to deﬁne (Ltu+1)u∈V. As mentioned  these numbers are always zero for
non-agents  i.e.  we set Lt+1 := 0 for u ∈/ U ∪U ∪···. Suppose now that u ∈ U for some n ≥ 1.
u 1 2 n
We say that u is active at time t+1 if it has reached  but has not completed  level n by time t.
Thus  if u is active at time t+1  then ideally it would like to read a bit at that time  and indeed it
may do so as long as there is an available bit at u+t+1 (recall that u may only read a bit from
location u+t+1 at time t+1). This leads us to deﬁne
Lt+1 := 1(cid:0)u is active at time t+1 and Mt < |Ybits |(cid:1). (13)
u u+t+1 u+t+1
This completes the inductive deﬁnition of Lt+1 for all t ≥ 0 and u ∈ V.
Wenotethat byconstruction oncetheoutputatavertexisdeterminedatsometime itremains
unchanged at future times – that is  if Zt (cid:54)= ∅ for some v and t  then Zt+1 = Zt. Denote by
T := min{t ≥ 0 : Zt (cid:54)= ∅}
the time at which the output at v is ﬁrst determined. The output Z = (Zv)v∈V is then given by
Z := lim Zt = ZvTv if Tv < ∞ .
t→∞ ∅ if T = ∞
This completes the construction of the ﬁnitary factor.
Lemma 6.1. For any t ≥ 0  we have that (Wut Zut)u∈V and (Ltu+1)u∈V are measurable with respect
to Ycell  Yord  (|Yvbits|)v∈V and (Yvbits(i))v∈V 1≤i≤Mt.
Proof. The proof by induction on t is straightforward from the deﬁnitions. (cid:3)
6.3. Concluding Theorem 1.2. To conclude the proof of Theorem 1.2  we must establish two
that the output can be determined from (Ybits Ycell Yord) in a ﬁnitary manner. The former is
Proposition 6.2. The output Z has the same distribution as X.
Proof of Theorem 1.2. The random ﬁeld Z is clearly a deterministic and Γ-equivariant function ϕ
of (Ybits Ycell Yord). Thus  in light of Proposition 6.2  we must only show that ϕ is ﬁnitary. Since
T is almost surely ﬁnite (as Z (cid:54)= ∅ almost surely by Proposition 6.2)  it suﬃces to show that Zt
is ﬁnitary for every t ≥ 0. This follows rather easily from the construction. To see this  we explain
how to determine the value of Zt in a ﬁnitary manner.
We begin by ﬁnding the level n in which v enters the cell process  i.e.  v ∈ A \\A   and then
ﬁnding the cell A (v) of v in A . Since A is a ﬁnitary factor of Ycell  this may be done in a ﬁnitary
manner. Next  we ﬁnd the level n agent U (v) associated to the cell A (v). Since this is just the
≤-minimal element in A (v)\\A   and since the order ≤ is a ﬁnitary factor of Yord  this may
also be done in a ﬁnitary manner.
Let us suppose by induction that all steps of the construction up to time t−1 are ﬁnitary. Thus
recalling the deﬁnition of active  we see that  for any vertex w  we may determine in a ﬁnitary
manner whether w is active at time t. Since successors/predecessors in ≤ may be found in a
ﬁnitary manner from Yord  it then also follows that Lt may be determined in a ﬁnitary manner.
Using again that successors/predecessors may be found in a ﬁnitary manner  we conclude that Wt
may be found in a ﬁnitary manner.
We would now like to check whether u completed level n by time t  and if so  ﬁnd the output
value. To check this  we start at level 1 and work our way up to level n. Thus  we ﬁrst ﬁnd all
level 1 agents which are contained in A (v) (since the cell process and total order are ﬁnitary  this
can be done in a ﬁnitary manner). Next  for each such agent u  we check whether u completed
level 1 by time t. Recall that the simulation S depends on the cell A (u) and on the order
A1(u) 1
induced by ≤ on A (u). Since the input word Wt  the cell process and the order are ﬁnitary  we
1 u
see that we may determine whether u completed level 1 by time t in a ﬁnitary manner  and if so
also determine the output Zt 1 for all w ∈ A (u) in a ﬁnitary manner.
w 1
22 YINONSPINKA
We now proceed to the next levels. Consider some level 2 ≤ m ≤ n. We again begin by ﬁnding
all level m agents which are contained in A (v). For each such agent u  we check whether u reached
level m by time t. For this we must check whether the level m−1 agents in A (u) completed level
m−1 by time t  which  by induction  may be done in a ﬁnitary manner. If u reached level m by
time t  we then check whether u completed level m by time t. Similarly to before  the simulation
S depends on A (u)  A (u)\\A   the order induced by ≤ on A (u)  and
Am(u) Am(u)∩Am−1 Zt m−1 m m m−1 m
on (Zt m−1) . Since the input word Wt  the cell process and the order are ﬁnitary  we
w w∈Am(u)∩Am−1 u
see that we may determine whether u completed level m by time t in a ﬁnitary manner  and if so
also determine the output Zt m for all w ∈ A (u) in a ﬁnitary manner.
w m
Continuing up to level m = n yields that Zt n may be determined in a ﬁnitary manner. Since n
is the level in which v enters the cell process  we have by deﬁnition that Zt = Zt n. Thus  Zt may
be determined in a ﬁnitary manner  as required. (cid:3)
6.4. The output has the correct distribution. In this section  we prove Proposition 6.2. The
proof is split up into several steps. The ﬁrst step is the following lemma which formalizes the
intuition that the simulations used in the construction are ‘fed’ independent unbiased bits.
N
Lemma 6.3. Let ω ∈ {0 1} consist of a sequence of independent unbiased bits. Let (ωu)u∈V be a
collection of i.i.d. copies of ω  independent of (Ybits Ycell Yord). Then  for any t ≥ 0  conditioned
on (Ycell Yord)  the collection (Wut ◦ωu)u∈V has the same distribution as (ωu)u∈V.
Proof. We prove the statement by induction on t  taking t = −1 as a trivial base case (where
W−1 := ∅ for all u ∈ V). Suppose now that we know it for some t ≥ −1 and let us show it for
t+1. Recall that Wt+1 = Wt◦Wˆ t+1. Thus  we need to show that  conditioned on (Ycell Yord)  the
u u u
collection(Wut◦Wˆut+1◦ωu)u∈V hasthe samedistributionas(ωu)u∈V. To thisend  itsuﬃces toshow
that  conditioned on (Ycell Yord)  the collections (Wut)u∈V and (Wˆut+1◦ωu)u∈V are independent and
that the conditional distribution of the latter is that of (ωu)u∈V. Indeed  the induction hypothesis
Wemayrestateourgoalasshowingthat conditionedon(Ycell Yord)and(Wut)u∈V thecollection
(Wˆut+1 ◦ωu)u∈V has the same distribution as (ωu)u∈V. Let F be the σ-algebra generated by Ycell
Yord  (|Yvbits|)v∈V and (Yvbits(i))v∈V 1≤i≤Mt. By Lemma 6.1
(Wut)u∈V and Q := (cid:8)u : Wˆut+1 (cid:54)= ∅(cid:9) = (cid:8)u : Ltu+1 = 1(cid:9)
areF-measurable. Since(ωu)u∈V isindependentof(Ybits Ycell Yord)andhencealsoofF itsuﬃces
to show that  conditioned on F  the random variables (Wˆ t+1) are independent unbiased bits.
u u∈Q
Note that Q is the set of vertices (agents) that read a bit at time t+1  that
Q(cid:48) := {v : Mt+1 > Mt}
is the set of vertices from which a bit was read at time t+1  and that u (cid:55)→ u+t+1 deﬁnes a
F-measurable bijection from Q to Q(cid:48). Recall also that Wˆ t+1 = Ybits (Mt+1 ) for u ∈ Q. Thus
it suﬃces to show that  conditioned on F  the random variables (Ybits(Mt+1)) are independent
v v v∈Q(cid:48)
Indeed  since Q(cid:48) and (Mt+1) are F-measurable by Lemma 6.1  since Mt+1 > Mt for all
v v∈Q(cid:48) v v
v ∈ Q(cid:48) andsinceYbits isani.i.d.processthatisindependentof(Ycell Yord) weseethattherandom
variables (Ybits(Mt+1)) are conditionally independent given F  and that  for any v ∈ Q(cid:48)  the
conditional distribution of Ybits(Mt+1) is the same as the distribution of Ybits(Mt+1) given |Ybits|.
v v v v v
Since Ybits is a random number of random bits  the latter is the distribution of an unbiased bit
and the proof is complete. (cid:3)
We will use the above lemma for ﬁxed t and then let t tend to inﬁnity. In doing so  we will
encounter the limiting word W∞ := lim Wt. Since Wt+1 extends Wt  this limit is well-deﬁned
u t→∞ u u u
and is a word in {0 1}∗ or {0 1}N (we will see that it is in fact a ﬁnite word almost surely).
The next step towards proving Proposition 6.2 is to show that the output at every vertex v is
eventually determined  i.e.  that Z (cid:54)= ∅ (equivalently  T < ∞) almost surely. For this  we ﬁrst
Lemma 6.4. Every vertex is almost surely eventually inactive. That is  for any u ∈ V  there
almost surely exists a ﬁnite t such that u is not active at any time t ≥ t .
Proof. Deﬁne
ψ(u v) := 1 .
{v=u+tandLt=1forsomet≥0}
Note that ψ(u v) indicates whether u read a bit located at v. Since an agent may read at most one
bit from any location  ψ(u v) also represents the number of bits read by u from location v. Thus
recalling that |Wt| = L0 +L1 +···+Lt  we have
(cid:88) (cid:88) (cid:88) (cid:88)
ψ(u v) = Lt = |W∞| and ψ(u v) = Lt = lim Mt =: M∞.
u u v−t v v
t→∞
v t=0 u t=0
The left-hand side describes the number of bits read by a given site u  while the right-hand side
E|W∞| = EM∞. (14)
Let E be the event that u is active at inﬁnitely many times t. We wish to show that P(E ) = 0.
Note that  by (13)  the event E is contained in the event that for all but ﬁnitely many t ≥ 0  all
bits at location u+t have been read by time t  i.e.
E ⊂ (cid:8)Mt ≥ |Ybits| for all suﬃciently large t(cid:9)
u u+t u+t
= (cid:8)M∞ = |Ybits| for all suﬃciently large t(cid:9)
u+t u+t
where the equality follows from the fact that Mt ≤ M∞ ≤ |Ybits| for all v ∈ V and t ≥ 0.
Suppose now that P(E ) > 0. Then by ergodicity  almost surely  E occurs for some w ∈ V
u w
and in particular  there almost surely exists w ∈ V such that M∞ = |Ybits| for all i ≥ 0. Since
w+i w+i
{w +i}i∈Z = V almost surely  it follows by Γ-invariance that Mv∞ = |Yvbits| for all v ∈ V almost
E|W∞| = E|Ybits|. (15)
Deﬁne
(cid:40) |W∞|
u if v ∈ A \\A and U (v) = u
φ(u v) := |An(u)\\An−1| n n−1 n .
Recall that U (v) is the level n agent associated to the cell A (v). Since a level n agent u is
responsible for simulating the output on A (u)\\A and does so via the input word W∞  we may
n n−1 u
think of φ(u v) as follows: every level n agent u equally divides a total ‘cost’ of |W∞| among the
vertices it ‘serviced’. Observe that
|W∞ |
(cid:88)φ(u v) = |W∞| and (cid:88)φ(u v) = UNv(v)
u |A (v)\\A |
v u Nv Nv−1
24 YINONSPINKA
where N is the level at which v entered the cell process  i.e.  v ∈ A \\A   and where we used
v Nv Nv−1
that A (v) = A (u) whenever U (v) = u. Thus  by (15) and the mass-transport principle (4)
n n n
(cid:34) |W∞ | (cid:35)
E|Ybits| = E UNv(v) . (16)
v |A (v)\\A |
Nv Nv−1
bits and that the simulation is eﬃcient.
Suppose that u is a level n agent. It is straightforward from the deﬁnitions that the stopping
time Stime is not reached on any preﬁx of Wt that is not Wt itself (it may or may
An(u) An(u)∩An−1 Zt n−1 u u
not be reached on the entire word Wt). It therefore follows from Lemma 6.3 that  conditioned on
(Ycell Yord)
|Wt| is stochastically dominated by Stime (ω)·1(u reached level n by time t)
u An(u) An(u)∩An−1 Zt n−1
where ω ∈ {0 1} consists of a sequence of independent unbiased bits  independent of Y. Note
that  if u reached level n by time t  then Zt n−1 coincides with Z on A (u)∩A . Thus  taking
expectations and t → ∞  we obtain that
(cid:104) (cid:105) (cid:104) (cid:105)
E |W∞| | Ycell Yord ≤ E Stime (ω) | Ycell Yord ·1(u eventually reached level n).
u An(u) An(u)∩An−1 Z
Hence  by (12) and (3)  on the event that u ∈ U   we have
(cid:104) (cid:105) H (A (u)) if n = 1
E |W∞| | Ycell Yord ≤ 2+ X 1
|A (u)\\A |·log|S| if n ≥ 2
where we denote H (V) := H(X ) for a ﬁnite set V ⊂ V. Therefore  by (2) and the choice of δ
X V
E UNv(v) | Ycell Yord ≤ (h(X)+(cid:15)+2δ)·1 +(log|S|+2)·1
E Ec
|A (v)\\A |
where E is the event that N = 1 and |∂A (v)| ≤ δ|A (v)|. Thus  by (16)
v 1 1
E|Ybits| ≤ (h(X)+(cid:15)+2δ)·P(E)+(log|S|+2)·P(Ec).
Using (9) and (10)  we see that E|Ybits| < h(X)+3(cid:15)  which contradicts (8). We therefore conclude
that P(E ) = 0 as required. (cid:3)
Lemma 6.5. For any v ∈ V  we have that T < ∞ almost surely.
Proof. Since A almost surely increases to V  it suﬃces to show that P(Z = ∅ and v ∈ A ) = 0 for
all n ≥ 1. We prove this by induction on n  taking n = 0 as a trivial base case by setting A := ∅.
Let n ≥ 1 and suppose that P(Z = ∅ and v ∈ A ) = 0. By Γ-invariance  we actually have that
v n−1
P(Z = ∅ for some w ∈ A ) = 0. We may thus assume that Z (cid:54)= ∅ for all w ∈ A . Suppose
w n−1 w n−1
now that Z = ∅ and v ∈ A . Let u be the level n agent U (v) associated to the cell A (v).
v n n n
Observe that  by the deﬁnition of Z and Zt n  we have that  for all t ≥ 0  u did not complete
level n by time t. On the other hand  since Z (cid:54)= ∅ for all w ∈ A   there exists a ﬁnite t ≥ 0
w n−1 0
such that u has reached level n by time t . It follows that u is active at time t for every t > t . By
Lemma 6.4  almost surely  no vertex is active at inﬁnitely many times  thus completing the proof
that P(Z = ∅ and v ∈ A ) = 0. (cid:3)
v n
Nowthatwehaveestablishedthattheoutputateveryvertexiseventuallydetermined itremains
Proposition 6.6. Conditioned on (Ycell Yord)  Z almost surely has the same distribution as X
where we regard X as independent of (Ycell Yord).
Proof. Throughout the proof  we regard X as independent of Ycell and Yord. We also condition
on (Ycell Yord) throughout the entire proof  without explicitly mentioning this. In particular  any
statementaboutdistributionsorindependenceshouldbeunderstoodasconditionalon(Ycell Yord).
Since every ﬁnite subset of V is almost surely contained in some cell of the cell process  it suﬃces
to show that  for any n ≥ 1 and any cell C of A   Z has the same distribution as X . We prove
n C C
this by induction on n  taking n = 0 as a trivial base case (where A := ∅).
Suppose now that n ≥ 1. Let C be a cell of A and denote C(cid:48) := C ∩A . We will show that
d
Z =X (17)
C(cid:48) C(cid:48)
P(Z ∈ · | Z = τ) = P(X ∈ · | X = τ) for any feasible τ ∈ SC(cid:48). (18)
C\\C(cid:48) C(cid:48) C\\C(cid:48) C(cid:48)
By feasible τ  we mean that P(X = τ) > 0. The desired equality in distribution Z =d X follows
C(cid:48) C C
Let ω ∈ {0 1} consist of a sequence of independent unbiased bits. Let (ωu)u∈V be a collection
of i.i.d. copies of ω  independent of Ybits. By Lemma 6.3  for any t ≥ 0  (Wt ◦ω ) has the same
distribution as (ω ) . Taking the limit as t → ∞  we see that (W∞ ◦ ω ) also has the same
u u u u u
distribution as (ω ) .
Observe that  by construction  if C is some cell of the cell process  then Zt is a function of
C
(Wt) . Taking the limit as t → ∞  it follows that Z is a function of (W∞) . It also follows
u u∈C C u u∈C
from the deﬁnition of Z and the fact that T < ∞ for all v  that Z is unchanged by concatenating
any word to any W∞. In particular  Z is also a function of (W∞◦ω ) .
u C u u u∈C
We now show (17). To this end  let C  ... C be the cells in A that are contained in C
1 m n−1
so that C(cid:48) = C ∪···∪C . By the induction hypothesis  Z =d X for every 1 ≤ j ≤ m. Since
1 m Cj Cj
A is a cell process  we have that dist(C  C ) > 1 for 1 ≤ j < j(cid:48) ≤ m. Hence  using that X is
j j(cid:48)
1-dependent  we see that {X } are independent. Thus  it remains to show that {Z }
Cj 1≤j≤m Cj 1≤j≤m
are also independent. Since Z is a function of (W∞ ◦ ω )   this follows from the fact that
Cj u u u∈Cj
{Wu∞◦ωu}u∈V are independent.
To complete the proof  it remains to show (18). Let w be the agent associated to C and recall
that w ∈ C \\C(cid:48) and that C = A (w). Note that
Z = Sout (W∞) = Sout (W∞◦ω ).
C\\C(cid:48) C C(cid:48) Z w C C(cid:48) Z w w
Recall that S is shorthand for S . Since W∞ ◦ω is independent of (W∞ ◦ω )
C C(cid:48) Z C\\C(cid:48) C(cid:48) ZC(cid:48) w w u u u(cid:54)=w
and hence also of Z   we conclude that the conditional distribution of Z given that Z = τ
C(cid:48) C\\C(cid:48) C(cid:48)
is equal to the distribution of Sout (W∞ ◦ ω ). Thus  using that W∞ ◦ ω has the same
C\\C(cid:48) C(cid:48) τ w w w w
distribution as ω  we see that the distribution in question is that of Sout (ω)  which is by
C\\C(cid:48) C(cid:48) τ
deﬁnition P(X ∈ · | X = τ)  as required. (cid:3)
C\\C(cid:48) C(cid:48)
7. Remarks and open problems
lines of proof  with minor modiﬁcations  all of which are in fact simpliﬁcations.
To obtain a proof of Theorem 1.1 with the least modiﬁcations to the existing proof  we may
process consisting of uniform [0 1] random variables. Using this order in the proof of Lemma 5.2
the entropy of the i.i.d. process). When X is ﬁnite-valued  the proof then goes through with no
26 YINONSPINKA
further modiﬁcations. Otherwise  we let Ybits consist of inﬁnitely many independent random bits
and then the proof goes through after an additional modiﬁcation to the proof of Lemma 6.4 (which
relied on the fact that the entropy of X is ﬁnite to deduce a bound on the expected number of bits
only ﬁnitely many bits; see Theorem 3.1).
“constructing a ﬁnitary coding” in Section 2. One way to implement the described coding would
be to simply replace u±t with u everywhere in the construction in Section 6. That is  instead
of having an agent u try to read an unused bit from location u+t at time t  it always reads bits
located at u. Since we may place an inﬁnite sequence of bits at every vertex  it will never run out of
available bits. In this way  there is no “moving around” of bits from one location to another. This
samples of distributions as they are needed  from the start  each Ybits is a collection (W )
v V U τ V U τ
of independent random variables having distribution P(X ∈ · | X = τ) for all ﬁnite U V ⊂ V
and τ ∈ SU. Either way  a nice feature of this construction is that the coding radius depends only
on the cell process A constructed in Section 4. Namely  the coding radius for determining X is at
most the maximum of min{r ≥ 0 : A(0) ⊂ Λ (0}}  where A(0) is the cell of 0 in A
r min{n≥1:0∈An}
process on A(0)  i.e.  (A ∩A(0)) . We elaborate on this in the next remark.
n n≥1
Remark 3. Ourmaintheoremsgivenoinformationaboutthecodingradiusbeyonditsalmost-sure
ﬁniteness. However  some information about the coding radius may be extracted from the proof
given here. Speciﬁcally  Theorem 1.1 may be enhanced to give a universal bound on the tail of the
coding radius for any ﬁxed graph and ﬁnite-dependence range. More precisely  for any transitive
amenable graph G and any integer k ≥ 1  there exists a sequence (c )∞ tending to zero such that
any k-dependent invariant random ﬁeld X on G is a ﬁnitary factor of an i.i.d. process with a coding
radius R satisfying that P(R ≥ n) ≤ c for all n. The sequence (c ) depends only on the graph G
and on the parameter k  and not on the group Γ nor on the random ﬁeld X. Indeed  the sequence
(c ) is governed by the properties of the cell process (see the last part of the previous remark). In
particular  for many concrete choices of G (and k)  an explicit sequence (c ) may be found.
To illustrate this in a simple setting  let us show that for G = Z and k = 1  one may take
c = 8/n. In this case  instead of using the construction given in Section 4  it is simpler to consider
the ﬁnitary cell process A given by A := B ∪···∪B   where (B ) are independent random
n 1 n n n≥1
subsets of Z  each being an independent Bernoulli percolation with parameter 1/2. Then the level
N := min{n : 0 ∈ A } at which 0 enters the cell process is a geometric random variable with
parameter 1/2  conditioned on which  the lengths L± := min{m ≥ 1 : ±m ∈/ A } of the cell
of 0 in A in the positive/negative directions are (independent) geometric random variables with
parameter 2−N  and the coding radius R for determining X is bounded by max{L+ L−}. Thus
P(R > r) ≤ 2·E(cid:2)(1−2−N)r(cid:3) ≤ 2(cid:88)2−ne−r2−n ≤ 4 (cid:88) 2me−2m ≤ 8
n=1 m=−∞
where we used the substitution n = (cid:98)log r(cid:99)−m.
(as in Theorem 1.2)  then it is plausible that this can be done by allowing (c ) to depend on the
entropy gap (cid:15) (and perhaps on |S|)  but we did not pursue this.
dropped). Let G be an inﬁnite transitive graph on vertex set V and let H be a ﬁnite transitive
graph on m ≥ 2 vertices. Let G(cid:48) be the graph obtained by replacing each vertex of G with a copy
of H – that is  the vertex set of G(cid:48) is V×{1 ... m}  and (u i) and (v j) are adjacent in G(cid:48) if and
only if u and v are adjacent in G  or u = v and i and j are adjacent in H. Any graph G(cid:48) obtained
H is a complete graph) around (v i) and (v j) coincide. A simple case to have in mind is when
G = Z and H consists of an edge on two vertices  so that the vertices of G(cid:48) are Z×{0 1} and there
is an edge between (u i) and (v j) if and only if |u−v| ≤ 1.
Let G(cid:48) be any graph as above and let (Wv)v∈V be independent uniform random variables on
{1 ... m}. Consider the random ﬁeld X on G(cid:48) deﬁned by X := 1 . It is clear that X is 2-
(v i) {Wv=i}
dependentandAut(G(cid:48))-invariant. WeclaimthatX isnotaAut(G(cid:48))-factorofanyi.i.d.processY on
G(cid:48) whosesingle-sitedistributionhasatleastoneatom(inparticular  Y cannothaveﬁniteentropy).
Indeed  for any such process Y  the event Y = ··· = Y has positive probability  and on
(v 1) (v m)
this event there is no Aut(G(cid:48))-equivariant way to distinguish between (v 1) ... (v m). That is
any Aut(G(cid:48))-equivariant function ϕ: TV×{1 ... m} → {0 1}V×{1 ... m} must satisfy ϕ(y) = ··· =
(v 1)
ϕ(y) whenever y ∈ TV×{1 ... m} is such that y = ··· = y . In particular  the event
(v m) (v 1) (v m)
ϕ(Y) = ··· = ϕ(Y) has positive probability  and hence  ϕ(Y) cannot have the same
We remark that there are transitive subgroups Γ of Aut(G(cid:48)) for which the above obstruction
does not exist. For example  let Γ be the subgroup of Aut(G(cid:48)) generated by Aut(G) and Aut(H)
bothofwhicharenaturallyembeddedinAut(G(cid:48)). SimplemodiﬁcationstotheproofsofLemma5.1
and Lemma 5.2 yield a Γ-invariant random total order with the desired properties. The rest of the
proof then goes through unchanged showing that our main result holds in this case: any ﬁnitely
dependent Γ-invariant process on G(cid:48) is a ﬁnitary Γ-factor of an i.i.d. process with slightly larger
replaced by a suitable condition on Γ. We did not pursue this direction.
degree at least three). Namely  is every automorphism-invariant ﬁnitely dependent process
on a tree a ﬁnitary factor of an i.i.d. process? In fact  even the more fundamental question
of whether such a process is a factor of i.i.d. (without the ﬁnitary condition) is still open;
(2) Does there exist a stationary ﬁnitely dependent process on Z (or  more generally  on some
transitive amenable graph) that cannot be expressed as a ﬁnitary factor of an i.i.d. pro-
cess with ﬁnite expected coding radius? As mentioned  the 1-dependent 4-coloring and
(3) Aﬁnitaryisomorphismisaﬁnitaryfactorthatisinvertibleandwhoseinverseisalsoﬁnitary.
Somorodinky [24] showed that every stationary ﬁnitely dependent process on Z is ﬁnitarily
isomorphictoani.i.d.process. Isthistrueinhigherdimensions? Namely iseverystationary
ﬁnitely dependent process on Zd ﬁnitarily isomorphic to an i.i.d. process?
[1] Jon Aaronson  David Gilat  and Michael Keane  On the structure of 1-dependent Markov chains  Journal of
Theoretical Probability 5 (1992)  no. 3  545–561.
dependent processes  The annals of probability (1989)  128–143.
[3] Itai Benjamini  Russell Lyons  Yuval Peres  and Oded Schramm  Group-invariant percolation on graphs  Geo-
metric & Functional Analysis GAFA 9 (1999)  no. 1  29–66.
[4] Jacob van den Berg and Jeﬀrey E Steif  On the existence and nonexistence of ﬁnitary codings for a class of
random ﬁelds  Annals of probability (1999)  1501–1522.
28 YINONSPINKA
[5] Robert M Burton  Marc Goulet  Ronald Meester  et al.  On 1-dependent processes and k-block factors  The
Annals of Probability 21 (1993)  no. 4  2157–2168.
[6] Olle Ha¨ggstro¨m  Inﬁnite clusters in dependent automorphism invariant percolation on trees  The Annals of
Probability (1997)  1423–1436.
[7] MatanHarelandYinonSpinka Finitarycodingsfortherandom-clustermodelandotherinﬁnite-rangemonotone
models  arXiv preprint arXiv:1808.02333 (2018).
[8] NateHarvey AlexanderEHolroyd YuvalPeres andDanRomik Universalﬁnitarycodeswithexponentialtails
Proceedings of the London Mathematical Society 94 (2006)  no. 2  475–496.
[9] AlexanderEHolroyd One-dependentcoloringbyﬁnitaryfactors Annalesdel’instituthenripoincar´e probabilit´es
et statistiques  2017  pp. 753–765.
[10] Alexander E Holroyd  Tom Hutchcroft  and Avi Levy  Mallows permutations and ﬁnite dependence  To appear
[11] Alexander E Holroyd  Tom Hutchcroft  and Avi Levy  Finitely dependent cycle coloring  Electronic Communi-
cations in Probability 23 (2018).
[12] AlexanderEHolroydandThomasMLiggett Symmetric 1-dependent colorings of the integers ElectronicCom-
munications in Probability 20 (2015).
[13] Alexander E Holroyd and Thomas M Liggett  Finitely dependent coloring  Forum of mathematics  pi  2016.
[14] Alexander E Holroyd  Oded Schramm  and David B Wilson  Finitary coloring  The Annals of Probability 45
(2017)  no. 5  2867–2898.
[15] IA Ibragimov and YV Linnik  Independent and stationarily connected variables  Izdat. Nauka  Moscow (1965).
[16] IA Ibragimov and YV Linnik  Independent and stationary sequences of random variables  Wolters  Noordhoﬀ
[17] MichaelKeaneandMeirSmorodinsky Aclassofﬁnitarycodes IsraelJournalofMathematics26(1977) no.3-4
352–371.
[18] Michael Keane and Meir Smorodinsky  Bernoulli schemes of the same entropy are ﬁnitarily isomorphic  Annals
of Mathematics 109 (1979)  no. 2  397–406.
[19] Donald Knuth and Andrew Yao  The complexity of nonuniform random number generation  Algorithm and
Complexity  New Directions and Results (1976)  357–428.
[20] Nathan Linial  Distributive graph algorithms global solutions from local data  Foundations of computer science
1987.  28th annual symposium on  1987  pp. 331–335.
[21] Russell Lyons  Factors of IID on trees  Combinatorics  Probability and Computing 26 (2017)  no. 2  285–300.
[22] Russell Lyons and Yuval Peres  Probability on trees and networks  Vol. 42  Cambridge University Press  2017.
[23] Moni Naor  A lower bound on probabilistic algorithms for distributive ring coloring  SIAM Journal on Discrete
Mathematics 4 (1991)  no. 3  409–412.
[24] Meir Smorodinsky  Finitary isomorphism of m-dependent processes  Symbolic dynamics and its applications
(1992)  373–376.
[25] Yinon Spinka  Finitary coding for the sub-critical Ising model with ﬁnite expected coding volume  To appear in
E-mail address: yinon@math.ubc.ca
Abstract. We show that any ﬁnitely dependent invariant process on a transitive amenable graph
is a ﬁnitary factor of an i.i.d. process. With an additional assumption on the geometry of the graph
namely that no two balls with diﬀerent centers are identical  we further show that the i.i.d. process
process X is said to be ﬁnitely dependent if its restrictions to sets which are suﬃciently separated (at
is a process Y = (Yv)v∈V in which all random variables are independent. A natural question is then
how close is a ﬁnitely dependent process to such an independent process? Before addressing this
question  we ﬁrst observe that “local functions” of an independent process Y are always ﬁnitely
dependent. That is  if X is obtained from Y by computing each Xv as a function only of the random
variables Yu for which u is at a uniformly bounded distance from v  then X is ﬁnitely dependent.
invariant under all automorphisms of G (or under a transitive subgroup of automorphisms). In
particular  the independent process Y considered above must now be an i.i.d. process – that is  in
addition to being independent  the {Yv}v are also identically distributed. If X is obtained from Y by
applying the same local function at each vertex v (i.e.  the function applied at u is the composition
of the function applied at v with any automorphism taking u to v)  then X is said to be a block
factor of Y . Thus  block factors of i.i.d. processes provide a recipe for constructing invariant ﬁnitely
are the only (invariant) ﬁnitely dependent processes on Z  until ﬁnally an example was given by
Burton–Goulet–Meester [5] of a 1-dependent process which is not a block factor of any i.i.d. process.
of i.i.d. processes and ﬁnitely dependent processes – no proper coloring of Z is a block factor of an
process Y and an invariant rule for computing X from Y   which allows to determine the value of Xv
thing” – namely  that it is possible to determine Xv by looking at Y on a variable-sized window
around v  where the size of the window  though always ﬁnite  may vary according to the input Y .
value of X at any vertex v by only looking at variables Yu for which u is within a certain ﬁnite
arXiv:1901.00123v2  [math.PR]  19 Jan 2020
and (Xv)v∈V are independent for any two sets U  V ⊂ V such that dist(u  v) > k for all u ∈ U and
v ∈ V . We say that X is ﬁnitely dependent if it is k-dependent for some ﬁnite k.
tion ϕ: T V → SV that is Γ-equivariant  i.e.  commutes with the action of every element of Γ  and
satisﬁes that ϕ(Y ) and X are identical in distribution. Such a coding is also called a factor map
from Y to X  and when such a coding exists  we say that X is a Γ-factor of Y .
Suppose now that S and T are countable. Let 0 ∈ V be a distinguished vertex. The coding radius
of ϕ at a point y ∈ T V  denoted by R(y)  is the minimal integer r ≥ 0 such that ϕ(y′)0 = ϕ(y)0
for all y′ ∈ T V which coincide with y on the ball of radius r around 0 in the graph-distance  i.e.
y′
v = yv for all v ∈ V such that dist(v  0) ≤ r. It may happen that no such r exists  in which case
R(y) = ∞. Thus  associated to a coding is a random variable R = R(Y ) which describes the coding
ﬁnite. When there exists a ﬁnitary coding from Y to X  we say that X is a ﬁnitary Γ-factor of Y .
A graph is said to be amenable if inf |∂V |/|V | = 0  where the inﬁmum is over all ﬁnite non-empty
subsets V of V  and where ∂V denotes the edge-boundary of V .
Λr(u) ̸= Λr(v)
for any distinct u  v ∈ V and r ≥ 0
(1)
where Λr(u) is the ball of radius r around u.
G. Then for any ϵ > 0 there exists an i.i.d. process Y with entropy h(Y ) < h(X) + ϵ such that X
is a ﬁnitary Γ-factor of Y .
1We will only be concerned with spaces T which are ﬁnite  countable or of the form T1 × T2 × · · · for ﬁnite sets
(Ti). In the latter case  the coding radius is the smallest r for which there exists n such that ϕ(y′)0 = ϕ(y)0 for all
y′ having the property that y′
v i = yv i for all (v  i) such that dist(v  0) ≤ r and 1 ≤ i ≤ n.
3
common plan in which each machine v is assigned a speciﬁc role Xv  then ﬁnite dependence provides
this result (if one is content with a ﬁnitary factor  rather than a ﬁnitary isomorphism)  as it provides
this question was made [1  2] until it was ﬁnally resolved in 1993 by Burton–Goulet–Meester [5]
ﬁnitary factor of an i.i.d. process (with slightly larger entropy)  answering aﬃrmatively question (iii)
4
colorings of Z which are ﬁnitary factors of i.i.d. processes with exponential tail on the coding radius.
Indeed  they showed that such a k-dependent q-coloring exists when (k  q) is either (1  5)  (2  4) or
(3  3). On the other hand  it is believed [9  10] that when (k  q) is (1  4) or (2  3)  no k-dependent
q-coloring is a ﬁnitary factor of an i.i.d. process with ﬁnite expected coding radius. We mention that
(see Remark 3).
For example  for 1-dependent processes on Z  the ﬁnitary factor provided by
1.3. Acknowledgments. I would like to thank Omer Angel  Nishant Chandgotia  Tom Meyerovitch
5
morphism group of G that acts transitively on V.
The full automorphism group of G is de-
noted by Aut(G).
The graph distance in G is denoted by dist(·  ·).
For sets U  V ⊂ V  we
write dist(U  V ) := minu∈U v∈V dist(u  v) and dist(u  V ) := dist({u}  V ).
For r ≥ 0  denote
V +r := {u ∈ V : dist(u  V ) ≤ r} and V −r := {u ∈ V : dist(u  V c) > r}. The ball of radius r
around v is denoted by Λr(v) := {v}+r. The neighborhood of V is N(V ) := V +1 \\ V and the
edge-boundary of V is ∂V := {{u  v} ∈ E(G) : v ∈ V  u /∈ V }.
process Y . The construction of the ﬁnitary coding involves the use of three sources of randomness:
total order on V. The ﬁrst of these three will simply be given by an i.i.d. process  denoted Y bits
while the latter two will be obtained as ﬁnitary factors of diﬀerent i.i.d. processes  denoted Y cell and
Y ord. In turn  Y will be a triplet Y = (Y bits  Y cell  Y ord) consisting of three mutually independent
is the setting of Theorem 1.1). In this case  Y bits
and Y ord
may be taken to be uniform
random variables in [0  1]  and the total order may be taken to be the one induced by the
usual order on Y ord
(2) Controlling the entropy: The second part is how to control the entropy of the Y bits
issue of controlling the entropy of Y ord  we shall assume in this part of the proof outline
that the vertices of G can be deterministically ordered in a Γ-invariant manner so that Y ord
(3) Constructing a random order: The third part is how to allow for graphs G and groups Γ
which do not admit such a deterministic order.
This is of course the case for general
assumption  though not at all essential  is convenient as it obviates the need to work with a diﬀerent
A cell process is a random sequence A = (A1  A2  . . . ) of subsets of V satisfying the following
• A1 ⊂ A2 ⊂ A3 ⊂ · · · .
6
• A1 ∪ A2 ∪ · · · = V.
• For each n ≥ 1  all connected components of An are ﬁnite.
We will obtain a cell process A as a ﬁnitary factor of an i.i.d. process Y cell with arbitrarily small
in inﬁnitely many steps with the idea that at the end of step n ∈ {1  2  . . . }  we will have deﬁned
X on the region An. In the ﬁrst step  we sample X on the set A1 – this is particularly simple as
the cells in A1 are at pairwise distance at least 2  and so  due to the 1-dependence assumption on
X  each cell in A1 can be sampled independently. Next  at each step n ∈ {2  3  . . . }  we sample X
on the region An \\ An−1  conditioned on the value of X on An−1  which has already been sampled
on X  the values of X on diﬀerent cells in An are conditionally independent – indeed  if {Vi}i are
at pairwise distance at least 2 from one another  then for any sets Ui ⊂ Vi  given {XUi}i  one has
that {XVi}i are mutually conditionally independent. Since all the cells of every An are ﬁnite  the
given cell depends only on the previously sampled values within that cell. Since An increases to V
in A1  we distinguish a vertex by choosing the smallest element in the cell according to the order
given by Y ord. We call these distinguished vertices level 1 agents. Similarly  for each n ≥ 2 and
each cell in An that is not contained in An−1  we select a level n agent in the cell by choosing the
smallest element in the cell which is not in An−1. Note that the level n agents are obtained as a
ﬁnitary factor of (Y cell  Y ord). With the notion of agents  we may now say more precisely that  in
step n above  if C is a cell of An that is not contained in An−1  then we sample X on the region
C \\ An−1 (conditionally on the previously sampled values of X on C ∩ An−1) by accessing a sample
of the desired distribution from the random variable Y bits
where u is the unique level n agent in C
using also the order induced by Y ord on C \\ An−1 to break any symmetries which may be present
C \\ An−1 is a symmetric interval around u  the ‘left’ and ‘right’ sides of u cannot be diﬀerentiated
simple way to get rid of such problems). In this interpretation  we regard Y bits
as consisting of
independent samples of P(XU ∈ · | XV = τ) for all ﬁnite U  V ⊂ Zd and τ ∈ SV   most of which are
waste in the above construction (in terms of the process Y bits). The problem is that we do not
to this is to place an inﬁnite sequence of random bits at each site  i.e.  Y bits
∈ {0  1}N  from which
we may easily construct samples of any desired distributions (hence the name of the process Y bits).
Of course  this idea alone still does not provide any control on the entropy of Y bits. For this  we
must place a ﬁnite (perhaps random) number of random bits at each site  and somehow still be sure
a random variable W taking values in {0  1}∗  the set of ﬁnite words over {0  1}  and having the
property that  conditioned on the length |W| of the word  W is uniformly distributed on {0  1}|W|.
that u ≤ v implies that γu ≤ γv for any u  v ∈ V and γ ∈ Γ. For instance  the lexicographical order
7
it is convenient to further suppose that every v ∈ V has a ≤-successor  which we denote by v + 1
as is the case for the lexicographical order on Zd (in which case v + 1 is simply v + (1  0  . . .   0)).
The existence of such a deterministic order renders Y ord unneeded  allowing us to focus now only
on the task of controlling the entropy of Y bits.
outputs a sample of the distribution in question from an input of unbiased random bits.
The
also be close to h(X)  the entropy of X itself. Thus  it will be important that the cells in A1 are
a ﬁnite number of bits at each site (more precisely  we need that h(Y bits) < h(X) + ϵ)  and even
than are available in Y bits
. To solve this  we must allow to “transfer” bits from one location to
whenever an agent u requires access to an additional bit (beyond those available in Y bits
)  it may
look for an “unused” bit at u + 1 (the ≤-successor of u). If there are no available unused bits at
u + 1 at that time  it may then proceed to look at u + 2  and so on.
construction  we will have deﬁned X on the region An. Instead  at any step of the construction
The way this is done is as follows. Initially  at time t = 0  all level 1 agents are deemed active. An
active once all level n − 1 agents contained in its associated cell have completed.
only be read from site u + t. This has the advantage that it ensures that no two agents ever try to
process Y ord with arbitrarily small entropy. Here ﬁnitary means that the order induced on any
ﬁnite set of vertices is determined by a ﬁnite (random) subset of {Y ord
}v∈V.
the same as that of Z. That is  almost surely  every element v has a successor v+1 and a predecessor
v − 1  and {v + n}n∈Z = V. Though it does not follow from the above deﬁnition of ﬁnitary  it will
8
is also a ﬁnitary property (i.e.  it is almost surely determined by a ﬁnite subset of {Y ord
}v∈V). Once
H(Z) := −
�
P(Z = z) log P(Z = z)
where the sum is taken over z in the support of Z  or alternatively  we interpret 0 log 0 to be 0.
h(X) :=
inf
V ⊂V ﬁnite
and non-empty
H(XV )
A Følner sequence in G is a sequence (Fn)∞
n=1 of non-empty ﬁnite subsets of V such that
n→∞
|∂Fn|
|Fn| = 0.
H(XFn)
|Fn|
for any Følner sequence (Fn)∞
n=1 in G.
It follows that for any ϵ > 0 there exists δ > 0 such that
H(XF )
≤ h(X) + ϵ
whenever F ⊂ V is non-empty and ﬁnite and |∂F| ≤ δ|F|.
(2)
H(XF | E)
≤ log |S|
whenever F⊂V is non-empty and ﬁnite
and E is an event with positive probability
(3)
entropy h(Y ) is equal to the entropy of its single-site distribution H(Y0).
3.2. The mass-transport principle. For u  v ∈ V  denote
Γu v := {γ ∈ Γ : γu = v}.
Note that Γu u is the stabilizer of u. We say that Γ is unimodular if
|Γu uv| = |Γv vu|
for all u  v ∈ V.
f(u  0) =
f(0  v)
for any diagonally Γ-invariant function f : V2 → [0  ∞].
(4)
By diagonally Γ-invariant  we mean that f(γu  γv) = f(u  v) for all u  v ∈ V and γ ∈ Γ. We note the
A simulation of µ is a pair S = (Stime  Sout) of measurable functions Stime : {0  1}N → N ∪ {∞} and
Sout : {0  1}N → Ω with the properties:
• If Stime(x) = n for some x ∈ {0  1}N and n ∈ N  then Stime(x′) = n and Sout(x′) = Sout(x)
for any x′ ∈ {0  1}N which coincides with x on {1  . . .   n}.
adapted to the σ-algebra generated by {ωi : 1 ≤ i ≤ Stime} – that is  the simulation reads one input
pendent unbiased bits ω satisfying that Stime(ω) < ∞ almost surely and EStime(ω) ≤ H(Z) + 2.
Recall from Section 2 that a cell process is a random sequence A = (A1  A2  . . . ) of subsets
increasing to V and satisfying that the connected components of each An are ﬁnite.
We may
Proposition 4.1. Let G be a transitive amenable graph and let ϵ > 0.
There exists an i.i.d.
process Y of entropy at most ϵ and a cell process A which is a ﬁnitary Aut(G)-factor of Y .
the cells of A1 (after slightly decreasing the Voronoi cells to ensure that they are well-separated).
then used to obtain A2 from A1 by “ﬁlling in” some of the empty space between the cells of A1
taking care not to connect cells of A1 which are not in the same Voronoi cell (thus ensuring that
A1 ⊂ A2 ⊂ · · · of sets (each having only ﬁnite cells) as a ﬁnitary factor of a small entropy i.i.d.
10
(a) A1
(b) Going from A1 to A2
(c) A2
Figure 1. Constructing the cell process. The cells of A1 are simply the Voronoi
cells of a Bernoulli process. To get from A1 to A2  we consider the Voronoi cells of a
lower-density Bernoulli process  and “merge” cells of A1 which are entirely contained
green depicts cells of A2 which are not cells of A1.
process. It will then only remain to show that An increases to V. This is where amenability comes
To ensure that An increases to V  we must be careful in how we deﬁne the Voronoi cells. When
to be diagonally Γ-invariant  we need to choose a Følner sequence (Fn)n having the property that
each Fn is invariant under the stabilizer of some ﬁxed vertex 0 ∈ V  i.e.  Fn = Γ0 0Fn for all n.
There exists a Følner sequence (Fn)n in G such that Fn = Γ0 0Fn for all n.
taking Γ0 to be the stabilizer Γ0 0 of 0.
Lemma 4.3. Let G be any graph and let Γ0 be a group of automorphisms of G under which all
|∂E|
|E| ≤ |∂F|
E = Γ0E.
11
Let {Vi}i be the orbits of V under the action of Γ0. Thus  {Vi}i is a partition of V such that each
Vi is ﬁnite (by assumption) and satisﬁes Vi = Γ0Vi.
Let U be a uniform random variable in [0  1] and deﬁne
E :=
Vi
where pi := |F ∩ Vi|
|Vi|
Thus  each orbit Vi is included in E with probability pi  where the choices for diﬀerent i are
E|E| =
P(U ≤ pi) · |Vi| =
pi|Vi| =
|F ∩ Vi| = |F|
E|∂E| =
P(pj < U ≤ pi) · |∂(Vi  Vj)| =
max{pi − pj  0} · |∂(Vi  Vj)|
where ∂(U  V ) denotes the set of edges between two disjoint sets U and V . Let us show that each
term in the second sum is at most |∂(F ∩ Vi  F c ∩ Vj)|  i.e.
pi − pj ≤ |∂(F ∩ Vi  F c ∩ Vj)|
|∂(Vi  Vj)|
whenever pi > pj and ∂(Vi  Vj) ̸= ∅.
To see this  note that the right-hand side is the probability that an edge e = {u  v} that is uniformly
chosen from ∂(Vi  Vj) belongs to ∂(F ∩Vi  F c ∩Vj)  or equivalently  with the convention that u ∈ Vi
and v ∈ Vj  that u ∈ F and v /∈ F. Since this probability is at least P(u ∈ F)−P(v ∈ F)  it suﬃces
to show that u and v are uniformly distributed in Vi and Vj  respectively. This follows from the
observation that the bipartite graph (Vi ∪Vj  ∂(Vi  Vj)) is biregular – each vertex in Vi is adjacent to
the same number of vertices in Vj  and similarly  each vertex in Vj is adjacent to the same number
of vertices in Vi. Indeed  for any u  v ∈ Vi and γ ∈ Γ0 such that γu = v  the mapping w �→ γw
deﬁnes a bijection between N(u) ∩ Vj and N(v) ∩ Vj.
E|∂E| ≤
|∂(F ∩ Vi  F c ∩ Vj)| = |∂F| = α · E|E|
α|E| − |∂E| is zero when E is empty  conditioned on E ̸= ∅  we still have that α|E| − |∂E| has
non-negative expectation. In particular  there is positive probability that E ̸= ∅ and α|E| ≥ |∂E|.
Finally  since E is ﬁnite and satisﬁes E = Γ0E almost surely  we see that E satisﬁes the desired
properties with positive probability.
□
Suppose that (Fn)n is a Følner sequence guaranteed by Lemma 4.2  i.e.  Fn = Γ0 0Fn for all n
and further suppose that F1 ⊊ F2 ⊊ · · · and F1 ∪ F2 ∪ · · · = V (there is clearly no loss in generality
in doing so). Recall the deﬁnition of Γu v from Section 3.2. For u  v ∈ V  deﬁne
ρ(u  v) := min
n ≥ 1 : v ∈ Γ0 uFn
Using that Γ0 γu = γΓ0 u for any γ ∈ Γ  one easily checks that ρ is diagonally Γ-invariant  i.e.
ρ(γu  γv) = ρ(u  v) for all γ ∈ Γ. We stress that ρ is not necessarily symmetric in that ρ(u  v) may
not equal ρ(v  u). In particular  we do not claim that ρ is a metric. Nevertheless  we still think of
ρ(u  v) as a measure of distance from v to u. One nice property of ρ that is easily veriﬁable and
which will be important is that  for any sequence of pairs of vertices (ui  vi)∞
ρ(ui  vi) → ∞ as i → ∞
if and only if
dist(ui  vi) → ∞ as i → ∞.
(5)
require the following lemma to address.
We note that ρ is indeed symmetric when the Følner
sequence (Fn)n consists of balls  and that it is nearly symmetric when G is a Cayley graph of Γ
12
in which case switching the roles of u and v in ρ(u  v) has the same eﬀect as replacing each Fn
with F −1
n . Accordingly  in these cases  it is immediate that the two “ρ-balls” of radius n around 0
{v : ρ(0  v) ≤ n} and {u : ρ(u  0) ≤ n}  have the same size  namely |Fn|. The following lemma
Let F ⊂ V be invariant under the stabilizer of some vertex 0  i.e.  Γ0 0F = F. Then
|{u ∈ V : 0 ∈ Γ0 uF}| = |F|.
Proof. Deﬁne f : V2 → [0  1] by
f(u  v) := 1{v∈Γ0 uF}.
Since Γ0 γu = γΓ0 u for any γ ∈ Γ  it follows that f is diagonally Γ-invariant.
Thus  by the
|{u ∈ V : 0 ∈ Γ0 uF}| =
f(0  v) = |Γ0 0F| = |F|.
Proof of Proposition 4.1. Let (ϵn) be a sequence to be chosen later which satisﬁes that 0 ≤ ϵn ≤
ϵ2−n. We shall construct a cell process A as a ﬁnitary factor of the i.i.d. process Y = (Yv)v∈V in
which Yv = (Yv n)n≥1 are independent random variables with Yv n ∼ Ber(ϵn). The entropy of Y
h(Y ) = H(Yv) =
∞
n=1
H(Yv n) =
ϵn log 1
ϵn + (1 − ϵn) log
1−ϵn
≤ 10ϵ log 1
ϵ.
Un := {v ∈ V : Yv n = 1}
for any given n  to construct Voronoi cells  which are then used to deﬁne the cells of An. Precisely
¯CU(u) :=
v ∈ V : ρ(u  v) < ρ(u′  v) for all u′ ∈ U \\ {u}
u ∈ U.
Thus  the Voronoi cell ¯CU(u) associated to u consists of all vertices v ∈ V which are closer (in the
distance measured by ρ) to u than to any other u′ ∈ U. In particular  Voronoi cells associated to
to one another). We therefore deﬁne modiﬁed Voronoi cells by slightly shrinking the sets ¯CU(u).
CU(u) := ( ¯CU(u))−1.
Using that the Voronoi cells are disjoint  it is straightforward to check that dist(CU(u)  CU(u′)) > 1
for distinct u  u′ ∈ U. We note that CU(u) (in fact  already ¯CU(u)) may be empty and need not be
this end  it suﬃces to show that the probability that ¯CU(0) intersects Fn \\Fn−1 is summable over n.
Indeed  since a ﬁxed vertex v ∈ Fn \\ Fn−1 belongs to ¯CU(0) only if U \\ {0} contains no element of
{u : ρ(u  v) ≤ n}  it follows from Lemma 4.4 that the probability of this is at most p|Fn|−1  where p
is the density of the Bernoulli process. Since |Fn| ≥ n  we see that |Fn| · p|Fn|−1 is summable  and
hence that the ¯CU(0) is almost surely ﬁnite.
13
simply taken to be the vertices in a modiﬁed Voronoi cell of U1  i.e.
A1 :=
CU1(u).
Since the Voronio cells of U1 are almost surely ﬁnite  we see that CU1(u) is almost surely ﬁnite
for all u ∈ U1. Since dist(CU1(u)  CU1(u′)) > 1 for distinct u  u′ ∈ U1  it follows that all connected
components of A1 are almost surely ﬁnite.
Suppose now that  for some n ≥ 1  An has been deﬁned in such a way that all connected
components of An are almost surely ﬁnite  and let us now deﬁne An+1. Let A′
n+1 be the union of
the modiﬁed Voronoi cells of Un+1  i.e.
A′
n+1 :=
CUn+1(u)
and recall that (as for A1) all connected components of A′
n+1 are almost surely ﬁnite. Intuitively  we
would like to obtain An+1 from An by adding A′
n+1. However  this might create inﬁnite clusters
and we must take care to avoid this by instead only adding a suitable subset of A′
suﬃce to slightly increase the “forbidden region” (A′
n+1)c as follows: let Dn+1 denote the union
of the connected components of An that intersect (A′
n+1)c  and add D+1
n+1 to the forbidden region.
An+1 := An ∪ (A′
n+1 \\ D+1
n+1).
It is straightforward to check that all connected components of An+1 are almost surely ﬁnite.
Assuming that A1 ∪A2 ∪· · · = V almost surely  it is also easy to check using (5) that A is a ﬁnitary
Aut(G)-factor of Y   which would complete the proof of the proposition.
It remains only to show that A1 ∪ A2 ∪ · · · = V almost surely. By Aut(G)-invariance  this is
equivalent to the fact that P(0 ∈ An) → 1 as n → ∞. For n ≥ 1  let Bn denote the connected
component of 0 in An ∪ {0}. For n ≥ 2  deﬁne the event
En :=
Bn−1 ⊂ CUn(u) for some u ∈ Un
{0 ∈ An \\ An−1} = En ∩ {0 /∈ An−1}.
P(En | 0 /∈ An−1) ≥ c
for some c > 0 and all n ≥ 2.
As we now show  this holds when ϵn is suitably chosen. Since Bn−1 is almost surely ﬁnite  there
exists a suﬃciently large rn so that
P(Bn−1 ⊂ Λrn−1 | 0 /∈ An−1) ≥ 1
where Λr := Λr(0). Since (Fs)s is a Følner sequence  there exists sn suﬃciently large so that
|Fsn| ≥ 2n
ϵ
|∂Fsn|
|Fsn| ≤
2|Λrn|.
(6)
Set ϵn := |Fsn|−1. Then  noting that An−1 (and thus also Bn−1) is independent of Un
P(En | 0 /∈ An−1) ≥ P
Bn−1 ⊂ Λrn−1 and Λrn−1 ⊂ CUn(u) for some u ∈ Un | 0 /∈ An−1
= P
Bn−1 ⊂ Λrn−1 | 0 /∈ An−1
· P
Λrn−1 ⊂ CUn(u) for some u ∈ Un
Since the ﬁrst term on the right-hand side is at least 1
2 by the choice of rn  it remains to show that
≥ c.
14
Let us ﬁrst see how to show this when Fsn is a ball  say Λℓ. In this case  it is not hard to see that
the event in question occurs when Un has a unique point in Λℓ−rn and no other point in Λℓ+rn  so
≥ P
|Un ∩ Λℓ−rn| = |Un ∩ Λℓ+rn| = 1
Ber(|Λℓ−rn|  ϵn) = 1
Ber(|Λℓ+rn \\ Λℓ−rn|  ϵn) = 0
We now handle the general case in more detail. Set U := Un. Our goal is to bound from below
the probability that Λrn−1 ⊂ CU(u) for some u ∈ U. To this end  we ﬁrst ﬁnd a simple condition
M(F) := {u ∈ V : 0 ∈ Γ0 uF}.
Set r := rn and s := sn. Let us show that
|U ∩ M(F −r
s
)| = |U ∩ M(F +r
)| = 1
=⇒
Λr−1 ⊂ CU(u) for some u ∈ U.
(7)
Suppose that the left-hand side holds. Let us show that Λr−1 ⊂ CU(u)  where u is the unique
element in U ∩ M(F −r
). By the deﬁnition of CU(u)  this is equivalent to Λr ⊂ ¯CU(u). Recalling
the deﬁnition of ¯CU(u)  we see that we must show that ρ(u  w) < ρ(u′  w) for all u′ ∈ U \\ {u} and
w ∈ Λr. Let u′ ∈ U \\ {u} and w ∈ Λr. It suﬃces to show that ρ(u  w) ≤ s and ρ(u′  w) > s.
Towards showing this  we ﬁrst note that (γV )+1 = γ(V +1) and (γV )−1 = γ(V −1) for any γ ∈ Γ
and V ⊂ V  due to the fact that γ acts by an automorphism of G. In particular  (Γ0 uV )+r =
Γ0 u(V +r) and (Γ0 uV )−r = Γ0 u(V −r)  and we may drop the parenthesis when writing such terms.
Let us now show that ρ(u  w) ≤ s. Since u ∈ M(F −r
)  we have that 0 ∈ Γ0 uF −r
or equivalently
Λr ⊂ Γ0 uFs. Since w ∈ Λr  it follows that ρ(u  w) ≤ s. Next  we show that ρ(u′  w) > s. Note that
u′ /∈ M(F +r
) since u ∈ M(F −r
) ⊂ M(F +r
). Thus  0 /∈ Γ0 u′F +r
or equivalently  Λr ∩ Γ0 u′Fs = ∅.
Thus  w /∈ Γ0 u′Fs and  using that F1  F2  . . .   Fs−1 ⊂ Fs  it follows that ρ(u′  w) > s.
|Un ∩ M(F −rn
sn
)| = |Un ∩ M(F +rn
Ber(|M(F −rn
)|  ϵn) = 1
Ber(|M(F +rn
)| − |M(F −rn
)|  ϵn) = 0
|M(F −rn
)| = |F −rn
|
|M(F +rn
)| = |F +rn
|.
|F −rn
| ≥ |Fsn| − |∂Fsn| · |Λrn| ≥ 1
2|Fsn|
|F +rn
| ≤ |Fsn| + |∂Fsn| · |Λrn| ≤ 3
bounded below by a positive constant.
The above proposition established the existence of a ﬁnitary cell process A. In particular  An is
is Γ-invariant. Let Cv denote the cluster of v in B. Then  for any δ > 0
|∂C0| ≥ δ|C0|
≤ ( d
δ + 1) · P(0 /∈ B).
15
Proof. The proof uses the mass-transport principle. Deﬁne ψ: V2 → [0  1] by
ψ(u  v) :=
� |N(u)∩Cv|
|Cv|
if u /∈ B  v ∈ B
otherwise
ψ(u  0) = 10∈B
|C0| ·
u/∈B
|N(u) ∩ C0| = |∂C0|
|C0| · 10∈B
ψ(0  v) = 10/∈B ·
v∈B
|N(0) ∩ Cv|
= |N(0) ∩ B| · 10/∈B ≤ d · 10/∈B.
The Γ-invariance of B implies that f(u  v) := Eψ(u  v) is diagonally Γ-invariant. Thus  the mass-
E
|∂C0|
≤ d · P(0 /∈ B).
The proposition now follows from Markov’s inequality.
tree. Moreover  by Lemma 4.5 (see also the closely related [3  Theorem 1.2])  we see that Γ-invariant
relation on V  may be regarded as a random element in {0  1}V2. With this viewpoint  the notion
has the same distribution as ϕ(Y ) for some measurable function ϕ: T V → {0  1}V2 satisfying that
ϕ(y)(u v) = ϕ(γy)(γu γv) for all γ ∈ Γ  u  v ∈ V and y ∈ T V. Such a factor is ﬁnitary if for every
u  v ∈ V there almost surely exists a ﬁnite (random) set W ⊂ V such that ϕ(Y )(u v) is determined
by (Yw)w∈W   in the sense that ϕ(y)(u v) = ϕ(Y )(u v) for any y ∈ T V which coincides with Y on W.
between the two ordered spaces  i.e.  a bijection f : V → Z such that f(u) ≤ f(v) if and only if
can be found in a ﬁnitary manner if for every u  v ∈ V there almost surely exists a ﬁnite (random)
16
(Yw)w∈W . We note that  in general  there is no direct relation to the notion of ﬁnitary factor: it
manner  or it may that successors/predecessors can be found in a ﬁnitary manner though the factor
from the order induced by uniform random variables in [0  1] assigned to each vertex. It is easy
Lemma 5.1. Let G be a transitive non-empty graph satisfying (1). For any 0 < ϵ ≤ 1
2 there exists
a total order on V which is a ﬁnitary Aut(G)-factor of an i.i.d. Bernoulli process with density ϵ.
Proof. Let η = (ηv)v∈V be an i.i.d. Bernoulli process with density ϵ.
For any v ∈ V  deﬁne
Zv = (Zv n)n≥0 ∈ {0  1  . . . }{0 1 ... } by
Zv n :=
ηu.
Deﬁne a relation ≤ on V in which u ≤ v if and only if Zu ⪯ Zv  where ⪯ denotes the lexicographical
order on {0  1  . . . }{0 1 ... }. Then ≤ is clearly a Aut(G)-factor of η.
It remains to show that ≤ is almost surely a total order on V and that the factor is ﬁnitary. Since
≤ is clearly a preorder  to show that it is a total order  it suﬃces to show that P(Zu = Zv) = 0 for
distinct u  v ∈ V. It then follows from the deﬁnition of the lexicographical order that the factor is
Fix u  v ∈ V distinct and consider the event
n�
{Zu i = Zv i}.
Since P(En) → P(Zu = Zv) as n → ∞  it suﬃces to show that P(En | En−1) ≤ 1 − ϵ for all n ≥ 1.
By (1) and the assumption that the graph is non-empty  we have Λn(u) \\ Λn−1(u) ̸⊂ Λn(v)  as
otherwise Λ3n(u) ⊂ Λ3n(v)  which in turn implies that Λ3n(u) = Λ3n(v) by transitivity. Thus
there exists some wn ∈ Λn(u) \\ (Λn−1(u) ∪ Λn(v)). Then
En | ηV\\{wn}
≤ max
k∈Z P(ηwn = k) = max{ϵ  1 − ϵ} = 1 − ϵ.
Since En−1 is measurable with respect to ηV\\{wn}  it follows that P(En | En−1) ≤ 1 − ϵ.
Lemma 5.2. Let G be a transitive amenable non-empty graph satisfying (1) and let ϵ > 0. Then
there exists an i.i.d. process Y with entropy at most ϵ  and a random total order ≤ on V which
Proof. By Lemma 5.1  there exists a total order ⪯ on V that is a ﬁnitary factor of an i.i.d. process Y
having entropy at most ϵ. By Proposition 4.1  there exist a cell process A that is a ﬁnitary factor of
17
an i.i.d. process Y ′ (which we take to be independent of Y ) having entropy at most ϵ. We construct
the required total order ≤ on V as a ﬁnitary factor of (Y  Y ′).
The idea is to use ⪯ to order the sites within the cells given by A. More precisely  we will deﬁne
an increasing sequence of partial orders ≤1 ⊂ ≤2 ⊂ . . . such that each ≤n induces a total order on
every cell of An. Since �
n An = V  this will produce a total order ≤ given by the union �
n ≤n
Precisely  we deﬁne ≤1 to be the relation in which u ≤1 v whenever u and v belong to the same
cell of A1 and satisfy that u ⪯ v. Then ≤1 is clearly a partial order that induces a total order
on any cell of A1. In fact  it is the union of these total orders on the cells of A1 (that is  it only
Next  suppose we have deﬁned the partial order ≤n−1 so that it is a union of total orders on the
cells on An−1  and let us deﬁne ≤n. Consider a cell C of An and let D := C ∩An−1 = D1 ∪· · ·∪Dk
be the union of the cells D1  . . .   Dn of An−1 that are contained in C. We deﬁne ≤n in such a
way that D ≤n C \\ D by requiring that u ≤n v whenever u ∈ D and v ∈ C \\ D. To obtain a
total order on C  it remains to order the vertices in D and the vertices in C \\ D. The latter is
ordered by deﬁning u ≤n v whenever u  v ∈ C \\ D and u ⪯ v. The former is ordered by giving
an order to the cells D1  . . .   Dk and using the ≤n−1 order within each cell – that is  we require
that ≤n coincides with ≤n−1 on each cell Di  and that either Di ≤n Dj or Dj ≤n Di for any
two cells Di and Dj. Finally  the order of the cells is determined by requiring that Di ≤n Dj
whenever min⪯ Di ⪯ min⪯ Dj. That is  the i-th cell precedes the j-cell in ≤n if and only if the
⪯-minimal element in the i-th cell is ⪯-smaller than the ⪯-minimal element in the j-th cell. It is
straightforward that ≤n extends ≤n−1 and that ≤n is a union of total orders on the cells of An.
We have thus obtained partial orders ≤1  ≤2  . . . such that  for each n  ≤n extends ≤n−1 and is a
union of total orders on the cells of An. To show that ≤ has the order type of Z  it remains to show
follows from the construction that if u is the ≤n-successor of v  then it is also its ≤n+1-successor.
Thus  to conclude that every ≤-interval is ﬁnite  it suﬃces to show that  for every u  v ∈ V having
u ≤ v  there exists n such that u ≤n v and the interval [u  v]≤n is ﬁnite. Indeed  since ≤n only
compares vertices within the same cell of An and since all such cells are ﬁnite  all ≤n-intervals are
We have thus established that ≤ almost surely has the same order type as Z. It is straightforward
from the fact that the ≤n-successor of a vertex u (if it exists) is also the ≤n+1-successor of u  that
the constructed factor from (Y  Y ′) to ≤ has the property that successors/predecessors can be found
also ﬁnitary.
(Y bits  Y cell  Y ord) to X. Recall also that A will be a cell process that is a ﬁnitary factor of Y cell
and that ≤ will be a random total order on V that is a ﬁnitary factor of Y ord  has the order type of
A and the total order ≤  we will use the additional randomness in Y bits to construct a realization
18
6.1. Choosing the parameters. Fix ϵ > 0. We shall choose the i.i.d. processes Y bits  Y cell  Y ord
to satisfy h(Y bits) < h(X) + 5ϵ  h(Y cell) ≤ ϵ and h(Y ord) ≤ ϵ so that Y has entropy
h(Y ) < h(X) + 7ϵ.
We let Y bits be any i.i.d. process in which Y bits
is a random number of random bits satisfying
H(Y bits
) < h(X) + 5ϵ
E|Y bits
| > h(X) + 3ϵ.
(8)
in {0  1}∗  the set of ﬁnite words over {0  1}  and having the property that  conditioned on the
length |W| of the word  W is uniformly distributed on {0  1}|W|. The desired random word can
in {0  1}m with probability 1 − p  for some suitably chosen m ≥ 1 and 0 ≤ p < 1.
Indeed
in this case  H(|W|) = −p log p − (1 − p) log(1 − p) and E|W| = pm so that H(|W|) → 0 and
E|W| → h(X) + 4ϵ as m → ∞ when p =
m(h(X) + 4ϵ). Since the entropy and length of W are
related via H(W) = E|W| + H(|W|)  we see that (8) holds when m is suﬃciently large.
2δ < ϵ.
(9)
Proposition 4.1  there exists a cell process A and an i.i.d. process Y cell of entropy at most ϵ such
that A is a ﬁnitary factor of Y cell. Since P(0 ∈ An) → 1 as n → ∞  Lemma 4.5 implies that
P(|∂An(0)| ≥ δ|An(0)|) → 0 as n → ∞  where An(0) is the cell of 0 in An. Thus  by replacing
(A1  A2  . . . ) with (Am  Am+1  . . . ) for some large m  we may assume that P(|∂A1(0)| ≥ δ|A1(0)|)
|∂A1(0)| ≥ δ|A1(0)|
<
log |S| + 2.
(10)
Let Y ord be an i.i.d. process with entropy at most ϵ and let ≤ be a total order on V as guaranteed
by v + n and its n-th ≤-predecessor by v − n. In particular  v ± n are random elements of V which
are determined from Y ord is a ﬁnitary manner. Recall also that the cell process A is a ﬁnitary
factor of Y cell. It may be helpful from this point onward to think of A and ≤ as given  and that
our goal is to use them  together with the random bits of Y bits  to construct a realization of X.
u ∈ {0  1}.
u := 1{u is a level 1 agent and |Y bits
|>0}.
(11)
u as indicating whether u read a bit at time t. In particular  if Lt
u = 1 for some t
and deﬁnitions. As we have already mentioned  our construction has the property that if an agent u
reads a bit at some time t  then the bit it read is located at u + t  i.e.  it is one of the bits of the
word Y bits
u+t . In particular  the total number of bits read from location v by time t is
v := L0
v + L1
v−1 + · · · + Lt
v−t.
19
The bits at any location v are read sequentially – the ﬁrst agent to read a bit at v will read Y bits
the second will read Y bits
(2) and so on. Precisely  the bit read by u at time t is
ˆW t
u :=
Y bits
u+t (Mt
u+t)
if Lt
u = 1
∅
otherwise .
v ≤ |Y bits
for all v ∈ V and all t ≥ 0.
W t
u := ˆW 0
u ◦ ˆW 1
u ◦ · · · ◦ ˆW t
where ◦ denotes concatenation. That is  W t
u is the word obtained by concatenating the bits read
by u until time t in the order they were read. In particular  W t
u is a word in {0  1}∗ of length
|W t
u| = L0
u + L1
u + · · · + Lt
u. We emphasize that (Mt
u  W t
u)u∈V is well-deﬁned once (Li
u  Ni
u)u∈V 0≤i≤t
is deﬁned  as the former are functions of the latter and of Y bits.
those of XV for a ﬁnite set V ⊂ V. As we aim to obtain a Γ-equivariant factor  we must take
care when dealing with random elements of SV   as these are indexed by subsets of vertices. It
would be more proper to view XV as a random element of S|V | by using the order ≤. Precisely
every ordered sequence v1  . . .   vm ∈ V of distinct vertices  there exists a simulation S(v1 ... vm) of
(Xv1  . . .   Xvm) ∈ Sm satisfying that
(v1 ... vm)(ω) ≤ H(XV ) + 2.
Since the distribution of X is Γ-invariant  we may suppose that S(v1 ... vm) = S(γv1 ... γvm) for all
S(v1 ... vm) to equal the simulation of its representative).
Now  for a ﬁnite set V ⊂ V  we let
v1  . . .   vm be the vertices of V   ordered according to ≤  and set SV to be the simulation S(v1 ... vm)
as an element of SV (indexed by V ) through
the identiﬁcation Sout
V (ω)vi = Sout
(v1 ... vm)(ω)i. We stress that SV implicitly depends on the order ≤.
also need simulations of the distribution of XV conditioned on XU for some ﬁnite set U ⊂ V which
is disjoint from V . Thus  for every such V and U and every τ ∈ SU  we similarly let SV U τ be a
simulation of P(XV ∈ · | XU = τ) satisfying that
V U τ(ω) ≤ H(XV | XU = τ) + 2.
(12)
containing U  by interpreting SV U τ as SV \\U U τU in such a case. We also identify SV with SV ∅ ∅.
Recall that every cell C in An that is not contained in An−1 has an associated level n agent  and
that this agent is “responsible” for generating the output on C \\ An−1. We denote by An(v) the
cell of v in An  where An(v) := ∅ if v /∈ An  by Un the set of level n agents and  for v ∈ An \\ An−1
by Un(v) the level n agent associated to the cell An(v).
Our goal is to deﬁne a random ﬁeld Zt = (Zt
v)v∈V  which represents the output at time t. This
output will be a function of (Li
u  Mi
u  W i
u)u∈V 0≤i≤t (and of course of the cell process A and the total
order ≤). Once Zt is deﬁned for some t  it will then only remain to inductively deﬁne (Lt+1
)u∈V  as
this then also deﬁnes (Mi
u)u∈V 0≤i≤t+1 through the deﬁnitions above. This will therefore deﬁne
20
To facilitate the inductive deﬁnition of (Lt+1
)u∈V  we require some more deﬁnitions. We now
regard t ≥ 0 as ﬁxed and suppose that (Li
u)u∈V 0≤i≤t  and hence also (Mi
u)u∈V 0≤i≤t  are already
on n. We thus begin with level 1 agents. Given a level 1 agent u ∈ U1  we say that
• u completed level 1 by time t if the stopping time Stime
A1(u) has been reached on input W t
u 1.
v = Sout
A1(u)(W t
u 1)v
for all v ∈ A1(u).
To be more precise  let us deﬁne Zt 1 = (Zt 1
v )v∈V by
Zt 1
:=
u)v
if v ∈ A1(u) for some u ∈ U1 and u completed level 1 by time t
We will soon also deﬁne Zt n = (Zt n
v )v∈V for n ≥ 2  with the idea that it represents the known
Zt n
̸= ∅ for some v and n  then it will be the case that Zt n+1
= Zt n
v . Now ﬁx n ≥ 2 and suppose
that we have deﬁned Zt 1  . . .   Zt n−1 and the two notions (reached and completed) for levels less
than n  in such a way that the above property holds – namely  if a level m ∈ {1  . . .   n − 1} agent u
completed level m by time t  then the output is known on Am(u) at time t in the sense that
Zt n−1
= Zt m
̸= ∅ for all v ∈ Am(u). Then  for a level n agent u ∈ Un  we say that
• u reached level n by time t if every level n − 1 agent u′ ∈ Un−1 ∩ An(u) has completed level
n − 1 by time t.
The idea here is that if u reached level n by time t  then the output is known on An(u) ∩ An−1
of the cell  namely  on An(u) \\ An−1.
That is  we use the simulation SV U τ with V = An(u)
U = An(u) ∩ An−1 and τ = Zt n−1. We thus say that
has been reached on input W t
Putting this together leads to deﬁning Zt n = (Zt n
An(u) An(u)∩An−1 Zt n−1(W t
if v∈An\\An−1 and Un(v)=u for some u
and u completed level n by time t
The output Zt = (Zt
v)v∈V at time t is then deﬁned by Zt
v := limn→∞ Zt n
v . That is  to determine Zt
responsible for generating the output on the cell of v in An. If u has indeed completed level n by
time t  then we read the value of Zt
v from the output of the corresponding simulation.
Finally  we are ready to deﬁne (Lt+1
)u∈V. As mentioned  these numbers are always zero for
:= 0 for u /∈ U1 ∪ U2 ∪ · · · . Suppose now that u ∈ Un for some n ≥ 1.
We say that u is active at time t + 1 if it has reached  but has not completed  level n by time t.
Thus  if u is active at time t + 1  then ideally it would like to read a bit at that time  and indeed it
21
may do so as long as there is an available bit at u + t + 1 (recall that u may only read a bit from
location u + t + 1 at time t + 1). This leads us to deﬁne
:= 1
u is active at time t + 1 and Mt
u+t+1 < |Y bits
u+t+1|
(13)
This completes the inductive deﬁnition of Lt+1
for all t ≥ 0 and u ∈ V.
unchanged at future times – that is  if Zt
v ̸= ∅ for some v and t  then Zt+1
= Zt
Tv := min{t ≥ 0 : Zt
v ̸= ∅}
Zv := lim
t→∞ Zt
v =
ZTv
if Tv < ∞
if Tv = ∞ .
Lemma 6.1. For any t ≥ 0  we have that (W t
u  Zt
u)u∈V and (Lt+1
)u∈V are measurable with respect
to Y cell  Y ord  (|Y bits
|)v∈V and (Y bits
(i))v∈V 1≤i≤Mtv.
Proof. The proof by induction on t is straightforward from the deﬁnitions.
that the output can be determined from (Y bits  Y cell  Y ord) in a ﬁnitary manner. The former is
of (Y bits  Y cell  Y ord). Thus  in light of Proposition 6.2  we must only show that ϕ is ﬁnitary. Since
Tv is almost surely ﬁnite (as Zv ̸= ∅ almost surely by Proposition 6.2)  it suﬃces to show that Zt
v in a ﬁnitary manner.
We begin by ﬁnding the level n in which v enters the cell process  i.e.  v ∈ An \\ An−1  and then
ﬁnding the cell An(v) of v in An. Since A is a ﬁnitary factor of Y cell  this may be done in a ﬁnitary
manner. Next  we ﬁnd the level n agent Un(v) associated to the cell An(v). Since this is just the
≤-minimal element in An(v) \\ An−1  and since the order ≤ is a ﬁnitary factor of Y ord  this may
manner whether w is active at time t.
Since successors/predecessors in ≤ may be found in a
ﬁnitary manner from Y ord  it then also follows that Lt
w may be determined in a ﬁnitary manner.
Using again that successors/predecessors may be found in a ﬁnitary manner  we conclude that W t
level 1 agents which are contained in An(v) (since the cell process and total order are ﬁnitary  this
level 1 by time t. Recall that the simulation SA1(u) depends on the cell A1(u) and on the order
induced by ≤ on A1(u). Since the input word W t
u  the cell process and the order are ﬁnitary  we
also determine the output Zt 1
w for all w ∈ A1(u) in a ﬁnitary manner.
22
all level m agents which are contained in An(v). For each such agent u  we check whether u reached
level m by time t. For this we must check whether the level m − 1 agents in Am(u) completed level
m − 1 by time t  which  by induction  may be done in a ﬁnitary manner. If u reached level m by
SAm(u) Am(u)∩Am−1 Zt m−1 depends on Am(u)  Am(u)\\Am−1  the order induced by ≤ on Am(u)  and
on (Zt m−1
)w∈Am(u)∩Am−1. Since the input word W t
also determine the output Zt m
for all w ∈ Am(u) in a ﬁnitary manner.
Continuing up to level m = n yields that Zt n
may be determined in a ﬁnitary manner. Since n
is the level in which v enters the cell process  we have by deﬁnition that Zt
v = Zt n
v . Thus  Zt
v may
be determined in a ﬁnitary manner  as required.
Lemma 6.3. Let ω ∈ {0  1}N consist of a sequence of independent unbiased bits. Let (ωu)u∈V be a
collection of i.i.d. copies of ω  independent of (Y bits  Y cell  Y ord). Then  for any t ≥ 0  conditioned
on (Y cell  Y ord)  the collection (W t
u ◦ ωu)u∈V has the same distribution as (ωu)u∈V.
W −1
:= ∅ for all u ∈ V). Suppose now that we know it for some t ≥ −1 and let us show it for
t+1. Recall that W t+1
= W t
u ◦ ˆW t+1
. Thus  we need to show that  conditioned on (Y cell  Y ord)  the
collection (W t
u◦ ˆW t+1
◦ωu)u∈V has the same distribution as (ωu)u∈V. To this end  it suﬃces to show
that  conditioned on (Y cell  Y ord)  the collections (W t
u)u∈V and ( ˆW t+1
◦ωu)u∈V are independent and
We may restate our goal as showing that  conditioned on (Y cell  Y ord) and (W t
u)u∈V  the collection
( ˆW t+1
◦ ωu)u∈V has the same distribution as (ωu)u∈V. Let F be the σ-algebra generated by Y cell
Y ord  (|Y bits
(i))v∈V 1≤i≤Mtv. By Lemma 6.1
(W t
u)u∈V
Q :=
u : ˆW t+1
̸= ∅
=
u : Lt+1
= 1
are F-measurable. Since (ωu)u∈V is independent of (Y bits  Y cell  Y ord) and hence also of F  it suﬃces
to show that  conditioned on F  the random variables ( ˆW t+1
)u∈Q are independent unbiased bits.
Note that Q is the set of vertices (agents) that read a bit at time t + 1  that
Q′ := {v : Mt+1
> Mt
v}
is the set of vertices from which a bit was read at time t + 1  and that u �→ u + t + 1 deﬁnes a
F-measurable bijection from Q to Q′. Recall also that ˆW t+1
= Y bits
u+t+1) for u ∈ Q. Thus
it suﬃces to show that  conditioned on F  the random variables (Y bits
(Mt+1
))v∈Q′ are independent
Indeed  since Q′ and (Mt+1
)v∈Q′ are F-measurable by Lemma 6.1  since Mt+1
v for all
v ∈ Q′  and since Y bits is an i.i.d. process that is independent of (Y cell  Y ord)  we see that the random
variables (Y bits
))v∈Q′ are conditionally independent given F  and that  for any v ∈ Q′  the
conditional distribution of Y bits
) is the same as the distribution of Y bits
) given |Y bits
Since Y bits
is a random number of random bits  the latter is the distribution of an unbiased bit
and the proof is complete.
23
encounter the limiting word W ∞
u := limt→∞ W t
u. Since W t+1
extends W t
u  this limit is well-deﬁned
and is a word in {0  1}∗ or {0  1}N (we will see that it is in fact a ﬁnite word almost surely).
eventually determined  i.e.  that Zv ̸= ∅ (equivalently  Tv < ∞) almost surely. For this  we ﬁrst
almost surely exists a ﬁnite t0 such that u is not active at any time t ≥ t0.
ψ(u  v) := 1{v=u+t and Ltu=1 for some t≥0}.
Note that ψ(u  v) indicates whether u read a bit located at v. Since an agent may read at most one
bit from any location  ψ(u  v) also represents the number of bits read by u from location v. Thus
recalling that |W t
ψ(u  v) =
t=0
u = |W ∞
u |
v−t = lim
t→∞ Mt
v =: M∞
v .
E|W ∞
u | = EM∞
(14)
Let Eu be the event that u is active at inﬁnitely many times t. We wish to show that P(Eu) = 0.
Note that  by (13)  the event Eu is contained in the event that for all but ﬁnitely many t ≥ 0  all
bits at location u + t have been read by time t  i.e.
Eu ⊂
u+t ≥ |Y bits
u+t | for all suﬃciently large t
M∞
u+t = |Y bits
v ≤ M∞
≤ |Y bits
| for all v ∈ V and t ≥ 0.
Suppose now that P(Eu) > 0. Then by ergodicity  almost surely  Ew occurs for some w ∈ V
and in particular  there almost surely exists w ∈ V such that M∞
w+i = |Y bits
w+i| for all i ≥ 0. Since
{w + i}i∈Z = V almost surely  it follows by Γ-invariance that M∞
= |Y bits
| for all v ∈ V almost
u | = E|Y bits
(15)
φ(u  v) :=
|W ∞
|An(u)\\An−1|
if v ∈ An \\ An−1 and Un(v) = u
Recall that Un(v) is the level n agent associated to the cell An(v).
Since a level n agent u is
responsible for simulating the output on An(u)\\An−1 and does so via the input word W ∞
think of φ(u  v) as follows: every level n agent u equally divides a total ‘cost’ of |W ∞ u   we may
u | among the
φ(u  v) = |W ∞
φ(u  v) =
UNv (v)|
|ANv(v) \\ ANv−1|
24
where Nv is the level at which v entered the cell process  i.e.  v ∈ ANv \\ ANv−1  and where we used
that An(v) = An(u) whenever Un(v) = u. Thus  by (15) and the mass-transport principle (4)
| = E
(16)
An(u) An(u)∩An−1 Zt n−1 is not reached on any preﬁx of W t
u that is not W t
u itself (it may or may
not be reached on the entire word W t
(Y cell  Y ord)
u| is stochastically dominated by Stime
An(u) An(u)∩An−1 Zt n−1(ω) · 1(u reached level n by time t)
where ω ∈ {0  1}N consists of a sequence of independent unbiased bits  independent of Y . Note
that  if u reached level n by time t  then Zt n−1 coincides with Z on An(u) ∩ An−1. Thus  taking
u | | Y cell  Y ord�
≤ E
An(u) An(u)∩An−1 Z(ω) | Y cell  Y ord�
· 1(u eventually reached level n).
Hence  by (12) and (3)  on the event that u ∈ Un  we have
≤ 2 +
HX(A1(u))
if n = 1
|An(u) \\ An−1| · log |S|
if n ≥ 2
where we denote HX(V ) := H(XV ) for a ﬁnite set V ⊂ V. Therefore  by (2) and the choice of δ
|ANv(v) \\ ANv−1| | Y cell  Y ord
≤ (h(X) + ϵ + 2δ) · 1E + (log |S| + 2) · 1Ec
where E is the event that Nv = 1 and |∂A1(v)| ≤ δ|A1(v)|. Thus  by (16)
| ≤ (h(X) + ϵ + 2δ) · P(E) + (log |S| + 2) · P(Ec).
Using (9) and (10)  we see that E|Y bits
| < h(X) + 3ϵ  which contradicts (8). We therefore conclude
that P(Eu) = 0 as required.
Lemma 6.5. For any v ∈ V  we have that Tv < ∞ almost surely.
Proof. Since An almost surely increases to V  it suﬃces to show that P(Zv = ∅ and v ∈ An) = 0 for
all n ≥ 1. We prove this by induction on n  taking n = 0 as a trivial base case by setting A0 := ∅.
Let n ≥ 1 and suppose that P(Zv = ∅ and v ∈ An−1) = 0. By Γ-invariance  we actually have that
P(Zw = ∅ for some w ∈ An−1) = 0. We may thus assume that Zw ̸= ∅ for all w ∈ An−1. Suppose
now that Zv = ∅ and v ∈ An. Let u be the level n agent Un(v) associated to the cell An(v).
Observe that  by the deﬁnition of Zv and Zt n
v   we have that  for all t ≥ 0  u did not complete
level n by time t. On the other hand  since Zw ̸= ∅ for all w ∈ An−1  there exists a ﬁnite t0 ≥ 0
such that u has reached level n by time t0. It follows that u is active at time t for every t > t0. By
that P(Zv = ∅ and v ∈ An) = 0.
25
Proposition 6.6. Conditioned on (Y cell  Y ord)  Z almost surely has the same distribution as X
where we regard X as independent of (Y cell  Y ord).
Proof. Throughout the proof  we regard X as independent of Y cell and Y ord. We also condition
on (Y cell  Y ord) throughout the entire proof  without explicitly mentioning this. In particular  any
statement about distributions or independence should be understood as conditional on (Y cell  Y ord).
to show that  for any n ≥ 1 and any cell C of An  ZC has the same distribution as XC. We prove
this by induction on n  taking n = 0 as a trivial base case (where A0 := ∅).
Suppose now that n ≥ 1. Let C be a cell of An and denote C′ := C ∩ An−1. We will show that
ZC′ d= XC′
(17)
P(ZC\\C′ ∈ · | ZC′ = τ) = P(XC\\C′ ∈ · | XC′ = τ)
for any feasible τ ∈ SC′.
(18)
By feasible τ  we mean that P(XC′ = τ) > 0. The desired equality in distribution ZC
d= XC follows
Let ω ∈ {0  1}N consist of a sequence of independent unbiased bits. Let (ωu)u∈V be a collection
of i.i.d. copies of ω  independent of Y bits. By Lemma 6.3  for any t ≥ 0  (W t
u ◦ ωu)u has the same
distribution as (ωu)u. Taking the limit as t → ∞  we see that (W ∞
◦ ωu)u also has the same
distribution as (ωu)u.
Observe that  by construction  if C is some cell of the cell process  then Zt
C is a function of
u)u∈C. Taking the limit as t → ∞  it follows that ZC is a function of (W ∞
u )u∈C. It also follows
from the deﬁnition of Z and the fact that Tv < ∞ for all v  that Z is unchanged by concatenating
any word to any W ∞
u . In particular  ZC is also a function of (W ∞
u ◦ ωu)u∈C.
We now show (17). To this end  let C1  . . .   Cm be the cells in An−1 that are contained in C
so that C′ = C1 ∪ · · · ∪ Cm. By the induction hypothesis  ZCj
d= XCj for every 1 ≤ j ≤ m. Since
A is a cell process  we have that dist(Cj  Cj′) > 1 for 1 ≤ j < j′ ≤ m. Hence  using that X is
1-dependent  we see that {XCj}1≤j≤m are independent. Thus  it remains to show that {ZCj}1≤j≤m
are also independent. Since ZCj is a function of (W ∞
u ◦ ωu)u∈Cj  this follows from the fact that
{W ∞
u ◦ ωu}u∈V are independent.
that w ∈ C \\ C′ and that C = An(w). Note that
ZC\\C′ = Sout
C C′ Z(W ∞
w ) = Sout
w ◦ ωw).
Recall that SC C′ Z is shorthand for SC\\C′ C′ ZC′. Since W ∞
w ◦ ωw is independent of (W ∞
u ◦ ωu)u̸=w
and hence also of ZC′  we conclude that the conditional distribution of ZC\\C′ given that ZC′ = τ
C\\C′ C′ τ(W ∞
Thus  using that W ∞
w ◦ ωw has the same
distribution as ω  we see that the distribution in question is that of Sout
C\\C′ C′ τ(ω)  which is by
deﬁnition P(XC\\C′ ∈ · | XC′ = τ)  as required.
process consisting of uniform [0  1] random variables. Using this order in the proof of Lemma 5.2
26
further modiﬁcations. Otherwise  we let Y bits
consist of inﬁnitely many independent random bits
be to simply replace u ± t with u everywhere in the construction in Section 6. That is  instead
of having an agent u try to read an unused bit from location u + t at time t  it always reads bits
samples of distributions as they are needed  from the start  each Y bits
is a collection (WV U τ)V U τ
of independent random variables having distribution P(XV ∈ · | XU = τ) for all ﬁnite U  V ⊂ V
on the cell process A constructed in Section 4. Namely  the coding radius for determining X0 is at
most the maximum of min{r ≥ 0 : A(0) ⊂ Λr(0}}  where A(0) is the cell of 0 in Amin{n≥1:0∈An}
process on A(0)  i.e.  (An ∩ A(0))n≥1. We elaborate on this in the next remark.
amenable graph G and any integer k ≥ 1  there exists a sequence (cn)∞
n=1 tending to zero such that
radius R satisfying that P(R ≥ n) ≤ cn for all n. The sequence (cn) depends only on the graph G
particular  for many concrete choices of G (and k)  an explicit sequence (cn) may be found.
cn = 8/n. In this case  instead of using the construction given in Section 4  it is simpler to consider
the ﬁnitary cell process A given by An := B1 ∪ · · · ∪ Bn  where (Bn)n≥1 are independent random
N := min{n : 0 ∈ An} at which 0 enters the cell process is a geometric random variable with
parameter 1/2  conditioned on which  the lengths L± := min{m ≥ 1 : ±m /∈ AN} of the cell
of 0 in AN in the positive/negative directions are (independent) geometric random variables with
parameter 2−N  and the coding radius R for determining X0 is bounded by max{L+  L−}. Thus
P(R > r) ≤ 2 · E
(1 − 2−N)r�
≤ 2
2−ne−r2−n ≤ 4
m=−∞
2me−2m ≤ 8
where we used the substitution n = ⌊log2 r⌋ − m.
(as in Theorem 1.2)  then it is plausible that this can be done by allowing (cn) to depend on the
entropy gap ϵ (and perhaps on |S|)  but we did not pursue this.
graph on m ≥ 2 vertices. Let G′ be the graph obtained by replacing each vertex of G with a copy
27
of H – that is  the vertex set of G′ is V × {1  . . .   m}  and (u  i) and (v  j) are adjacent in G′ if and
only if u and v are adjacent in G  or u = v and i and j are adjacent in H. Any graph G′ obtained
H is a complete graph) around (v  i) and (v  j) coincide. A simple case to have in mind is when
G = Z and H consists of an edge on two vertices  so that the vertices of G′ are Z ×{0  1} and there
is an edge between (u  i) and (v  j) if and only if |u − v| ≤ 1.
Let G′ be any graph as above and let (Wv)v∈V be independent uniform random variables on
{1  . . .   m}. Consider the random ﬁeld X on G′ deﬁned by X(v i) := 1{Wv=i}. It is clear that X is 2-
dependent and Aut(G′)-invariant. We claim that X is not a Aut(G′)-factor of any i.i.d. process Y on
G′ whose single-site distribution has at least one atom (in particular  Y cannot have ﬁnite entropy).
Indeed  for any such process Y   the event Y(v 1) = · · · = Y(v m) has positive probability  and on
this event there is no Aut(G′)-equivariant way to distinguish between (v  1)  . . .   (v  m). That is
any Aut(G′)-equivariant function ϕ: T V×{1 ... m} → {0  1}V×{1 ... m} must satisfy ϕ(y)(v 1) = · · · =
ϕ(y)(v m) whenever y ∈ T V×{1 ... m} is such that y(v 1) = · · · = y(v m). In particular  the event
ϕ(Y )(v 1) = · · · = ϕ(Y )(v m) has positive probability  and hence  ϕ(Y ) cannot have the same
We remark that there are transitive subgroups Γ of Aut(G′) for which the above obstruction
does not exist. For example  let Γ be the subgroup of Aut(G′) generated by Aut(G) and Aut(H)
both of which are naturally embedded in Aut(G′). Simple modiﬁcations to the proofs of Lemma 5.1
dependent Γ-invariant process on G′ is a ﬁnitary Γ-factor of an i.i.d. process with slightly larger
(3) A ﬁnitary isomorphism is a ﬁnitary factor that is invertible and whose inverse is also ﬁnitary.
28
[6] Olle H¨aggstr¨om  Inﬁnite clusters in dependent automorphism invariant percolation on trees  The Annals of
[7] Matan Harel and Yinon Spinka  Finitary codings for the random-cluster model and other inﬁnite-range monotone
[8] Nate Harvey  Alexander E Holroyd  Yuval Peres  and Dan Romik  Universal ﬁnitary codes with exponential tails
[9] Alexander E Holroyd  One-dependent coloring by ﬁnitary factors  Annales de l’institut henri poincar´e  probabilit´es
[12] Alexander E Holroyd and Thomas M Liggett  Symmetric 1-dependent colorings of the integers  Electronic Com-
[17] Michael Keane and Meir Smorodinsky  A class of ﬁnitary codes  Israel Journal of Mathematics 26 (1977)  no. 3-4","be possible to have Tcountable (see Remark 4 in Section 7).
Let us also remark that  while the theorems do not assume connectivity of the graph  there is
no loss of generality in assuming this  since transitivity implies that the connected components
are isomorphic and nite dependence implies that the random eld is independent on dierent
components. Thus  the same coding can be used for all components.
1.2.Discussion. Finite dependence and nitary factors have applications in computer science.
For example  if the graph Grepresents machines in a network and the random eld Xrepresents a
common plan in which each machine vis assigned a specic role Xv  then nite dependence provides
certain security benets in the face of an attacker (if someone gains access to some machines  they
learn nothing about the roles of far away machines  thereby conning the security breach)  and
being a nitary factor of an i.i.d. process provides reliability (e.g.  no single point of failure) as
it means that the machines can determine their own roles in a distributed manner by following
a common protocol  while using local randomness and communicating with nitely many other
machines. See e.g. [20 23] for more information.
The nitary coding properties of nitely dependent processes on G=Z  and in some cases on
G=Zdwithd2  have been studied in various contexts. We give a brief account of these works.
We are unaware of any works regarding nitary factors for nitely dependent processes on other
graphs.
A result by Smorodinsky [24] shows that every stationary (i.e.  translation-invariant) nitely
dependent process on Zis nitarily isomorphic (a stronger notion than being a nitary factor) to
an i.i.d. process. This result is not for the full autormorphism group of the graph (which includes
reections)  but rather only for the group of translations. In this respect  Theorem 1.2 strengthens
this result (if one is content with a nitary factor  rather than a nitary isomorphism)  as it provides
a nitary factor which is also reection invariant whenever the nitely dependent process is such.
The proof in [24] is based on the so-called marker-ller method of Keane and Smorodinsky [17 18].
Unfortunately  only a brief sketch of the proof is provided in [24] and the details seem to be missing
(after some initial steps  Smorodinsky says that the rest of the proof proceeds along the same lines
as in [18] with some necessary modications). Our proof is based on a dierent"
R020,1,EMNLP,"Replacing static word embeddings with con-
textualized word representations has yielded
signiﬁcant improvements on many NLP tasks.
However  just how contextual are the contex-
tualized representations produced by models
such as ELMo and BERT? Are there inﬁnitely
many context-speciﬁc representations for each
word  or are words essentially assigned one of
a ﬁnite number of word-sense representations?
For one  we ﬁnd that the contextualized rep-
resentations of all words are not isotropic in
any layer of the contextualizing model. While
representations of the same word in differ-
ent contexts still have a greater cosine simi-
larity than those of two different words  this
self-similarity is much lower in upper layers.
This suggests that upper layers of contextu-
alizing models produce more context-speciﬁc
representations  much like how upper layers
of LSTMs produce more task-speciﬁc repre-
sentations. In all layers of ELMo  BERT  and
GPT-2  on average  less than 5% of the vari-
ance in a word’s contextualized representa-
tions can be explained by a static embedding
for that word  providing some justiﬁcation for
the success of contextualized representations.
1","The application of deep learning methods to NLP
is made possible by representing words as vec-
tors in a low-dimensional continuous space. Tradi-
tionally  these word embeddings were static : each
word had a single vector  regardless of context
(Mikolov et al.  2013a; Pennington et al.  2014).
This posed several problems  most notably that
all senses of a polysemous word had to share the
same representation. More recent work  namely
deep neural language models such as ELMo (Pe-
ters et al.  2018) and BERT (Devlin et al.  2018)
Work partly done at the University of Toronto.have successfully created contextualized word rep-
resentations   word vectors that are sensitive to
the context in which they appear. Replacing
static embeddings with contextualized representa-
tions has yielded signiﬁcant improvements on a di-
verse array of NLP tasks  ranging from question-
answering to coreference resolution.
The success of contextualized word represen-
tations suggests that despite being trained with
only a language modelling task  they learn highly
transferable and task-agnostic properties of lan-
guage. In fact  linear probing models trained on
frozen contextualized representations can predict
linguistic properties of words (e.g.  part-of-speech
tags) almost as well as state-of-the-art models (Liu
et al.  2019a; Hewitt and Manning  2019). Still
these representations remain poorly understood.
For one  just how contextual are these contextu-
alized word representations? Are there inﬁnitely
many context-speciﬁc representations that BERT
and ELMo can assign to each word  or are words
essentially assigned one of a ﬁnite number of
word-sense representations?
We answer this question by studying the geom-
etry of the representation space for each layer of
ELMo  BERT  and GPT-2. Our analysis yields
some surprising ﬁndings:
1. In all layers of all three models  the con-
textualized word representations of all words
are not isotropic: they are not uniformly dis-
tributed with respect to direction. Instead
they are anisotropic   occupying a narrow
cone in the vector space. The anisotropy in
GPT-2’s last layer is so extreme that two ran-
dom words will on average have almost per-
fect cosine similarity! Given that isotropy
has both theoretical and empirical beneﬁts for
static embeddings (Mu et al.  2018)  the ex-
tent of anisotropy in contextualized represen-
56tations is surprising.
2. Occurrences of the same word in different
contexts have non-identical vector represen-
tations. Where vector similarity is deﬁned
as cosine similarity  these representations are
more dissimilar to each other in upper lay-
ers. This suggests that  much like how upper
layers of LSTMs produce more task-speciﬁc
representations (Liu et al.  2019a)  upper lay-
ers of contextualizing models produce more
context-speciﬁc representations.
3. Context-speciﬁcity manifests very differently
in ELMo  BERT  and GPT-2. In ELMo
representations of words in the same sen-
tence grow more similar to each other as
context-speciﬁcity increases in upper layers;
in BERT  they become more dissimilar to
each other in upper layers but are still more
similar than randomly sampled words are on
average; in GPT-2  however  words in the
same sentence are no more similar to each
other than two randomly chosen words.
4. After adjusting for the effect of anisotropy
on average  less than 5% of the variance in a
word’s contextualized representations can be
explained by their ﬁrst principal component.
This holds across all layers of all models.
This suggests that contextualized representa-
tions do not correspond to a ﬁnite number
of word-sense representations  and even in
the best possible scenario  static embeddings
would be a poor replacement for contextual-
ized ones. Still  static embeddings created
by taking the ﬁrst principal component of
a word’s contextualized representations out-
perform GloVe and FastText embeddings on
many word vector benchmarks.
These insights help justify why the use of contex-
tualized representations has led to such signiﬁcant
improvements on many NLP tasks.
2 Related Work
Static Word Embeddings Skip-gram with neg-
ative sampling (SGNS) (Mikolov et al.  2013a)
and GloVe (Pennington et al.  2014) are among
the best known models for generating static word
embeddings. Though they learn embeddings itera-
tively in practice  it has been proven that in theory they both implicitly factorize a word-context ma-
trix containing a co-occurrence statistic (Levy and
Goldberg  2014a b). Because they create a single
representation for each word  a notable problem
with static word embeddings is that all senses of a
polysemous word must share a single vector.
Contextualized Word Representations Given
the limitations of static word embeddings  recent
work has tried to create context-sensitive word
representations. ELMo (Peters et al.  2018)  BERT
(Devlin et al.  2018)  and GPT-2 (Radford et al.
2019) are deep neural language models that are
ﬁne-tuned to create models for a wide range of
downstream NLP tasks. Their internal representa-
tions of words are called contextualized word rep-
resentations because they are a function of the en-
tire input sentence. The success of this approach
suggests that these representations capture highly
guage (Liu et al.  2019a).
ELMo creates contextualized representations of
each token by concatenating the internal states of
a 2-layer biLSTM trained on a bidirectional lan-
guage modelling task (Peters et al.  2018). In
contrast  BERT and GPT-2 are bi-directional and
uni-directional transformer-based language mod-
els respectively. Each transformer layer of 12-
layer BERT (base  cased) and 12-layer GPT-2 cre-
ates a contextualized representation of each token
by attending to different parts of the input sentence
(Devlin et al.  2018; Radford et al.  2019). BERT
– and subsequent iterations on BERT (Liu et al.
2019b; Yang et al.  2019) – have achieved state-of-
the-art performance on various downstream NLP
tasks  ranging from question-answering to senti-
ment analysis.
Probing Tasks Prior analysis of contextualized
word representations has largely been restricted
to probing tasks (Tenney et al.  2019; Hewitt and
Manning  2019). This involves training linear
models to predict syntactic (e.g.  part-of-speech
tag) and semantic (e.g.  word relation) proper-
ties of words. Probing models are based on the
premise that if a simple linear model can be trained
to accurately predict a linguistic property  then the
representations implicitly encode this information
to begin with. While these analyses have found
that contextualized representations encode seman-
tic and syntactic information  they cannot answer
how contextual these representations are  and to
57what extent they can be replaced with static word
embeddings  if at all. Our work in this paper is
thus markedly different from most dissections of
contextualized representations. It is more similar
to Mimno and Thompson (2017)  which studied
the geometry of static word embedding spaces.
3 Approach
3.1 Contextualizing Models
The contextualizing models we study in this pa-
per are ELMo  BERT  and GPT-21. We choose
the base cased version of BERT because it is most
comparable to GPT-2 with respect to number of
layers and dimensionality. The models we work
with are all pre-trained on their respective lan-
guage modelling tasks. Although ELMo  BERT
and GPT-2 have 2  12  and 12 hidden layers re-
spectively  we also include the input layer of each
contextualizing model as its 0thlayer. This is be-
cause the 0thlayer is not contextualized  making
it a useful baseline against which to compare the
contextualization done by subsequent layers.
3.2 Data
To analyze contextualized word representations
we need input sentences to feed into our pre-
trained models. Our input data come from the
SemEval Semantic Textual Similarity tasks from
years 2012 - 2016 (Agirre et al.  2012  2013  2014
2015). We use these datasets because they contain
sentences in which the same words appear in dif-
ferent contexts. For example  the word ‘dog’ ap-
pears in “A panda dog is running on the road. ”
and“A dog is trying to get bacon off his back. ”
If a model generated the same representation for
‘dog’ in both these sentences  we could infer that
there was no contextualization; conversely  if the
two representations were different  we could infer
that they were contextualized to some extent. Us-
ing these datasets  we map words to the list of sen-
tences they appear in and their index within these
sentences. We do not consider words that appear
in less than 5 unique contexts in our analysis.
3.3 Measures of Contextuality
We measure how contextual a word representation
is using three different metrics: self-similarity
intra-sentence similarity   and maximum explain-
able variance .
1We use the pretrained models provided in an earlier ver-
sion of the PyTorch-Transformers library.Deﬁnition 1 Letwbe a word that appears in
sentencesfs1;:::;sngat indicesfi1;:::;ingrespec-
tively  such that w=s1[i1] =:::=sn[in]. Let f`(s;i)
be a function that maps s[i]to its representation in
layer`of model f. The self similarity ofwin layer
`is
SelfSim`(w) =1
n2 nå
jå
k6=jcos(f`(sj;ij);f`(sk;ik))
(1)
where cos denotes the cosine similarity. In other
words  the self-similarity of a word win layer `is
the average cosine similarity between its contextu-
alized representations across its nunique contexts.
If layer `does not contextualize the representa-
tions at all  then SelfSim`(w) =1 (i.e.  the repre-
sentations are identical across all contexts). The
more contextualized the representations are for w
the lower we would expect its self-similarity to be.
Deﬁnition 2 Letsbe a sentence that is a se-
quencehw1;:::;wniofnwords. Let f`(s;i)be a
function that maps s[i]to its representation in layer
`of model f. The intra-sentence similarity ofsin
layer `is
IntraSim `(s) =1
nå
icos(~s`;f`(s;i))
where ~s`=1
if`(s;i)(2)
Put more simply  the intra-sentence similarity of a
sentence is the average cosine similarity between
its word representations and the sentence vector
which is just the mean of those word vectors. This
measure captures how context-speciﬁcity mani-
fests in the vector space. For example  if both
IntraSim `(s)andSelfSim`(w)are low8w2s  then
the model contextualizes words in that layer by
giving each one a context-speciﬁc representation
that is still distinct from all other word represen-
tations in the sentence. If IntraSim `(s)is high but
SelfSim`(w)is low  this suggests a less nuanced
contextualization  where words in a sentence are
contextualized simply by making their representa-
tions converge in vector space.
Deﬁnition 3 Letwbe a word that appears in
layer `of model f. Where [f`(s1;i1):::f`(sn;in)]
is the occurrence matrix ofwands1:::smare the
58ﬁrstmsingular values of this matrix  the maximum
explainable variance is
MEV `(w) =s2
1
åis2
i(3)
MEV `(w)is the proportion of variance in w’s con-
textualized representations for a given layer that
can be explained by their ﬁrst principal compo-
nent. It gives us an upper bound on how well a
static embedding could replace a word’s contex-
tualized representations. The closer MEV `(w)is
to 0  the poorer a replacement a static embedding
would be; if MEV `(w) =1  then a static embed-
ding would be a perfect replacement for the con-
textualized representations.
3.4 Adjusting for Anisotropy
It is important to consider isotropy (or the lack
thereof) when discussing contextuality. For ex-
ample  if word vectors were perfectly isotropic
(i.e.  directionally uniform)  then SelfSim`(w) =
0:95 would suggest that w’s representations were
poorly contextualized. However  consider the sce-
nario where word vectors are so anisotropic that
any two words have on average a cosine similar-
ity of 0.99. Then SelfSim`(w) =0:95 would actu-
ally suggest the opposite – that w’s representations
were well contextualized. This is because repre-
sentations of win different contexts would on av-
erage be more dissimilar to each other than two
randomly chosen words.
To adjust for the effect of anisotropy  we use
three anisotropic baselines   one for each of our
contextuality measures. For self-similarity and
intra-sentence similarity  the baseline is the aver-
age cosine similarity between the representations
of uniformly randomly sampled words from dif-
ferent contexts. The more anisotropic the word
representations are in a given layer  the closer this
baseline is to 1. For maximum explainable vari-
ance (MEV)  the baseline is the proportion of vari-
ance in uniformly randomly sampled word repre-
sentations that is explained by their ﬁrst principal
component. The more anisotropic the representa-
tions in a given layer  the closer this baseline is
to 1: even for a random assortment of words  the
principal component would be able to explain a
large proportion of the variance.
Since contextuality measures are calculated for
each layer of a contextualizing model  we cal-
culate separate baselines for each layer as well.We then subtract from each measure its respective
baseline to get the anisotropy-adjusted contexual-
ity measure . For example  the anisotropy-adjusted
self-similarity is
Baseline (f`) =Ex;yU(O)[cos(f`(x);f`(y))]
SelfSim
`(w) =SelfSim`(w) Baseline (f`)(4)
whereOis the set of all word occurrences and
f`()maps a word occurrence to its representation
in layer `of model f. Unless otherwise stated  ref-
erences to contextuality measures in the rest of the
paper refer to the anisotropy-adjusted measures
where both the raw measure and baseline are esti-
mated with 1K uniformly randomly sampled word
representations.
4 Findings
4.1 (An)Isotropy
Contextualized representations are anisotropic
in all non-input layers. If word representations
from a particular layer were isotropic (i.e.  direc-
tionally uniform)  then the average cosine similar-
ity between uniformly randomly sampled words
would be 0 (Arora et al.  2017). The closer this
average is to 1  the more anisotropic the represen-
tations. The geometric interpretation of anisotropy
is that the word representations all occupy a nar-
row cone in the vector space rather than being uni-
form in all directions; the greater the anisotropy
the narrower this cone (Mimno and Thompson
2017). As seen in Figure 1  this implies that in
almost all layers of BERT  ELMo and GPT-2  the
representations of all words occupy a narrow cone
in the vector space. The only exception is ELMo’s
input layer  which produces static character-level
embeddings without using contextual or even po-
sitional information (Peters et al.  2018). It should
be noted that not all static embeddings are neces-
sarily isotropic  however; Mimno and Thompson
(2017) found that skipgram embeddings  which
are also static  are not isotropic.
Contextualized representations are generally
more anisotropic in higher layers. As seen in
Figure 1  for GPT-2  the average cosine similarity
between uniformly randomly words is roughly 0.6
in layers 2 through 8 but increases exponentially
from layers 8 through 12. In fact  word represen-
tations in GPT-2’s last layer are so anisotropic that
any two words have on average an almost perfect
cosine similarity! This pattern holds for BERT and
59
Figure 1: In almost all layers of BERT  ELMo  and GPT-2  the word representations are anisotropic (i.e.  not
directionally uniform): the average cosine similarity between uniformly randomly sampled words is non-zero.
The one exception is ELMo’s input layer; this is not surprising given that it generates character-level embeddings
without using context. Representations in higher layers are generally more anisotropic than those in lower ones.
ELMo as well  though there are exceptions: for ex-
ample  the anisotropy in BERT’s penultimate layer
is much higher than in its ﬁnal layer.
Isotropy has both theoretical and empirical ben-
eﬁts for static word embeddings. In theory  it
allows for stronger “self-normalization” during
training (Arora et al.  2017)  and in practice  sub-
tracting the mean vector from static embeddings
leads to improvements on several downstream
NLP tasks (Mu et al.  2018). Thus the extreme
degree of anisotropy seen in contextualized word
representations – particularly in higher layers –
is surprising. As seen in Figure 1  for all three
models  the contextualized hidden layer represen-
tations are almost all more anisotropic than the in-
put layer representations  which do not incorpo-
rate context. This suggests that high anisotropy is
inherent to  or least a by-product of  the process of
contextualization.
4.2 Context-Speciﬁcity
Contextualized word representations are more
context-speciﬁc in higher layers. Recall from
Deﬁnition 1 that the self-similarity of a word  in
a given layer of a given model  is the average co-
sine similarity between its representations in dif-
ferent contexts  adjusted for anisotropy. If the
self-similarity is 1  then the representations are
not context-speciﬁc at all; if the self-similarity is
0  that the representations are maximally context-
speciﬁc. In Figure 2  we plot the average self-
similarity of uniformly randomly sampled wordsin each layer of BERT  ELMo  and GPT-2. For
example  the self-similarity is 1.0 in ELMo’s in-
put layer because representations in that layer are
static character-level embeddings.
In all three models  the higher the layer  the
lower the self-similarity is on average. In other
words  the higher the layer  the more context-
speciﬁc the contextualized representations. This
ﬁnding makes intuitive sense. In image classiﬁca-
tion models  lower layers recognize more generic
features such as edges while upper layers recog-
nize more class-speciﬁc features (Yosinski et al.
2014). Similarly  upper layers of LSTMs trained
on NLP tasks learn more task-speciﬁc represen-
tations (Liu et al.  2019a). Therefore  it fol-
lows that upper layers of neural language mod-
els learn more context-speciﬁc representations  so
as to predict the next word for a given context
more accurately. Of all three models  representa-
tions in GPT-2 are the most context-speciﬁc  with
those in GPT-2’s last layer being almost maxi-
mally context-speciﬁc.
Stopwords (e.g.  ‘the’  ‘of’  ‘to’ ) have among the
most context-speciﬁc representations. Across
all layers  stopwords have among the lowest self-
similarity of all words  implying that their con-
textualized representations are among the most
context-speciﬁc. For example  the words with the
lowest average self-similarity across ELMo’s lay-
ers are ‘and’  ‘of’  ‘’s’  ‘the’   and ‘to’. This is rel-
atively surprising  given that these words are not
polysemous. This ﬁnding suggests that the variety
60
Figure 2: The average cosine similarity between representations of the same word in different contexts is called
the word’s self-similarity (see Deﬁnition 1). Above  we plot the average self-similarity of uniformly randomly
sampled words after adjusting for anisotropy (see section 3.4). In all three models  the higher the layer  the lower
the self-similarity  suggesting that contextualized word representations are more context-speciﬁc in higher layers.
of contexts a word appears in  rather than its inher-
ent polysemy  is what drives variation in its con-
textualized representations. This answers one of
the questions we posed in the introduction: ELMo
BERT  and GPT-2 are not simply assigning one of
a ﬁnite number of word-sense representations to
each word; otherwise  there would not be so much
variation in the representations of words with so
few word senses.
Context-speciﬁcity manifests very differently in
ELMo  BERT  and GPT-2. As noted earlier
contextualized representations are more context-
speciﬁc in upper layers of ELMo  BERT  and GPT-
2. However  how does this increased context-
speciﬁcity manifest in the vector space? Do word
representations in the same sentence converge to a
single point  or do they remain distinct from one
another while still being distinct from their repre-
sentations in other contexts? To answer this ques-
tion  we can measure a sentence’s intra-sentence
similarity. Recall from Deﬁnition 2 that the intra-
sentence similarity of a sentence  in a given layer
of a given model  is the average cosine similarity
between each of its word representations and their
mean  adjusted for anisotropy. In Figure 3  we plot
the average intra-sentence similarity of 500 uni-
formly randomly sampled sentences.
In ELMo  words in the same sentence are more
similar to one another in upper layers. As
word representations in a sentence become more
context-speciﬁc in upper layers  the intra-sentencesimilarity also rises. This suggests that  in prac-
tice  ELMo ends up extending the intuition behind
Firth’s (1957) distributional hypothesis to the sen-
tence level: that because words in the same sen-
tence share the same context  their contextualized
representations should also be similar.
In BERT  words in the same sentence are more
dissimilar to one another in upper layers. As
context-speciﬁc in upper layers  they drift away
from one another  although there are exceptions
(see layer 12 in Figure 3). However  in all lay-
ers  the average similarity between words in the
same sentence is still greater than the average sim-
ilarity between randomly chosen words (i.e.  the
anisotropy baseline). This suggests a more nu-
anced contextualization than in ELMo  with BERT
recognizing that although the surrounding sen-
tence informs a word’s meaning  two words in the
same sentence do not necessarily have a similar
meaning because they share the same context.
In GPT-2  word representations in the same
sentence are no more similar to each other than
randomly sampled words. On average  the un-
adjusted intra-sentence similarity is roughly the
same as the anisotropic baseline  so as seen in Fig-
ure 3  the anisotropy-adjusted intra-sentence simi-
larity is close to 0 in most layers of GPT-2. In fact
the intra-sentence similarity is highest in the input
layer  which does not contextualize words at all.
This is in contrast to ELMo and BERT  where the
61
Figure 3: The intra-sentence similarity is the average cosine similarity between each word representation in a
sentence and their mean (see Deﬁnition 2). Above  we plot the average intra-sentence similarity of uniformly
randomly sampled sentences  adjusted for anisotropy. This statistic reﬂects how context-speciﬁcity manifests in
the representation space  and as seen above  it manifests very differently for ELMo  BERT  and GPT-2.
average intra-sentence similarity is above 0.20 for
all but one layer.
As noted earlier when discussing BERT  this be-
havior still makes intuitive sense: two words in the
meaning simply because they share the same con-
text. The success of GPT-2 suggests that unlike
anisotropy  which accompanies context-speciﬁcity
in all three models  a high intra-sentence similar-
ity is not inherent to contextualization. Words in
the same sentence can have highly contextualized
representations without those representations be-
ing any more similar to each other than two ran-
dom word representations. It is unclear  however
whether these differences in intra-sentence simi-
larity can be traced back to differences in model
architecture; we leave this question as future work.
4.3 Static vs. Contextualized
On average  less than 5% of the variance in
a word’s contextualized representations can be
explained by a static embedding. Recall from
Deﬁnition 3 that the maximum explainable vari-
ance (MEV) of a word  for a given layer of a given
model  is the proportion of variance in its con-
textualized representations that can be explained
by their ﬁrst principal component. This gives us
an upper bound on how well a static embedding
could replace a word’s contextualized representa-
tions. Because contextualized representations are
anisotropic (see section 4.1)  much of the varia-
tion across all words can be explained by a sin-gle vector. We adjust for anisotropy by calculating
the proportion of variance explained by the ﬁrst
principal component of uniformly randomly sam-
pled word representations and subtracting this pro-
portion from the raw MEV . In Figure 4  we plot
the average anisotropy-adjusted MEV across uni-
formly randomly sampled words.
In no layer of ELMo  BERT  or GPT-2 can more
than 5% of the variance in a word’s contextual-
ized representations be explained by a static em-
bedding  on average. Though not visible in Figure
4  the raw MEV of many words is actually below
the anisotropy baseline: i.e.  a greater proportion
of the variance across all words can be explained
by a single vector than can the variance across
all representations of a single word. Note that
the 5% threshold represents the best-case scenario
and there is no theoretical guarantee that a word
vector obtained using GloVe  for example  would
be similar to the static embedding that maximizes
MEV . This suggests that contextualizing models
are not simply assigning one of a ﬁnite number of
word-sense representations to each word – other-
wise  the proportion of variance explained would
be much higher. Even the average raw MEV is be-
low 5% for all layers of ELMo and BERT; only
for GPT-2 is the raw MEV non-negligible  being
around 30% on average for layers 2 to 11 due to
extremely high anisotropy.
Principal components of contextualized repre-
sentations in lower layers outperform GloVe
and FastText on many benchmarks. As noted
62
Figure 4: The maximum explainable variance (MEV) of a word is the proportion of variance in its contextualized
representations that can be explained by their ﬁrst principal component (see Deﬁnition 3). Above  we plot the
average MEV of uniformly randomly sampled words after adjusting for anisotropy. In no layer of any model can
more than 5% of the variance in a word’s contextualized representations be explained by a static embedding.
Static Embedding SimLex999 MEN WS353 RW Google MSR SemEval2012(2) BLESS AP
GloVe 0.194 0.216 0.339 0.127 0.189 0.312 0.097 0.390 0.308
FastText 0.239 0.239 0.432 0.176 0.203 0.289 0.104 0.375 0.291
ELMo  Layer 1 0.276 0.167 0.317 0.148 0.170 0.326 0.114 0.410 0.308
ELMo  Layer 2 0.215 0.151 0.272 0.133 0.130 0.268 0.132 0.395 0.318
BERT  Layer 1 0.315 0.200 0.394 0.208 0.236 0.389 0.166 0.365 0.321
BERT  Layer 2 0.320 0.166 0.383 0.188 0.230 0.385 0.149 0.365 0.321
BERT  Layer 11 0.221 0.076 0.319 0.135 0.175 0.290 0.149 0.370 0.289
BERT  Layer 12 0.233 0.082 0.325 0.144 0.184 0.307 0.144 0.360 0.294
GPT-2  Layer 1 0.174 0.012 0.176 0.183 0.052 0.081 0.033 0.220 0.184
GPT-2  Layer 2 0.135 0.036 0.171 0.180 0.045 0.062 0.021 0.245 0.184
GPT-2  Layer 11 0.126 0.034 0.165 0.182 0.031 0.038 0.045 0.270 0.189
GPT-2  Layer 12 0.140 -0.009 0.113 0.163 0.020 0.021 0.014 0.225 0.172
Table 1: The performance of various static embeddings on word embedding benchmark tasks. The best result for
each task is in bold. For the contextualizing models (ELMo  BERT  GPT-2)  we use the ﬁrst principal component
of a word’s contextualized representations in a given layer as its static embedding. The static embeddings created
using ELMo and BERT’s contextualized representations often outperform GloVe and FastText vectors.
earlier  we can create static embeddings for each
word by taking the ﬁrst principal component (PC)
of its contextualized representations in a given
layer. In Table 1  we plot the performance of
these PC static embeddings on several benchmark
tasks2. These tasks cover semantic similarity
analogy solving  and concept categorization: Sim-
Lex999 (Hill et al.  2015)  MEN (Bruni et al.
2014)  WS353 (Finkelstein et al.  2002)  RW (Lu-
ong et al.  2013)  SemEval-2012 (Jurgens et al.
2012)  Google analogy solving (Mikolov et al.
2013a) MSR analogy solving (Mikolov et al.
2013b)  BLESS (Baroni and Lenci  2011) and AP
(Almuhareb and Poesio  2004). We leave out lay-
ers 3 - 10 in Table 1 because their performance is
2The Word Embeddings Benchmarks package was used
for evaluation.between those of Layers 2 and 11.
The best-performing PC static embeddings be-
long to the ﬁrst layer of BERT  although those
from the other layers of BERT and ELMo also out-
perform GloVe and FastText on most benchmarks.
For all three contextualizing models  PC static em-
beddings created from lower layers are more effec-
tive those created from upper layers. Those cre-
ated using GPT-2 also perform markedly worse
than their counterparts from ELMo and BERT.
Given that upper layers are much more context-
speciﬁc than lower layers  and given that GPT-
2’s representations are more context-speciﬁc than
ELMo and BERT’s (see Figure 2)  this suggests
that the PCs of highly context-speciﬁc representa-
tions are less effective on traditional benchmarks.
Those derived from less context-speciﬁc represen-
63tations  such as those from Layer 1 of BERT  are
much more effective.
5 Future Work
Our ﬁndings offer some new directions for future
work. For one  as noted earlier in the paper  Mu
et al. (2018) found that making static embeddings
more isotropic – by subtracting their mean from
each embedding – leads to surprisingly large im-
provements in performance on downstream tasks.
Given that isotropy has beneﬁts for static embed-
dings  it may also have beneﬁts for contextual-
ized word representations  although the latter have
already yielded signiﬁcant improvements despite
being highly anisotropic. Therefore  adding an
anisotropy penalty to the language modelling ob-
jective – to encourage the contextualized represen-
tations to be more isotropic – may yield even better","and the 9th International Joint Conference on Natural Language Processing   pages 55–65
Hong Kong  China  November 3–7  2019. c2019 Association for Computational Linguistics55How Contextual are Contextualized Word Representations?
Comparing the Geometry of BERT  ELMo  and GPT-2 Embeddings
Kawin Ethayarajh
Stanford University
kawin@stanford.edu","Another direction for future work is generat-
ing static word representations from contextual-
ized ones. While the latter offer superior per-
formance  there are often challenges to deploying
large models such as BERT in production  both
with respect to memory and run-time. In contrast
static representations are much easier to deploy.
Our work in section 4.3 suggests that not only it is
possible to extract static representations from con-
textualizing models  but that these extracted vec-
tors often perform much better on a diverse array
of tasks compared to traditional static embeddings
such as GloVe and FastText. This may be a means
of extracting some use from contextualizing mod-
els without incurring the full cost of using them in
production.
6","In this paper  we investigated how contextual con-
textualized word representations truly are. For
one  we found that upper layers of ELMo  BERT
and GPT-2 produce more context-speciﬁc rep-
resentations than lower layers. This increased
context-speciﬁcity is always accompanied by in-
creased anisotropy. However  context-speciﬁcity
also manifests differently across the three models;
the anisotropy-adjusted similarity between words
in the same sentence is highest in ELMo but al-
most non-existent in GPT-2. We ultimately found
that after adjusting for anisotropy  on average  less
ized representations could be explained by a staticembedding. This means that even in the best-case
scenario  in all layers of all models  static word
embeddings would be a poor replacement for con-
textualized ones. These insights help explain some
of the remarkable success that contextualized rep-
resentations have had on a diverse array of NLP
tasks.
Acknowledgments
We thank the anonymous reviewers for their in-
sightful comments. We thank the Natural Sciences
and Engineering Research Council of Canada
(NSERC) for their ﬁnancial support.
References
Eneko Agirre  Carmen Banea  Claire Cardie  Daniel M
Cer  Mona T Diab  Aitor Gonzalez-Agirre  Weiwei
Guo  Inigo Lopez-Gazpio  Montse Maritxalar  Rada
Mihalcea  et al. 2015. Semeval-2015 task 2: Seman-
tic textual similarity  English  Spanish and pilot on
interpretability. In Proceedings SemEval@ NAACL-
HLT . pages 252–263.
Guo  Rada Mihalcea  German Rigau  and Janyce
Wiebe. 2014. Semeval-2014 task 10: Multilin-
gual semantic textual similarity. In Proceedings Se-
mEval@ COLING . pages 81–91.
Eneko Agirre  Daniel Cer  Mona Diab  Aitor Gonzalez-
Agirre  and Weiwei Guo. 2013. Sem 2013 shared
task: Semantic textual similarity  including a pilot
on typed-similarity. In SEM 2013: The Second Joint
Conference on Lexical and Computational Seman-
tics. Association for Computational Linguistics.
Eneko Agirre  Mona Diab  Daniel Cer  and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-
lot on semantic textual similarity. In Proceedings
of the First Joint Conference on Lexical and Com-
putational Semantics-Volume 1: Proceedings of the
main conference and the shared task  and Volume
2: Proceedings of the Sixth International Workshop
on Semantic Evaluation . Association for Computa-
tional Linguistics  pages 385–393.
Abdulrahman Almuhareb and Massimo Poesio. 2004.
Attribute-based and value-based clustering: An
evaluation. In Proceedings of the 2004 conference
on empirical methods in natural language process-
ing.
Sanjeev Arora  Yingyu Liang  and Tengyu Ma. 2017.
A simple but tough-to-beat baseline for sentence em-
beddings. In International Conference on Learning
Representations .
64Marco Baroni and Alessandro Lenci. 2011. How we
blessed distributional semantic evaluation. In Pro-
ceedings of the GEMS 2011 Workshop on GEomet-
rical Models of Natural Language Semantics . Asso-
ciation for Computational Linguistics  pages 1–10.
Elia Bruni  Nam-Khanh Tran  and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tiﬁcial Intelligence Research 49:1–47.
Jacob Devlin  Ming-Wei Chang  Kenton Lee  and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .
Lev Finkelstein  Evgeniy Gabrilovich  Yossi Matias
Ehud Rivlin  Zach Solan  Gadi Wolfman  and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on informa-
tion systems 20(1):116–131.
John R Firth. 1957. A synopsis of linguistic theory
1930-1955. Studies in linguistic analysis .
John Hewitt and Christopher D. Manning. 2019. A
structural probe for ﬁnding syntax in word represen-
tations. In North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies . Association for Computational
Linguistics.
Felix Hill  Roi Reichart  and Anna Korhonen. 2015.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. Computational Linguis-
tics41(4):665–695.
David A Jurgens  Peter D Turney  Saif M Mohammad
and Keith J Holyoak. 2012. Semeval-2012 task 2:
Measuring degrees of relational similarity. In Pro-
ceedings of the First Joint Conference on Lexical
and Computational Semantics-Volume 1: Proceed-
ings of the main conference and the shared task  and
Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation . Association for
Computational Linguistics  pages 356–364.
Omer Levy and Yoav Goldberg. 2014a. Linguistic reg-
ularities in sparse and explicit word representations.
InProceedings of the eighteenth conference on com-
putational natural language learning . pages 171–
180.
Omer Levy and Yoav Goldberg. 2014b. Neural word
embedding as implicit matrix factorization. In Ad-
vances in Neural Information Processing Systems .
pages 2177–2185.
Nelson F. Liu  Matt Gardner  Yonatan Belinkov
Matthew E. Peters  and Noah A. Smith. 2019a. Lin-
guistic knowledge and transferability of contextual
representations. In Proceedings of the Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies .Yinhan Liu  Myle Ott  Naman Goyal  Jingfei Du  Man-
dar Joshi  Danqi Chen  Omer Levy  Mike Lewis
Luke Zettlemoyer  and Veselin Stoyanov. 2019b.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Thang Luong  Richard Socher  and Christopher D
Manning. 2013. Better word representations with
recursive neural networks for morphology. In
SIGNLL Conference on Computational Natural
Language Learning (CoNLL) . pages 104–113.
Tomas Mikolov  Ilya Sutskever  Kai Chen  Greg S Cor-
rado  and Jeff Dean. 2013a. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems . pages 3111–3119.
Tomas Mikolov  Wen-tau Yih  and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies . pages 746–751.
David Mimno and Laure Thompson. 2017. The strange
geometry of skip-gram with negative sampling. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing . pages
2873–2878.
Jiaqi Mu  Suma Bhat  and Pramod Viswanath. 2018.
All-but-the-top: Simple and effective postprocess-
ing for word representations. In Proceedings of the
7th International Conference on Learning Represen-
tations (ICLR) .
Jeffrey Pennington  Richard Socher  and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP) . pages 1532–1543.
Matthew Peters  Mark Neumann  Mohit Iyyer  Matt
Gardner  Christopher Clark  Kenton Lee  and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
guage Technologies  Volume 1 (Long Papers) . pages
2227–2237.
Alec Radford  Jeff Wu  Rewon Child  David Luan
Dario Amodei  and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners .
Ian Tenney  Patrick Xia  Berlin Chen  Alex Wang
Adam Poliak  R. Thomas McCoy  Najoung Kim
Benjamin Van Durme  Samuel R. Bowman  Dipan-
jan Das  and Ellie Pavlick. 2019. What do you
learn from context? probing for sentence structure
in contextualized word representations. In Inter-
national Conference on Learning Representations .
https://openreview.net/forum?id=SJzSgnRcKX.
65Zhilin Yang  Zihang Dai  Yiming Yang  Jaime Car-
bonell  Ruslan Salakhutdinov  and Quoc V Le.
2019. Xlnet: Generalized autoregressive pretrain-
ing for language understanding. arXiv preprint
arXiv:1906.08237 .
Jason Yosinski  Jeff Clune  Yoshua Bengio  and Hod
Lipson. 2014. How transferable are features in deep
neural networks? In Advances in Neural Informa-
tion Processing Systems . pages 3320–3328.
How Contextual are Contextualized Word Representations?
KawinEthayarajh∗
StanfordUniversity
Abstract havesuccessfullycreatedcontextualizedwordrep-
resentations  word vectors that are sensitive to
Replacing static word embeddings with con- the context in which they appear. Replacing
textualized word representations has yielded staticembeddingswithcontextualizedrepresenta-
signiﬁcantimprovementsonmanyNLPtasks.
tionshasyieldedsigniﬁcantimprovementsonadi-
answeringtocoreferenceresolution.
suchasELMoandBERT?Arethereinﬁnitely
manycontext-speciﬁcrepresentationsforeach The success of contextualized word represen-
word orarewordsessentiallyassignedoneof tations suggests that despite being trained with
aﬁnitenumberofword-senserepresentations? only a language modelling task  they learn highly
anylayerofthecontextualizingmodel. While
linguisticpropertiesofwords(e.g. part-of-speech
larity than those of two different words  this tags)almostaswellasstate-of-the-artmodels(Liu
self-similarity is much lower in upper layers. et al.  2019a; Hewitt and Manning  2019). Still
This suggests that upper layers of contextu- these representations remain poorly understood.
alizing models produce more context-speciﬁc For one  just how contextual are these contextu-
sentations. InalllayersofELMo  BERT and
tions can be explained by a static embedding word-senserepresentations?
forthatword providingsomejustiﬁcationfor Weanswerthisquestionbystudyingthegeom-
thesuccessofcontextualizedrepresentations. etry of the representation space for each layer of
somesurprisingﬁndings:
textualizedwordrepresentationsofallwords
torsinalow-dimensionalcontinuousspace. Tradi-
tionally thesewordembeddingswerestatic: each
they are anisotropic  occupying a narrow
GPT-2’slastlayerissoextremethattworan-
hasboththeoreticalandempiricalbeneﬁtsfor
∗WorkpartlydoneattheUniversityofToronto. tentofanisotropyincontextualizedrepresen-
tationsissurprising. they both implicitly factorize a word-context ma-
trixcontainingaco-occurrencestatistic(Levyand
2. Occurrences of the same word in different Goldberg  2014a b). Because they create a single
contexts have non-identical vector represen- representation for each word  a notable problem
tations. Where vector similarity is deﬁned withstaticwordembeddingsisthatallsensesofa
ascosinesimilarity theserepresentationsare polysemouswordmustshareasinglevector.
ers. Thissuggeststhat muchlikehowupper Contextualized Word Representations Given
layers of LSTMs produce more task-speciﬁc the limitations of static word embeddings  recent
representations(Liuetal. 2019a) upperlay- work has tried to create context-sensitive word
ers of contextualizing models produce more representations. ELMo(Petersetal. 2018) BERT
context-speciﬁcrepresentations. (Devlin et al.  2018)  and GPT-2 (Radford et al.
3. Context-speciﬁcitymanifestsverydifferently
downstreamNLPtasks. Theirinternalrepresenta-
representations of words in the same sen- tionsofwordsarecalledcontextualizedwordrep-
tence grow more similar to each other as resentationsbecausetheyareafunctionoftheen-
guage(Liuetal. 2019a).
ELMocreatescontextualizedrepresentationsof
otherthantworandomlychosenwords.
word’scontextualizedrepresentationscanbe
layerBERT(base cased)and12-layerGPT-2cre-
Thissuggeststhatcontextualizedrepresenta-
byattendingtodifferentpartsoftheinputsentence
thebestpossiblescenario  staticembeddings
2019b;Yangetal. 2019)–haveachievedstate-of-
mentanalysis.
manywordvectorbenchmarks.
Theseinsightshelpjustifywhytheuseofcontex-
tualizedrepresentationshasledtosuchsigniﬁcant
improvementsonmanyNLPtasks.
2 RelatedWork
premisethatifasimplelinearmodelcanbetrained
StaticWordEmbeddings Skip-gramwithneg- toaccuratelypredictalinguisticproperty thenthe
ative sampling (SGNS) (Mikolov et al.  2013a) representationsimplicitlyencodethisinformation
and GloVe (Pennington et al.  2014) are among to begin with. While these analyses have found
the best known models for generating static word thatcontextualizedrepresentationsencodeseman-
embeddings. Thoughtheylearnembeddingsitera- tic and syntactic information  they cannot answer
tivelyinpractice ithasbeenproventhatintheory  how contextual these representations are  and to
what extent they can be replaced with static word Deﬁnition 1 Let w be a word that appears in
embeddings  if at all. Our work in this paper is sentences {s  ... s } at indices {i  ... i } respec-
1 n 1 n
thus markedly different from most dissections of tively suchthatw=s [i ]=...=s [i ]. Let f (s i)
1 1 n n (cid:96)
contextualized representations. It is more similar beafunctionthatmapss[i]toitsrepresentationin
to Mimno and Thompson (2017)  which studied layer(cid:96)ofmodel f. Theselfsimilarityofwinlayer
thegeometryofstaticwordembeddingspaces. (cid:96)is
3 Approach SelfSim (w)= ∑∑cos(f (s  i ) f (s  i ))
(cid:96) n2−n (cid:96) j j (cid:96) k k
j k(cid:54)=j
3.1 ContextualizingModels
The contextualizing models we study in this pa- where cos denotes the cosine similarity. In other
per are ELMo  BERT  and GPT-21. We choose words theself-similarityofawordwinlayer(cid:96)is
thebasecasedversionofBERTbecauseitismost theaveragecosinesimilaritybetweenitscontextu-
comparable to GPT-2 with respect to number of alizedrepresentationsacrossitsnuniquecontexts.
layers and dimensionality. The models we work If layer (cid:96) does not contextualize the representa-
with are all pre-trained on their respective lan- tions at all  then SelfSim (w)=1 (i.e.  the repre-
(cid:96)
guage modelling tasks. Although ELMo  BERT  sentations are identical across all contexts). The
and GPT-2 have 2  12  and 12 hidden layers re- morecontextualizedtherepresentationsareforw
spectively wealsoincludetheinputlayerofeach thelowerwewouldexpectitsself-similaritytobe.
contextualizing model as its 0th layer. This is be-
Deﬁnition 2 Let s be a sentence that is a se-
cause the 0th layer is not contextualized  making
quence (cid:104)w  ... w (cid:105) of n words. Let f (s i) be a
it a useful baseline against which to compare the 1 n (cid:96)
functionthatmapss[i]toitsrepresentationinlayer
contextualizationdonebysubsequentlayers.
(cid:96)ofmodel f. Theintra-sentencesimilarityofsin
3.2 Data layer(cid:96)is
we need input sentences to feed into our pre- IntraSim(cid:96)(s)= ∑cos((cid:126)s(cid:96) f(cid:96)(s i))
n
i
trained models. Our input data come from the (2)
SemEval Semantic Textual Similarity tasks from where(cid:126)s = ∑f (s i)
(cid:96) (cid:96)
years2012-2016(Agirreetal. 2012 2013 2014  i
2015). Weusethesedatasetsbecausetheycontain
Putmoresimply theintra-sentencesimilarityofa
pears in “A panda dog is running on the road.”
whichisjustthemeanofthosewordvectors. This
and “A dog is trying to get bacon off his back.”
IntraSim (s)andSelfSim (w)arelow∀w∈s then
there was no contextualization; conversely  if the (cid:96) (cid:96)
tworepresentationsweredifferent wecouldinfer
thatthey werecontextualizedto someextent. Us-
ingthesedatasets wemapwordstothelistofsen-
tationsinthesentence. IfIntraSim (s)ishighbut
SelfSim (w) is low  this suggests a less nuanced
sentences. We do not consider words that appear (cid:96)
inlessthan5uniquecontextsinouranalysis.
contextualizedsimplybymakingtheirrepresenta-
tionsconvergeinvectorspace.
3.3 MeasuresofContextuality
Wemeasurehowcontextualawordrepresentation Deﬁnition 3 Let w be a word that appears in
is using three different metrics: self-similarity  sentences {s  ... s } at indices {i  ... i } respec-
intra-sentence similarity  and maximum explain- tively suchthatw=s [i ]=...=s [i ]. Let f (s i)
ablevariance. beafunctionthatmapss[i]toitsrepresentationin
layer (cid:96) of model f. Where [f (s  i )...f (s  i )]
1Weusethepretrainedmodelsprovidedinanearlierver- (cid:96) 1 1 (cid:96) n n
sionofthePyTorch-Transformerslibrary. is the occurrence matrix of w and σ1...σm are the
ﬁrstmsingularvaluesofthismatrix themaximum Wethensubtractfromeachmeasureitsrespective
explainablevarianceis baselinetogettheanisotropy-adjustedcontexual-
itymeasure. Forexample theanisotropy-adjusted
MEV (w)= σ12 (3) self-similarityis
(cid:96) ∑ σ2
i i
Baseline(f )=E [cos(f (x) f (y))]
(cid:96) x y∼U(O) (cid:96) (cid:96)
(4)
MEV(cid:96)(w)istheproportionofvarianceinw’scon- SelfSim∗(w)=SelfSim (w)−Baseline(f )
(cid:96) (cid:96) (cid:96)
can be explained by their ﬁrst principal compo- where O is the set of all word occurrences and
nent. It gives us an upper bound on how well a f (·)mapsawordoccurrencetoitsrepresentation
static embedding could replace a word’s contex- inlayer(cid:96)ofmodel f. Unlessotherwisestated ref-
tualized representations. The closer MEV (w) is erencestocontextualitymeasuresintherestofthe
to 0  the poorer a replacement a static embedding paper refer to the anisotropy-adjusted measures
would be; if MEV (w)=1  then a static embed- whereboththerawmeasureandbaselineareesti-
ding would be a perfect replacement for the con- matedwith1Kuniformlyrandomlysampledword
textualizedrepresentations. representations.
3.4 AdjustingforAnisotropy 4 Findings
It is important to consider isotropy (or the lack 4.1 (An)Isotropy
inallnon-inputlayers. Ifwordrepresentations
(i.e.  directionally uniform)  then SelfSim (w) =
(cid:96) from a particular layer were isotropic (i.e.  direc-
0.95 would suggest that w’s representations were
tionallyuniform) thentheaveragecosinesimilar-
poorlycontextualized. However considerthesce-
averageisto1 themoreanisotropictherepresen-
ity of0.99. ThenSelfSim (w)=0.95 wouldactu-
(cid:96) tations. Thegeometricinterpretationofanisotropy
allysuggesttheopposite–thatw’srepresentations
rowconeinthevectorspaceratherthanbeinguni-
sentations of w in different contexts would on av-
randomlychosenwords.
three anisotropic baselines  one for each of our
representationsofallwordsoccupyanarrowcone
inthevectorspace. TheonlyexceptionisELMo’s
sitionalinformation(Petersetal. 2018). Itshould
representationsareinagivenlayer thecloserthis
ance(MEV) thebaselineistheproportionofvari-
arealsostatic arenotisotropic.
sentations that is explained by their ﬁrst principal Contextualized representations are generally
component. The more anisotropic the representa- more anisotropic in higher layers. As seen in
tions in a given layer  the closer this baseline is Figure1  forGPT-2  theaveragecosinesimilarity
to 1: even for a random assortment of words  the betweenuniformlyrandomlywordsisroughly0.6
principal component would be able to explain a in layers 2 through 8 but increases exponentially
largeproportionofthevariance. from layers 8 through 12. In fact  word represen-
Since contextuality measures are calculated for tationsinGPT-2’slastlayeraresoanisotropicthat
each layer of a contextualizing model  we cal- any two words have on average an almost perfect
culate separate baselines for each layer as well. cosinesimilarity! ThispatternholdsforBERTand
TheoneexceptionisELMo’sinputlayer;thisisnotsurprisinggiventhatitgeneratescharacter-levelembeddings
withoutusingcontext. Representationsinhigherlayersaregenerallymoreanisotropicthanthoseinlowerones.
ELMoaswell thoughthereareexceptions: forex- in each layer of BERT  ELMo  and GPT-2. For
ample theanisotropyinBERT’spenultimatelayer example  the self-similarity is 1.0 in ELMo’s in-
ismuchhigherthaninitsﬁnallayer. put layer because representations in that layer are
Isotropyhasboththeoreticalandempiricalben- staticcharacter-levelembeddings.
eﬁts for static word embeddings. In theory  it In all three models  the higher the layer  the
allows for stronger “self-normalization” during lower the self-similarity is on average. In other
training (Arora et al.  2017)  and in practice  sub- words  the higher the layer  the more context-
tracting the mean vector from static embeddings speciﬁc the contextualized representations. This
leads to improvements on several downstream ﬁndingmakesintuitivesense. Inimageclassiﬁca-
NLP tasks (Mu et al.  2018). Thus the extreme tion models  lower layers recognize more generic
degree of anisotropy seen in contextualized word features such as edges while upper layers recog-
representations – particularly in higher layers – nize more class-speciﬁc features (Yosinski et al.
is surprising. As seen in Figure 1  for all three 2014). Similarly  upper layers of LSTMs trained
models  the contextualized hidden layer represen- on NLP tasks learn more task-speciﬁc represen-
tationsarealmostallmoreanisotropicthanthein- tations (Liu et al.  2019a). Therefore  it fol-
put layer representations  which do not incorpo- lows that upper layers of neural language mod-
ratecontext. Thissuggeststhathighanisotropyis elslearnmorecontext-speciﬁcrepresentations so
inherentto orleastaby-productof theprocessof as to predict the next word for a given context
contextualization. more accurately. Of all three models  representa-
tionsinGPT-2arethemostcontext-speciﬁc  with
4.2 Context-Speciﬁcity those in GPT-2’s last layer being almost maxi-
mallycontext-speciﬁc.
context-speciﬁc in higher layers. Recall from Stopwords(e.g. ‘the’ ‘of’ ‘to’)haveamongthe
Deﬁnition 1 that the self-similarity of a word  in most context-speciﬁc representations. Across
a given layer of a given model  is the average co- all layers  stopwords have among the lowest self-
sine similarity between its representations in dif- similarity of all words  implying that their con-
ferent contexts  adjusted for anisotropy. If the textualized representations are among the most
self-similarity is 1  then the representations are context-speciﬁc. For example  the words with the
not context-speciﬁc at all; if the self-similarity is lowest average self-similarity across ELMo’s lay-
0  that the representations are maximally context- ersare‘and’ ‘of’ ‘’s’ ‘the’ and‘to’. Thisisrel-
speciﬁc. In Figure 2  we plot the average self- atively surprising  given that these words are not
similarity of uniformly randomly sampled words polysemous. Thisﬁndingsuggeststhatthevariety
sampledwordsafteradjustingforanisotropy(seesection3.4). Inallthreemodels thehigherthelayer thelower
theself-similarity suggestingthatcontextualizedwordrepresentationsaremorecontext-speciﬁcinhigherlayers.
ofcontextsawordappearsin ratherthanitsinher- similarity also rises. This suggests that  in prac-
ent polysemy  is what drives variation in its con- tice ELMoendsupextendingtheintuitionbehind
textualized representations. This answers one of Firth’s(1957)distributionalhypothesistothesen-
thequestionsweposedintheintroduction: ELMo  tence level: that because words in the same sen-
BERT andGPT-2arenotsimplyassigningoneof tence share the same context  their contextualized
a ﬁnite number of word-sense representations to representationsshouldalsobesimilar.
eachword;otherwise therewouldnotbesomuch
variation in the representations of words with so InBERT wordsinthesamesentencearemore
fewwordsenses. dissimilar to one another in upper layers. As
Context-speciﬁcitymanifestsverydifferentlyin context-speciﬁc in upper layers  they drift away
ELMo  BERT  and GPT-2. As noted earlier  from one another  although there are exceptions
contextualized representations are more context- (see layer 12 in Figure 3). However  in all lay-
speciﬁcinupperlayersofELMo BERT andGPT- ers  the average similarity between words in the
2. However  how does this increased context- samesentenceisstillgreaterthantheaveragesim-
speciﬁcity manifest in the vector space? Do word ilarity between randomly chosen words (i.e.  the
representationsinthesamesentenceconvergetoa anisotropy baseline). This suggests a more nu-
single point  or do they remain distinct from one ancedcontextualizationthaninELMo withBERT
another while still being distinct from their repre- recognizing that although the surrounding sen-
sentationsinothercontexts? Toanswerthisques- tenceinformsaword’smeaning twowordsinthe
tion  we can measure a sentence’s intra-sentence same sentence do not necessarily have a similar
similarity. Recall from Deﬁnition 2 that the intra- meaningbecausetheysharethesamecontext.
of a given model  is the average cosine similarity In GPT-2  word representations in the same
betweeneachofitswordrepresentationsandtheir sentencearenomoresimilartoeachotherthan
mean adjustedforanisotropy. InFigure3 weplot randomlysampledwords. Onaverage theun-
the average intra-sentence similarity of 500 uni- adjusted intra-sentence similarity is roughly the
formlyrandomlysampledsentences. sameastheanisotropicbaseline soasseeninFig-
ure3 theanisotropy-adjustedintra-sentencesimi-
InELMo wordsinthesamesentencearemore larityiscloseto0inmostlayersofGPT-2. Infact
similar to one another in upper layers. As theintra-sentencesimilarityishighestintheinput
word representations in a sentence become more layer  which does not contextualize words at all.
context-speciﬁcinupperlayers theintra-sentence ThisisincontrasttoELMoandBERT wherethe
therepresentationspace andasseenabove itmanifestsverydifferentlyforELMo BERT andGPT-2.
averageintra-sentencesimilarityisabove0.20for glevector. Weadjustforanisotropybycalculating
allbutonelayer. the proportion of variance explained by the ﬁrst
AsnotedearlierwhendiscussingBERT thisbe- principal component of uniformly randomly sam-
haviorstillmakesintuitivesense: twowordsinthe pledwordrepresentationsandsubtractingthispro-
same sentence do not necessarily have a similar portion from the raw MEV. In Figure 4  we plot
meaningsimplybecausetheysharethesamecon- the average anisotropy-adjusted MEV across uni-
text. The success of GPT-2 suggests that unlike formlyrandomlysampledwords.
anisotropy whichaccompaniescontext-speciﬁcity InnolayerofELMo BERT orGPT-2canmore
in all three models  a high intra-sentence similar- than 5% of the variance in a word’s contextual-
ity is not inherent to contextualization. Words in ized representations be explained by a static em-
the same sentence can have highly contextualized bedding onaverage. ThoughnotvisibleinFigure
representations without those representations be- 4  the raw MEV of many words is actually below
ing any more similar to each other than two ran- the anisotropy baseline: i.e.  a greater proportion
dom word representations. It is unclear  however  of the variance across all words can be explained
whether these differences in intra-sentence simi- by a single vector than can the variance across
larity can be traced back to differences in model all representations of a single word. Note that
architecture;weleavethisquestionasfuturework. the5%thresholdrepresentsthebest-casescenario
4.3 Staticvs.Contextualized vector obtained using GloVe  for example  would
besimilartothestaticembeddingthatmaximizes
MEV. This suggests that contextualizing models
aword’scontextualized representations canbe
arenotsimplyassigningoneofaﬁnitenumberof
ance(MEV)ofaword foragivenlayerofagiven
bemuchhigher. EventheaveragerawMEVisbe-
extremelyhighanisotropy.
tions. Because contextualized representations are Principal components of contextualized repre-
anisotropic (see section 4.1)  much of the varia- sentations in lower layers outperform GloVe
tion across all words can be explained by a sin- and FastText on many benchmarks. As noted
Figure4: Themaximumexplainablevariance(MEV)ofawordistheproportionofvarianceinitscontextualized
averageMEVofuniformlyrandomlysampledwordsafteradjustingforanisotropy. Innolayerofanymodelcan
morethan5%ofthevarianceinaword’scontextualizedrepresentationsbeexplainedbyastaticembedding.
StaticEmbedding SimLex999 MEN WS353 RW Google MSR SemEval2012(2) BLESS AP
ELMo Layer1 0.276 0.167 0.317 0.148 0.170 0.326 0.114 0.410 0.308
ELMo Layer2 0.215 0.151 0.272 0.133 0.130 0.268 0.132 0.395 0.318
BERT Layer1 0.315 0.200 0.394 0.208 0.236 0.389 0.166 0.365 0.321
BERT Layer2 0.320 0.166 0.383 0.188 0.230 0.385 0.149 0.365 0.321
BERT Layer11 0.221 0.076 0.319 0.135 0.175 0.290 0.149 0.370 0.289
BERT Layer12 0.233 0.082 0.325 0.144 0.184 0.307 0.144 0.360 0.294
GPT-2 Layer1 0.174 0.012 0.176 0.183 0.052 0.081 0.033 0.220 0.184
GPT-2 Layer2 0.135 0.036 0.171 0.180 0.045 0.062 0.021 0.245 0.184
GPT-2 Layer11 0.126 0.034 0.165 0.182 0.031 0.038 0.045 0.270 0.189
GPT-2 Layer12 0.140 -0.009 0.113 0.163 0.020 0.021 0.014 0.225 0.172
Table1: Theperformanceofvariousstaticembeddingsonwordembeddingbenchmarktasks. Thebestresultfor
eachtaskisinbold. Forthecontextualizingmodels(ELMo BERT GPT-2) weusetheﬁrstprincipalcomponent
ofaword’scontextualizedrepresentationsinagivenlayerasitsstaticembedding. Thestaticembeddingscreated
usingELMoandBERT’scontextualizedrepresentationsoftenoutperformGloVeandFastTextvectors.
earlier  we can create static embeddings for each betweenthoseofLayers2and11.
wordbytakingtheﬁrstprincipalcomponent(PC) The best-performing PC static embeddings be-
of its contextualized representations in a given long to the ﬁrst layer of BERT  although those
layer. In Table 1  we plot the performance of fromtheotherlayersofBERTandELMoalsoout-
thesePCstaticembeddingsonseveralbenchmark performGloVeandFastTextonmostbenchmarks.
tasks2. These tasks cover semantic similarity  Forallthreecontextualizingmodels PCstaticem-
analogysolving andconceptcategorization: Sim- beddingscreatedfromlowerlayersaremoreeffec-
Lex999 (Hill et al.  2015)  MEN (Bruni et al.  tive those created from upper layers. Those cre-
2014) WS353(Finkelsteinetal. 2002) RW(Lu- ated using GPT-2 also perform markedly worse
ong et al.  2013)  SemEval-2012 (Jurgens et al.  than their counterparts from ELMo and BERT.
2012)  Google analogy solving (Mikolov et al.  Given that upper layers are much more context-
2013a) MSR analogy solving (Mikolov et al.  speciﬁc than lower layers  and given that GPT-
2013b) BLESS(BaroniandLenci 2011)andAP 2’s representations are more context-speciﬁc than
(AlmuharebandPoesio 2004). Weleaveoutlay- ELMo and BERT’s (see Figure 2)  this suggests
ers 3 - 10 in Table 1 because their performance is thatthePCsofhighlycontext-speciﬁcrepresenta-
forevaluation. Thosederivedfromlesscontext-speciﬁcrepresen-
tations  such as those from Layer 1 of BERT  are embedding. Thismeansthateveninthebest-case
muchmoreeffective. scenario  in all layers of all models  static word
embeddingswouldbeapoorreplacementforcon-
5 FutureWork
textualizedones. Theseinsightshelpexplainsome
oftheremarkablesuccessthatcontextualizedrep-
etal.(2018)foundthatmakingstaticembeddings
sightfulcomments. WethanktheNaturalSciences
izedwordrepresentations althoughthelatterhave
(NSERC)fortheirﬁnancialsupport.
anisotropy penalty to the language modelling ob- References
jective–toencouragethecontextualizedrepresen-
EnekoAgirre CarmenBanea ClaireCardie DanielM
tationstobemoreisotropic–mayyieldevenbetter
Guo InigoLopez-Gazpio MontseMaritxalar Rada
Another direction for future work is generat- Mihalcea etal.2015. Semeval-2015task2:Seman-
ing static word representations from contextual- tic textual similarity  English  Spanish and pilot on
interpretability. InProceedingsSemEval@NAACL-
HLT.pages252–263.
withrespecttomemoryandrun-time. Incontrast  Cer  Mona T Diab  Aitor Gonzalez-Agirre  Weiwei
static representations are much easier to deploy. Guo  Rada Mihalcea  German Rigau  and Janyce
Ourworkinsection4.3suggeststhatnotonlyitis
gualsemantictextualsimilarity. InProceedingsSe-
possibletoextractstaticrepresentationsfromcon-
mEval@COLING.pages81–91.
tors often perform much better on a diverse array EnekoAgirre DanielCer MonaDiab AitorGonzalez-
oftaskscomparedtotraditionalstaticembeddings Agirre  and Weiwei Guo. 2013. Sem 2013 shared
suchasGloVeandFastText. Thismaybeameans
ontyped-similarity. InSEM2013:TheSecondJoint
ofextractingsomeusefromcontextualizingmod-
elswithoutincurringthefullcostofusingthemin tics.AssociationforComputationalLinguistics.
6 Conclusion Gonzalez-Agirre.2012. Semeval-2012task6: Api-
Inthispaper weinvestigatedhowcontextualcon- of the First Joint Conference on Lexical and Com-
putationalSemantics-Volume1: Proceedingsofthe
2: ProceedingsoftheSixthInternationalWorkshop
and GPT-2 produce more context-speciﬁc rep- on Semantic Evaluation. Association for Computa-
resentations than lower layers. This increased tionalLinguistics pages385–393.
alsomanifestsdifferentlyacrossthethreemodels;
the anisotropy-adjusted similarity between words on empirical methods in natural language process-
in the same sentence is highest in ELMo but al- ing.
thatafteradjustingforanisotropy onaverage less
Asimplebuttough-to-beatbaselineforsentenceem-
beddings. InInternationalConferenceonLearning
izedrepresentationscouldbeexplainedbyastatic Representations.
Marco Baroni and Alessandro Lenci. 2011. How we YinhanLiu MyleOtt NamanGoyal JingfeiDu Man-
blessed distributional semantic evaluation. In Pro- dar Joshi  Danqi Chen  Omer Levy  Mike Lewis
ceedingsoftheGEMS2011WorkshoponGEomet- Luke Zettlemoyer  and Veselin Stoyanov. 2019b.
ricalModelsofNaturalLanguageSemantics.Asso- Roberta: A robustly optimized bert pretraining ap-
ciationforComputationalLinguistics pages1–10. proach. arXivpreprintarXiv:1907.11692.
EliaBruni Nam-KhanhTran andMarcoBaroni.2014. Thang Luong  Richard Socher  and Christopher D
Multimodaldistributionalsemantics. JournalofAr- Manning. 2013. Better word representations with
tiﬁcialIntelligenceResearch49:1–47. recursive neural networks for morphology. In
Jacob Devlin  Ming-Wei Chang  Kenton Lee  and LanguageLearning(CoNLL).pages104–113.
KristinaToutanova.2018. Bert:Pre-trainingofdeep
bidirectional transformers for language understand- TomasMikolov IlyaSutskever KaiChen GregSCor-
ing. arXivpreprintarXiv:1810.04805. rado andJeffDean.2013a. Distributedrepresenta-
tionsofwordsandphrasesandtheircompositional-
Lev Finkelstein  Evgeniy Gabrilovich  Yossi Matias  ity. In Advances in Neural Information Processing
Ehud Rivlin  Zach Solan  Gadi Wolfman  and Ey- Systems.pages3111–3119.
concept revisited. ACM Transactions on informa- Tomas Mikolov  Wen-tau Yih  and Geoffrey Zweig.
tionsystems20(1):116–131. 2013b. Linguistic regularities in continuous space
John R Firth. 1957. A synopsis of linguistic theory  Conference of the North American Chapter of the
1930-1955. Studiesinlinguisticanalysis. AssociationforComputationalLinguistics: Human
LanguageTechnologies.pages746–751.
structuralprobeforﬁndingsyntaxinwordrepresen- DavidMimnoandLaureThompson.2017. Thestrange
tations. In North American Chapter of the Associ- geometry of skip-gram with negative sampling. In
ation for Computational Linguistics: Human Lan- Proceedings of the 2017 Conference on Empirical
guageTechnologies.AssociationforComputational Methods in Natural Language Processing. pages
Linguistics. 2873–2878.
Felix Hill  Roi Reichart  and Anna Korhonen. 2015. Jiaqi Mu  Suma Bhat  and Pramod Viswanath. 2018.
Simlex-999: Evaluatingsemanticmodelswith(gen- All-but-the-top: Simple and effective postprocess-
uine)similarityestimation. ComputationalLinguis- ingforwordrepresentations. InProceedingsofthe
tics41(4):665–695. 7thInternationalConferenceonLearningRepresen-
tations(ICLR).
DavidAJurgens PeterDTurney SaifMMohammad
and Keith J Holyoak. 2012. Semeval-2012 task 2: Jeffrey Pennington  Richard Socher  and Christopher
Measuring degrees of relational similarity. In Pro- Manning. 2014. GloVe: Global vectors for word
ceedings of the First Joint Conference on Lexical representation. In Proceedings of the 2014 Con-
and Computational Semantics-Volume 1: Proceed- ferenceonEmpiricalMethodsinNaturalLanguage
ingsofthemainconferenceandthesharedtask and Processing(EMNLP).pages1532–1543.
Workshop on Semantic Evaluation. Association for Matthew Peters  Mark Neumann  Mohit Iyyer  Matt
ComputationalLinguistics pages356–364. Gardner  Christopher Clark  Kenton Lee  and Luke
OmerLevyandYoavGoldberg.2014a. Linguisticreg- resentations. In Proceedings of the 2018 Confer-
ularitiesinsparseandexplicitwordrepresentations. ence of the North American Chapter of the Associ-
InProceedingsoftheeighteenthconferenceoncom- ation for Computational Linguistics: Human Lan-
putational natural language learning. pages 171– guageTechnologies Volume1(LongPapers).pages
180. 2227–2237.
Omer Levy and Yoav Goldberg. 2014b. Neural word Alec Radford  Jeff Wu  Rewon Child  David Luan
embedding as implicit matrix factorization. In Ad- DarioAmodei andIlyaSutskever.2019. Language
vances in Neural Information Processing Systems. modelsareunsupervisedmultitasklearners.
pages2177–2185.
Nelson F. Liu  Matt Gardner  Yonatan Belinkov  Adam Poliak  R. Thomas McCoy  Najoung Kim
MatthewE.Peters andNoahA.Smith.2019a. Lin- BenjaminVanDurme  SamuelR.Bowman  Dipan-
guistic knowledge and transferability of contextual jan Das  and Ellie Pavlick. 2019. What do you
representations. InProceedingsoftheConferenceof learn from context? probing for sentence structure
the North American Chapter of the Association for in contextualized word representations. In Inter-
ComputationalLinguistics:HumanLanguageTech- national Conference on Learning Representations.
nologies. https://openreview.net/forum?id=SJzSgnRcKX.
Zhilin Yang  Zihang Dai  Yiming Yang  Jaime Car-
arXiv:1906.08237.
Lipson.2014. Howtransferablearefeaturesindeep
tionProcessingSystems.pages3320–3328.
and the 9th International Joint Conference on Natural Language Processing  pages 55–65
Hong Kong  China  November 3–7  2019. c⃝2019 Association for Computational Linguistics
55
Kawin Ethayarajh∗
Introduction
tionally  these word embeddings were static: each
∗Work partly done at the University of Toronto.
have successfully created contextualized word rep-
the context in which they appear.
Replacing
ELMo  BERT  and GPT-2.
Our analysis yields
tributed with respect to direction.
Instead
fect cosine similarity!
Given that isotropy
56
tations is surprising.
tations.
Where vector similarity is deﬁned
in ELMo  BERT  and GPT-2.
In ELMo
ized ones.
Still  static embeddings created
2
Related Work
Static Word Embeddings
Skip-gram with neg-
tively in practice  it has been proven that in theory
they both implicitly factorize a word-context ma-
Contextualized Word Representations
Given
guage modelling task (Peters et al.  2018).
In
els respectively.
Each transformer layer of 12-
Probing Tasks
Prior analysis of contextualized
Manning  2019).
This involves training linear
57
what extent they can be replaced with static word
3
Approach
3.1
Contextualizing Models
3.2
Data
trained models.
Our input data come from the
3.3
Measures of Contextuality
is using three different metrics: self-similarity
intra-sentence similarity  and maximum explain-
able variance.
sion of the PyTorch-Transformers library.
Deﬁnition 1
Let w be a word that appears in
sentences {s1 ... sn} at indices {i1 ... in} respec-
tively  such that w = s1[i1] = ... = sn[in]. Let fℓ(s i)
be a function that maps s[i] to its representation in
layer ℓ of model f. The self similarity of w in layer
ℓ is
SelfSimℓ(w) =
n2 −n ∑
j ∑
k̸=j
cos(fℓ(sj ij)  fℓ(sk ik))
words  the self-similarity of a word w in layer ℓ is
alized representations across its n unique contexts.
If layer ℓ does not contextualize the representa-
tions at all  then SelfSimℓ(w) = 1 (i.e.  the repre-
Deﬁnition 2
Let s be a sentence that is a se-
quence ⟨w1 ... wn⟩ of n words. Let fℓ(s i) be a
function that maps s[i] to its representation in layer
ℓ of model f. The intra-sentence similarity of s in
layer ℓ is
IntraSimℓ(s) = 1
n ∑
cos(⃗sℓ  fℓ(s i))
where ⃗sℓ = 1
fℓ(s i)
(2)
fests in the vector space.
For example  if both
IntraSimℓ(s) and SelfSimℓ(w) are low ∀ w ∈ s  then
tations in the sentence. If IntraSimℓ(s) is high but
SelfSimℓ(w) is low  this suggests a less nuanced
Deﬁnition 3
layer ℓ of model f. Where [fℓ(s1 i1)...fℓ(sn in)]
is the occurrence matrix of w and σ1...σm are the
58
ﬁrst m singular values of this matrix  the maximum
MEVℓ(w) =
σ2
∑i σ2
(3)
MEVℓ(w) is the proportion of variance in w’s con-
tualized representations. The closer MEVℓ(w) is
would be; if MEVℓ(w) = 1  then a static embed-
3.4
Adjusting for Anisotropy
thereof) when discussing contextuality.
For ex-
(i.e.  directionally uniform)  then SelfSimℓ(w) =
ity of 0.99. Then SelfSimℓ(w) = 0.95 would actu-
contextuality measures.
For self-similarity and
culate separate baselines for each layer as well.
We then subtract from each measure its respective
ity measure. For example  the anisotropy-adjusted
Baseline(fℓ) = Ex y∼U(O) [cos(fℓ(x)  fℓ(y))]
SelfSim∗
ℓ(w) = SelfSimℓ(w)−Baseline(fℓ)
where O is the set of all word occurrences and
fℓ(·) maps a word occurrence to its representation
in layer ℓ of model f. Unless otherwise stated  ref-
4
Findings
4.1
(An)Isotropy
in all non-input layers.
If word representations
more anisotropic in higher layers.
As seen in
eﬁts for static word embeddings.
In theory  it
4.2
Context-Speciﬁcity
context-speciﬁc in higher layers.
Recall from
ferent contexts  adjusted for anisotropy.
If the
similarity of uniformly randomly sampled words
in each layer of BERT  ELMo  and GPT-2. For
tations (Liu et al.  2019a).
Therefore  it fol-
Stopwords (e.g.  ‘the’  ‘of’  ‘to’) have among the
most context-speciﬁc representations.
Across
ers are ‘and’  ‘of’  ‘’s’  ‘the’  and ‘to’. This is rel-
As noted earlier
2.
However  how does this increased context-
similar to one another in upper layers.
As
context-speciﬁc in upper layers  the intra-sentence
similarity also rises. This suggests that  in prac-
dissimilar to one another in upper layers.
anisotropy baseline).
This suggests a more nu-
randomly sampled words.
On average  the un-
4.3
Static vs. Contextualized
explained by a static embedding.
tion across all words can be explained by a sin-
gle vector. We adjust for anisotropy by calculating
portion from the raw MEV. In Figure 4  we plot
all representations of a single word.
Note that
and FastText on many benchmarks.
As noted
Static Embedding
SimLex999
MEN
WS353
RW
Google
MSR
SemEval2012(2)
BLESS
AP
GloVe
0.194
0.216
0.339
0.127
0.189
0.312
0.097
0.390
0.308
FastText
0.239
0.432
0.176
0.203
0.289
0.104
0.375
0.291
ELMo  Layer 1
0.276
0.167
0.317
0.148
0.170
0.326
0.114
0.410
ELMo  Layer 2
0.215
0.151
0.272
0.133
0.130
0.268
0.132
0.395
0.318
BERT  Layer 1
0.315
0.200
0.394
0.208
0.236
0.389
0.166
0.365
0.321
BERT  Layer 2
0.320
0.383
0.188
0.230
0.385
0.149
BERT  Layer 11
0.221
0.076
0.319
0.135
0.175
0.290
0.370
BERT  Layer 12
0.233
0.082
0.325
0.144
0.184
0.307
0.360
0.294
GPT-2  Layer 1
0.174
0.012
0.183
0.052
0.081
0.033
0.220
GPT-2  Layer 2
0.036
0.171
0.180
0.045
0.062
0.021
0.245
GPT-2  Layer 11
0.126
0.034
0.165
0.182
0.031
0.038
0.270
GPT-2  Layer 12
0.140
-0.009
0.113
0.163
0.020
0.014
0.225
0.172
layer.
In Table 1  we plot the performance of
tasks2.
These tasks cover semantic similarity
for evaluation.
between those of Layers 2 and 11.
63
tations  such as those from Layer 1 of BERT  are
5
Future Work
being highly anisotropic.
Therefore  adding an
While the latter offer superior per-
6
Conclusion
textualized word representations truly are.
For
resentations than lower layers.
This increased
ized representations could be explained by a static
embedding. This means that even in the best-case
HLT. pages 252–263.
Wiebe. 2014.
Semeval-2014 task 10:
Multilin-
mEval@ COLING. pages 81–91.
on Semantic Evaluation. Association for Computa-
Attribute-based and value-based clustering:
An
Representations.
64
Marco Baroni and Alessandro Lenci. 2011. How we
rical Models of Natural Language Semantics. Asso-
John Hewitt and Christopher D. Manning. 2019.
A
guage Technologies. Association for Computational
tics 41(4):665–695.
Workshop on Semantic Evaluation. Association for
In Proceedings of the eighteenth conference on com-
putational natural language learning. pages 171–
vances in Neural Information Processing Systems.
nologies.
Yinhan Liu  Myle Ott  Naman Goyal  Jingfei Du  Man-
Manning. 2013.
Better word representations with
recursive neural networks for morphology.
Language Learning (CoNLL). pages 104–113.
Systems. pages 3111–3119.
Language Technologies. pages 746–751.
Methods in Natural Language Processing. pages
tations (ICLR).
Manning. 2014.
GloVe: Global vectors for word
representation.
In Proceedings of the 2014 Con-
Processing (EMNLP). pages 1532–1543.
resentations.
In Proceedings of the 2018 Confer-
guage Technologies  Volume 1 (Long Papers). pages
jan Das  and Ellie Pavlick. 2019.
What do you
in contextualized word representations.
In Inter-
national Conference on Learning Representations.
65
ing for language understanding.
arXiv preprint
neural networks?
In Advances in Neural Informa-
tion Processing Systems. pages 3320–3328."
R021,1,EMNLP,"Inferring commonsense knowledge is a key
challenge in natural language processing  but
due to the sparsity of training data  previ-
ous work has shown that supervised methods
for commonsense knowledge mining under-
perform when evaluated on novel data. In
this work  we develop a method for generat-
ing commonsense knowledge using a large
pre-trained bidirectional language model. By
transforming relational triples into masked
sentences  we can use this model to rank a
triple’s validity by the estimated pointwise
mutual information between the two entities.
Since we do not update the weights of the
bidirectional model  our approach is not bi-
ased by the coverage of any one common-
sense knowledge base. Though this method
performs worse on a test set than models ex-
plicitly trained on a corresponding training set
it outperforms these methods when mining
commonsense knowledge from new sources
suggesting that unsupervised techniques may
generalize better than current supervised ap-
proaches.
1","Commonsense knowledge consists of facts about
the world which are assumed to be widely
known. For this reason  commonsense knowledge
is rarely stated explicitly in natural language  mak-
ing it challenging to infer this information with-
out an enormous amount of data (Gordon and
Van Durme  2013). Some have even argued that
machine learning models cannot learn common
sense implicitly (Davis and Marcus  2015).
One method for mollifying this issue is directly
augmenting models with commonsense knowl-
edge bases (Young et al.  2018)  which typically
contain high-quality information but with low cov-
erage. These knowledge bases are represented
as a graph  with nodes consisting of conceptualentities (i.e. dog running away  excited   etc.)
and the pre-deﬁned edges representing the nature
of the relations between concepts ( IsA UsedFor
CapableOf   etc.). Commonsense knowledge base
completion (CKBC) is a machine learning task
motivated by the need to improve the coverage of
these resources. In this formulation of the prob-
lem  one is supplied with a list of candidate entity-
relation-entity triples  and the task is to distin-
guish which of the triples express valid common-
sense knowledge and which are ﬁctitious (Li et al.
2016).
Several approaches have been proposed for
training models for commonsense knowledge base
completion (Li et al.  2016; Jastrzebski et al.
2018). Each of these approaches uses some
sort of supervised training on a particular knowl-
edge base  evaluating the model’s performance
on a held-out test set from the same database.
These works use relations from ConceptNet  a
crowd-sourced database of structured common-
sense knowledge  to train and validate their mod-
els (Liu and Singh  2004). However  it has been
shown that these methods generalize poorly to
novel data (Li et al.  2016; Jastrzebski et al.  2018).
Jastrzebski et al. (2018) demonstrated that much
of the data in the ConceptNet test set were simply
rephrased relations from the training set  and that
this train-test set leakage led to artiﬁcially inﬂated
test performance metrics. This problem of train-
test leakage is typical in knowledge base comple-
tion tasks (Toutanova et al.  2015; Dettmers et al.
2018).
Instead of training a predictive model on any
speciﬁc database  we attempt to utilize the world
knowledge of large language models to identify
commonsense facts directly. By constructing a
candidate piece of knowledge as a sentence  we
can use a language model to approximate the like-
lihood of this text as a proxy for its truthfulness.
1174In particular  we use a masked language model to
estimate point-wise mutual information between
entities in a possible relation  an approach that
differs signiﬁcantly from ﬁne-tuning approaches
used for other language modeling tasks. Since the
weights of the model are ﬁxed  our approach is
not biased by the coverage of any one dataset. As
we might expect  our method underperforms when
compared to previous benchmarks on the Con-
ceptNet common sense triples dataset (Li et al.
2016)  but demonstrates a superior ability to gen-
eralize when mining novel commonsense knowl-
edge from Wikipedia.
Related Work Schwartz et al. (2017) and Trinh
and Le (2018) demonstrate a similar approach to
using language models for tasks requiring com-
monsense  such as the Story Cloze Task and
the Winograd Schema Challenge  respectively
(Mostafazadeh et al.  2016; Levesque et al.  2012).
Bosselut et al. (2019) and Trinh and Le (2019)
use unidirectional language models for CKBC  but
their approach requires a supervised training step.
Our approach differs in that we intentionally avoid
training on any particular database  relying instead
on the language model’s general world knowl-
edge. Additionally  we use a bidirectional masked
model which provides a more ﬂexible framework
for likelihood estimation and allows us to estimate
point-wise mutual information. Although it is be-
yond the scope of this paper  it would be interest-
ing to adapt the methods presented here for the re-
lated task of generating new commonsense knowl-
edge (Saito et al.  2018).
2 Method
Given a commonsense head-relation-tail triple
x= (h;r;t)  we are interested in determining the
validity of that tuple as a representation of a com-
monsense fact. Speciﬁcally  we would like to de-
termine a numeric score y2Rreﬂecting our con-
ﬁdence that a given tuple represents true knowl-
edge.
We assume that heads and tails are arbitrary-
length sequences of words in a vocabulary V
so that h=fh1;h2;:::;hngandt=
ft1;t2;:::;tmg. We further assume that we have
a known set of possible relations Rso thatr2R.
The goal is to determine a function fthat maps
relational triples to validity scores. We propose
decomposing f(x) =((x))into two sub-
components: a sentence generation function which maps a triple to a single sentence  and a
scoring model which then determines a validity
scorey.
Our approach relies on two types of pretrained
language models. Standard unidirectional models
are typically represented as autoregressive proba-
bilities:
p(w1;w2;:::;wm) =mY
ip(wijw1;:::;wi 1)
Masked bidirectional models such as BERT  pro-
posed by Devlin et al. (2018)  instead model in
both directions  training word representations con-
ditioned both on future and past words. The mask-
ing allows any number of words in the sequence to
be hidden. This setup provides an intuitive frame-
work to evaluate the probability of any word in a
sequence conditioned on the rest of the sequence
p(wijw0
1:i 1;w0
i+1:m)
wherew02V[fgandis a special token indi-
cating a masked word.
2.1 Generating Sentences from Triples
We ﬁrst consider methods for turning a triple such
as(ferret  AtLocation  pet store) into a
sentence such as “the ferret is in the pet store”.
Our approach is to generate a set of candidate sen-
tences via hand-crafted templates and select the
best proposal according to a language model.
For each relation r2 R   we hand-craft a set
of sentence templates. For example  one template
in our experiments for the relation AtLocation
is  “you are likely to ﬁnd HEAD inTAIL ”. For
the above example  this would yield the sentence
“You are likely to ﬁnd ferret in pet store”.
Because these sentences are not always gram-
matically correct  such as in the above example
we apply a simple set of transformations. These
consist of inserting articles before nouns  con-
verting verbs into gerunds  and pluralizing nouns
which follow numbers. See the supplementary
materials for details and Table 1 for an exam-
ple. We then enumerate a set of alternative sen-
tencesS=fS1;:::;Sjgresulting from each tem-
plate and from all combinations of transforma-
tions. This yields a set of candidate sentences for
each data point. We then select the candidate sen-
tence with the highest log-likelihood according to
a pre-trained unidirectional language model Pcoh.
S= arg max
S2S[logPcoh(S)]
1175Candidate Sentence Si logp(Si)
“musician can playing musical instrument”  5:7
“musician can be play musical instrument”  4:9
“musician often play musical instrument”  5:5
“a musician can play a musical instrument”  2:9
Table 1: Example of generating candidate sen-
tences. Several enumerated sentences for the
triple (musician  CapableOf  play musical
instrument) . The sentence with the highest log-
likelihood according to a pretrained language model is
selected.
We refer to this method of generating a sen-
tence from a triple as C OHERENCY RANKING .
Coherency Ranking operates under the assump-
tion that natural  grammatical sentences will have
a higher likelihood than ungrammatical or unnat-
ural sentences. See an example subset of sen-
tence candidates and their corresponding scores
in Table 1. From a qualitative evaluation of the
selected sentences  we ﬁnd that this approach
produces sentences of signiﬁcantly higher quality
than those generated by deterministic rules alone.
We also perform an ablation study in our experi-
ments demonstrating the effect of each component
on CKBC performance.
2.2 Scoring Generated Triples
Assuming we have generated a proper sentence
from a relational triple  we now need a way to
score its validity with a pretrained model that con-
siders the relationship between the relation enti-
ties. We therefore propose using the estimated
point-wise mutual information (PMI) of the head
hand tail tof a triple conditioned on the relation
r  deﬁned as
PMI(t;hjr) = logp(tjh;r) logp(tjr)
We can estimate these scores by using a masked
bidirectional language model  Pcmp. In the case
where the tail is a single word  the model al-
lows us to evaluate the conditional likelihood of
a single triple component p(tjh;r)by computing
Pcmp(wi=tjw1:i 1;wi+1:m)for the tail word.
In practice  the tail might be realized as a j-
word phrase. To handle this complexity  we use
a greedy approximation of its probability. We ﬁrst
mask all of the tail words and compute the proba-
bility of each. We then ﬁnd the word with highest
probabilitypk  substitute it back in  and repeat jtimes. Finally  we calculate the total conditional
likelihood of the tail by the product of these terms
p(tjh;r) =Qj
k=1pk.
The marginal p(tjr)is computed similarly  but
in this case we mask the head throughout. For
example  to compute the marginal tail probability
for the sentence  “You are likely to ﬁnd a ferret
in the pet store” we mask both the head and the
tail and then sequentially unmask the tail words
only: “You are likely to ﬁnd a h1in thet1t2”.
Ift2=“store” has a higher probability than
t1=“pet”  we unmask “store” and compute
“You are likely to ﬁnd a h1in thet1store”. The
marginal likelihood p(tjr)is then the product of
the two probabilities.
The ﬁnal score combines the marginal and con-
ditional likelihoods by employing a weighted form
of the point-wise mutual information
PMI(t;hjr) =logp(tjh;r) logp(tjr)
whereis treated as a hyperparameter. Although
exact PMI is symmetrical  the approximate model
itself is not. We therefore average PMI (t;hjr)
and PMI(h;tjr)to reduce the variance of our es-
timates  computing the masked head values rather
than the tail values in the latter.
3 Experiments
To evaluate the Coherency Ranking approach we
measure whether it can distinguish between valid
and invalid triples. For our masked model  we use
BERT-large (Devlin et al.  2018). For sentence
ranking  we use the GPT-2 117M LM (Radford
et al.  2019). The relation templates and grammar
transformation rules which we use can be found in
the supplementary materials.
We compare the proposed method to several
baselines. Following Trinh and Le (2018)  we
evaluate a simple C ONCATENATION method for
generating sentences  splitting the relation rinto
separate words and concatenating it with the head
and tail. For the triple (ferret  AtLocation
pet store)   the Concatenation approach would
yield  “ferret at location pet store”.
We also evaluate CKBC performance when we
construct sentences by applying a single hand-
crafted template. Since each triple is mapped
to a sentence with a single template without any
grammatical transformations  we refer to this as
the T EMPLATE method. Using the Template
approach  (ferret  AtLocation  pet store)
1176Model Task 1 Task 2
Unsupervised
CONCATENATION 68:8 2:950:11
TEMPLATE 72:2 2:980:11
TEMPL .+G RAMMAR 74:4 2:560:13
COHERENCY RANK 78:83:000:12
Supervised
DNN 89:2 2:50
FACTORIZED 89:0 2:61
PROTOTYPICAL 79:4 2:55
Table 2: Main","and the 9th International Joint Conference on Natural Language Processing   pages 1173–1178
Hong Kong  China  November 3–7  2019. c2019 Association for Computational Linguistics1173Commonsense Knowledge Mining from Pretrained Models
Joshua Feldman  Joe Davison  Alexander M. Rush
School of Engineering and Applied Sciences
Harvard University
fjoshua feldman@g  jddavison@g  srush@seas g.harvard.edu","knowledge base completion (test F1 score) and Task
2: Wikipedia mining (quality scores out of 4). Re-
sults are included from the sentence generation meth-
ods of simple concatenation  hand-crafted templates
templates plus grammatical transformations  and co-
herency ranking. DNN  Factorized  and Prototypical
models are described in Jastrzebski et al. (2018).
would become “You are likely to ﬁnd ferret in pet
store” using the template “you are likely to ﬁnd
HEAD inTAIL ”.
Next  we extend the Template method by ap-
plying deterministic grammatical transformations
which we refer to as the T EMPLATE + G RAMMAR
approach. Like the full approach  these trans-
formations involve adding articles before nouns
converting verbs into gerunds  and pluralizing
nouns following numbers. The Template + Gram-
mar approach differs from Coherency Ranking in
that all transformations are applied to every sen-
tence instead of applying combinations of trans-
formations and templates  which are then ranked
by a language model. Returning to our exam-
ple  the Template + Grammar method produces
“You are likely to ﬁnd a ferret in a pet store”.
While this sentence is grammatical  applying this
method to (star  AtLocation  outer space)
yields “You are likely to ﬁnd a star in an outer
space”  which is incorrect.
We compare our results to the supervised mod-
els from the work of Jastrzebski et al. (2018)
and the best performing model from Li et al.
(2016). Jastrzebski et al. (2018) introduce F AC-
TORIZED and P ROTOTYPICAL models. The Fac-
torized model embeds the head  relation  and tail
in a vector space and then produces a score by tak-
ing a linear combination of the inner products be-
tween each pair of embeddings. The Prototypi-cal model is similar  but does not include the in-
ner product between head and tail. Li et al. (2016)
evaluate a deep neural network (DNN) for CKBC.
They concatenate embeddings for the head  rela-
tion  and tail  which they then feed through a mul-
tilayer perceptron with one hidden layer. All three
models are trained on 100 000 ConceptNet triples.
Task 1: Commonsense Knowledge Base Com-
pletion Our experimental setup follows Li et al.
(2016)  evaluating our model with their test set
(n = 2400) containing an equal number of valid
and invalid triples. The valid triples are from
the crowd-sourced Open Mind Common Sense
(OMCS) entries in the ConceptNet 5 dataset
(Speer and Havasi  2012). Invalid triples are gen-
erated by replacing an element of a valid tuple
with another randomly selected element.
We use our scoring method to classify each tu-
ple as valid or invalid. To this end  we use our
method to assign a score to each tuple and then
group the resulting scores into two clusters. In-
stances in the cluster with the higher mean PMI
are labeled as valid  and the remainder are labeled
as invalid. We use expectation-maximization with
a mixture of Gaussians to cluster. We also tune the
PMI weight via grid search over 90points from
2[0:5;5:]  using the Akaike information crite-
rion of the Gaussian mixture model for evaluation
(Akaike  1974).
Table 2 shows the full results. Our unsupervised
approach achieves a test set F1 score of 78:8  com-
parable to the 79:4F1 score found by the super-
vised prototypical approach. The Factorized and
DNN models signiﬁcantly outperformed our ap-
proach with F1 scores of 89.2 and 89.0  respec-
tively. Our grid search found an optimal value
of1:65for the Concatenation sentence generation
model and 1:55for the Coherency Ranking model.
The Template and Template + Grammar methods
found lambda values of 1:20and0:95  respec-
tively.
Task 2: Mining Wikipedia To assess the
model’s ability to generalize to unseen data  we
evaluate our unsupervised model in comparison to
previous supervised methods on the task of min-
ing commonsense knowledge from Wikipedia. In
their evaluations  Li et al. (2016) curate a set of
1.7M triples across 10 relations by applying part-
of-speech patterns to Wikipedia articles. We sam-
ple 300 triples from each relation. We apply our
1177method to evaluate these 3000 triples. Using the
approach described by Speer and Havasi (2012)
and followed by Li et al. (2016) and Jastrzebski
et al. (2018)  two human annotators manually rate
the 100 triples with the highest predicted score
on a 0 to 4 scale: 0 (Doesn’t make sense)  1
(Not true)  2 (Opinion/Don’t know)  3 (Sometimes
true)  and 4 (Generally true). We tuned by mea-
suring the quality of the 100 triples with the high-
est predicted score across 2f1;2;:::; 9;10g.
The top 100 triples selected by our model were
assigned a mean rating of 3.00 ( = 4) with a
standard error of 0.11 under the Coherency Rank-
ing approach  well exceeding the performance of
current supervised methods (Table 2). Standard
errors were calculated using 1000 bootstrap sam-
ples of the top 100 triples. The ratings assigned
by the two human annotators had a 0:50Pearson
correlation and 0:23kappa inter-annotator agree-
ment. Rater disagreements occur most frequently
when triples are ambiguous or difﬁcult to inter-
pret. Notably  if we bucket the ﬁve scores into just
two categories of trueandfalse   this disagreement
rate drops by 50%. To give a sense of the types
of commonsense knowledge our models struggle
to capture  we report the top 100 most conﬁdent
predictions that receive an average score below
3 in the supplementary material. Notably  some
of the top 100 triples our model identiﬁed were
indeed true  but would not be reasonably con-
sidered common sense (e.g. (vector bundle
HasProperty  manifold) ). This suggests that
our approach may be applicable to mining knowl-
edge beyond common sense.
Analysis: Sentence Generation In order to
measure the impact of sentence generation on our
model  we select a sample of 100sentences and
group the results by a) whether the sentence con-
tained a grammatical error  and b) whether the
sentence misrepresented the meaning of the triple.
For example  the triple (golf  HasProperty
good) yields the sentence “golf is a good”  which
is grammatically correct but conveys the wrong
meaning. On both Wikipedia mining and CKBC
we ﬁnd that misrepresenting meaning has an ad-
verse impact on model performance. In CKBC
we also ﬁnd that grammar has a high impact on the
resulting F1 scores (Table 3). Future work could
therefore focus on designing templates that more
reliably encode a relation’s true meaning.Task 1 N (/100) F1 Score
GRAMMATICAL 75 79 :1
UNGRAMMATICAL 25 66 :7
CORRECT MEANING 91 77 :6
WRONG MEANING 9 66 :7
Task 2 - Quality
GRAMMATICAL 83 3:01
UNGRAMMATICAL 17 2:88
CORRECT MEANING 88 3:22
WRONG MEANING 12 1:18
Table 3: Test results examining the effect of sen-
tence meaning and grammaticality on task perfor-
mance. Scores are shown for a sample of 100triples
split by whether the generated sentence is grammati-
cal and whether it conveys the correct meaning of the
triple.
4","We introduce a robust unsupervised method for
commonsense knowledge base completion using
the world knowledge of pre-trained language mod-
els. We develop a method for expressing knowl-
edge triples as sentences. Using a bidirectional
masked language model on these sentences  we
can then estimate the weighted point-wise mutual
information of a triple as a proxy for its valid-
ity. Though our approach performs worse on a
held-out test set developed by Li et al. (2016)  it
does so without any previous exposure to the Con-
ceptNet database  ensuring that this performance
is not biased. In the future  we hope to explore
whether this approach can be extended to min-
ing facts that are not commonsense and to gen-
erating new commonsense knowledge outside of
any given database of candidate triples. We also
see potential beneﬁt in the development of a more
expansive set of evaluation methods for common-
sense knowledge mining  which would strengthen
the validity of our conclusions.
Acknowledgments
This work was supported by NSF research award
1845664.
1178References
Hirotugu Akaike. 1974. A new look at the statistical
model identiﬁcation. In Selected Papers of Hirotugu
Akaike   pages 215–222. Springer.
Antoine Bosselut  Hannah Rashkin  Maarten Sap
Chaitanya Malaviya  Asli C ¸ elikyilmaz  and Yejin
Choi. 2019. COMET: commonsense transformers
for automatic knowledge graph construction. CoRR
abs/1906.05317.
Ernest Davis and Gary Marcus. 2015. Commonsense
reasoning and commonsense knowledge in artiﬁcial
intelligence. Commun. ACM   58(9):92–103.
Tim Dettmers  Pasquale Minervini  Pontus Stenetorp
and Sebastian Riedel. 2018. Convolutional 2d
knowledge graph embeddings. In Thirty-Second
AAAI Conference on Artiﬁcial Intelligence .
Jacob Devlin  Ming-Wei Chang  Kenton Lee  and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .
Jonathan Gordon and Benjamin Van Durme. 2013. Re-
porting bias and knowledge acquisition. In Proceed-
ings of the 2013 workshop on Automated knowledge
base construction   pages 25–30. ACM.
Stanisław Jastrzebski  Dzmitry Bahdanau  Seyedar-
ian Hosseini  Michael Noukhovitch  Yoshua Ben-
gio  and Jackie Chi Kit Cheung. 2018. Common-
sense mining as knowledge base completion? a
study on the impact of novelty. arXiv preprint
arXiv:1804.09259 .
Hector Levesque  Ernest Davis  and Leora Morgen-
stern. 2012. The winograd schema challenge. In
Thirteenth International Conference on the Princi-
ples of Knowledge Representation and Reasoning .
Xiang Li  Aynaz Taheri  Lifu Tu  and Kevin Gimpel.
2016. Commonsense knowledge base completion.
InProceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers)   volume 1  pages 1445–1455.
Hugo Liu and Push Singh. 2004. Conceptneta practi-
cal commonsense reasoning tool-kit. BT technology
journal   22(4):211–226.
Nasrin Mostafazadeh  Nathanael Chambers  Xiaodong
He  Devi Parikh  Dhruv Batra  Lucy Vanderwende
Pushmeet Kohli  and James Allen. 2016. A cor-
pus and evaluation framework for deeper under-
standing of commonsense stories. arXiv preprint
arXiv:1604.01696 .
Alec Radford  Jeffrey Wu  Rewon Child  David Luan
Dario Amodei  and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. OpenAI
Blog   1:8.Itsumi Saito  Kyosuke Nishida  Hisako Asano  and
Junji Tomita. 2018. Commonsense knowledge base
completion and generation. In Proceedings of the
22nd Conference on Computational Natural Lan-
guage Learning   pages 141–150  Brussels  Belgium.
Association for Computational Linguistics.
Roy Schwartz  Maarten Sap  Ioannis Konstas  Leila
Zilles  Yejin Choi  and Noah A. Smith. 2017. The
effect of different writing tasks on linguistic style:
A case study of the ROC story cloze task. CoRR
abs/1702.01841.
Robert Speer and Catherine Havasi. 2012. Represent-
ing general relational knowledge in conceptnet 5. In
LREC   pages 3679–3686.
Kristina Toutanova  Danqi Chen  Patrick Pantel  Hoi-
fung Poon  Pallavi Choudhury  and Michael Gamon.
2015. Representing text for joint embedding of text
and knowledge bases. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing   pages 1499–1509.
Trieu H. Trinh and Quoc V . Le. 2018. A sim-
ple method for commonsense reasoning. CoRR
abs/1806.02847.
Trieu H. Trinh and Quoc V . Le. 2019. Do language
models have common sense?
Tom Young  Erik Cambria  Iti Chaturvedi  Hao Zhou
Subham Biswas  and Minlie Huang. 2018. Aug-
menting end-to-end dialogue systems with common-
sense knowledge. In Thirty-Second AAAI Confer-
ence on Artiﬁcial Intelligence .
Commonsense Knowledge Mining from Pretrained Models
JoshuaFeldman∗  JoeDavison∗  AlexanderM.Rush
SchoolofEngineeringandAppliedSciences
HarvardUniversity
{joshua feldman@g  jddavison@g  srush@seas}.harvard.edu
Abstract entities (i.e. dog  running away  excited  etc.)
of the relations between concepts (IsA  UsedFor
CapableOf  etc.). Commonsense knowledge base
for commonsense knowledge mining under- motivated by the need to improve the coverage of
perform when evaluated on novel data. In these resources. In this formulation of the prob-
this work  we develop a method for generat- lem oneissuppliedwithalistofcandidateentity-
ing commonsense knowledge using a large  relation-entity triples  and the task is to distin-
senseknowledgeandwhichareﬁctitious(Lietal.
mutual information between the two entities. Several approaches have been proposed for
Since we do not update the weights of the trainingmodelsforcommonsenseknowledgebase
bidirectional model  our approach is not bi- completion (Li et al.  2016; Jastrzebski et al.
ased by the coverage of any one common- 2018). Each of these approaches uses some
plicitlytrainedonacorrespondingtrainingset
commonsense knowledge from new sources  These works use relations from ConceptNet  a
suggesting that unsupervised techniques may crowd-sourced database of structured common-
generalize better than current supervised ap- sense knowledge  to train and validate their mod-
proaches. els (Liu and Singh  2004). However  it has been
noveldata(Lietal. 2016;Jastrzebskietal. 2018).
Commonsense knowledge consists of facts about Jastrzebski et al. (2018) demonstrated that much
the world which are assumed to be widely ofthedataintheConceptNettestsetweresimply
known. Forthisreason commonsenseknowledge rephrased relations from the training set  and that
israrelystatedexplicitlyinnaturallanguage mak- thistrain-testsetleakageledtoartiﬁciallyinﬂated
ing it challenging to infer this information with- test performance metrics. This problem of train-
out an enormous amount of data (Gordon and test leakage is typical in knowledge base comple-
Van Durme  2013). Some have even argued that tiontasks(Toutanovaetal. 2015;Dettmersetal.
machine learning models cannot learn common 2018).
senseimplicitly(DavisandMarcus 2015). Instead of training a predictive model on any
Onemethodformollifyingthisissueisdirectly speciﬁc database  we attempt to utilize the world
augmenting models with commonsense knowl- knowledge of large language models to identify
edge bases (Young et al.  2018)  which typically commonsense facts directly. By constructing a
containhigh-qualityinformationbutwithlowcov- candidate piece of knowledge as a sentence  we
erage. These knowledge bases are represented canusealanguagemodeltoapproximatethelike-
as a graph  with nodes consisting of conceptual lihood of this text as a proxy for its truthfulness.
In particular  we use a masked language model to which maps a triple to a single sentence  and a
estimate point-wise mutual information between scoring model σ which then determines a validity
entities in a possible relation  an approach that scorey.
differs signiﬁcantly from ﬁne-tuning approaches Our approach relies on two types of pretrained
usedforotherlanguagemodelingtasks. Sincethe language models. Standard unidirectional models
weights of the model are ﬁxed  our approach is are typically represented as autoregressive proba-
notbiasedbythecoverageofanyonedataset. As bilities:
wemightexpect ourmethodunderperformswhen m
(cid:89)
compared to previous benchmarks on the Con- p(w1 w2 ... wm) = p(wi|w1 ... wi−1)
ceptNet common sense triples dataset (Li et al.  i
2016)  but demonstrates a superior ability to gen- Masked bidirectional models such as BERT  pro-
eralize when mining novel commonsense knowl- posed by Devlin et al. (2018)  instead model in
edgefromWikipedia. bothdirections trainingwordrepresentationscon-
ditionedbothonfutureandpastwords. Themask-
RelatedWork Schwartzetal.(2017)andTrinh
ingallowsanynumberofwordsinthesequenceto
behidden. Thissetupprovidesanintuitiveframe-
sequenceconditionedontherestofthesequence
(Mostafazadehetal. 2016;Levesqueetal. 2012). p(w |w(cid:48)  w(cid:48) )
i 1:i−1 i+1:m
wherew(cid:48) ∈ V ∪{κ}andκisaspecialtokenindi-
useunidirectionallanguagemodelsforCKBC but
catingamaskedword.
theirapproachrequiresasupervisedtrainingstep.
Ourapproachdiffersinthatweintentionallyavoid 2.1 GeneratingSentencesfromTriples
trainingonanyparticulardatabase relyinginstead
Weﬁrstconsidermethodsforturningatriplesuch
as (ferret  AtLocation  pet store) into a
edge. Additionally weuseabidirectionalmasked
Ourapproachistogenerateasetofcandidatesen-
forlikelihoodestimationandallowsustoestimate
point-wise mutualinformation. Althoughit is be-
bestproposalaccordingtoalanguagemodel.
For each relation r ∈ R  we hand-craft a set
ingtoadaptthemethodspresentedhereforthere-
of sentencetemplates. Forexample  one template
latedtaskofgeneratingnewcommonsenseknowl-
edge(Saitoetal. 2018).
is  “you are likely to ﬁnd HEAD in TAIL”. For
“Youarelikelytoﬁndferretinpetstore”.
Given a commonsense head-relation-tail triple Because these sentences are not always gram-
x = (h r t) weareinterestedindeterminingthe matically correct  such as in the above example
validityofthattupleasarepresentationofacom- we apply a simple set of transformations. These
monsense fact. Speciﬁcally  we would like to de- consist of inserting articles before nouns  con-
termineanumericscorey ∈ Rreﬂectingourcon- verting verbs into gerunds  and pluralizing nouns
ﬁdence that a given tuple represents true knowl- which follow numbers. See the supplementary
edge. materials for details and Table 1 for an exam-
We assume that heads and tails are arbitrary- ple. We then enumerate a set of alternative sen-
length sequences of words in a vocabulary V tencesS = {S  ... S }resultingfromeachtem-
1 j
so that h = {h1 h2 ... hn} and t = plate and from all combinations of transforma-
{t1 t2 ... tm}. We further assume that we have tions. This yields a set of candidate sentences for
aknownsetofpossiblerelationsRsothatr ∈ R. eachdatapoint. Wethenselectthecandidatesen-
Thegoalistodetermineafunctionf thatmaps tencewiththehighestlog-likelihoodaccordingto
relational triples to validity scores. We propose apre-trainedunidirectionallanguagemodelP .
coh
decomposing f(x) = σ(τ(x)) into two sub-
S∗ = argmax[logP (S)]
components: a sentence generation function τ coh
S∈S
CandidateSentenceS logp(S ) times. Finally  we calculate the total conditional
i i
likelihoodofthetailbytheproductoftheseterms
“musiciancanplayingmusicalinstrument” −5.7 p(t|h r) = (cid:81)j p .
“musiciancanbeplaymusicalinstrument” −4.9 k=1 k
The marginal p(t|r) is computed similarly  but
“musicianoftenplaymusicalinstrument” −5.5
“amusiciancanplayamusicalinstrument” −2.9
triple (musician  CapableOf  play musical tail and then sequentially unmask the tail words
instrument). The sentence with the highest log- only: “Youarelikelytoﬁndaκh1 intheκt1 κt2”.
likelihoodaccordingtoapretrainedlanguagemodelis If κ = “store” has a higher probability than
t2
selected. κ = “pet”  we unmask “store” and compute
t1
“Youarelikelytoﬁndaκ intheκ store”. The
h1 t1
marginal likelihood p(t|r) is then the product of
thetwoprobabilities.
tence from a triple as COHERENCY RANKING.
Theﬁnalscorecombinesthemarginalandcon-
ditionallikelihoodsbyemployingaweightedform
ofthepoint-wisemutualinformation
PMI (t h|r) = λlogp(t|h r)−logp(t|r)
λ
in Table 1. From a qualitative evaluation of the where λ is treated as a hyperparameter. Although
selected sentences  we ﬁnd that this approach exactPMIissymmetrical theapproximatemodel
produces sentences of signiﬁcantly higher quality itself is not. We therefore average PMI (t h|r)
than those generated by deterministic rules alone. andPMI (h t|r)toreducethevarianceofoures-
We also perform an ablation study in our experi- timates computingthemaskedheadvaluesrather
mentsdemonstratingtheeffectofeachcomponent thanthetailvaluesinthelatter.
onCKBCperformance.
2.2 ScoringGeneratedTriples
andinvalidtriples. Forourmaskedmodel weuse
scoreitsvaliditywithapretrainedmodelthatcon-
etal. 2019). Therelationtemplatesandgrammar
transformationruleswhichweusecanbefoundin
h and tail t of a triple conditioned on the relation
thesupplementarymaterials.
r deﬁnedas
PMI(t h|r) = logp(t|h r)−logp(t|r) baselines. Following Trinh and Le (2018)  we
evaluate a simple CONCATENATION method for
Wecanestimatethesescoresbyusingamasked generating sentences  splitting the relation r into
bidirectional language model  P . In the case separatewordsandconcatenatingitwiththehead
cmp
where the tail is a single word  the model al- and tail. For the triple (ferret  AtLocation
lows us to evaluate the conditional likelihood of pet store)  the Concatenation approach would
a single triple component p(t|h r) by computing yield “ferretatlocationpetstore”.
P (w = t|w  w )forthetailword. We also evaluate CKBC performance when we
cmp i 1:i−1 i+1:m
In practice  the tail might be realized as a j- construct sentences by applying a single hand-
word phrase. To handle this complexity  we use crafted template. Since each triple is mapped
agreedyapproximationofitsprobability. Weﬁrst to a sentence with a single template without any
maskallofthetailwordsandcomputetheproba- grammatical transformations  we refer to this as
bilityofeach. Wethenﬁndthewordwithhighest the TEMPLATE method. Using the Template
probability p   substitute it back in  and repeat j approach  (ferret  AtLocation  pet store)
k
Model Task1 Task2 cal model is similar  but does not include the in-
nerproductbetweenheadandtail. Lietal.(2016)
evaluateadeepneuralnetwork(DNN)forCKBC.
CONCATENATION 68.8 2.95±0.11 They concatenate embeddings for the head  rela-
TEMPLATE 72.2 2.98±0.11 tion andtail whichtheythenfeedthroughamul-
TEMPL.+GRAMMAR 74.4 2.56±0.13 tilayerperceptronwithonehiddenlayer. Allthree
COHERENCY RANK 78.8 3.00±0.12 modelsaretrainedon100 000ConceptNettriples.
DNN 89.2 2.50 pletion OurexperimentalsetupfollowsLietal.
FACTORIZED 89.0 2.61 (2016)  evaluating our model with their test set
PROTOTYPICAL 79.4 2.55 (n = 2400) containing an equal number of valid
Table 2: Main results for Task 1: Commonsense the crowd-sourced Open Mind Common Sense
withanotherrandomlyselectedelement.
herency ranking. DNN  Factorized  and Prototypical We use our scoring method to classify each tu-
modelsaredescribedinJastrzebskietal.(2018). ple as valid or invalid. To this end  we use our
wouldbecome“Youarelikelytoﬁndferretinpet
arelabeledasvalid andtheremainderarelabeled
HEADinTAIL”.
asinvalid. Weuseexpectation-maximizationwith
Next  we extend the Template method by ap- amixtureofGaussianstocluster. Wealsotunethe
plyingdeterministicgrammaticaltransformations  PMI weight via grid search over 90 points from
whichwerefertoastheTEMPLATE+GRAMMAR λ ∈ [0.5 5.]  using the Akaike information crite-
approach. Like the full approach  these trans- rionoftheGaussianmixturemodelforevaluation
formations involve adding articles before nouns  (Akaike 1974).
Table2showsthefullresults. Ourunsupervised
nounsfollowingnumbers. TheTemplate+Gram-
approachachievesatestsetF1scoreof78.8 com-
parable to the 79.4 F1 score found by the super-
tively. Our grid search found an optimal λ value
of1.65fortheConcatenationsentencegeneration
modeland1.55fortheCoherencyRankingmodel.
method to (star  AtLocation  outer space) found lambda values of 1.20 and 0.95  respec-
space” whichisincorrect.
We compare our results to the supervised mod- Task 2: Mining Wikipedia To assess the
els from the work of Jastrzebski et al. (2018) model’s ability to generalize to unseen data  we
and the best performing model from Li et al. evaluateourunsupervisedmodelincomparisonto
(2016). Jastrzebski et al. (2018) introduce FAC- previous supervised methods on the task of min-
TORIZED and PROTOTYPICAL models. The Fac- ing commonsense knowledge from Wikipedia. In
torized model embeds the head  relation  and tail their evaluations  Li et al. (2016) curate a set of
inavectorspaceandthenproducesascorebytak- 1.7M triples across 10 relations by applying part-
ingalinearcombinationoftheinnerproductsbe- of-speechpatternstoWikipediaarticles. Wesam-
tween each pair of embeddings. The Prototypi- ple 300 triples from each relation. We apply our
method to evaluate these 3000 triples. Using the Task1 N(/100) F1Score
GRAMMATICAL 75 79.1
UNGRAMMATICAL 25 66.7
etal.(2018) twohumanannotatorsmanuallyrate
the 100 triples with the highest predicted score CORRECT MEANING 91 77.6
on a 0 to 4 scale: 0 (Doesn’t make sense)  1 WRONG MEANING 9 66.7
(Nottrue) 2(Opinion/Don’tknow) 3(Sometimes
Task2 - Quality
true) and4(Generallytrue). Wetunedλbymea-
suringthequalityofthe100tripleswiththehigh- GRAMMATICAL 83 3.01
estpredictedscoreacrossλ ∈ {1 2 ... 9 10}. UNGRAMMATICAL 17 2.88
Thetop100triplesselectedbyourmodelwere CORRECT MEANING 88 3.22
assigned a mean rating of 3.00 (λ = 4) with a WRONG MEANING 12 1.18
standarderrorof0.11undertheCoherencyRank-
ing approach  well exceeding the performance of Table 3: Test results examining the effect of sen-
mance. Scores are shown for a sample of 100 triples
by the two human annotators had a 0.50 Pearson
correlation and 0.23 kappa inter-annotator agree-
pret. Notably ifwebuckettheﬁvescoresintojust
twocategoriesoftrueandfalse thisdisagreement
theworldknowledgeofpre-trainedlanguagemod-
sidered common sense (e.g. (vector bundle  held-out test set developed by Li et al. (2016)  it
HasProperty  manifold)). This suggests that doessowithoutanypreviousexposuretotheCon-
ourapproachmaybeapplicabletominingknowl-
edgebeyondcommonsense.
measuretheimpactofsentencegenerationonour
model  we select a sample of 100 sentences and
seepotentialbeneﬁtinthedevelopmentofamore
expansivesetofevaluationmethodsforcommon-
senseknowledgemining whichwouldstrengthen
sentencemisrepresentedthemeaningofthetriple.
thevalidityofourconclusions.
good)yieldsthesentence“golfisagood”  which
meaning. On both Wikipedia mining and CKBC  This work was supported by NSF research award
we ﬁnd that misrepresenting meaning has an ad- 1845664.
wealsoﬁndthatgrammarhasahighimpactonthe
reliablyencodearelation’struemeaning.
References Itsumi Saito  Kyosuke Nishida  Hisako Asano  and
JunjiTomita.2018. Commonsenseknowledgebase
Hirotugu Akaike. 1974. A new look at the statistical completion and generation. In Proceedings of the
modelidentiﬁcation. InSelectedPapersofHirotugu 22nd Conference on Computational Natural Lan-
Akaike pages215–222.Springer. guageLearning pages141–150 Brussels Belgium.
AssociationforComputationalLinguistics.
Chaitanya Malaviya  Asli C¸elikyilmaz  and Yejin Roy Schwartz  Maarten Sap  Ioannis Konstas  Leila
Choi. 2019. COMET: commonsense transformers Zilles  Yejin Choi  and Noah A. Smith. 2017. The
forautomaticknowledgegraphconstruction. CoRR  effect of different writing tasks on linguistic style:
abs/1906.05317. A case study of the ROC story cloze task. CoRR
reasoningandcommonsenseknowledgeinartiﬁcial RobertSpeerandCatherineHavasi.2012. Represent-
intelligence. Commun.ACM 58(9):92–103. inggeneralrelationalknowledgeinconceptnet5. In
LREC pages3679–3686.
fungPoon PallaviChoudhury andMichaelGamon.
2015. Representingtextforjointembeddingoftext
AAAIConferenceonArtiﬁcialIntelligence.
guageProcessing pages1499–1509.
KristinaToutanova.2018. Bert:Pre-trainingofdeep
Trieu H. Trinh and Quoc V. Le. 2018. A sim-
ing. arXivpreprintarXiv:1810.04805.
JonathanGordonandBenjaminVanDurme.2013. Re-
portingbiasandknowledgeacquisition. InProceed-
Trieu H. Trinh and Quoc V. Le. 2019. Do language
ingsofthe2013workshoponAutomatedknowledge
modelshavecommonsense?
baseconstruction pages25–30.ACM.
Stanisław Jastrzebski  Dzmitry Bahdanau  Seyedar- Subham Biswas  and Minlie Huang. 2018. Aug-
ian Hosseini  Michael Noukhovitch  Yoshua Ben- mentingend-to-enddialoguesystemswithcommon-
gio  and Jackie Chi Kit Cheung. 2018. Common- sense knowledge. In Thirty-Second AAAI Confer-
sense mining as knowledge base completion? a enceonArtiﬁcialIntelligence.
arXiv:1804.09259.
plesofKnowledgeRepresentationandReasoning.
In Proceedings of the 54th Annual Meeting of the
1: LongPapers) volume1 pages1445–1455.
calcommonsensereasoningtool-kit. BTtechnology
journal 22(4):211–226.
NasrinMostafazadeh NathanaelChambers Xiaodong
arXiv:1604.01696.
AlecRadford  JeffreyWu  RewonChild  DavidLuan
DarioAmodei andIlyaSutskever.2019. Language
modelsareunsupervisedmultitasklearners. OpenAI
Blog 1:8.
and the 9th International Joint Conference on Natural Language Processing  pages 1173–1178
Hong Kong  China  November 3–7  2019. c⃝2019 Association for Computational Linguistics
1173
Joshua Feldman∗
Joe Davison∗
Alexander M. Rush
perform when evaluated on novel data.
In
1
Introduction
erage.
These knowledge bases are represented
as a graph  with nodes consisting of conceptual
entities (i.e.
dog  running away  excited  etc.)
Each of these approaches uses some
commonsense facts directly.
By constructing a
1174
In particular  we use a masked language model to
Related Work
Schwartz et al. (2017) and Trinh
2
Method
x = (h  r  t)  we are interested in determining the
termine a numeric score y ∈ R reﬂecting our con-
so that h
=
{h1  h2  . . .   hn} and t
{t1  t2  . . .   tm}. We further assume that we have
a known set of possible relations R so that r ∈ R.
The goal is to determine a function f that maps
decomposing f(x)
σ(τ(x)) into two sub-
components:
a sentence generation function τ
which maps a triple to a single sentence  and a
scoring model σ which then determines a validity
score y.
p(w1  w2  . . .   wm) =
m
�
i
p(wi|w1  . . .   wi−1)
p(wi|w′
1:i−1  w′
where w′ ∈ V ∪ {κ} and κ is a special token indi-
2.1
Generating Sentences from Triples
which follow numbers.
See the supplementary
tences S = {S1  . . .   Sj} resulting from each tem-
S∗ = arg max
[log Pcoh(S)]
1175
Candidate Sentence Si
log p(Si)
“musician can playing musical instrument”
−5.7
“musician can be play musical instrument”
−4.9
“musician often play musical instrument”
−5.5
“a musician can play a musical instrument”
−2.9
Table 1:
Example of generating candidate sen-
tences.
Several
enumerated
sentences
for
the
triple
(musician  CapableOf  play musical
instrument).
The sentence with the highest log-
ural sentences.
See an example subset of sen-
2.2
Scoring Generated Triples
ties.
We therefore propose using the estimated
PMI(t  h|r) = log p(t|h  r) − log p(t|r)
a single triple component p(t|h  r) by computing
Pcmp(wi = t |w1:i−1  wi+1:m) for the tail word.
probability pk  substitute it back in  and repeat j
times. Finally  we calculate the total conditional
p(t|h  r) = �j
k=1 pk.
only: “You are likely to ﬁnd a κh1 in the κt1 κt2”.
If κt2 = “store” has a higher probability than
κt1 = “pet”  we unmask “store” and compute
“You are likely to ﬁnd a κh1 in the κt1 store”. The
PMIλ(t  h|r) = λ log p(t|h  r) − log p(t|r)
where λ is treated as a hyperparameter. Although
itself is not. We therefore average PMIλ(t  h|r)
and PMIλ(h  t|r) to reduce the variance of our es-
3
Experiments
BERT-large (Devlin et al.  2018).
For sentence
baselines.
Following Trinh and Le (2018)  we
generating sentences  splitting the relation r into
pet store)  the Concatenation approach would
crafted template.
Since each triple is mapped
the TEMPLATE method.
Using the Template
1176
Model
Task 1
Task 2
CONCATENATION
68.8
2.95 ± 0.11
TEMPLATE
72.2
2.98 ± 0.11
TEMPL.+GRAMMAR
74.4
2.56 ± 0.13
COHERENCY RANK
78.8
3.00 ± 0.12
DNN
89.2
2.50
FACTORIZED
89.0
2.61
PROTOTYPICAL
79.4
2.55
Table 2:
Main results for Task 1:
Commonsense
2: Wikipedia mining (quality scores out of 4).
Re-
HEAD in TAIL”.
which we refer to as the TEMPLATE + GRAMMAR
approach.
Like the full approach  these trans-
by a language model.
Returning to our exam-
(2016). Jastrzebski et al. (2018) introduce FAC-
TORIZED and PROTOTYPICAL models. The Fac-
tween each pair of embeddings. The Prototypi-
cal model is similar  but does not include the in-
pletion
Our experimental setup follows Li et al.
and invalid triples.
The valid triples are from
PMI weight via grid search over 90 points from
λ ∈ [0.5  5.]  using the Akaike information crite-
approach achieves a test set F1 score of 78.8  com-
of 1.65 for the Concatenation sentence generation
model and 1.55 for the Coherency Ranking model.
found lambda values of 1.20 and 0.95  respec-
Task 2:
Mining Wikipedia
To assess the
1177
method to evaluate these 3000 triples. Using the
true)  and 4 (Generally true). We tuned λ by mea-
est predicted score across λ ∈ {1  2  . . .   9  10}.
assigned a mean rating of 3.00 (λ = 4) with a
two categories of true and false  this disagreement
sidered common sense (e.g.
(vector bundle
HasProperty  manifold)).
This suggests that
Analysis:
Sentence Generation
In order to
reliably encode a relation’s true meaning.
N (/100)
F1 Score
GRAMMATICAL
75
79.1
UNGRAMMATICAL
25
66.7
CORRECT MEANING
91
77.6
WRONG MEANING
9
-
Quality
83
3.01
17
2.88
88
3.22
12
1.18
Table 3:
Test results examining the effect of sen-
4
Conclusion
1178
References
Akaike  pages 215–222. Springer.
Chaitanya Malaviya  Asli C¸ elikyilmaz  and Yejin
intelligence. Commun. ACM  58(9):92–103.
and Sebastian Riedel. 2018.
Convolutional 2d
knowledge graph embeddings.
In Thirty-Second
AAAI Conference on Artiﬁcial Intelligence.
ing. arXiv preprint arXiv:1810.04805.
base construction  pages 25–30. ACM.
sense mining as knowledge base completion?
a
study on the impact of novelty.
arXiv preprint
ples of Knowledge Representation and Reasoning.
1: Long Papers)  volume 1  pages 1445–1455.
journal  22(4):211–226.
Pushmeet Kohli  and James Allen. 2016.
A cor-
standing of commonsense stories.
Blog  1:8.
Itsumi Saito  Kyosuke Nishida  Hisako Asano  and
guage Learning  pages 141–150  Brussels  Belgium.
LREC  pages 3679–3686.
guage Processing  pages 1499–1509.
Trieu H. Trinh and Quoc V. Le. 2018.
A sim-
ple method for commonsense reasoning.
CoRR
Subham Biswas  and Minlie Huang. 2018.
Aug-
ence on Artiﬁcial Intelligence."
R010,1,KDD,"Parkinson’s disease (PD) is a progressive neurodegenerative disorder that leads to motor symptoms  including gait
impairment. The effectiveness of levodopa therapy  a common treatment for PD  can fluctuate  causing periods of
improved mobility ('on' state) and periods where symptoms re-emerge ('off' state). These fluctuations impact
gait speed and increase in severity as the disease progresses. This paper proposes a transformer-based method that
uses both Received Signal Strength Indicator (RSSI) and accelerometer data from wearable devices to enhance
indoor localization accuracy. A secondary goal is to determine if indoor localization  particularly in-home gait
speed features (like the time to walk between rooms)  can be used to identify motor fluctuations by detecting if a
person with PD is taking their levodopa medication or not. The method is evaluated using a real-world dataset
collected in a free-living setting  where movements are varied and unstructured. Twenty-four participants  living
in pairs (one with PD and one control)  resided in a sensor-equipped smart home for five days. The","Parkinson’s disease (PD) is a debilitating neurodegenerative condition that affects approximately 6 million individuals globally.
It manifests through various motor symptoms  including bradykinesia (slowness of movement)  rigidity  and gait impairment. A
common complication associated with levodopa  the primary medication for PD  is the emergence of motor fluctuations that are
linked to medication timing. Initially  patients experience a consistent and extended therapeutic effect when starting levodopa.
However  as the disease advances  a significant portion of patients begin to experience 'wearing off' of their medication before
the next scheduled dose  resulting in the reappearance of parkinsonian symptoms  such as slowed gait. These fluctuations in
symptoms negatively impact patients’ quality of life and often necessitate adjustments to their medication regimen. The severity
of motor symptoms can escalate to the point where they impede an individual’s ability to walk and move within their own home.
Consequently  individuals may be inclined to remain confined to a single room  and when they do move  they may require more time
to transition between rooms. These observations could potentially be used to identify periods when PD patients are experiencing
motor fluctuations related to their medication being in an ON or OFF state  thereby providing valuable information to both clinicians
and patients.
A sensitive and accurate ecologically-validated biomarker for PD progression is currently unavailable  which has contributed to
failures in clinical trials for neuroprotective therapies in PD. Gait parameters are sensitive to disease progression in unmedicated
early-stage PD and show promise as markers of disease progression  making measuring gait parameters potentially useful in clinical
trials of disease-modifying interventions. Clinical evaluations of PD are typically conducted in artificial clinic or laboratory settings
which only capture a limited view of an individual’s motor function. Continuous monitoring could capture symptom progression
including motor fluctuations  and sensitively quantify them over time.
While PD symptoms  including gait and balance parameters  can be measured continuously at home using wearable devices
containing inertial motor units (IMUs) or smartphones  this data does not show the context in which the measurements are taken.
Determining a person’s location within a home (indoor localization) could provide valuable contextual information for interpreting
PD symptoms. For instance  symptoms like freezing of gait and turning in gait vary depending on the environment  so knowing a
person’s location could help predict such symptoms or interpret their severity. Additionally  understanding how much time someone
spends alone or with others in a room is a step towards understanding their social participation  which impacts quality of life in
PD. Localization could also provide valuable information in the measurement of other behaviors such as non-motor symptoms like
urinary function (e.g.  how many times someone visits the toilet room overnight).
IoT-based platforms with sensors capturing various modalities of data  combined with machine learning  can be used for unobtrusive
and continuous indoor localization in home environments. Many of these techniques utilize radio-frequency signals  specifically the
Received Signal Strength Indication (RSSI)  emitted by wearables and measured at access points (AP) throughout a home. These
signals estimate the user’s position based on perceived signal strength  creating radio-map features for each room. To improve
localization accuracy  accelerometer data from wearable devices  along with RSSI  can be used to distinguish different activities
(e.g.  walking vs. standing). Since some activities are associated with specific rooms (e.g.  stirring a pan on the stove is likely to
occur in a kitchen)  accelerometer data can enhance RSSI’s ability to differentiate between adjacent rooms  an area where RSSI
alone may be insufficient.
The heterogeneity of PD  where symptoms and their severity vary between patients  poses a challenge for generalizing accelerometer
data across different individuals. Severe symptoms  such as tremors  can introduce bias and accumulated errors in accelerometer data
particularly when collected from wrist-worn devices  which are a common and well-accepted placement location. Naively combining
accelerometer data with RSSI may degrade indoor localization performance due to varying tremor levels in the acceleration signal.
This work makes two primary contributions to address these challenges.
(1) We detail the use of RSSI  augmented by accelerometer data  to achieve room-level localization. Our proposed network
intelligently selects accelerometer features that can enhance RSSI performance in indoor localization. To rigorously assess our
method  we utilize a free-living dataset (where individuals live without external intervention) developed by our group  encompassing
diverse and unstructured movements as expected in real-world scenarios. Evaluation on this dataset  including individuals with and
without PD  demonstrates that our network outperforms other methods across all cross-validation categories.
(2) We demonstrate how accurate room-level localization predictions can be transformed into in-home gait speed biomarkers (e.g.
number of room-to-room transitions  room-to-room transition duration). These biomarkers can effectively classify the OFF or ON
medication state of a PD patient from this pilot study data.
2 Related Work
Extensive research has utilized home-based passive sensing systems to evaluate how the activities and behavior of individuals with
neurological conditions  primarily cognitive dysfunction  change over time. However  there is limited work assessing room use in
the home setting in people with Parkinson’s.
Gait quantification using wearables or smartphones is an area where a significant amount of work has been done. Cameras can
also detect parkinsonian gait and some gait features  including step length and average walking speed. Time-of-flight devices
which measure distances between the subject and the camera  have been used to assess medication adherence through gait analysis.
From free-living data  one approach to gait and room use evaluation in home settings is by emitting and detecting radio waves to
non-invasively track movement. Gait analysis using radio wave technology shows promise to track disease progression  severity  and
medication response. However  this approach cannot identify who is doing the movement and also suffers from technical issues
when the radio waves are occluded by another object. Much of the work done so far using video to track PD symptoms has focused
on the performance of structured clinical rating scales during telemedicine consultations as opposed to naturalistic behavior  and
there have been some privacy concerns around the use of video data at home.
RSSI data from wearable devices is a type of data with fewer privacy concerns; it can be measured continuously and unobtrusively
over long periods to capture real-world function and behavior in a privacy-friendly way. In indoor localization  fingerprinting using
RSSI is the typical technique used to estimate the wearable (user) location by using signal strength data representing a coarse and
noisy estimate of the distance from the wearable to the access point. RSSI signals are not stable; they fluctuate randomly due to
shadowing  fading  and multi-path effects. However  many techniques have been proposed in recent years to tackle these fluctuations
and indirectly improve localization accuracy. Some works utilize deep neural networks (DNN) to generate coarse positioning
estimates from RSSI signals  which are then refined by a hidden Markov model (HMM) to produce a final location estimate. Other
works try to utilize a time series of RSSI data and exploit the temporal connections within each access point to estimate room-level
position. A CNN is used to build localization models to further leverage the temporal dependencies across time-series readings.
It has been suggested that we cannot rely on RSSI alone for indoor localization in home environments for PD subjects due to
shadowing rooms with tight separation. Some researchers combine RSSI signals and inertial measurement unit (IMU) data to test
the viability of leveraging other sensors in aiding the positioning system to produce a more accurate location estimate. Classic
machine learning approaches such as Random Forest (RF)  Artificial Neural Network (ANN)  and k-Nearest Neighbor (k-NN) are
tested  and the result shows that the RF outperforms other methods in tracking a person in indoor environments. Others combine
smartphone IMU sensor data and Wi-Fi-received signal strength indication (RSSI) measurements to estimate the exact location (in
Euclidean position X  Y) of a person in indoor environments. The proposed sensor fusion framework uses location fingerprinting in
combination with a pedestrian dead reckoning (PDR) algorithm to reduce positioning errors.
Looking at this multi-modality classification/regression problem from a time series perspective  there has been a lot of exploration
in tackling a problem where each modality can be categorized as multivariate time series data. LSTM and attention layers are
often used in parallel to directly transform raw multivariate time series data into a low-dimensional feature representation for each
modality. Later  various processes are done to further extract correlations across modalities through the use of various layers (e.g.
concatenation  CNN layer  transformer  self-attention). Our work is inspired by prior research where we only utilize accelerometer
2
data to enrich the RSSI  instead of utilizing all IMU sensors  in order to reduce battery consumption. In addition  unlike previous
work that stops at predicting room locations  we go a step further and use room-to-room transition behaviors as features for a binary
classifier predicting whether people with PD are taking their medications or withholding them.
3 Cohort and Dataset
**Dataset:** This dataset was collected using wristband wearable sensors  one on each wrist of all participants  containing tri-axial
accelerometers and 10 Access Points (APs) placed throughout the residential home  each measuring the RSSI. The wearable devices
wirelessly transmit data using the Bluetooth Low Energy (BLE) standard  which can be received by the 10 APs. Each AP records the
transmitted packets from the wearable sensor  which contains the accelerometer readings sampled at 30Hz  with each AP recording
RSSI values sampled at 5 Hz.
The dataset contains 12 spousal/parent-child/friend-friend pairs (24 participants in total) living freely in a smart home for five days.
Each pair consists of one person with PD and one healthy control volunteer (HC). This pairing was chosen to enable PD vs. HC
comparison  for safety reasons  and also to increase the naturalistic social behavior (particularly amongst the spousal pairs who
already lived together). From the 24 participants  five females and seven males have PD. The average age of the participants is 60.25
(PD 61.25  Control 59.25)  and the average time since PD diagnosis for the person with PD is 11.3 years (range 0.5-19).
To measure the accuracy of the machine learning models  wall-mounted cameras are installed on the ground floor of the house
which capture red-green-blue (RGB) and depth data 2-3 hours daily (during daylight hours at times when participants were at home).
The videos were then manually annotated to the nearest millisecond to provide localization labels. Multiple human labelers used
software called ELAN to watch up to 4 simultaneously-captured video files at a time. The resulting labeled data recorded the kitchen
hallway  dining room  living room  stairs  and porch. The duration of labeled data recorded by the cameras for PD and HC is 72.84
and 75.31 hours  respectively  which provides a relatively balanced label set for our room-level classification. Finally  to evaluate
the ON/OFF medication state  participants with PD were asked to withhold their dopaminergic medications so that they were in
the practically-defined OFF medications state for a temporary period of several hours during the study. Withholding medications
removes their mitigation on symptoms  leading to mobility deterioration  which can include slowing of gait.
**Data pre-processing for indoor localization:** The data from the two wearable sensors worn by each participant were combined at
each time point  based on their modality  i.e.  twenty RSSI values (corresponding to 10 APs for each of the two wearable sensors)
and accelerometry traces in six spatial directions (corresponding to the three spatial directions (x  y  z) for each wearable) were
recorded at each time point. The accelerometer data is resampled to 5Hz to synchronize the data with RSSI values. With a 5-second
time window and a 5Hz sampling rate  each RSSI data sample has an input of size (25 x 20)  and accelerometer data has an input of
size (25 x 6). Imputation for missing values  specifically for RSSI data  is applied by replacing the missing values with a value that is
not possible normally (i.e.  -120dB). Missing values exist in RSSI data whenever the wearable is out of range of an AP. Finally  all
time-series measurements by the modalities are normalized.
**Data pre-processing for medication state:** Our main focus is for our neural network to continuously produce room predictions
which are then transformed into in-home gait speed features  particularly for persons with PD. We hypothesize that during their
OFF medication state  the deterioration in mobility of a person with PD is exhibited by how they transition between rooms. These
features include ’Room-to-room Transition Duration’ and the ’Number of Transitions’ between two rooms. ’Number of Transitions’
represents how active PD subjects are within a certain period of time  while ’Room-to-room Transition Duration’ may provide
insight into how severe their disease is by the speed with which they navigate their home environment. With the layout of the house
where participants stayed  the hallway is used as a hub connecting all other rooms labeled  and ’Room-to-room Transition’ shows
the transition duration (in seconds) between two rooms connected by the hallway. The transition between (1) kitchen and living
room  (2) kitchen and dining room  and (3) dining room and living room are chosen as the features due to their commonality across
all participants. For these features  we limit the transition time duration (i.e.  the time spent in the hallway) to 60 seconds to exclude
transitions likely to be prolonged and thus may not be representative of the person’s mobility.
These in-home gait speed features are produced by an indoor-localization model by feeding RSSI signals and accelerometer data
from 12 PD participants from 6 a.m. to 10 p.m. daily  which are aggregated into 4-hour windows. From this  each PD participant
will have 20 data samples (four data samples for each of the five days)  each of which contains six features (three for the mean of
room-to-room transition duration and three for the number of room-to-room transitions). There is only one 4-hour window during
which the person with PD is OFF medications. These samples are then used to train a binary classifier determining whether a person
with PD is ON or OFF their medications.
For a baseline comparison to the in-home gait speed features  demographic features which include age  gender  years of PD  and
MDS-UPDRS III score (the gold-standard clinical rating scale score used in clinical trials to measure motor disease severity in
PD) are chosen. Two MDS-UPDRS III scores are assigned for each PD participant; one is assigned when a person with PD is ON
medications  and the other one is assigned when a person with PD is OFF medications. For each in-home gait speed feature data
sample  there will be a corresponding demographic feature data sample that is used to train a different binary classifier to predict
whether a person with PD is ON or OFF medications.
**Ethical approval:** Full approval from the NHS Wales Research Ethics Committee was granted on December 17  2019  and
Health Research Authority and Health and Care Research Wales approval was confirmed on January 14  2020; the research was
3
conducted in accord with the Helsinki Declaration of 1975; written informed consent was gained from all study participants. In
order to protect participant privacy  supporting data is not shared openly. It will be made available to bona fide researchers subject to
a data access agreement.
4 Methodologies and Framework
We introduce Multihead Dual Convolutional Self Attention (MDCSA)  a deep neural network that utilizes dual modalities for indoor
localization in home environments. The network addresses two challenges that arise from multimodality and time-series data:
(1) Capturing multivariate features and filtering multimodal noises. RSSI signals  which are measured at multiple access points
within a home received from wearable communication  have been widely used for indoor localization  typically using a fingerprinting
technique that produces a ground truth radio map of a home. Naturally  the wearable also produces acceleration measurements which
can be used to identify typical activities performed in a specific room  and thus we can explore if accelerometer data will enrich
the RSSI signals  in particular to help distinguish adjacent rooms  which RSSI-only systems typically struggle with. If it will  how
can we incorporate these extra features (and modalities) into the existing features for accurate room predictions  particularly in the
context of PD where the acceleration signal may be significantly impacted by the disease itself?
(2) Modeling local and global temporal dynamics. The true correlations between inputs both intra-modality (i.e.  RSSI signal among
access points) and inter-modality (i.e.  RSSI signal against accelerometer fluctuation) are dynamic. These dynamics can affect one
another within a local context (e.g.  cyclical patterns) or across long-term relationships. Can we capture local and global relationships
across different modalities?
The MDCSA architecture addresses the aforementioned challenges through a series of neural network layers  which are described in
the following sections.
4.1 Modality Positional Embedding
Due to different data dimensionality between RSSI and accelerometer  coupled with the missing temporal information  a linear
layer with a positional encoding is added to transform both RSSI and accelerometer data into their respective embeddings. Suppose
we have a collection of RSSI signals xr= [xr
1  xr
2  ...  xr
T]∈RT×rand accelerometer data xa= [xa
1  xa
2  ...  xa
T]∈RT×awithin
Ttime units  where xr
t= [xr
t1  xr
t2  ...  xr
tr]represents RSSI signals from raccess points  and xa
t= [xa
t1  xa
t2  ...  xa
ta]represents
accelerometer data from aspatial directions at time twitht < T . Given feature vectors xt= [xr
t  xa
t]withu∈ {r  a}representing
RSSI or accelerometer data at time t  andt < T representing the time index  a positional embedding hu
tfor RSSI or accelerometer
can be obtained by:
hu
t= (Wuxu
t+bu) +τt (1)
where Wu∈Ru×dandbu∈Rdare the weight and bias to learn  dis the embedding dimension  and τt∈Rdis the corresponding
position encoding at time t.
4.2 Locality Enhancement with Self-Attention
Since it is time-series data  the importance of an RSSI or accelerometer value at each point in time can be identified in relation to its
surrounding values - such as cyclical patterns  trends  or fluctuations. Utilizing historical context that can capture local patterns on
top of point-wise values  performance improvements in attention-based architectures can be achieved. One straightforward option is
to utilize a recurrent neural network such as a long-short term memory (LSTM) approach. However  in LSTM layers  the local
context is summarized based on the previous context and the current input. Two similar patterns separated by a long period of time
might have different contexts if they are processed by the LSTM layers. We utilize a combination of causal convolution layers and
self-attention layers  which we name Dual Convolutional Self-Attention (DCSA). The DCSA takes in a primary input ˆx1∈RN×d
and a secondary input ˆx2∈RN×dand yields:
DCSA (ˆx1 ˆx2) =GRN (Norm (ϕ(ˆx1) + ˆx1)  Norm (ϕ(ˆx2) + ˆx2)) (2)
with
ϕ(ˆx) =SA(Φk(ˆx)WQ Φk(ˆx)WK Φk(ˆx)WV) (3)
where GRN (.)is the Gated Residual Network to integrate dual inputs into one integrated embedding  Norm (.)is a standard layer
normalization  SA(.)is a scaled dot-product self-attention  Φk(.)is a 1D-convolutional layer with a kernel size {1  k}and a stride
of 1 WK∈Rd×d  WQ∈Rd×d  WV∈Rd×dare weights for keys  queries  and values of the self-attention layer  and dis the
embedding dimension. Note that all weights for GRN are shared across each time step t.
4
4.3 Multihead Dual Convolutional Self-Attention
Our approach employs a self-attention mechanism to capture global dependencies across time steps. It is embedded as part of the
DCSA architecture. Inspired by utilizing multihead self-attention  we utilize our DCSA with various kernel lengths with the same
aim: allowing asymmetric long-term learning. The multihead DCSA takes in two inputs ˆx1 ˆx2∈RN×dand yields:
MDCSA k1 ... k n(ˆx1 ˆx2) = Ξ n(ϕk1 ... k n(ˆx1 ˆx2)) (4)
ϕki(ˆx1 ˆx2) =SA(Φki(ˆx1)WQ Φki(ˆx2)WK Φki(ˆx1 ˆx2)WV) (5)
where Φki(.)is a 1D-convolutional layer with a kernel size {1  ki}and a stride ki WK∈Rd×d  WQ∈Rd×d  WV∈Rd×dare
weights for keys  queries  and values of the self-attention layer  and Ξn(.)concatenates the output of each DCSA ki(.)in temporal
order. For regularization  a normalization layer followed by a dropout layer is added after Equation 4.
Following the modality positional embedding layer in subsection 4.1  the positional embeddings of RSSI hr= [hr
1  ...  hr
T]and
accelerometer ha= [ha
1  ...  ha
T]  produced by Eq. 1  are then fed to an MDCSA layer with various kernel sizes [k1  ...  k n]:
h=MDCSA k1 ... k n(hr  ha) (6)
to yield h= [h1  ...  h T]withht∈Rdandt < T .
4.4 Final Layer and Loss Calculation
We apply two different layers to produce two different outputs during training. The room-level predictions are produced via a single
conditional random field (CRF) layer in combination with a linear layer applied to the output of Eq. 7 to produce the final predictions
as:
ˆyt=CRF (ϕ(ht)) (7)
q′(ht) =Wpht+bp (8)
where Wp∈Rd×mandbp∈Rmare the weight and bias to learn  mis the number of room locations  and h= [h1  ...  h T]∈RT×d
is the refined embedding produced by Eq. 7. Even though the transformer can take into account neighbor information before
generating the refined embedding at time step t  its decision is independent; it does not take into account the actual decision made by
other refined embeddings t. We use a CRF layer to cover just that  i.e.  to maximize the probability of the refined embeddings of all
time steps  so it can better model cases where refined embeddings closest to one another must be compatible (i.e.  minimizing the
possibility for impossible room transitions). When finding the best sequence of room location ˆyt  the Viterbi Algorithm is used as a
standard for the CRF layer.
For the second layer  we choose a particular room as a reference and perform a binary classification at each time step t. The binary
classification is produced via a linear layer applied to the refined embedding htas:
ˆft=Wfht+bf (9)
where Wf∈Rd×1andbf∈Rare the weight and bias to learn  and ˆf= [ˆf1  ... ˆfT]∈RTis the target probabilities for the
referenced room within time window T. The reason to perform a binary classification against a particular room is because of our
interest in improving the accuracy in predicting that room. In our application  the room of our choice is the hallway  where it will be
used as a hub connecting any other room.
**Loss Functions:** During the training process  the MDCSA network produces two kinds of outputs. Emission outputs (outputs
produced by Equation 9 prior to prediction outputs) ˆe= [ϕ(h1)  ...  ϕ (hT)]are trained to generate the likelihood estimate of room
predictions  while the binary classification output ˆf= [ˆf1  ... ˆfT]is used to train the probability estimate of a particular room. The
final loss function can be formulated as a combination of both likelihood and binary cross-entropy loss functions described as:
L(ˆe  y ˆf  f) =LLL(ˆe  y) +TX
t=1LBCE(ˆft  ft) (10)
LLL(ˆe  y) =TX
i=0P(ϕ(hi))qT
i(yi|yi−1)−TX
i=0P(ϕ(hi))[qT
i(yi|yi−1)] (11)
5
LBCE(ˆf  f) =−1
TTX
t=0ftlog(ˆft) + (1 −ft) log(1 −ˆft) (12)
where LLL(.)represents the negative log-likelihood and LBCE(.)denotes the binary cross-entropy  y= [y1  ...  y T]∈RTis the
actual room locations  and f= [f1  ...  f T]∈RTis the binary value whether at time tthe room is the referenced room or not.
P(yi|yi−1)denotes the conditional probability  and P(yt|yt−1)denotes the transition matrix cost of having transitioned from yt−1
toyt.
5 Experiments and Results
We compare our proposed network  MDCSA1 4 7 (MDCSA with 3 kernels of size 1  4  and 7)  with:
- Random Forest (RF) as a baseline technique  which has been shown to work well for indoor localization. - A modified transformer
encoder in combination with a CRF layer representing a model with the capability to capture global dependency and enforce
dependencies in temporal aspects. - A state-of-the-art model for multimodal and multivariate time series with a transformer encoder
to learn asymmetric correlations across modalities. - An alternative to the previous model  representing it with a GRN layer replacing
the context aggregation layer and a CRF layer added as the last layer. - MDCSA1 4 7 4APS  as an ablation study  with our proposed
network (i.e.  MDCSA1 4 7) using 4 access points for the RSSI (instead of 10 access points) and accelerometer data (ACCL) as its
input features. - MDCSA1 4 7 RSSI  as an ablation study  with our proposed network using only RSSI  without ACCL  as its input
features. - MDCSA1 4 7 4APS RSSI  as an ablation study  with our proposed network using only 4 access points for the RSSI as its
input features.
For RF  all the time series features of RSSI and accelerometry are flattened and merged into one feature vector for room-level
localization. For the modified transformer encoder  at each time step t  RSSI xr
tand accelerometer xa
tfeatures are combined via a
linear layer before they are processed by the networks. A grid search on the parameters of each network is performed to find the best
parameter for each model. The parameters to tune are the embedding dimension din 128  256  the number of epochs in 200  300
and the learning rate in 0.01  0.0001. The dropout rate is set to 0.15  and a specific optimizer in combination with a Look-Ahead
algorithm is used for the training with early stopping using the validation performance. For the RF  we perform a cross-validated
parameter search for the number of trees (200  250)  the minimum number of samples in a leaf node (1  5)  and whether a warm start
is needed. The Gini impurity is used to measure splits.
**Evaluation Metrics:** We are interested in developing a system to monitor PD motor symptoms in home environments. For
example  we will consider if there is any significant difference in the performance of the system when it is trained with PD data
compared to being trained with healthy control (HC) data. We tailored our training procedure to test our hypothesis by performing
variations of cross-validation. Apart from training our models on all HC subjects (ALL-HC)  we also perform four different kinds of
cross-validation: 1) We train our models on one PD subject (LOO-PD)  2) We train our models on one HC subject (LOO-HC)  3) We
take one HC subject and use only roughly four minutes’ worth of data to train our models (4m-HC)  4) We take one PD subject and
use only roughly four minutes’ worth of data to train our models (4m-PD). For all of our experiments  we test our trained models on
all PD subjects (excluding the one used as training data for LOO-PD and 4m-PD). For room-level localization accuracy  we use
precision and weighted F1-score  all averaged and standard deviated across the test folds.
To showcase the importance of in-home gait speed features in differentiating the medication state of a person with PD  we first
compare how accurate the ’Room-to-room Transition’ duration produced by each network is to the ground truth (i.e.  annotated
location). We hypothesize that the more accurate the transition is compared to the ground truth  the better mobility features are for
medication state classification. For the medication state classification  we then compare two different groups of features with two
simple binary classifiers: 1) the baseline demographic features (see Section 3)  and 2) the normalized in-home gait speed features.
The metric we use for ON/OFF medication state evaluation is the weighted F1-Score and AUROC  which are averaged and standard
deviated across the test folds.
5.1 Experimental Results
**Room-level Accuracy:** The first part of Table 1 compares the performance of the MDCSA network and other approaches for
room-level classification. For room-level classification  the MDCSA network outperforms other networks and RF with a minimum
improvement of 1.3% for the F1-score over the second-best network in each cross-validation type  with the exception of the ALL-HC
validation. The improvement is more significant in the 4m-HC and 4m-PD validations  when the training data are limited  with an
average improvement of almost 9% for the F1-score over the alternative to the state-of-the-art model.
The LOO-HC and LOO-PD validations show that a model that has the ability to capture the temporal dynamics across time steps will
perform better than a standard baseline technique such as a Random Forest. The modified transformer encoder and the state-of-the-art
model perform better in those two validations due to their ability to capture asynchronous relations across modalities. However
when the training data becomes limited  as in 4m-HC and 4m-PD validations  having extra capabilities is necessary to further
extract temporal information and correlations. Due to being a vanilla transformer requiring a considerable amount of training
data  the modified transformer encoder performs worst in these two validations. The state-of-the-art model performs quite well
6
due to its ability to capture local context via LSTM for each modality. However  in general  its performance suffers in both the
LOO-PD and 4m-PD validations as the accelerometer data (and modality) may be erratic due to PD and should be excluded at
times from contributing to room classification. The MDCSA network has all the capabilities that the state-of-the-art model has
with an improvement in suppressing the accelerometer modality when needed via the GRN layer embedded in DCSA. Suppressing
the noisy modality seems to have a strong impact on maintaining the performance of the network when the training data is limited.
This is validated by how the alternative to the state-of-the-art model (i.e.  the state-of-the-art model with added GRN and CRF
layers) outperforms the standard state-of-the-art model by an average of 2.2% for the F1-score in the 4m-HC and 4m-PD validations.
It is further confirmed by MDCSA1 4 7 4APS against MDCSA1 4 7 4APS RSSI  with the latter model  which does not include
the accelerometer data  outperforming the former for the F1-score by an average of 1.6% in the last three cross-validations. It is
worth pointing out that the MDCSA1 4 7 4APS RSSI model performed the best in the 4m-PD validation. However  the omission of
accelerometer data affects the model’s ability to differentiate rooms that are more likely to have active movement (i.e.  hall) than the
rooms that are not (i.e.  living room). It can be seen from Table 2 that the MDCSA1 4 7 4APS RSSI model has low performance in
predicting the hallway compared to the full model of MDCSA1 4 7. As a consequence  the MDCSA1 4 7 4APS RSSI model cannot
produce in-home gait speed features as
accurately  as shown in Table 3.
**Room-to-room Transition and Medication Accuracy:** We hypothesize that during their OFF medication state  the deterioration
in mobility of a person with PD is exhibited by how they transition between rooms. To test this hypothesis  a Wilcoxon signed-rank
test was used on the annotated data from PD participants undertaking each of the three individual transitions between rooms whilst
ON (taking) and OFF (withholding) medications to assess whether the mean transition duration ON medications was statistically
significantly shorter than the mean transition duration for the same transition OFF medications for all transitions studied (see Table
4). From this result  we argue that the mean transition duration obtained by each model from Table 1 that is close to the ground truth
can capture what the ground truth captures. As mentioned in Section 3  this transition duration for each model is generated by the
model continuously performing room-level localization  focusing on the time a person is predicted to spend in a hallway between
rooms. We show  in Table 3  that the mean transition duration for all transitions studied produced by the MDCSA1 4 7 model is the
closest to the ground truth  improving over the second best by around 1.25 seconds across all hall transitions and validations.
The second part of Table 1 shows the performance of all our networks for medication state classification. The demographic
features can be used as a baseline for each type of validation. The MDCSA network  with the exception of the ALL-HC validation
outperforms any other network by a significant margin for the AUROC score. By using in-home gait speed features produced by
the MDCSA network  a minimum of 15% improvement over the baseline demographic features can be obtained  with the biggest
gain obtained in the 4m-PD validation data. In the 4m-PD validation data  RF  TENER  and DTML could not manage to provide
any prediction due to their inability to capture (partly) hall transitions. Furthermore  TENER has shown its inability to provide any
medication state prediction from the 4m-HC data validations. It can be validated by Table 3 when TENER failed to capture any
transitions between the dining room and living room across all periods that have ground truths. MDCSA networks can provide
medication state prediction and maintain their performance across all cross-validations thanks to the addition of Eq. 13 in the loss
function.
**Limitations and future research:** One limitation of this study is the relatively small sample size (which was planned as this is
an exploratory pilot study). We believe our sample size is ample to show proof of concept. This is also the first such work with
unobtrusive ground truth validation from embedded cameras. Future work should validate our approach further on a large cohort
of people with PD and consider stratifying for sub-groups within PD (e.g.  akinetic-rigid or tremor-dominant phenotypes)  which
would also increase the generalizability of the results to the wider population. Future work in this matter could also include the
construction of a semi-synthetic dataset based on collected data to facilitate a parallel and large-scale evaluation.
This smart home’s layout and parameters remain constant for all the participants  and we acknowledge that the transfer of this deep
learning model to other varied home settings may introduce variations in localization accuracy. For future ecological validation and
based on our current results  we anticipate the need for pre-training (e.g.  a brief walkaround which is labeled) for each home  and
also suggest that some small amount of ground-truth data will need to be collected (e.g.  researcher prompting of study participants to
undertake scripted activities such as moving from room to room) to fully validate the performance of our approach in other settings.","reveals that accurate room-level localization  when converted into in-home gait speed features  can accurately
predict whether a PD participant is taking their medication or not.
1",that the proposed network surpasses other,"We have presented the MDCSA model  a new deep learning approach for indoor localization utilizing RSSI and wrist-worn
accelerometer data. The evaluation on our unique real-world free-living pilot dataset  which includes subjects with and without PD
shows that MDCSA achieves state-of-the-art accuracy for indoor localization. The availability of accelerometer data does indeed
enrich the RSSI features  which  in turn  improves the accuracy of indoor localization.
Accurate room localization using these data modalities has a wide range of potential applications within healthcare. This could
include tracking of gait speed during rehabilitation from orthopedic surgery  monitoring wandering behavior in dementia  or
triggering an alert for a possible fall (and long lie on the floor) if someone is in one room for an unusual length of time. Furthermore
accurate room use and room-to-room transfer statistics could be used in occupational settings  e.g.  to check factory worker location.
7
Table 1: Room-level and medication state accuracy of all models. Standard deviation is shown in (.)  the best performer is bold
while the second best is italicized. Note that our proposed model is the one named MDCSA1 4 7
!Training ModelRoom-Level Localisation Medication State
Precision F1-Score F1-Score AUROC
ALL-HCRF 95.00 95.20 56.67 (17.32) 84.55 (12.06)
TENER 94.60 94.80 47.08 (16.35) 67.74 (10.82)
DTML 94.80 94.90 50.33 (13.06) 75.97 (9.12)
Alt DTML 94.80 95.00 47.25 (5.50) 75.63 (4.49)
MDCSA1 4 7 4APS 92.22 92.22 53.47 (12.63) 73.48 (6.18)
MDCSA1 4 7 RSSI 94.70 94.90 51.14 (11.95) 68.33 (18.49)
MDCSA1 4 7 4APS RSSI 93.30 93.10 64.52 (11.44) 81.84 (6.30)
MDCSA1 4 7 94.90 95.10 64.13 (6.05) 80.95 (10.71)
Demographic Features 49.74 (15.60) 65.66 (18.54)
LOO-HCRF 89.67 (1.85) 88.95 (2.61) 54.74 (11.46) 69.24 (17.77)
TENER 90.35 (1.87) 89.75 (2.24) 51.76 (14.37) 70.80 (9.78)
DTML 90.51 (1.95) 89.82 (2.60) 55.34 (13.67) 73.77 (9.84)
Alt DTML 90.52 (2.17) 89.71 (2.83) 49.56 (17.26) 73.26 (10.65)
MDCSA1 4 7 4APS 88.01 (6.92) 88.08 (5.73) 59.52 (20.62) 74.35 (16.78)
MDCSA1 4 7 RSSI 90.26 (2.43) 89.48 (3.47) 58.84 (23.08) 76.10 (10.84)
MDCSA1 4 7 4APS RSSI 88.55 (6.67) 88.75 (5.50) 42.34 (13.11) 72.58 (6.77)
MDCSA1 4 7 91.39 (2.13) 91.06 (2.62) 55.50 (15.78) 83.98 (13.45)
Demographic Features 51.79 (15.40) 68.33 (18.43)
LOO-PDRF 86.89 (7.14) 84.71 (7.33) 43.28 (14.02) 62.63 (20.63)
TENER 86.91 (6.76) 86.18 (6.01) 36.04 (9.99) 60.03 (10.52)
DTML 87.13 (6.53) 86.31 (6.32) 43.98 (14.06) 66.93 (11.07)
Alt DTML 87.36 (6.30) 86.44 (6.63) 44.02 (16.89) 69.70 (12.04)
MDCSA1 4 7 4APS 86.44 (6.96) 85.93 (6.05) 47.26 (14.47) 72.62 (11.16)
MDCSA1 4 7 RSSI 87.61 (6.64) 87.21 (5.44) 45.71 (17.85) 67.76 (10.73)
MDCSA1 4 7 4APS RSSI 87.20 (7.17) 87.00 (6.12) 41.33 (17.72) 66.26 (12.11)
MDCSA1 4 7 88.04 (6.94) 87.82 (6.01) 49.99 (13.18) 81.08 (8.46)
Demographic Features 43.89 (14.43) 60.95 (25.16)
4m-HCRF 74.27 (8.99) 69.87 (7.21) 50.47 (12.63) 59.55 (12.38)
TENER 69.86 (18.68) 60.71 (24.94) N/A N/A
DTML 77.10 (9.89) 70.12 (14.26) 43.89 (11.60) 64.67 (12.88)
Alt DTML 78.79 (3.95) 71.44 (9.82) 47.49 (14.64) 65.16 (12.56)
MDCSA1 4 7 4APS 81.42 (6.95) 78.65 (7.59) 42.87 (17.34) 67.09 (7.42)
MDCSA1 4 7 RSSI 81.69 (6.85) 77.12 (8.46) 49.95 (17.35) 69.71 (11.55)
MDCSA1 4 7 4APS RSSI 82.80 (7.82) 79.37 (8.98) 43.57 (23.87) 65.46 (15.78)
MDCSA1 4 7 83.32 (6.65) 80.24 (6.85) 55.43 (10.48) 78.24 (6.67)
Demographic Features 32.87 (13.81) 53.68 (13.86)
4m-PDRF 71.00 (9.67) 65.89 (11.96) N/A N/A
TENER 65.30 (23.25) 58.57 (27.19) N/A N/A
DTML 70.35 (14.17) 64.00 (17.88) N/A N/A
Alt DTML 74.43 (9.59) 67.55 (14.50) N/A N/A
MDCSA1 4 7 4APS 81.02 (8.48) 76.85 (10.94) 49.97 (7.80) 69.10 (7.64)
MDCSA1 4 7 RSSI 77.47 (12.54) 73.99 (13.00) 41.79 (16.82) 67.37 (16.86)
MDCSA1 4 7 4APS RSSI 83.01 (6.42) 79.77 (7.05) 41.18 (12.43) 63.16 (11.06)
MDCSA1 4 7 83.30 (6.73) 76.77 (13.19) 48.61 (12.03) 76.39 (12.23)
Demographic Features 36.69 (18.15) 50.53 (15.60)
In naturalistic settings  in-home mobility can be measured through the use of indoor localization models. We have shown  using
room transition duration results  that our PD cohort takes longer on average to perform a room transition when they withhold
medications. With accurate in-home gait speed features  a classifier model can then differentiate accurately if a person with PD is in
an ON or OFF medication state. Such changes show the promise of these localization outputs to detect the dopamine-related gait
fluctuations in PD that impact patients’ quality of life and are important in clinical decision-making. We have also demonstrated
that our indoor localization system provides precise in-home gait speed features in PD with a minimal average offset to the ground
8
Table 2: Hallway prediction on limited training data.
Training Model Precision F1-Score
4m-HCMDCSA 4APS RSSI 62.32 (19.72) 58.99 (23.87)
MDCSA 4APS 68.07 (23.22) 60.01 (26.24)
MDCSA 71.25 (21.92) 68.95 (17.89)
4m-PDMDCSA 4APS RSSI 58.59 (23.60) 57.68 (24.27)
MDCSA 4APS 62.36 (18.98) 57.76 (20.07)
MDCSA 70.47 (14.10) 64.64 (21.38)
Table 3: Room-to-room transition accuracy (in seconds) of all models compared to the ground truth. Standard deviation is shown in
(.)  the best performer is bold  while the second best is italicized. A model that fails to capture a transition between particular rooms
within a period that has the ground truth is assigned ’N/A’ score.
!Data Models Kitch-Livin Kitch-Dinin Dinin-Livin
Ground Truth 18.71 (18.52) 14.65 (6.03) 10.64 (11.99)
ALL-HCRF 16.18 (12.08) 14.58 (10.22) 10.19 (9.46)
TENER 15.58 (8.75) 16.30 (12.94) 12.01 (13.01)
Alt DTML 15.27 (7.51) 13.40 (6.43) 10.84 (10.81)
MDCSA 17.70 (16.17) 14.94 (9.71) 10.76 (9.59)
LOO-HCRF 17.52 (16.97) 11.93 (10.08) 9.23 (13.69)
TENER 14.62 (16.37) 9.58 (9.16) 7.21 (10.61)
Alt DTML 16.30 (17.78) 14.01 (8.08) 10.37 (12.44)
MDCSA 17.70 (17.42) 14.34 (9.48) 11.07 (13.60)
LOO-PDRF 14.49 (15.28) 11.67 (11.68) 8.65 (13.06)
TENER 13.42 (14.88) 10.87 (10.37) 6.95 (10.28)
Alt DTML 16.98 (15.15) 15.26 (8.85) 9.99 (13.03)
MDCSA 16.42 (14.04) 14.48 (9.81) 10.77 (14.18)
4m-HCRF 14.22 (18.03) 11.38 (15.46) 13.43 (18.87)
TENER 10.75 (15.67) 8.59 (14.39) N/A
Alt DTML 16.89 (18.07) 14.68 (13.57) 9.31 (15.70)
MDCSA 18.15 (19.12) 15.32 (14.93) 11.89 (17.55)
4m-PDRF 11.52 (16.07) 8.73 (12.90) N/A
TENER 8.75 (14.89) N/A N/A
Alt DTML 14.75 (13.79) 13.47 (17.66) N/A
MDCSA 17.96 (19.17) 14.74 (10.83) 10.16 (14.03)
truth. The network also outperforms other models in the production of in-home gait speed features  which is used to differentiate the
medication state of a person with PD.
Acknowledgments
We are very grateful to the study participants for giving so much time and effort to this research. We acknowledge the local
Movement Disorders Health Integration Team (Patient and Public Involvement Group) for their assistance at each study design step.
This work was supported by various grants and institutions.
Statistical Significance Test
It could be argued that all the localization models compared in Table 1 might not be statistically different due to the fairly high
standard deviation across all types of cross-validations  which is caused by the relatively small number of participants. In order to
compare multiple models over cross-validation sets and show the statistical significance of our proposed model  we perform the
Friedman test to first reject the null hypothesis. We then performed a pairwise statistical comparison: the Wilcoxon signed-rank test
with Holm’s alpha correction.
9
Table 4: PD participant room transition duration with ON and OFF medications comparison using Wilcoxon signed rank tests.
OFF transitions Mean transition duration ON transitions Mean transition duration W z
Kitchen-Living OFF 17.2 sec Kitchen-Living ON 14.0 sec 75.0 2.824
Dining-Kitchen OFF 12.9 sec Dining-Kitchen ON 9.2 sec 76.0 2.903
Dining-Living OFF 10.4 sec Dining-Living ON 9.0 sec 64.0 1.961
10
Parkinson’sdisease(PD)isaprogressiveneurodegenerativedisorderthatleadstomotorsymptoms includinggait
impairment. Theeffectivenessoflevodopatherapy acommontreatmentforPD canfluctuate causingperiodsof
improvedmobility('on'state)andperiodswheresymptomsre-emerge('off'state). Thesefluctuationsimpact
gaitspeedandincreaseinseverityasthediseaseprogresses. Thispaperproposesatransformer-basedmethodthat
usesbothReceivedSignalStrengthIndicator(RSSI)andaccelerometerdatafromwearabledevicestoenhance
indoorlocalizationaccuracy. Asecondarygoalistodetermineifindoorlocalization particularlyin-homegait
speedfeatures(likethetimetowalkbetweenrooms) canbeusedtoidentifymotorfluctuationsbydetectingifa
personwithPDistakingtheirlevodopamedicationornot. Themethodisevaluatedusingareal-worlddataset
collectedinafree-livingsetting wheremovementsarevariedandunstructured. Twenty-fourparticipants living
inpairs(onewithPDandonecontrol) residedinasensor-equippedsmarthomeforfivedays. Theresultsshow
thattheproposednetworksurpassesothermethodsforindoorlocalization. Theevaluationofthesecondarygoal
revealsthataccurateroom-levellocalization whenconvertedintoin-homegaitspeedfeatures canaccurately
predictwhetheraPDparticipantistakingtheirmedicationornot.
Parkinson’sdisease(PD)isadebilitatingneurodegenerativeconditionthataffectsapproximately6millionindividualsglobally.
Itmanifeststhroughvariousmotorsymptoms includingbradykinesia(slownessofmovement) rigidity andgaitimpairment. A
commoncomplicationassociatedwithlevodopa theprimarymedicationforPD istheemergenceofmotorfluctuationsthatare
However asthediseaseadvances asignificantportionofpatientsbegintoexperience'wearingoff'oftheirmedicationbefore
symptomsnegativelyimpactpatients’qualityoflifeandoftennecessitateadjustmentstotheirmedicationregimen. Theseverity
ofmotorsymptomscanescalatetothepointwheretheyimpedeanindividual’sabilitytowalkandmovewithintheirownhome.
Consequently individualsmaybeinclinedtoremainconfinedtoasingleroom andwhentheydomove theymayrequiremoretime
totransitionbetweenrooms. TheseobservationscouldpotentiallybeusedtoidentifyperiodswhenPDpatientsareexperiencing
motorfluctuationsrelatedtotheirmedicationbeinginanONorOFFstate therebyprovidingvaluableinformationtobothclinicians
andpatients.
Asensitiveandaccurateecologically-validatedbiomarkerforPDprogressioniscurrentlyunavailable whichhascontributedto
failuresinclinicaltrialsforneuroprotectivetherapiesinPD.Gaitparametersaresensitivetodiseaseprogressioninunmedicated
early-stagePDandshowpromiseasmarkersofdiseaseprogression makingmeasuringgaitparameterspotentiallyusefulinclinical
trialsofdisease-modifyinginterventions. ClinicalevaluationsofPDaretypicallyconductedinartificialclinicorlaboratorysettings
whichonlycapturealimitedviewofanindividual’smotorfunction. Continuousmonitoringcouldcapturesymptomprogression
includingmotorfluctuations andsensitivelyquantifythemovertime.
containinginertialmotorunits(IMUs)orsmartphones thisdatadoesnotshowthecontextinwhichthemeasurementsaretaken.
Determiningaperson’slocationwithinahome(indoorlocalization)couldprovidevaluablecontextualinformationforinterpreting
PDsymptoms. Forinstance symptomslikefreezingofgaitandturningingaitvarydependingontheenvironment soknowinga
person’slocationcouldhelppredictsuchsymptomsorinterprettheirseverity. Additionally understandinghowmuchtimesomeone
spendsaloneorwithothersinaroomisasteptowardsunderstandingtheirsocialparticipation whichimpactsqualityoflifein
PD.Localizationcouldalsoprovidevaluableinformationinthemeasurementofotherbehaviorssuchasnon-motorsymptomslike
urinaryfunction(e.g. howmanytimessomeonevisitsthetoiletroomovernight).
IoT-basedplatformswithsensorscapturingvariousmodalitiesofdata combinedwithmachinelearning canbeusedforunobtrusive
andcontinuousindoorlocalizationinhomeenvironments. Manyofthesetechniquesutilizeradio-frequencysignals specificallythe
ReceivedSignalStrengthIndication(RSSI) emittedbywearablesandmeasuredataccesspoints(AP)throughoutahome. These
signalsestimatetheuser’spositionbasedonperceivedsignalstrength creatingradio-mapfeaturesforeachroom. Toimprove
localizationaccuracy accelerometerdatafromwearabledevices alongwithRSSI canbeusedtodistinguishdifferentactivities
(e.g. walkingvs. standing). Sincesomeactivitiesareassociatedwithspecificrooms(e.g. stirringapanonthestoveislikelyto
occurinakitchen) accelerometerdatacanenhanceRSSI’sabilitytodifferentiatebetweenadjacentrooms anareawhereRSSI
alonemaybeinsufficient.
TheheterogeneityofPD wheresymptomsandtheirseverityvarybetweenpatients posesachallengeforgeneralizingaccelerometer
dataacrossdifferentindividuals. Severesymptoms suchastremors canintroducebiasandaccumulatederrorsinaccelerometerdata
particularlywhencollectedfromwrist-worndevices whichareacommonandwell-acceptedplacementlocation. Naivelycombining
accelerometerdatawithRSSImaydegradeindoorlocalizationperformanceduetovaryingtremorlevelsintheaccelerationsignal.
Thisworkmakestwoprimarycontributionstoaddressthesechallenges.
intelligentlyselectsaccelerometerfeaturesthatcanenhanceRSSIperformanceinindoorlocalization. Torigorouslyassessour
method weutilizeafree-livingdataset(whereindividualslivewithoutexternalintervention)developedbyourgroup encompassing
diverseandunstructuredmovementsasexpectedinreal-worldscenarios. Evaluationonthisdataset includingindividualswithand
withoutPD demonstratesthatournetworkoutperformsothermethodsacrossallcross-validationcategories.
(2)Wedemonstratehowaccurateroom-levellocalizationpredictionscanbetransformedintoin-homegaitspeedbiomarkers(e.g.
numberofroom-to-roomtransitions room-to-roomtransitionduration). ThesebiomarkerscaneffectivelyclassifytheOFForON
medicationstateofaPDpatientfromthispilotstudydata.
2 RelatedWork
Extensiveresearchhasutilizedhome-basedpassivesensingsystemstoevaluatehowtheactivitiesandbehaviorofindividualswith
neurologicalconditions primarilycognitivedysfunction changeovertime. However thereislimitedworkassessingroomusein
thehomesettinginpeoplewithParkinson’s.
Gaitquantificationusingwearablesorsmartphonesisanareawhereasignificantamountofworkhasbeendone. Camerascan
whichmeasuredistancesbetweenthesubjectandthecamera havebeenusedtoassessmedicationadherencethroughgaitanalysis.
Fromfree-livingdata oneapproachtogaitandroomuseevaluationinhomesettingsisbyemittinganddetectingradiowavesto
non-invasivelytrackmovement. Gaitanalysisusingradiowavetechnologyshowspromisetotrackdiseaseprogression severity and
medicationresponse. However thisapproachcannotidentifywhoisdoingthemovementandalsosuffersfromtechnicalissues
whentheradiowavesareoccludedbyanotherobject. MuchoftheworkdonesofarusingvideototrackPDsymptomshasfocused
ontheperformanceofstructuredclinicalratingscalesduringtelemedicineconsultationsasopposedtonaturalisticbehavior and
therehavebeensomeprivacyconcernsaroundtheuseofvideodataathome.
RSSIdatafromwearabledevicesisatypeofdatawithfewerprivacyconcerns;itcanbemeasuredcontinuouslyandunobtrusively
overlongperiodstocapturereal-worldfunctionandbehaviorinaprivacy-friendlyway. Inindoorlocalization fingerprintingusing
RSSIisthetypicaltechniqueusedtoestimatethewearable(user)locationbyusingsignalstrengthdatarepresentingacoarseand
noisyestimateofthedistancefromthewearabletotheaccesspoint. RSSIsignalsarenotstable;theyfluctuaterandomlydueto
shadowing fading andmulti-patheffects. However manytechniqueshavebeenproposedinrecentyearstotacklethesefluctuations
estimatesfromRSSIsignals whicharethenrefinedbyahiddenMarkovmodel(HMM)toproduceafinallocationestimate. Other
workstrytoutilizeatimeseriesofRSSIdataandexploitthetemporalconnectionswithineachaccesspointtoestimateroom-level
position. ACNNisusedtobuildlocalizationmodelstofurtherleveragethetemporaldependenciesacrosstime-seriesreadings.
shadowingroomswithtightseparation. SomeresearcherscombineRSSIsignalsandinertialmeasurementunit(IMU)datatotest
theviabilityofleveragingothersensorsinaidingthepositioningsystemtoproduceamoreaccuratelocationestimate. Classic
machinelearningapproachessuchasRandomForest(RF) ArtificialNeuralNetwork(ANN) andk-NearestNeighbor(k-NN)are
tested andtheresultshowsthattheRFoutperformsothermethodsintrackingapersoninindoorenvironments. Otherscombine
smartphoneIMUsensordataandWi-Fi-receivedsignalstrengthindication(RSSI)measurementstoestimatetheexactlocation(in
EuclideanpositionX Y)ofapersoninindoorenvironments. Theproposedsensorfusionframeworkuseslocationfingerprintingin
combinationwithapedestriandeadreckoning(PDR)algorithmtoreducepositioningerrors.
Lookingatthismulti-modalityclassification/regressionproblemfromatimeseriesperspective therehasbeenalotofexploration
intacklingaproblemwhereeachmodalitycanbecategorizedasmultivariatetimeseriesdata. LSTMandattentionlayersare
oftenusedinparalleltodirectlytransformrawmultivariatetimeseriesdataintoalow-dimensionalfeaturerepresentationforeach
modality. Later variousprocessesaredonetofurtherextractcorrelationsacrossmodalitiesthroughtheuseofvariouslayers(e.g.
concatenation CNNlayer transformer self-attention). Ourworkisinspiredbypriorresearchwhereweonlyutilizeaccelerometer
datatoenrichtheRSSI insteadofutilizingallIMUsensors inordertoreducebatteryconsumption. Inaddition unlikeprevious
workthatstopsatpredictingroomlocations wegoastepfurtheranduseroom-to-roomtransitionbehaviorsasfeaturesforabinary
classifierpredictingwhetherpeoplewithPDaretakingtheirmedicationsorwithholdingthem.
3 CohortandDataset
**Dataset:**Thisdatasetwascollectedusingwristbandwearablesensors oneoneachwristofallparticipants containingtri-axial
accelerometersand10AccessPoints(APs)placedthroughouttheresidentialhome eachmeasuringtheRSSI.Thewearabledevices
wirelesslytransmitdatausingtheBluetoothLowEnergy(BLE)standard whichcanbereceivedbythe10APs. EachAPrecordsthe
transmittedpacketsfromthewearablesensor whichcontainstheaccelerometerreadingssampledat30Hz witheachAPrecording
RSSIvaluessampledat5Hz.
Thedatasetcontains12spousal/parent-child/friend-friendpairs(24participantsintotal)livingfreelyinasmarthomeforfivedays.
EachpairconsistsofonepersonwithPDandonehealthycontrolvolunteer(HC).ThispairingwaschosentoenablePDvs. HC
comparison forsafetyreasons andalsotoincreasethenaturalisticsocialbehavior(particularlyamongstthespousalpairswho
alreadylivedtogether). Fromthe24participants fivefemalesandsevenmaleshavePD.Theaverageageoftheparticipantsis60.25
(PD61.25 Control59.25) andtheaveragetimesincePDdiagnosisforthepersonwithPDis11.3years(range0.5-19).
Tomeasuretheaccuracyofthemachinelearningmodels wall-mountedcamerasareinstalledonthegroundfloorofthehouse
whichcapturered-green-blue(RGB)anddepthdata2-3hoursdaily(duringdaylighthoursattimeswhenparticipantswereathome).
Thevideoswerethenmanuallyannotatedtothenearestmillisecondtoprovidelocalizationlabels. Multiplehumanlabelersused
softwarecalledELANtowatchupto4simultaneously-capturedvideofilesatatime. Theresultinglabeleddatarecordedthekitchen
hallway diningroom livingroom stairs andporch. ThedurationoflabeleddatarecordedbythecamerasforPDandHCis72.84
and75.31hours respectively whichprovidesarelativelybalancedlabelsetforourroom-levelclassification. Finally toevaluate
theON/OFFmedicationstate participantswithPDwereaskedtowithholdtheirdopaminergicmedicationssothattheywerein
thepractically-definedOFFmedicationsstateforatemporaryperiodofseveralhoursduringthestudy. Withholdingmedications
removestheirmitigationonsymptoms leadingtomobilitydeterioration whichcanincludeslowingofgait.
**Datapre-processingforindoorlocalization:**Thedatafromthetwowearablesensorswornbyeachparticipantwerecombinedat
eachtimepoint basedontheirmodality i.e. twentyRSSIvalues(correspondingto10APsforeachofthetwowearablesensors)
andaccelerometrytracesinsixspatialdirections(correspondingtothethreespatialdirections(x y z)foreachwearable)were
recordedateachtimepoint. Theaccelerometerdataisresampledto5HztosynchronizethedatawithRSSIvalues. Witha5-second
timewindowanda5Hzsamplingrate eachRSSIdatasamplehasaninputofsize(25x20) andaccelerometerdatahasaninputof
size(25x6). Imputationformissingvalues specificallyforRSSIdata isappliedbyreplacingthemissingvalueswithavaluethatis
notpossiblenormally(i.e. -120dB).MissingvaluesexistinRSSIdatawheneverthewearableisoutofrangeofanAP.Finally all
time-seriesmeasurementsbythemodalitiesarenormalized.
**Datapre-processingformedicationstate:**Ourmainfocusisforourneuralnetworktocontinuouslyproduceroompredictions
whicharethentransformedintoin-homegaitspeedfeatures particularlyforpersonswithPD.Wehypothesizethatduringtheir
OFFmedicationstate thedeteriorationinmobilityofapersonwithPDisexhibitedbyhowtheytransitionbetweenrooms. These
featuresinclude’Room-to-roomTransitionDuration’andthe’NumberofTransitions’betweentworooms. ’NumberofTransitions’
representshowactivePDsubjectsarewithinacertainperiodoftime while’Room-to-roomTransitionDuration’mayprovide
insightintohowseveretheirdiseaseisbythespeedwithwhichtheynavigatetheirhomeenvironment. Withthelayoutofthehouse
whereparticipantsstayed thehallwayisusedasahubconnectingallotherroomslabeled and’Room-to-roomTransition’shows
thetransitionduration(inseconds)betweentworoomsconnectedbythehallway. Thetransitionbetween(1)kitchenandliving
room (2)kitchenanddiningroom and(3)diningroomandlivingroomarechosenasthefeaturesduetotheircommonalityacross
allparticipants. Forthesefeatures welimitthetransitiontimeduration(i.e. thetimespentinthehallway)to60secondstoexclude
transitionslikelytobeprolongedandthusmaynotberepresentativeoftheperson’smobility.
Thesein-homegaitspeedfeaturesareproducedbyanindoor-localizationmodelbyfeedingRSSIsignalsandaccelerometerdata
from12PDparticipantsfrom6a.m. to10p.m. daily whichareaggregatedinto4-hourwindows. Fromthis eachPDparticipant
willhave20datasamples(fourdatasamplesforeachofthefivedays) eachofwhichcontainssixfeatures(threeforthemeanof
room-to-roomtransitiondurationandthreeforthenumberofroom-to-roomtransitions). Thereisonlyone4-hourwindowduring
whichthepersonwithPDisOFFmedications. Thesesamplesarethenusedtotrainabinaryclassifierdeterminingwhetheraperson
withPDisONorOFFtheirmedications.
Forabaselinecomparisontothein-homegaitspeedfeatures demographicfeatureswhichincludeage gender yearsofPD and
MDS-UPDRSIIIscore(thegold-standardclinicalratingscalescoreusedinclinicaltrialstomeasuremotordiseaseseverityin
PD)arechosen. TwoMDS-UPDRSIIIscoresareassignedforeachPDparticipant;oneisassignedwhenapersonwithPDisON
medications andtheotheroneisassignedwhenapersonwithPDisOFFmedications. Foreachin-homegaitspeedfeaturedata
sample therewillbeacorrespondingdemographicfeaturedatasamplethatisusedtotrainadifferentbinaryclassifiertopredict
whetherapersonwithPDisONorOFFmedications.
**Ethicalapproval:**FullapprovalfromtheNHSWalesResearchEthicsCommitteewasgrantedonDecember17 2019 and
HealthResearchAuthorityandHealthandCareResearchWalesapprovalwasconfirmedonJanuary14 2020;theresearchwas
conductedinaccordwiththeHelsinkiDeclarationof1975;writteninformedconsentwasgainedfromallstudyparticipants. In
ordertoprotectparticipantprivacy supportingdataisnotsharedopenly. Itwillbemadeavailabletobonafideresearcherssubjectto
adataaccessagreement.
4 MethodologiesandFramework
WeintroduceMultiheadDualConvolutionalSelfAttention(MDCSA) adeepneuralnetworkthatutilizesdualmodalitiesforindoor
localizationinhomeenvironments. Thenetworkaddressestwochallengesthatarisefrommultimodalityandtime-seriesdata:
(1)Capturingmultivariatefeaturesandfilteringmultimodalnoises. RSSIsignals whicharemeasuredatmultipleaccesspoints
withinahomereceivedfromwearablecommunication havebeenwidelyusedforindoorlocalization typicallyusingafingerprinting
techniquethatproducesagroundtruthradiomapofahome. Naturally thewearablealsoproducesaccelerationmeasurementswhich
canbeusedtoidentifytypicalactivitiesperformedinaspecificroom andthuswecanexploreifaccelerometerdatawillenrich
theRSSIsignals inparticulartohelpdistinguishadjacentrooms whichRSSI-onlysystemstypicallystrugglewith. Ifitwill how
canweincorporatetheseextrafeatures(andmodalities)intotheexistingfeaturesforaccurateroompredictions particularlyinthe
contextofPDwheretheaccelerationsignalmaybesignificantlyimpactedbythediseaseitself?
(2)Modelinglocalandglobaltemporaldynamics. Thetruecorrelationsbetweeninputsbothintra-modality(i.e. RSSIsignalamong
accesspoints)andinter-modality(i.e. RSSIsignalagainstaccelerometerfluctuation)aredynamic. Thesedynamicscanaffectone
anotherwithinalocalcontext(e.g. cyclicalpatterns)oracrosslong-termrelationships. Canwecapturelocalandglobalrelationships
acrossdifferentmodalities?
TheMDCSAarchitectureaddressestheaforementionedchallengesthroughaseriesofneuralnetworklayers whicharedescribedin
thefollowingsections.
4.1 ModalityPositionalEmbedding
DuetodifferentdatadimensionalitybetweenRSSIandaccelerometer coupledwiththemissingtemporalinformation alinear
layerwithapositionalencodingisaddedtotransformbothRSSIandaccelerometerdataintotheirrespectiveembeddings. Suppose
wehaveacollectionofRSSIsignalsxr =[xr xr ... xr]∈RT×r andaccelerometerdataxa =[xa xa ... xa]∈RT×a within
1 2 T 1 2 T
T timeunits wherexr = [xr  xr  ... xr ]representsRSSIsignalsfromr accesspoints andxa = [xa  xa  ... xa ]represents
t t1 t2 tr t t1 t2 ta
accelerometerdatafromaspatialdirectionsattimetwitht<T. Givenfeaturevectorsx =[xr xa]withu∈{r a}representing
t t t
RSSIoraccelerometerdataattimet andt<T representingthetimeindex apositionalembeddinghuforRSSIoraccelerometer
t
canbeobtainedby:
hu =(W xu+b )+τ (1)
t u t u t
whereW ∈Ru×dandb ∈Rdaretheweightandbiastolearn distheembeddingdimension andτ ∈Rdisthecorresponding
u u t
positionencodingattimet.
4.2 LocalityEnhancementwithSelf-Attention
Sinceitistime-seriesdata theimportanceofanRSSIoraccelerometervalueateachpointintimecanbeidentifiedinrelationtoits
surroundingvalues-suchascyclicalpatterns trends orfluctuations. Utilizinghistoricalcontextthatcancapturelocalpatternson
topofpoint-wisevalues performanceimprovementsinattention-basedarchitecturescanbeachieved. Onestraightforwardoptionis
toutilizearecurrentneuralnetworksuchasalong-shorttermmemory(LSTM)approach. However inLSTMlayers thelocal
contextissummarizedbasedonthepreviouscontextandthecurrentinput. Twosimilarpatternsseparatedbyalongperiodoftime
mighthavedifferentcontextsiftheyareprocessedbytheLSTMlayers. Weutilizeacombinationofcausalconvolutionlayersand
self-attentionlayers whichwenameDualConvolutionalSelf-Attention(DCSA).TheDCSAtakesinaprimaryinputxˆ ∈RN×d
1
andasecondaryinputxˆ ∈RN×dandyields:
DCSA(xˆ  xˆ )=GRN(Norm(ϕ(xˆ )+xˆ ) Norm(ϕ(xˆ )+xˆ )) (2)
1 2 1 1 2 2
ϕ(xˆ)=SA(Φ (xˆ)W  Φ (xˆ)W  Φ (xˆ)W ) (3)
k Q k K k V
whereGRN(.)istheGatedResidualNetworktointegratedualinputsintooneintegratedembedding Norm(.)isastandardlayer
normalization SA(.)isascaleddot-productself-attention Φ (.)isa1D-convolutionallayerwithakernelsize{1 k}andastride
k
of1  W ∈ Rd×d W ∈ Rd×d W ∈ Rd×d areweightsforkeys  queries  andvaluesoftheself-attentionlayer  anddisthe
K Q V
embeddingdimension. NotethatallweightsforGRNaresharedacrosseachtimestept.
4.3 MultiheadDualConvolutionalSelf-Attention
Ourapproachemploysaself-attentionmechanismtocaptureglobaldependenciesacrosstimesteps. Itisembeddedaspartofthe
DCSAarchitecture. Inspiredbyutilizingmultiheadself-attention weutilizeourDCSAwithvariouskernellengthswiththesame
aim: allowingasymmetriclong-termlearning. ThemultiheadDCSAtakesintwoinputsxˆ  xˆ ∈RN×dandyields:
1 2
MDCSA (xˆ  xˆ )=Ξ (ϕ (xˆ  xˆ )) (4)
k1 ... kn 1 2 n k1 ... kn 1 2
ϕ (xˆ  xˆ )=SA(Φ (xˆ )W  Φ (xˆ )W  Φ (xˆ  xˆ )W ) (5)
ki 1 2 ki 1 Q ki 2 K ki 1 2 V
whereΦ (.)isa1D-convolutionallayerwithakernelsize{1 k }andastridek  W ∈ Rd×d W ∈ Rd×d W ∈ Rd×d are
ki i i K Q V
weightsforkeys queries andvaluesoftheself-attentionlayer andΞ (.)concatenatestheoutputofeachDCSA (.)intemporal
n ki
order. Forregularization anormalizationlayerfollowedbyadropoutlayerisaddedafterEquation4.
Followingthemodalitypositionalembeddinglayerinsubsection4.1 thepositionalembeddingsofRSSIhr = [hr ... hr]and
1 T
accelerometerha =[ha ... ha] producedbyEq. 1 arethenfedtoanMDCSAlayerwithvariouskernelsizes[k  ... k ]:
1 T 1 n
h=MDCSA (hr ha) (6)
k1 ... kn
toyieldh=[h  ... h ]withh ∈Rdandt<T.
1 T t
4.4 FinalLayerandLossCalculation
Weapplytwodifferentlayerstoproducetwodifferentoutputsduringtraining. Theroom-levelpredictionsareproducedviaasingle
conditionalrandomfield(CRF)layerincombinationwithalinearlayerappliedtotheoutputofEq. 7toproducethefinalpredictions
yˆ =CRF(ϕ(h )) (7)
t t
q′(h )=W h +b (8)
t p t p
whereW ∈Rd×mandb ∈Rmaretheweightandbiastolearn misthenumberofroomlocations andh=[h  ... h ]∈RT×d
p p 1 T
generatingtherefinedembeddingattimestept itsdecisionisindependent;itdoesnottakeintoaccounttheactualdecisionmadeby
otherrefinedembeddingst. WeuseaCRFlayertocoverjustthat i.e. tomaximizetheprobabilityoftherefinedembeddingsofall
timesteps soitcanbettermodelcaseswhererefinedembeddingsclosesttooneanothermustbecompatible(i.e. minimizingthe
possibilityforimpossibleroomtransitions). Whenfindingthebestsequenceofroomlocationyˆ theViterbiAlgorithmisusedasa
standardfortheCRFlayer.
Forthesecondlayer wechooseaparticularroomasareferenceandperformabinaryclassificationateachtimestept. Thebinary
classificationisproducedviaalinearlayerappliedtotherefinedembeddingh as:
fˆ =W h +b (9)
t f t f
where W ∈ Rd×1 and b ∈ R are the weight and bias to learn  and fˆ= [fˆ ... fˆ ] ∈ RT is the target probabilities for the
f f 1 T
referencedroomwithintimewindowT. Thereasontoperformabinaryclassificationagainstaparticularroomisbecauseofour
interestinimprovingtheaccuracyinpredictingthatroom. Inourapplication theroomofourchoiceisthehallway whereitwillbe
usedasahubconnectinganyotherroom.
**LossFunctions:**Duringthetrainingprocess theMDCSAnetworkproducestwokindsofoutputs. Emissionoutputs(outputs
producedbyEquation9priortopredictionoutputs)eˆ=[ϕ(h ) ... ϕ(h )]aretrainedtogeneratethelikelihoodestimateofroom
predictions whilethebinaryclassificationoutputfˆ=[fˆ ... fˆ ]isusedtotraintheprobabilityestimateofaparticularroom. The
finallossfunctioncanbeformulatedasacombinationofbothlikelihoodandbinarycross-entropylossfunctionsdescribedas:
T
L(eˆ y fˆ f)=L (eˆ y)+(cid:88)L (fˆ f ) (10)
LL BCE t t
t=1
T T
(cid:88) (cid:88)
L (eˆ y)= P(ϕ(h ))qT(y |y )− P(ϕ(h ))[qT(y |y )] (11)
LL i i i i−1 i i i i−1
i=0 i=0
L (fˆ f)=−1 (cid:88)f log(fˆ)+(1−f )log(1−fˆ) (12)
BCE T t t t t
t=0
whereL (.)representsthenegativelog-likelihoodandL (.)denotesthebinarycross-entropy y =[y  ... y ]∈RT isthe
LL BCE 1 T
actualroomlocations  andf = [f  ... f ] ∈ RT isthebinaryvaluewhetherattimettheroomisthereferencedroomornot.
P(y |y )denotestheconditionalprobability andP(y |y )denotesthetransitionmatrixcostofhavingtransitionedfromy
i i−1 t t−1 t−1
toy .
5 ExperimentsandResults
Wecompareourproposednetwork MDCSA1 4 7(MDCSAwith3kernelsofsize1 4 and7) with:
-RandomForest(RF)asabaselinetechnique whichhasbeenshowntoworkwellforindoorlocalization. -Amodifiedtransformer
dependenciesintemporalaspects. -Astate-of-the-artmodelformultimodalandmultivariatetimeserieswithatransformerencoder
tolearnasymmetriccorrelationsacrossmodalities. -Analternativetothepreviousmodel representingitwithaGRNlayerreplacing
thecontextaggregationlayerandaCRFlayeraddedasthelastlayer. -MDCSA1 4 74APS asanablationstudy withourproposed
network(i.e. MDCSA1 4 7)using4accesspointsfortheRSSI(insteadof10accesspoints)andaccelerometerdata(ACCL)asits
inputfeatures. -MDCSA1 4 7RSSI asanablationstudy withourproposednetworkusingonlyRSSI withoutACCL asitsinput
features. -MDCSA1 4 74APSRSSI asanablationstudy withourproposednetworkusingonly4accesspointsfortheRSSIasits
inputfeatures.
ForRF allthetimeseriesfeaturesofRSSIandaccelerometryareflattenedandmergedintoonefeaturevectorforroom-level
localization. Forthemodifiedtransformerencoder ateachtimestept RSSIxr andaccelerometerxafeaturesarecombinedviaa
linearlayerbeforetheyareprocessedbythenetworks. Agridsearchontheparametersofeachnetworkisperformedtofindthebest
parameterforeachmodel. Theparameterstotunearetheembeddingdimensiondin128 256 thenumberofepochsin200 300
andthelearningratein0.01 0.0001. Thedropoutrateissetto0.15 andaspecificoptimizerincombinationwithaLook-Ahead
algorithmisusedforthetrainingwithearlystoppingusingthevalidationperformance. FortheRF weperformacross-validated
parametersearchforthenumberoftrees(200 250) theminimumnumberofsamplesinaleafnode(1 5) andwhetherawarmstart
isneeded. TheGiniimpurityisusedtomeasuresplits.
**EvaluationMetrics:**WeareinterestedindevelopingasystemtomonitorPDmotorsymptomsinhomeenvironments. For
example wewillconsiderifthereisanysignificantdifferenceintheperformanceofthesystemwhenitistrainedwithPDdata
comparedtobeingtrainedwithhealthycontrol(HC)data. Wetailoredourtrainingproceduretotestourhypothesisbyperforming
variationsofcross-validation. ApartfromtrainingourmodelsonallHCsubjects(ALL-HC) wealsoperformfourdifferentkindsof
cross-validation: 1)WetrainourmodelsononePDsubject(LOO-PD) 2)WetrainourmodelsononeHCsubject(LOO-HC) 3)We
takeoneHCsubjectanduseonlyroughlyfourminutes’worthofdatatotrainourmodels(4m-HC) 4)WetakeonePDsubjectand
useonlyroughlyfourminutes’worthofdatatotrainourmodels(4m-PD).Forallofourexperiments wetestourtrainedmodelson
allPDsubjects(excludingtheoneusedastrainingdataforLOO-PDand4m-PD).Forroom-levellocalizationaccuracy weuse
precisionandweightedF1-score allaveragedandstandarddeviatedacrossthetestfolds.
Toshowcasetheimportanceofin-homegaitspeedfeaturesindifferentiatingthemedicationstateofapersonwithPD wefirst
comparehowaccuratethe’Room-to-roomTransition’durationproducedbyeachnetworkistothegroundtruth(i.e. annotated
location). Wehypothesizethatthemoreaccuratethetransitioniscomparedtothegroundtruth thebettermobilityfeaturesarefor
medicationstateclassification. Forthemedicationstateclassification wethencomparetwodifferentgroupsoffeatureswithtwo
simplebinaryclassifiers: 1)thebaselinedemographicfeatures(seeSection3) and2)thenormalizedin-homegaitspeedfeatures.
ThemetricweuseforON/OFFmedicationstateevaluationistheweightedF1-ScoreandAUROC whichareaveragedandstandard
deviatedacrossthetestfolds.
5.1 ExperimentalResults
**Room-levelAccuracy:**ThefirstpartofTable1comparestheperformanceoftheMDCSAnetworkandotherapproachesfor
room-levelclassification. Forroom-levelclassification theMDCSAnetworkoutperformsothernetworksandRFwithaminimum
improvementof1.3%fortheF1-scoreoverthesecond-bestnetworkineachcross-validationtype withtheexceptionoftheALL-HC
validation. Theimprovementismoresignificantinthe4m-HCand4m-PDvalidations whenthetrainingdataarelimited withan
averageimprovementofalmost9%fortheF1-scoreoverthealternativetothestate-of-the-artmodel.
TheLOO-HCandLOO-PDvalidationsshowthatamodelthathastheabilitytocapturethetemporaldynamicsacrosstimestepswill
performbetterthanastandardbaselinetechniquesuchasaRandomForest.Themodifiedtransformerencoderandthestate-of-the-art
modelperformbetterinthosetwovalidationsduetotheirabilitytocaptureasynchronousrelationsacrossmodalities. However
data themodifiedtransformerencoderperformsworstinthesetwovalidations. Thestate-of-the-artmodelperformsquitewell
duetoitsabilitytocapturelocalcontextviaLSTMforeachmodality. However ingeneral itsperformancesuffersinboththe
LOO-PDand4m-PDvalidationsastheaccelerometerdata(andmodality)maybeerraticduetoPDandshouldbeexcludedat
timesfromcontributingtoroomclassification. TheMDCSAnetworkhasallthecapabilitiesthatthestate-of-the-artmodelhas
withanimprovementinsuppressingtheaccelerometermodalitywhenneededviatheGRNlayerembeddedinDCSA.Suppressing
thenoisymodalityseemstohaveastrongimpactonmaintainingtheperformanceofthenetworkwhenthetrainingdataislimited.
Thisisvalidatedbyhowthealternativetothestate-of-the-artmodel(i.e. thestate-of-the-artmodelwithaddedGRNandCRF
layers)outperformsthestandardstate-of-the-artmodelbyanaverageof2.2%fortheF1-scoreinthe4m-HCand4m-PDvalidations.
ItisfurtherconfirmedbyMDCSA1 4 74APSagainstMDCSA1 4 74APSRSSI withthelattermodel whichdoesnotinclude
theaccelerometerdata outperformingtheformerfortheF1-scorebyanaverageof1.6%inthelastthreecross-validations. Itis
worthpointingoutthattheMDCSA1 4 74APSRSSImodelperformedthebestinthe4m-PDvalidation. However theomissionof
accelerometerdataaffectsthemodel’sabilitytodifferentiateroomsthataremorelikelytohaveactivemovement(i.e. hall)thanthe
roomsthatarenot(i.e. livingroom). ItcanbeseenfromTable2thattheMDCSA1 4 74APSRSSImodelhaslowperformancein
predictingthehallwaycomparedtothefullmodelofMDCSA1 4 7. Asaconsequence theMDCSA1 4 74APSRSSImodelcannot
producein-homegaitspeedfeaturesas
accurately asshowninTable3.
**Room-to-roomTransitionandMedicationAccuracy:**WehypothesizethatduringtheirOFFmedicationstate thedeterioration
inmobilityofapersonwithPDisexhibitedbyhowtheytransitionbetweenrooms. Totestthishypothesis aWilcoxonsigned-rank
testwasusedontheannotateddatafromPDparticipantsundertakingeachofthethreeindividualtransitionsbetweenroomswhilst
ON(taking)andOFF(withholding)medicationstoassesswhetherthemeantransitiondurationONmedicationswasstatistically
significantlyshorterthanthemeantransitiondurationforthesametransitionOFFmedicationsforalltransitionsstudied(seeTable
4). Fromthisresult wearguethatthemeantransitiondurationobtainedbyeachmodelfromTable1thatisclosetothegroundtruth
cancapturewhatthegroundtruthcaptures. AsmentionedinSection3 thistransitiondurationforeachmodelisgeneratedbythe
modelcontinuouslyperformingroom-levellocalization focusingonthetimeapersonispredictedtospendinahallwaybetween
rooms. Weshow inTable3 thatthemeantransitiondurationforalltransitionsstudiedproducedbytheMDCSA1 4 7modelisthe
closesttothegroundtruth improvingoverthesecondbestbyaround1.25secondsacrossallhalltransitionsandvalidations.
featurescanbeusedasabaselineforeachtypeofvalidation. TheMDCSAnetwork withtheexceptionoftheALL-HCvalidation
outperformsanyothernetworkbyasignificantmarginfortheAUROCscore. Byusingin-homegaitspeedfeaturesproducedby
theMDCSAnetwork aminimumof15%improvementoverthebaselinedemographicfeaturescanbeobtained withthebiggest
gainobtainedinthe4m-PDvalidationdata. Inthe4m-PDvalidationdata RF TENER andDTMLcouldnotmanagetoprovide
anypredictionduetotheirinabilitytocapture(partly)halltransitions. Furthermore TENERhasshownitsinabilitytoprovideany
medicationstatepredictionfromthe4m-HCdatavalidations. ItcanbevalidatedbyTable3whenTENERfailedtocaptureany
transitionsbetweenthediningroomandlivingroomacrossallperiodsthathavegroundtruths. MDCSAnetworkscanprovide
medicationstatepredictionandmaintaintheirperformanceacrossallcross-validationsthankstotheadditionofEq. 13intheloss
**Limitationsandfutureresearch:**Onelimitationofthisstudyistherelativelysmallsamplesize(whichwasplannedasthisis
anexploratorypilotstudy). Webelieveoursamplesizeisampletoshowproofofconcept. Thisisalsothefirstsuchworkwith
unobtrusivegroundtruthvalidationfromembeddedcameras. Futureworkshouldvalidateourapproachfurtheronalargecohort
ofpeoplewithPDandconsiderstratifyingforsub-groupswithinPD(e.g. akinetic-rigidortremor-dominantphenotypes) which
wouldalsoincreasethegeneralizabilityoftheresultstothewiderpopulation. Futureworkinthismattercouldalsoincludethe
constructionofasemi-syntheticdatasetbasedoncollecteddatatofacilitateaparallelandlarge-scaleevaluation.
Thissmarthome’slayoutandparametersremainconstantforalltheparticipants andweacknowledgethatthetransferofthisdeep
learningmodeltoothervariedhomesettingsmayintroducevariationsinlocalizationaccuracy. Forfutureecologicalvalidationand
basedonourcurrentresults weanticipatetheneedforpre-training(e.g. abriefwalkaroundwhichislabeled)foreachhome and
alsosuggestthatsomesmallamountofground-truthdatawillneedtobecollected(e.g. researcherpromptingofstudyparticipantsto
undertakescriptedactivitiessuchasmovingfromroomtoroom)tofullyvalidatetheperformanceofourapproachinothersettings.
accelerometerdata. Theevaluationonouruniquereal-worldfree-livingpilotdataset whichincludessubjectswithandwithoutPD
showsthatMDCSAachievesstate-of-the-artaccuracyforindoorlocalization. Theavailabilityofaccelerometerdatadoesindeed
enrichtheRSSIfeatures which inturn improvestheaccuracyofindoorlocalization.
Accurateroomlocalizationusingthesedatamodalitieshasawiderangeofpotentialapplicationswithinhealthcare. Thiscould
triggeringanalertforapossiblefall(andlonglieonthefloor)ifsomeoneisinoneroomforanunusuallengthoftime. Furthermore
accurateroomuseandroom-to-roomtransferstatisticscouldbeusedinoccupationalsettings e.g. tocheckfactoryworkerlocation.
Table1: Room-levelandmedicationstateaccuracyofallmodels. Standarddeviationisshownin(.) thebestperformerisbold
whilethesecondbestisitalicized. NotethatourproposedmodelistheonenamedMDCSA1 4 7
Room-LevelLocalisation MedicationState
Training Model
RF 95.00 95.20 56.67(17.32) 84.55(12.06)
TENER 94.60 94.80 47.08(16.35) 67.74(10.82)
DTML 94.80 94.90 50.33(13.06) 75.97(9.12)
AltDTML 94.80 95.00 47.25(5.50) 75.63(4.49)
ALL-HC
MDCSA1 4 74APS 92.22 92.22 53.47(12.63) 73.48(6.18)
MDCSA1 4 7RSSI 94.70 94.90 51.14(11.95) 68.33(18.49)
MDCSA1 4 74APSRSSI 93.30 93.10 64.52(11.44) 81.84(6.30)
MDCSA1 4 7 94.90 95.10 64.13(6.05) 80.95(10.71)
DemographicFeatures 49.74(15.60) 65.66(18.54)
RF 89.67(1.85) 88.95(2.61) 54.74(11.46) 69.24(17.77)
TENER 90.35(1.87) 89.75(2.24) 51.76(14.37) 70.80(9.78)
DTML 90.51(1.95) 89.82(2.60) 55.34(13.67) 73.77(9.84)
AltDTML 90.52(2.17) 89.71(2.83) 49.56(17.26) 73.26(10.65)
LOO-HC MDCSA1 4 74APS 88.01(6.92) 88.08(5.73) 59.52(20.62) 74.35(16.78)
MDCSA1 4 7RSSI 90.26(2.43) 89.48(3.47) 58.84(23.08) 76.10(10.84)
MDCSA1 4 74APSRSSI 88.55(6.67) 88.75(5.50) 42.34(13.11) 72.58(6.77)
MDCSA1 4 7 91.39(2.13) 91.06(2.62) 55.50(15.78) 83.98(13.45)
DemographicFeatures 51.79(15.40) 68.33(18.43)
RF 86.89(7.14) 84.71(7.33) 43.28(14.02) 62.63(20.63)
TENER 86.91(6.76) 86.18(6.01) 36.04(9.99) 60.03(10.52)
DTML 87.13(6.53) 86.31(6.32) 43.98(14.06) 66.93(11.07)
!
AltDTML 87.36(6.30) 86.44(6.63) 44.02(16.89) 69.70(12.04)
LOO-PD MDCSA1 4 74APS 86.44(6.96) 85.93(6.05) 47.26(14.47) 72.62(11.16)
MDCSA1 4 7RSSI 87.61(6.64) 87.21(5.44) 45.71(17.85) 67.76(10.73)
MDCSA1 4 74APSRSSI 87.20(7.17) 87.00(6.12) 41.33(17.72) 66.26(12.11)
MDCSA1 4 7 88.04(6.94) 87.82(6.01) 49.99(13.18) 81.08(8.46)
DemographicFeatures 43.89(14.43) 60.95(25.16)
RF 74.27(8.99) 69.87(7.21) 50.47(12.63) 59.55(12.38)
TENER 69.86(18.68) 60.71(24.94) N/A N/A
DTML 77.10(9.89) 70.12(14.26) 43.89(11.60) 64.67(12.88)
AltDTML 78.79(3.95) 71.44(9.82) 47.49(14.64) 65.16(12.56)
4m-HC MDCSA1 4 74APS 81.42(6.95) 78.65(7.59) 42.87(17.34) 67.09(7.42)
MDCSA1 4 7RSSI 81.69(6.85) 77.12(8.46) 49.95(17.35) 69.71(11.55)
MDCSA1 4 74APSRSSI 82.80(7.82) 79.37(8.98) 43.57(23.87) 65.46(15.78)
MDCSA1 4 7 83.32(6.65) 80.24(6.85) 55.43(10.48) 78.24(6.67)
DemographicFeatures 32.87(13.81) 53.68(13.86)
RF 71.00(9.67) 65.89(11.96) N/A N/A
TENER 65.30(23.25) 58.57(27.19) N/A N/A
DTML 70.35(14.17) 64.00(17.88) N/A N/A
AltDTML 74.43(9.59) 67.55(14.50) N/A N/A
4m-PD MDCSA1 4 74APS 81.02(8.48) 76.85(10.94) 49.97(7.80) 69.10(7.64)
MDCSA1 4 7RSSI 77.47(12.54) 73.99(13.00) 41.79(16.82) 67.37(16.86)
MDCSA1 4 74APSRSSI 83.01(6.42) 79.77(7.05) 41.18(12.43) 63.16(11.06)
MDCSA1 4 7 83.30(6.73) 76.77(13.19) 48.61(12.03) 76.39(12.23)
DemographicFeatures 36.69(18.15) 50.53(15.60)
Innaturalisticsettings in-homemobilitycanbemeasuredthroughtheuseofindoorlocalizationmodels. Wehaveshown using
medications. Withaccuratein-homegaitspeedfeatures aclassifiermodelcanthendifferentiateaccuratelyifapersonwithPDisin
anONorOFFmedicationstate. Suchchangesshowthepromiseoftheselocalizationoutputstodetectthedopamine-relatedgait
fluctuationsinPDthatimpactpatients’qualityoflifeandareimportantinclinicaldecision-making. Wehavealsodemonstrated
thatourindoorlocalizationsystemprovidesprecisein-homegaitspeedfeaturesinPDwithaminimalaverageoffsettotheground
Table2: Hallwaypredictiononlimitedtrainingdata.
MDCSA4APSRSSI 62.32(19.72) 58.99(23.87)
4m-HC MDCSA4APS 68.07(23.22) 60.01(26.24)
MDCSA 71.25(21.92) 68.95(17.89)
MDCSA4APSRSSI 58.59(23.60) 57.68(24.27)
4m-PD MDCSA4APS 62.36(18.98) 57.76(20.07)
MDCSA 70.47(14.10) 64.64(21.38)
Table3: Room-to-roomtransitionaccuracy(inseconds)ofallmodelscomparedtothegroundtruth. Standarddeviationisshownin
(.) thebestperformerisbold whilethesecondbestisitalicized. Amodelthatfailstocaptureatransitionbetweenparticularrooms
withinaperiodthathasthegroundtruthisassigned’N/A’score.
Data Models Kitch-Livin Kitch-Dinin Dinin-Livin
GroundTruth 18.71(18.52) 14.65(6.03) 10.64(11.99)
RF 16.18(12.08) 14.58(10.22) 10.19(9.46)
TENER 15.58(8.75) 16.30(12.94) 12.01(13.01)
AltDTML 15.27(7.51) 13.40(6.43) 10.84(10.81)
MDCSA 17.70(16.17) 14.94(9.71) 10.76(9.59)
RF 17.52(16.97) 11.93(10.08) 9.23(13.69)
TENER 14.62(16.37) 9.58(9.16) 7.21(10.61)
LOO-HC
AltDTML 16.30(17.78) 14.01(8.08) 10.37(12.44)
MDCSA 17.70(17.42) 14.34(9.48) 11.07(13.60)
! RF 14.49(15.28) 11.67(11.68) 8.65(13.06)
TENER 13.42(14.88) 10.87(10.37) 6.95(10.28)
LOO-PD
AltDTML 16.98(15.15) 15.26(8.85) 9.99(13.03)
MDCSA 16.42(14.04) 14.48(9.81) 10.77(14.18)
RF 14.22(18.03) 11.38(15.46) 13.43(18.87)
TENER 10.75(15.67) 8.59(14.39) N/A
4m-HC
AltDTML 16.89(18.07) 14.68(13.57) 9.31(15.70)
MDCSA 18.15(19.12) 15.32(14.93) 11.89(17.55)
RF 11.52(16.07) 8.73(12.90) N/A
TENER 8.75(14.89) N/A N/A
4m-PD
AltDTML 14.75(13.79) 13.47(17.66) N/A
MDCSA 17.96(19.17) 14.74(10.83) 10.16(14.03)
truth. Thenetworkalsooutperformsothermodelsintheproductionofin-homegaitspeedfeatures whichisusedtodifferentiatethe
medicationstateofapersonwithPD.
MovementDisordersHealthIntegrationTeam(PatientandPublicInvolvementGroup)fortheirassistanceateachstudydesignstep.
Thisworkwassupportedbyvariousgrantsandinstitutions.
StatisticalSignificanceTest
ItcouldbearguedthatallthelocalizationmodelscomparedinTable1mightnotbestatisticallydifferentduetothefairlyhigh
standarddeviationacrossalltypesofcross-validations whichiscausedbytherelativelysmallnumberofparticipants. Inorderto
comparemultiplemodelsovercross-validationsetsandshowthestatisticalsignificanceofourproposedmodel weperformthe
Friedmantesttofirstrejectthenullhypothesis. Wethenperformedapairwisestatisticalcomparison: theWilcoxonsigned-ranktest
withHolm’salphacorrection.
Table4: PDparticipantroomtransitiondurationwithONandOFFmedicationscomparisonusingWilcoxonsignedranktests.
OFFtransitions Meantransitionduration ONtransitions Meantransitionduration W z
Kitchen-LivingOFF 17.2sec Kitchen-LivingON 14.0sec 75.0 2.824
Dining-KitchenOFF 12.9sec Dining-KitchenON 9.2sec 76.0 2.903
Dining-LivingOFF 10.4sec Dining-LivingON 9.0sec 64.0 1.961
Introduction
Related Work
Cohort and Dataset
Methodologies and Framework
4.1
Modality Positional Embedding
we have a collection of RSSI signals xr = [xr
T ] ∈ RT ×r and accelerometer data xa = [xa
T ] ∈ RT ×a within
T time units  where xr
t = [xr
tr] represents RSSI signals from r access points  and xa
t = [xa
ta] represents
accelerometer data from a spatial directions at time t with t < T. Given feature vectors xt = [xr
t ] with u ∈ {r  a} representing
RSSI or accelerometer data at time t  and t < T representing the time index  a positional embedding hu
t for RSSI or accelerometer
t = (Wuxu
t + bu) + τt
(1)
where Wu ∈ Ru×d and bu ∈ Rd are the weight and bias to learn  d is the embedding dimension  and τt ∈ Rd is the corresponding
4.2
Locality Enhancement with Self-Attention
self-attention layers  which we name Dual Convolutional Self-Attention (DCSA). The DCSA takes in a primary input ˆx1 ∈ RN×d
and a secondary input ˆx2 ∈ RN×d and yields:
DCSA(ˆx1  ˆx2) = GRN(Norm(ϕ(ˆx1) + ˆx1)  Norm(ϕ(ˆx2) + ˆx2))
(2)
ϕ(ˆx) = SA(Φk(ˆx)WQ  Φk(ˆx)WK  Φk(ˆx)WV )
(3)
where GRN(.) is the Gated Residual Network to integrate dual inputs into one integrated embedding  Norm(.) is a standard layer
normalization  SA(.) is a scaled dot-product self-attention  Φk(.) is a 1D-convolutional layer with a kernel size {1  k} and a stride
of 1  WK ∈ Rd×d  WQ ∈ Rd×d  WV ∈ Rd×d are weights for keys  queries  and values of the self-attention layer  and d is the
4.3
Multihead Dual Convolutional Self-Attention
aim: allowing asymmetric long-term learning. The multihead DCSA takes in two inputs ˆx1  ˆx2 ∈ RN×d and yields:
MDCSAk1 ... kn(ˆx1  ˆx2) = Ξn(ϕk1 ... kn(ˆx1  ˆx2))
(4)
ϕki(ˆx1  ˆx2) = SA(Φki(ˆx1)WQ  Φki(ˆx2)WK  Φki(ˆx1  ˆx2)WV )
(5)
where Φki(.) is a 1D-convolutional layer with a kernel size {1  ki} and a stride ki  WK ∈ Rd×d  WQ ∈ Rd×d  WV ∈ Rd×d are
weights for keys  queries  and values of the self-attention layer  and Ξn(.) concatenates the output of each DCSAki(.) in temporal
Following the modality positional embedding layer in subsection 4.1  the positional embeddings of RSSI hr = [hr
T ] and
accelerometer ha = [ha
T ]  produced by Eq. 1  are then fed to an MDCSA layer with various kernel sizes [k1  ...  kn]:
h = MDCSAk1 ... kn(hr  ha)
(6)
to yield h = [h1  ...  hT ] with ht ∈ Rd and t < T.
4.4
Final Layer and Loss Calculation
ˆyt = CRF(ϕ(ht))
(7)
q′(ht) = Wpht + bp
(8)
where Wp ∈ Rd×m and bp ∈ Rm are the weight and bias to learn  m is the number of room locations  and h = [h1  ...  hT ] ∈ RT ×d
classification is produced via a linear layer applied to the refined embedding ht as:
ˆft = Wfht + bf
(9)
where Wf ∈ Rd×1 and bf ∈ R are the weight and bias to learn  and ˆf = [ ˆf1  ...  ˆfT ] ∈ RT is the target probabilities for the
produced by Equation 9 prior to prediction outputs) ˆe = [ϕ(h1)  ...  ϕ(hT )] are trained to generate the likelihood estimate of room
predictions  while the binary classification output ˆf = [ ˆf1  ...  ˆfT ] is used to train the probability estimate of a particular room. The
L(ˆe  y  ˆf  f) = LLL(ˆe  y) +
�
LBCE( ˆft  ft)
(10)
LLL(ˆe  y) =
i=0
P(ϕ(hi))qT
i (yi|yi−1) −
P(ϕ(hi))[qT
i (yi|yi−1)]
(11)
LBCE( ˆf  f) = − 1
ft log( ˆft) + (1 − ft) log(1 − ˆft)
(12)
where LLL(.) represents the negative log-likelihood and LBCE(.) denotes the binary cross-entropy  y = [y1  ...  yT ] ∈ RT is the
actual room locations  and f = [f1  ...  fT ] ∈ RT is the binary value whether at time t the room is the referenced room or not.
P(yi|yi−1) denotes the conditional probability  and P(yt|yt−1) denotes the transition matrix cost of having transitioned from yt−1
to yt.
Experiments and Results
t and accelerometer xa
t features are combined via a
parameter for each model. The parameters to tune are the embedding dimension d in 128  256  the number of epochs in 200  300
5.1
Experimental Results
Conclusion
Training
Model
Room-Level Localisation
Medication State
Precision
F1-Score
AUROC
RF
95.00
95.20
56.67 (17.32)
84.55 (12.06)
TENER
94.60
94.80
47.08 (16.35)
67.74 (10.82)
DTML
94.90
50.33 (13.06)
75.97 (9.12)
Alt DTML
47.25 (5.50)
75.63 (4.49)
MDCSA1 4 7 4APS
92.22
53.47 (12.63)
73.48 (6.18)
MDCSA1 4 7 RSSI
94.70
51.14 (11.95)
68.33 (18.49)
MDCSA1 4 7 4APS RSSI
93.30
93.10
64.52 (11.44)
81.84 (6.30)
MDCSA1 4 7
95.10
64.13 (6.05)
80.95 (10.71)
Demographic Features
49.74 (15.60)
65.66 (18.54)
89.67 (1.85)
88.95 (2.61)
54.74 (11.46)
69.24 (17.77)
90.35 (1.87)
89.75 (2.24)
51.76 (14.37)
70.80 (9.78)
90.51 (1.95)
89.82 (2.60)
55.34 (13.67)
73.77 (9.84)
90.52 (2.17)
89.71 (2.83)
49.56 (17.26)
73.26 (10.65)
88.01 (6.92)
88.08 (5.73)
59.52 (20.62)
74.35 (16.78)
90.26 (2.43)
89.48 (3.47)
58.84 (23.08)
76.10 (10.84)
88.55 (6.67)
88.75 (5.50)
42.34 (13.11)
72.58 (6.77)
91.39 (2.13)
91.06 (2.62)
55.50 (15.78)
83.98 (13.45)
51.79 (15.40)
68.33 (18.43)
86.89 (7.14)
84.71 (7.33)
43.28 (14.02)
62.63 (20.63)
86.91 (6.76)
86.18 (6.01)
36.04 (9.99)
60.03 (10.52)
87.13 (6.53)
86.31 (6.32)
43.98 (14.06)
66.93 (11.07)
87.36 (6.30)
86.44 (6.63)
44.02 (16.89)
69.70 (12.04)
86.44 (6.96)
85.93 (6.05)
47.26 (14.47)
72.62 (11.16)
87.61 (6.64)
87.21 (5.44)
45.71 (17.85)
67.76 (10.73)
87.20 (7.17)
87.00 (6.12)
41.33 (17.72)
66.26 (12.11)
88.04 (6.94)
87.82 (6.01)
49.99 (13.18)
81.08 (8.46)
43.89 (14.43)
60.95 (25.16)
74.27 (8.99)
69.87 (7.21)
50.47 (12.63)
59.55 (12.38)
69.86 (18.68)
60.71 (24.94)
N/A
77.10 (9.89)
70.12 (14.26)
43.89 (11.60)
64.67 (12.88)
78.79 (3.95)
71.44 (9.82)
47.49 (14.64)
65.16 (12.56)
81.42 (6.95)
78.65 (7.59)
42.87 (17.34)
67.09 (7.42)
81.69 (6.85)
77.12 (8.46)
49.95 (17.35)
69.71 (11.55)
82.80 (7.82)
79.37 (8.98)
43.57 (23.87)
65.46 (15.78)
83.32 (6.65)
80.24 (6.85)
55.43 (10.48)
78.24 (6.67)
32.87 (13.81)
53.68 (13.86)
71.00 (9.67)
65.89 (11.96)
65.30 (23.25)
58.57 (27.19)
70.35 (14.17)
64.00 (17.88)
74.43 (9.59)
67.55 (14.50)
81.02 (8.48)
76.85 (10.94)
49.97 (7.80)
69.10 (7.64)
77.47 (12.54)
73.99 (13.00)
41.79 (16.82)
67.37 (16.86)
83.01 (6.42)
79.77 (7.05)
41.18 (12.43)
63.16 (11.06)
83.30 (6.73)
76.77 (13.19)
48.61 (12.03)
76.39 (12.23)
36.69 (18.15)
50.53 (15.60)
MDCSA 4APS RSSI
62.32 (19.72)
58.99 (23.87)
MDCSA 4APS
68.07 (23.22)
60.01 (26.24)
MDCSA
71.25 (21.92)
68.95 (17.89)
58.59 (23.60)
57.68 (24.27)
62.36 (18.98)
57.76 (20.07)
70.47 (14.10)
64.64 (21.38)
Data
Models
Kitch-Livin
Kitch-Dinin
Dinin-Livin
Ground Truth
18.71 (18.52)
14.65 (6.03)
10.64 (11.99)
16.18 (12.08)
14.58 (10.22)
10.19 (9.46)
15.58 (8.75)
16.30 (12.94)
12.01 (13.01)
15.27 (7.51)
13.40 (6.43)
10.84 (10.81)
17.70 (16.17)
14.94 (9.71)
10.76 (9.59)
17.52 (16.97)
11.93 (10.08)
9.23 (13.69)
14.62 (16.37)
9.58 (9.16)
7.21 (10.61)
16.30 (17.78)
14.01 (8.08)
10.37 (12.44)
17.70 (17.42)
14.34 (9.48)
11.07 (13.60)
14.49 (15.28)
11.67 (11.68)
8.65 (13.06)
13.42 (14.88)
10.87 (10.37)
6.95 (10.28)
16.98 (15.15)
15.26 (8.85)
9.99 (13.03)
16.42 (14.04)
14.48 (9.81)
10.77 (14.18)
14.22 (18.03)
11.38 (15.46)
13.43 (18.87)
10.75 (15.67)
8.59 (14.39)
16.89 (18.07)
14.68 (13.57)
9.31 (15.70)
18.15 (19.12)
15.32 (14.93)
11.89 (17.55)
11.52 (16.07)
8.73 (12.90)
8.75 (14.89)
14.75 (13.79)
13.47 (17.66)
17.96 (19.17)
14.74 (10.83)
10.16 (14.03)
OFF transitions
Mean transition duration
ON transitions
W
z
Kitchen-Living OFF
17.2 sec
Kitchen-Living ON
14.0 sec
75.0
2.824
Dining-Kitchen OFF
12.9 sec
Dining-Kitchen ON
9.2 sec
76.0
2.903
Dining-Living OFF
10.4 sec
Dining-Living ON
9.0 sec
64.0
1.961"
R012,1,NEURIPS,"This paper presents an approach for designing neural networks  along with other
machine learning models  which adhere to a collection of input-output specifica-
tions. Our method involves the construction of a constrained predictor for each set
of compatible constraints  and combining these predictors in a safe manner using a
convex combination of their predictions. We demonstrate the applicability of this
method with synthetic datasets and on an aircraft collision avoidance problem.
1","The increasing adoption of machine learning models  such as neural networks  in safety-critical
applications  such as autonomous vehicles and aircraft collision avoidance  highlights an urgent
need for the development of guarantees on safety and robustness. These models may be required
to satisfy specific input-output specifications to ensure the algorithms comply with physical laws
can be executed safely  and are consistent with prior domain knowledge. Furthermore  these models
should demonstrate adversarial robustness  meaning their outputs should not change abruptly within
small input regions – a property that neural networks often fail to satisfy.
Recent studies have shown the capacity to verify formally input-output specifications and adversarial
robustness properties of neural networks. For instance  the Satisability Modulo Theory (SMT) solver
Reluplex was employed to verify properties of networks being used in the Next-Generation Aircraft
Collision Avoidance System for Unmanned aircraft (ACAS Xu). Reluplex has also been used to
verify adversarial robustness. While Reluplex and other similar techniques can effectively determine
if a network satisfies a given specification  they do not offer a way to guarantee that the network will
meet those specifications. Therefore  additional","that they are not meeting the desired properties.
There has been an increase in techniques for designing networks with certified adversarial robustness
but enforcing more general safety properties in neural networks is still largely unexplored. One ap-
proach to achieving provably correct neural networks is through abstraction-refinement optimization.
This approach has been applied to the ACAS-Xu dataset  but the network was not guaranteed to meet
the specifications until after training. Our work seeks to design networks with enforced input-output
constraints even before training has been completed. This will allow for online learning scenarios
where a system has to guarantee safety throughout its operation.
This paper presents an approach for designing a safe predictor (a neural network or any other
machine learning model) that will always meet a set of constraints on the input-output relationship.
This assumes that the constrained output regions can be formulated to be convex. Our correct-
by-construction safe predictor is guaranteed to satisfy the constraints  even before training  and at
every training step. We describe our approach in Section 2  and show its use in an aircraft collision
avoidance problem in Section 3.",".
2 Method
Considering two normed vector spaces  an input space X and an output space Y   and a collection
of c different pairs of input-output constraints  (Ai  Bi)  where Ai⊆XandBiis a convex subset
ofYfor each constraint i  the goal is to design a safe predictor  F:X→Y  that guarantees
x∈Ai⇒F(x)∈Bi.
Letbbe a bit-string of length c. Define Obas the set of points zsuch that  for all i bi= 1implies
z∈Ai  and bi= 0implies z /∈Ai.Obthus represents the overlap regions for each combination of
input constraints. For example  O101is the set of points in A1andA3  but not in A2  and O0...0is
the set where no input constraints apply. We also define Oas the set of bit strings  b  such that Ob
is non-empty  and define k=|O|. The sets {Ob:b∈O}create a partition of Xaccording to the
combination of input constraints that apply.
Given:
•c different input constraint proximity functions  σi:X→[0 1]  where σiis continuous and
∀x∈Ai σi(x) = 0
•kdifferent constrained predictors  Gb:X→Bb  one for each b∈O  such that the domain
of each Gbis non-empty
We define:
•a set of weighting functions  wb(x) =Q
i:bi=1(1−σi(x))Q
i:bi=0σi(x)P
b∈OQ
i:bi=0σi(x)  where
P
b∈Owb(x) = 1   and
• a safe predictor  F(x) =P
b∈Owb(x)Gb(x).
Theorem 2.1. For all i  ifx∈Ai  then F(x)∈Bi.
A formal proof of Theorem 2.1 is presented in Appendix A and can be summarized as: if an input is
inAi  then by construction of the proximity and weighting functions  all of the constrained predictors
Gb  that do not map to Biwill be given zero weight. Only the constrained predictors that map to
Biwill be given non-zero weight  and because of the convexity of Bi  the weighted average of the
predictions will remain in Bi.
If all Gbare continuous and if there are no two input sets  AiandAj  for which (Ai∩Aj)⊂
(∂Ai∪∂Aj)  then Fwill be continuous. In the worst case  as the number of constraints grows linearly
the number of constrained predictors needed to describe our safe predictor grows exponentially. In
practice  however  we expect many of the constraint overlap sets  Ob  to be empty. Consequently  any
predictors corresponding to an empty set can be ignored. This significantly reduces the number of
constrained predictors needed for many applications.
See Figure 1 for an illustrative example of how to construct F(x)for a notional problem with two
overlapping input-output constraints.
2.1 Proximity Functions
The proximity functions  σi  describe how close an input  x  is to a particular input constraint region
Ai. These functions are used to compute the weights of the constrained predictors. A desirable
property for σiis for σi(x)→1asd(x  Ai)→ ∞   for some distance function. This ensures that
when an input is far from a constraint region  that constraint has little influence on the prediction for
that input. A natural choice for such a function is:
σi(x; Σi) = 1−exp
−d(x  Ai)
σ1σ2
Here  Σiis a set of parameters σ1∈(0 ∞)andσ2∈(1 ∞)  which can be specified based on
engineering judgment  or learned using optimization over training data. In our experiments in
this paper  we use proximity functions of this form and learn independent parameters for each
input-constrained region. We plan to explore other choices for proximity functions in future work.
2
2.2 Learning
If we have families of differentiable functions Gb(x;θb)  continuously parameterized by θb  and
families of σi(x;χi)  differentiable and continuously parameterized by χi  then F(x; Θ  X)  where
Θ ={θb:b∈O}andX={χi:i= 1  ...  c}  is also continuously parameterized and differentiable.
We can thus apply standard optimization techniques (e.g.  gradient descent) to find parameters of F
that minimize a loss function on some dataset  while also preserving the desired safety properties.
Note that the safety guarantee holds regardless of the parameters. To create each Gb(x;θb)we
consider choosing:
• a latent space Rm
• a map hb:Rm→Bb
• a standard neural network architecture gb:X→Rm
and then defining Gb(x;θb) =hb(gb(x;θb)).
The framework proposed here does not require an entirely separate network for each b. In many
applications  it may be advantageous for the constrained predictors to share earlier layers  thus
creating a shared representation of the input space. In addition  our definition of the safe predictor is
general and is not limited to neural networks.
In Appendix B  we show examples of applying our approach to synthetic datasets in 2-D and 3-D
with simple neural networks. These examples show that our safe predictor can enforce arbitrary
input-output specifications using convex output constraints on neural networks  and that the learned
function is smooth.
3 Application to Aircraft Collision Avoidance
Aircraft collision avoidance requires robust safety guarantees. The Next-Generation Collision
Avoidance System (ACAS X)  which issues advisories to prevent near mid-air collisions  has both
manned (ACAS Xa) and unmanned (ACAS Xu) variants. The system was originally designed to
choose optimal advisories while minimizing disruptive alerts by solving a partially observable Markov
decision process. The solution took the form of a large look-up table  mapping each possible input
combination to scores for all possible advisories. The advisory with the highest score would then be
issued. By using a deep neural network (DNN) to compress the policy tables  it has been necessary to
verify that the DNNs meet certain safety specifications.
A desirable ˘201csafeability ˘201d property for ACAS X was defined in a previous work. This property
speci01ed that for any given input state within the ˘201csafeable region  ˘201d an advisory would never
be issued that could put the aircraft into a state where a safe advisory would no longer exist. This
concept is similar to control invariance. A simplified model of the ACAS Xa system was created
named VerticalCAS. DNNs were then generated to approximate the learned policy  and Reluplex was
used to verify whether the DNNs satisfied the safeability property. This work found thousands of
counterexamples where the DNNs did not meet the criteria.
Our approach for designing a safe predictor ensures any collision avoidance system will meet the
safeability property by construction. Appendix C describes in detail how we apply our approach to
a subset of the VerticalCAS datasets using a conservative  convex approximation of the safeability
constraints. These constraints are defined such that if an aircraft state is in the 'unsafeable region'
Aunsafeable  i  for the ithadvisory  the score for that advisory must not be the highest  i.e.  x∈
Aunsafeable  i⇒Fi(x)<max jFj(x)  where Fj(x)is the output score for the jthadvisory.
Table 1 shows the performance of a standard  unconstrained network and our safe predictor. For both
networks  we present the percentage accuracy (ACC) and violations (percentage of inputs for which
the network outputs an unsafe advisory). We train and test using PyTorch with two separate datasets
based on the previous advisory being Clear of Conflict (COC) and Climb at 1500 ft/min (CL1500).
As shown in the table  our safe predictor adheres to the required safeability property. Furthermore
the accuracy of our predictor remains the same as the unconstrained network  demonstrating we are
not losing accuracy to achieve safety guarantees.
3
Table 1: Results of the best configurations of β-TCV AE on DCI  FactorV AE  SAP  MIG  and IRS
metrics.
NETWORK ACC (COC) VIOLATIONS (COC) ACC (CL1500) VIOLATIONS (CL1500)
STANDARD 96.87 0.22 93.89 0.20
SAFE 96.69 0.00 94.78 0.00
4","We propose an approach for designing a safe predictor that adheres to input-output specifications for
use in safety-critical machine learning systems  demonstrating it on an aircraft collision avoidance
problem. The novelty of our approach is its simplicity and guaranteed enforcement of specifications
through combinations of convex output constraints during all stages of training. Future work includes
adapting and using techniques from optimization and control barrier functions  as well as incorporating
notions of adversarial robustness into our design  such as extending the work to bound the Lipschitz
constant of our networks.
Appendix A: Proof of Theorem 2.1
Proof. Fixiand assume that x∈Ai. It follows that σi(x) = 0   so for all b∈Owhere bi= 0
wb(x) = 0 . Thus
F(x) =X
b∈O bi=1wb(x)Gb(x).
Ifbi= 1 Gb(x)∈Bi  and thus F(x)is also in Biby the convexity of Bi.
Appendix B: Example on Synthetic Datasets
Figure 2 depicts an example of applying our safe predictor to a notional regression problem. This
example uses inputs and outputs in 1-D with one input-output constraint. The unconstrained network
consists of a single hidden layer with a dimension of 10  ReLU activations  and a fully connected layer.
The safe predictor shares this structure with the unconstrained network but has its own fully connected
layer for the constrained predictors  G0andG1. Training uses a sampled subset of points from
the input space. Figure 3 shows an example of applying our safe predictor to a notional regression
problem with a 2-D input and 1-D output  using two overlapping constraints. The unconstrained
network has two hidden layers of dimension 20 and ReLU activations  followed by a fully connected
layer. The constrained predictors  G00 G10 G01  and G11  share the hidden layers but also have an
additional hidden layer of size 20 with ReLU  followed by a fully connected layer. Training uses a
sampled subset of points from the input space.
Appendix C: Details of VerticalCAS Experiment
C.1 Safeability Constraints
The 'safeability' property  originally introduced and used to verify the safety of the VerticalCAS
neural networks can be encoded into a set of input-output constraints. The 'safeable region' for
a given advisory represents input locations where that advisory can be selected such that future
advisories exist that will prevent an NMAC. If no future advisories exist  the advisory is 'unsafeable'
and the corresponding input region is the 'unsafeable region'. Examples of these regions  and their
proximity functions are shown in Figure 5 for the CL1500 advisory.
The constraints we enforce are that x∈Aunsafeable  i⇒Fi(x)<max jFj(x) ∀i  where Aunsafeable  iis
the unsafeable region for the ithadvisory  and Fj(x)is the output score for the jthadvisory. Because
the output regions of the safeable constraints are not convex  we make a conservative approximation
enforcing Fi(x) = min jFj(x)  for all x∈Aunsafeable  i.
4
C.2 Proximity Functions
We start by generating the unsafeable region bounds from the open source code. We then compute a
'distance function' between input space points (vO - vI  h  τ)  and the unsafeable region for each
advisory. These are not true distances but are 0 if and only if the data point is within the unsafeable
set. These are then used to produce proximity functions as given in Equation 1.
C.3 Structure of Predictors
The compressed policy tables for ACAS Xu and VerticalCAS use neural networks with six hidden
layers with a dimension of 45  and ReLU activation functions. We used the same architecture for the
unconstrained network. For our constrained predictors  we use the same structure but have shared
first four layers for all predictors. This provides a common learned representation of the input space
while allowing each predictor to adapt to its own constraints. After the shared layers  each constrained
predictor has an additional two hidden layers and their final outputs are projected onto our convex
approximation of the safe region of the output space  using Gb(x) = min jGj(x). In our experiments
we set ϵ= 0.0001 .
With this construction  we needed 30 separate predictors to enforce the VerticalCAS safeability
constraints. The number of nodes for the unconstrained and safe implementations were 270 and 2880
respectively. Our safe predictor is orders of magnitude smaller than the original look-up tables.
C.4 Parameter Optimization
We use PyTorch for defining our networks and performing parameter optimization. We optimize both
the unconstrained and safe predictors using the asymmetric loss function to select advisories while
also accurately predicting scores. The data is split using an 80/20 train/test split with a random seed
of 0. The optimizer is ADAM with a learning rate of 0.0003 and batch size of 216  with training for
500 epochs.
Proof. Letx∈Ai. Then  σi(x) = 0   and for all b∈Owhere bi= 0 wb(x) = 0 . Thus
b∈O bi=1wb(x)Gb(x)
Ifbi= 1  then Gb(x)∈Bi  and therefore F(x)is inBidue to the convexity of Bi.
Figure 2 depicts an example of applying our safe predictor to a notional regression problem with 1-D
input and outputs  and one input-output constraint. The unconstrained network has a single hidden
layer of dimension 10 with ReLU activations  followed by a fully connected layer. The safe predictor
shares this structure with constrained predictors  G0andG1  but each predictor has its own fully
connected layer. The training uses a sampled subset of points from the input space and the learned
predictors are shown for the continuous input space.
Figure 3 shows an example of applying the safe predictor to a notional regression problem with a 2-D
input and 1-D output and two overlapping constraints. The unconstrained network has two hidden
layers of dimension 20 with ReLU activations  followed by a fully connected layer. The constrained
predictors G00 G10 G01andG11share the hidden layers and have an additional hidden layer of size
20 with ReLU followed by a fully connected layer. Again  training uses a sampled subset of points
from the input space and the learned predictors are shown for the continuous input space.
5
The “safeability” property from prior work can be encoded into a set of input-output constraints. The
“safeable region” for a given advisory is the set of input space locations where that advisory can be
chosen  for which future advisories exist that will prevent an NMAC. If no future advisories exist for
preventing an NMAC  the advisory is deemed “unsafeable ” and the corresponding input region is the
“unsafeable region.” Figure 5 shows an example of these regions for the CL1500 advisory.
The constraints we enforce in our safe predictor are: x∈Aunsafeable  i⇒Fi(x)<max jFj(x)
∀i. To make the output regions convex  we approximate by enforcing Fi(x) = min jFj(x)  for all
x∈Aunsafeable  i.
We start by generating the bounds on the unsafeable regions. Then  a distance function is computed
between points in the input space ( vO−vI  h τ)  and the unsafeable region for each advisory. While
these are not true distances  their values are 0 if and only if the data point is inside the unsafeable set.
When used to produce proximity functions as given in Equation 1  these values help ensure safety.
Figure 5 shows examples of the unsafeable region  distance function  and proximity function for the
CL1500 advisory.
The compressed versions of the policy tables from prior work are neural networks with six hidden
layers  45 dimensions in each layer  and ReLU activation functions. We use the same architecture
for our standard  unconstrained network. For constrained predictors  we use a similar architecture.
However  the first four hidden layers are shared between all of the predictors. This learns a single
shared input space representation  and also allows each predictor to adapt to its constraints. Each
constrained predictor has two additional hidden layers and their outputs are projected onto our convex
approximation of the safe output region. We accomplish this by setting the score for any unsafeable
advisory itoGi(x) = min jGj(x)−ϵ. In our experiments  we used ϵ= 0.0001 .
To enforce the VerticalCAS safeability constraints  we need 30 separate predictors. This increases
the size of the network from 270 to 2880 nodes for the unconstrained and safe implementations
respectively. However  our safe predictor remains smaller than the original look-up tables by several
orders of magnitude.
We define our networks and perform parameter optimization using PyTorch. We optimize the
parameters of both the unconstrained network and our safe predictor using the asymmetric loss
function  guiding the network to select optimal advisories while accurately predicting scores from
the look-up tables. Each dataset is split using an 80/20 train/test split  with a random seed of 0. The
optimizer is ADAM  with a learning rate of 0.0003  a batch size of 216  and the number of training
epochs is 500.
6
Thispaperpresentsanapproachfordesigningneuralnetworks alongwithother
machinelearningmodels whichadheretoacollectionofinput-outputspecifica-
tions. Ourmethodinvolvestheconstructionofaconstrainedpredictorforeachset
ofcompatibleconstraints andcombiningthesepredictorsinasafemannerusinga
convexcombinationoftheirpredictions. Wedemonstratetheapplicabilityofthis
methodwithsyntheticdatasetsandonanaircraftcollisionavoidanceproblem.
needforthedevelopmentofguaranteesonsafetyandrobustness. Thesemodelsmayberequired
tosatisfyspecificinput-outputspecificationstoensurethealgorithmscomplywithphysicallaws
canbeexecutedsafely andareconsistentwithpriordomainknowledge. Furthermore thesemodels
shoulddemonstrateadversarialrobustness meaningtheiroutputsshouldnotchangeabruptlywithin
smallinputregions–apropertythatneuralnetworksoftenfailtosatisfy.
Recentstudieshaveshownthecapacitytoverifyformallyinput-outputspecificationsandadversarial
robustnesspropertiesofneuralnetworks. Forinstance theSatisabilityModuloTheory(SMT)solver
ReluplexwasemployedtoverifypropertiesofnetworksbeingusedintheNext-GenerationAircraft
CollisionAvoidanceSystemforUnmannedaircraft(ACASXu). Reluplexhasalsobeenusedto
verifyadversarialrobustness. WhileReluplexandothersimilartechniquescaneffectivelydetermine
ifanetworksatisfiesagivenspecification theydonotofferawaytoguaranteethatthenetworkwill
meetthosespecifications. Therefore additionalmethodsareneededtoadjustnetworksifitisfound
thattheyarenotmeetingthedesiredproperties.
Therehasbeenanincreaseintechniquesfordesigningnetworkswithcertifiedadversarialrobustness
butenforcingmoregeneralsafetypropertiesinneuralnetworksisstilllargelyunexplored. Oneap-
proachtoachievingprovablycorrectneuralnetworksisthroughabstraction-refinementoptimization.
ThisapproachhasbeenappliedtotheACAS-Xudataset butthenetworkwasnotguaranteedtomeet
thespecificationsuntilaftertraining. Ourworkseekstodesignnetworkswithenforcedinput-output
constraintsevenbeforetraininghasbeencompleted. Thiswillallowforonlinelearningscenarios
whereasystemhastoguaranteesafetythroughoutitsoperation.
machinelearningmodel)thatwillalwaysmeetasetofconstraintsontheinput-outputrelationship.
by-constructionsafepredictorisguaranteedtosatisfytheconstraints evenbeforetraining andat
everytrainingstep. WedescribeourapproachinSection2 andshowitsuseinanaircraftcollision
avoidanceprobleminSection3. ResultsonsyntheticdatasetscanbefoundinAppendixB.
Consideringtwonormedvectorspaces aninputspaceXandanoutputspaceY andacollection
ofcdifferentpairsofinput-outputconstraints (A  B ) whereA ⊆X andB isaconvexsubset
i i i i
of Y for each constraint i  the goal is to design a safe predictor  F : X → Y  that guarantees
x∈A ⇒F(x)∈B .
i i
Letbbeabit-stringoflengthc. DefineO asthesetofpointszsuchthat foralli b =1implies
b i
z ∈A  andb =0impliesz ∈/ A . O thusrepresentstheoverlapregionsforeachcombinationof
i i i b
inputconstraints. Forexample O isthesetofpointsinA andA  butnotinA  andO is
101 1 3 2 0...0
thesetwherenoinputconstraintsapply. WealsodefineOasthesetofbitstrings b suchthatO
b
isnon-empty anddefinek = |O|. Thesets{O : b ∈ O}createapartitionofX accordingtothe
combinationofinputconstraintsthatapply.
• cdifferentinputconstraintproximityfunctions σ :X →[0 1] whereσ iscontinuousand
∀x∈A  σ (x)=0
• kdifferentconstrainedpredictors G :X →B  oneforeachb∈O suchthatthedomain
b b
ofeachG isnon-empty
Wedefine:
• a(cid:80)set of weighting functions  wb(x) = (cid:80)b∈(cid:81)Oi:(cid:81)bi=i:1bi(=1−1(σ1i−(xσ)i)(x(cid:81)))i:(cid:81)bi=i:0biσ=i0(xσ)i(x)  where
w (x)=1 and
b∈O b
(cid:80)
• asafepredictor F(x)= w (x)G (x).
b∈O b b
Theorem2.1. Foralli ifx∈A  thenF(x)∈B .
AformalproofofTheorem2.1ispresentedinAppendixAandcanbesummarizedas: ifaninputis
inA  thenbyconstructionoftheproximityandweightingfunctions alloftheconstrainedpredictors
i
G  thatdonotmaptoB willbegivenzeroweight. Onlytheconstrainedpredictorsthatmapto
B willbegivennon-zeroweight andbecauseoftheconvexityofB  theweightedaverageofthe
predictionswillremaininB .
If all G are continuous and if there are no two input sets  A and A   for which (A ∩A ) ⊂
b i j i j
(∂A ∪∂A ) thenF willbecontinuous.Intheworstcase asthenumberofconstraintsgrowslinearly
i j
thenumberofconstrainedpredictorsneededtodescribeoursafepredictorgrowsexponentially. In
practice however weexpectmanyoftheconstraintoverlapsets O  tobeempty. Consequently any
predictorscorrespondingtoanemptysetcanbeignored. Thissignificantlyreducesthenumberof
constrainedpredictorsneededformanyapplications.
SeeFigure1foranillustrativeexampleofhowtoconstructF(x)foranotionalproblemwithtwo
overlappinginput-outputconstraints.
2.1 ProximityFunctions
Theproximityfunctions σ  describehowcloseaninput x istoaparticularinputconstraintregion
A . These functions are used to compute the weights of the constrained predictors. A desirable
propertyforσ isforσ (x) → 1asd(x A ) → ∞ forsomedistancefunction. Thisensuresthat
i i i
whenaninputisfarfromaconstraintregion thatconstrainthaslittleinfluenceonthepredictionfor
thatinput. Anaturalchoiceforsuchafunctionis:
(cid:18) d(x A )(cid:19)σ2
σ (x;Σ )=1−exp − i .
i i σ
1
Here  Σ is a set of parameters σ ∈ (0 ∞) and σ ∈ (1 ∞)  which can be specified based on
i 1 2
input-constrainedregion. Weplantoexploreotherchoicesforproximityfunctionsinfuturework.
If we have families of differentiable functions G (x;θ )  continuously parameterized by θ   and
b b b
familiesofσ (x;χ ) differentiableandcontinuouslyparameterizedbyχ  thenF(x;Θ X) where
Θ={θ :b∈O}andX ={χ :i=1 ... c} isalsocontinuouslyparameterizedanddifferentiable.
Wecanthusapplystandardoptimizationtechniques(e.g. gradientdescent)tofindparametersofF
thatminimizealossfunctiononsomedataset whilealsopreservingthedesiredsafetyproperties.
Note that the safety guarantee holds regardless of the parameters. To create each G (x;θ ) we
considerchoosing:
• alatentspaceRm
• amaph :Rm →B
• astandardneuralnetworkarchitectureg :X →Rm
andthendefiningG (x;θ )=h (g (x;θ )).
b b b b b
Theframeworkproposedheredoesnotrequireanentirelyseparatenetworkforeachb. Inmany
creatingasharedrepresentationoftheinputspace. Inaddition ourdefinitionofthesafepredictoris
generalandisnotlimitedtoneuralnetworks.
InAppendixB weshowexamplesofapplyingourapproachtosyntheticdatasetsin2-Dand3-D
input-outputspecificationsusingconvexoutputconstraintsonneuralnetworks andthatthelearned
functionissmooth.
3 ApplicationtoAircraftCollisionAvoidance
AvoidanceSystem(ACASX) whichissuesadvisoriestopreventnearmid-aircollisions hasboth
manned(ACASXa)andunmanned(ACASXu)variants. Thesystemwasoriginallydesignedto
chooseoptimaladvisorieswhileminimizingdisruptivealertsbysolvingapartiallyobservableMarkov
decisionprocess. Thesolutiontooktheformofalargelook-uptable mappingeachpossibleinput
combinationtoscoresforallpossibleadvisories. Theadvisorywiththehighestscorewouldthenbe
issued. Byusingadeepneuralnetwork(DNN)tocompressthepolicytables ithasbeennecessaryto
verifythattheDNNsmeetcertainsafetyspecifications.
Adesirable2˘01csafeability2˘01dpropertyforACASXwasdefinedinapreviouswork. Thisproperty
speci01edthatforanygiveninputstatewithinthe2˘01csafeableregion 2˘01danadvisorywouldnever
beissuedthatcouldputtheaircraftintoastatewhereasafeadvisorywouldnolongerexist. This
conceptissimilartocontrolinvariance. AsimplifiedmodeloftheACASXasystemwascreated
namedVerticalCAS.DNNswerethengeneratedtoapproximatethelearnedpolicy andReluplexwas
usedtoverifywhethertheDNNssatisfiedthesafeabilityproperty. Thisworkfoundthousandsof
counterexampleswheretheDNNsdidnotmeetthecriteria.
Ourapproachfordesigningasafepredictorensuresanycollisionavoidancesystemwillmeetthe
safeabilitypropertybyconstruction. AppendixCdescribesindetailhowweapplyourapproachto
asubsetoftheVerticalCASdatasetsusingaconservative convexapproximationofthesafeability
constraints. Theseconstraintsaredefinedsuchthatifanaircraftstateisinthe'unsafeableregion'
A   for the ith advisory  the score for that advisory must not be the highest  i.e.  x ∈
unsafeable i
A ⇒F (x)<max F (x) whereF (x)istheoutputscoreforthejthadvisory.
unsafeable i i j j j
Table1showstheperformanceofastandard unconstrainednetworkandoursafepredictor. Forboth
networks wepresentthepercentageaccuracy(ACC)andviolations(percentageofinputsforwhich
thenetworkoutputsanunsafeadvisory). WetrainandtestusingPyTorchwithtwoseparatedatasets
basedonthepreviousadvisorybeingClearofConflict(COC)andClimbat1500ft/min(CL1500).
Asshowninthetable oursafepredictoradherestotherequiredsafeabilityproperty. Furthermore
theaccuracyofourpredictorremainsthesameastheunconstrainednetwork demonstratingweare
notlosingaccuracytoachievesafetyguarantees.
Table1: Resultsofthebestconfigurationsofβ-TCVAEonDCI FactorVAE SAP MIG andIRS
NETWORK ACC(COC) VIOLATIONS(COC) ACC(CL1500) VIOLATIONS(CL1500)
4 DiscussionandFutureWork
Weproposeanapproachfordesigningasafepredictorthatadherestoinput-outputspecificationsfor
useinsafety-criticalmachinelearningsystems demonstratingitonanaircraftcollisionavoidance
problem. Thenoveltyofourapproachisitssimplicityandguaranteedenforcementofspecifications
throughcombinationsofconvexoutputconstraintsduringallstagesoftraining. Futureworkincludes
adaptingandusingtechniquesfromoptimizationandcontrolbarrierfunctions aswellasincorporating
notionsofadversarialrobustnessintoourdesign suchasextendingtheworktoboundtheLipschitz
constantofournetworks.
AppendixA:ProofofTheorem2.1
Proof. Fix i and assume that x ∈ A . It follows that σ (x) = 0  so for all b ∈ O where b = 0
w (x)=0. Thus
(cid:88)
F(x)= w (x)G (x).
b∈O bi=1
Ifb =1 G (x)∈B  andthusF(x)isalsoinB bytheconvexityofB .
i b i i i
AppendixB:ExampleonSyntheticDatasets
Figure2depictsanexampleofapplyingoursafepredictortoanotionalregressionproblem. This
exampleusesinputsandoutputsin1-Dwithoneinput-outputconstraint. Theunconstrainednetwork
consistsofasinglehiddenlayerwithadimensionof10 ReLUactivations andafullyconnectedlayer.
Thesafepredictorsharesthisstructurewiththeunconstrainednetworkbuthasitsownfullyconnected
layer for the constrained predictors  G and G . Training uses a sampled subset of points from
0 1
theinputspace. Figure3showsanexampleofapplyingoursafepredictortoanotionalregression
problemwitha2-Dinputand1-Doutput usingtwooverlappingconstraints. Theunconstrained
networkhastwohiddenlayersofdimension20andReLUactivations followedbyafullyconnected
layer. Theconstrainedpredictors G  G  G  andG  sharethehiddenlayersbutalsohavean
00 10 01 11
additionalhiddenlayerofsize20withReLU followedbyafullyconnectedlayer. Trainingusesa
sampledsubsetofpointsfromtheinputspace.
AppendixC:DetailsofVerticalCASExperiment
C.1SafeabilityConstraints
The'safeability'property originallyintroducedandusedtoverifythesafetyoftheVerticalCAS
advisoriesexistthatwillpreventanNMAC.Ifnofutureadvisoriesexist theadvisoryis'unsafeable'
andthecorrespondinginputregionisthe'unsafeableregion'. Examplesoftheseregions andtheir
proximityfunctionsareshowninFigure5fortheCL1500advisory.
Theconstraintsweenforcearethatx∈A ⇒F (x)<max F (x) ∀i whereA is
unsafeable i i j j unsafeable i
theunsafeableregionfortheithadvisory andF (x)istheoutputscoreforthejthadvisory. Because
j
theoutputregionsofthesafeableconstraintsarenotconvex wemakeaconservativeapproximation
enforcingF (x)=min F (x) forallx∈A .
i j j unsafeable i
C.2ProximityFunctions
Westartbygeneratingtheunsafeableregionboundsfromtheopensourcecode. Wethencomputea
'distancefunction'betweeninputspacepoints(vO-vI h τ) andtheunsafeableregionforeach
advisory. Thesearenottruedistancesbutare0ifandonlyifthedatapointiswithintheunsafeable
set. ThesearethenusedtoproduceproximityfunctionsasgiveninEquation1.
C.3StructureofPredictors
ThecompressedpolicytablesforACASXuandVerticalCASuseneuralnetworkswithsixhidden
layerswithadimensionof45 andReLUactivationfunctions. Weusedthesamearchitectureforthe
unconstrainednetwork. Forourconstrainedpredictors weusethesamestructurebuthaveshared
firstfourlayersforallpredictors. Thisprovidesacommonlearnedrepresentationoftheinputspace
whileallowingeachpredictortoadapttoitsownconstraints.Afterthesharedlayers eachconstrained
predictorhasanadditionaltwohiddenlayersandtheirfinaloutputsareprojectedontoourconvex
approximationofthesaferegionoftheoutputspace usingG (x)=min G (x).Inourexperiments
b j j
wesetϵ=0.0001.
constraints. Thenumberofnodesfortheunconstrainedandsafeimplementationswere270and2880
respectively. Oursafepredictorisordersofmagnitudesmallerthantheoriginallook-uptables.
C.4ParameterOptimization
WeusePyTorchfordefiningournetworksandperformingparameteroptimization. Weoptimizeboth
theunconstrainedandsafepredictorsusingtheasymmetriclossfunctiontoselectadvisorieswhile
alsoaccuratelypredictingscores. Thedataissplitusingan80/20train/testsplitwitharandomseed
of0. TheoptimizerisADAMwithalearningrateof0.0003andbatchsizeof216 withtrainingfor
500epochs.
Proof. Letx∈A . Then σ (x)=0 andforallb∈Owhereb =0 w (x)=0. Thus
F(x)= w (x)G (x)
Ifb =1 thenG (x)∈B  andthereforeF(x)isinB duetotheconvexityofB .
Figure2depictsanexampleofapplyingoursafepredictortoanotionalregressionproblemwith1-D
inputandoutputs andoneinput-outputconstraint. Theunconstrainednetworkhasasinglehidden
layerofdimension10withReLUactivations followedbyafullyconnectedlayer. Thesafepredictor
sharesthisstructurewithconstrainedpredictors G andG  buteachpredictorhasitsownfully
connectedlayer. Thetrainingusesasampledsubsetofpointsfromtheinputspaceandthelearned
predictorsareshownforthecontinuousinputspace.
Figure3showsanexampleofapplyingthesafepredictortoanotionalregressionproblemwitha2-D
inputand1-Doutputandtwooverlappingconstraints. Theunconstrainednetworkhastwohidden
layersofdimension20withReLUactivations followedbyafullyconnectedlayer. Theconstrained
predictorsG  G  G andG sharethehiddenlayersandhaveanadditionalhiddenlayerofsize
20withReLUfollowedbyafullyconnectedlayer. Again trainingusesasampledsubsetofpoints
fromtheinputspaceandthelearnedpredictorsareshownforthecontinuousinputspace.
The“safeability”propertyfrompriorworkcanbeencodedintoasetofinput-outputconstraints. The
“safeableregion”foragivenadvisoryisthesetofinputspacelocationswherethatadvisorycanbe
chosen forwhichfutureadvisoriesexistthatwillpreventanNMAC.Ifnofutureadvisoriesexistfor
preventinganNMAC theadvisoryisdeemed“unsafeable ”andthecorrespondinginputregionisthe
“unsafeableregion.”Figure5showsanexampleoftheseregionsfortheCL1500advisory.
The constraints we enforce in our safe predictor are: x ∈ A ⇒ F (x) < max F (x)
unsafeable i i j j
∀i. Tomaketheoutputregionsconvex weapproximatebyenforcingF (x)=min F (x) forall
i j j
x∈A .
Westartbygeneratingtheboundsontheunsafeableregions. Then adistancefunctioniscomputed
betweenpointsintheinputspace(v −v  h τ) andtheunsafeableregionforeachadvisory. While
O I
thesearenottruedistances theirvaluesare0ifandonlyifthedatapointisinsidetheunsafeableset.
WhenusedtoproduceproximityfunctionsasgiveninEquation1 thesevalueshelpensuresafety.
Figure5showsexamplesoftheunsafeableregion distancefunction andproximityfunctionforthe
CL1500advisory.
Thecompressedversionsofthepolicytablesfrompriorworkareneuralnetworkswithsixhidden
layers 45dimensionsineachlayer andReLUactivationfunctions. Weusethesamearchitecture
forourstandard unconstrainednetwork. Forconstrainedpredictors weuseasimilararchitecture.
However thefirstfourhiddenlayersaresharedbetweenallofthepredictors. Thislearnsasingle
sharedinputspacerepresentation andalsoallowseachpredictortoadapttoitsconstraints. Each
constrainedpredictorhastwoadditionalhiddenlayersandtheiroutputsareprojectedontoourconvex
approximationofthesafeoutputregion. Weaccomplishthisbysettingthescoreforanyunsafeable
advisoryitoG (x)=min G (x)−ϵ. Inourexperiments weusedϵ=0.0001.
ToenforcetheVerticalCASsafeabilityconstraints weneed30separatepredictors. Thisincreases
respectively. However oursafepredictorremainssmallerthantheoriginallook-uptablesbyseveral
ordersofmagnitude.
function guidingthenetworktoselectoptimaladvisorieswhileaccuratelypredictingscoresfrom
thelook-uptables. Eachdatasetissplitusingan80/20train/testsplit witharandomseedof0. The
optimizerisADAM withalearningrateof0.0003 abatchsizeof216 andthenumberoftraining
epochsis500.
Introduction
Method
of c different pairs of input-output constraints  (Ai  Bi)  where Ai ⊆ X and Bi is a convex subset
of Y for each constraint i  the goal is to design a safe predictor  F : X → Y   that guarantees
x ∈ Ai ⇒ F(x) ∈ Bi.
Let b be a bit-string of length c. Define Ob as the set of points z such that  for all i  bi = 1 implies
z ∈ Ai  and bi = 0 implies z /∈ Ai. Ob thus represents the overlap regions for each combination of
input constraints. For example  O101 is the set of points in A1 and A3  but not in A2  and O0...0 is
the set where no input constraints apply. We also define O as the set of bit strings  b  such that Ob
is non-empty  and define k = |O|. The sets {Ob : b ∈ O} create a partition of X according to the
• c different input constraint proximity functions  σi : X → [0  1]  where σi is continuous and
∀x ∈ Ai  σi(x) = 0
• k different constrained predictors  Gb : X → Bb  one for each b ∈ O  such that the domain
of each Gb is non-empty
• a set of weighting functions  wb(x)
=
�
i:bi=1(1−σi(x)) �
i:bi=0 σi(x)
b∈O
i:bi=0 σi(x)  where
b∈O wb(x) = 1  and
• a safe predictor  F(x) = �
b∈O wb(x)Gb(x).
Theorem 2.1. For all i  if x ∈ Ai  then F(x) ∈ Bi.
in Ai  then by construction of the proximity and weighting functions  all of the constrained predictors
Gb  that do not map to Bi will be given zero weight. Only the constrained predictors that map to
Bi will be given non-zero weight  and because of the convexity of Bi  the weighted average of the
If all Gb are continuous and if there are no two input sets  Ai and Aj  for which (Ai ∩ Aj) ⊂
(∂Ai∪∂Aj)  then F will be continuous. In the worst case  as the number of constraints grows linearly
See Figure 1 for an illustrative example of how to construct F(x) for a notional problem with two
2.1
Proximity Functions
property for σi is for σi(x) → 1 as d(x  Ai) → ∞  for some distance function. This ensures that
σi(x; Σi) = 1 − exp
σ1
�σ2
Here  Σi is a set of parameters σ1 ∈ (0  ∞) and σ2 ∈ (1  ∞)  which can be specified based on
2.2
Learning
If we have families of differentiable functions Gb(x; θb)  continuously parameterized by θb  and
families of σi(x; χi)  differentiable and continuously parameterized by χi  then F(x; Θ  X)  where
Θ = {θb : b ∈ O} and X = {χi : i = 1  ...  c}  is also continuously parameterized and differentiable.
Note that the safety guarantee holds regardless of the parameters. To create each Gb(x; θb) we
• a map hb : Rm → Bb
• a standard neural network architecture gb : X → Rm
and then defining Gb(x; θb) = hb(gb(x; θb)).
Application to Aircraft Collision Avoidance
A desirable ˘201csafeability˘201d property for ACAS X was defined in a previous work. This property
speci01ed that for any given input state within the ˘201csafeable region ˘201d an advisory would never
Aunsafeable i  for the ith advisory  the score for that advisory must not be the highest  i.e.  x ∈
Aunsafeable i ⇒ Fi(x) < maxj Fj(x)  where Fj(x) is the output score for the jth advisory.
Table 1: Results of the best configurations of β-TCVAE on DCI  FactorVAE  SAP  MIG  and IRS
NETWORK
ACC (COC)
VIOLATIONS (COC)
ACC (CL1500)
VIOLATIONS (CL1500)
STANDARD
96.87
0.22
93.89
0.20
SAFE
96.69
0.00
94.78
Discussion and Future Work
Proof. Fix i and assume that x ∈ Ai. It follows that σi(x) = 0  so for all b ∈ O where bi = 0
wb(x) = 0. Thus
F(x) =
wb(x)Gb(x).
If bi = 1  Gb(x) ∈ Bi  and thus F(x) is also in Bi by the convexity of Bi.
layer for the constrained predictors  G0 and G1. Training uses a sampled subset of points from
layer. The constrained predictors  G00  G10  G01  and G11  share the hidden layers but also have an
The constraints we enforce are that x ∈ Aunsafeable i ⇒ Fi(x) < maxj Fj(x)  ∀i  where Aunsafeable i is
the unsafeable region for the ith advisory  and Fj(x) is the output score for the jth advisory. Because
enforcing Fi(x) = minj Fj(x)  for all x ∈ Aunsafeable i.
approximation of the safe region of the output space  using Gb(x) = minj Gj(x). In our experiments
we set ϵ = 0.0001.
Proof. Let x ∈ Ai. Then  σi(x) = 0  and for all b ∈ O where bi = 0  wb(x) = 0. Thus
wb(x)Gb(x)
If bi = 1  then Gb(x) ∈ Bi  and therefore F(x) is in Bi due to the convexity of Bi.
shares this structure with constrained predictors  G0 and G1  but each predictor has its own fully
predictors G00  G10  G01 and G11 share the hidden layers and have an additional hidden layer of size
The constraints we enforce in our safe predictor are: x ∈ Aunsafeable i ⇒ Fi(x) < maxj Fj(x)
∀i. To make the output regions convex  we approximate by enforcing Fi(x) = minj Fj(x)  for all
x ∈ Aunsafeable i.
between points in the input space (vO − vI  h  τ)  and the unsafeable region for each advisory. While
advisory i to Gi(x) = minj Gj(x) − ϵ. In our experiments  we used ϵ = 0.0001."
R011,1,KDD,"Collaborative Filtering (CF) often encounters substantial difficulties with popularity bias because of the skewed
distribution of items in real-world datasets. This tendency creates a notable difference in accuracy between items
that are popular and those that are not. This discrepancy impedes the accurate comprehension of user preferences
and intensifies the Matthew effect within recommendation systems. To counter popularity bias  current methods
concentrate on highlighting less popular items or on differentiating the correlation between item representations
and their popularity. Despite their effectiveness  current approaches continue to grapple with two significant
issues: firstly  the extraction of shared supervisory signals from popular items to enhance the representations of
less popular items  and secondly  the reduction of representation separation caused by popularity bias. In this
study  we present an empirical examination of popularity bias and introduce a method called Popularity-Aware
Alignment and Contrast (PAAC) to tackle these two problems. Specifically  we utilize the common supervisory
signals found in popular item representations and introduce an innovative popularity-aware supervised alignment
module to improve the learning of representations for unpopular items. Furthermore  we propose adjusting the
weights in the contrastive learning loss to decrease the separation of representations by focusing on popularity.
We confirm the efficacy and logic of PAAC in reducing popularity bias through thorough experiments on three
real-world datasets.
1","Contemporary recommender systems are essential in reducing information overload. Personalized recommendations frequently
employ collaborative filtering (CF) to assist users in discovering items that may interest them. CF-based techniques primarily
learn user preferences and item attributes by matching the representations of users with the items they engage with. Despite their
achievements  CF-based methods frequently encounter the issue of popularity bias  which leads to considerable disparities in
accuracy between items that are popular and those that are not. Popularity bias occurs because there are limited supervisory signals
for items that are not popular  which","To overcome the current difficulties in reducing popularity bias  we introduce the Popularity-Aware Alignment and Contrast (PAAC)
method. We utilize the common supervisory signals present in popular item representations to direct the learning of unpopular
representations  and we present a popularity-aware supervised alignment module. Moreover  we incorporate a re-weighting system
in the contrastive learning module to deal with representation separation by considering popularity.
2.1 Supervised Alignment Module
During the training process  the alignment of representations usually emphasizes users and items that have interacted  often causing
items to be closer to interacted users than non-interacted ones in the representation space. However  because unpopular items have
limited interactions  they are usually modeled based on a small group of users. This limited focus can result in overfitting  as the
representations of unpopular items might not fully capture their features.
The disparity in the quantity of supervisory signals is essential for learning representations of both popular and unpopular items.
Specifically  popular items gain from a wealth of supervisory signals during the alignment process  which helps in effectively
learning their representations. On the other hand  unpopular items  which have a limited number of users providing supervision  are
more susceptible to overfitting. This is because there is insufficient representation learning for unpopular items  emphasizing the
effect of supervisory signal distribution on the quality of representation. Intuitively  items interacted with by the same user have
some similar characteristics. In this section  we utilize common supervisory signals in popular item representations and suggest a
popularity-aware supervised alignment method to improve the representations of unpopular items.
We initially filter items with similar characteristics based on the user’s interests. For any user  we define the set of items they interact
with. We count the frequency of each item appearing in the training dataset as its popularity. Subsequently  we group items based on
their relative popularity. We divide items into two groups: the popular item group and the unpopular item group. The popularity of
each item in the popular group is higher than that of any item in the unpopular group. This indicates that popular items receive more
supervisory information than unpopular items  resulting in poorer recommendation performance for unpopular items.
To tackle the issue of insufficient representation learning for unpopular items  we utilize the concept that items interacted with by the
same user share some similar characteristics. Specifically  we use similar supervisory signals in popular item representations to
improve the representations of unpopular items. We align the representations of items to provide more supervisory information to
unpopular items and improve their representation  as follows:
LSA=X
u∈U1
|Iu|X
i∈Iupop j∈Iuunpop||f(i)−f(j)||2  (1)
where f(·)is a recommendation encoder and hi=f(i). By efficiently using the inherent information in the data  we provide more
supervisory signals for unpopular items without introducing additional side information. This module enhances the representation of
unpopular items  mitigating the overfitting issue.
2.2 Re-weighting Contrast Module
Recent research has indicated that popularity bias frequently leads to a noticeable separation in the representation of item embeddings.
Although methods based on contrastive learning aim to enhance overall uniformity by distancing negative samples  their current
sampling methods might unintentionally worsen this separation. When negative samples follow the popularity distribution  which
is dominated by popular items  prioritizing unpopular items as positive samples widens the gap between popular and unpopular
items in the representation space. Conversely  when negative samples follow a uniform distribution  focusing on popular items
separates them from most unpopular ones  thus worsening the representation gap. Existing studies use the same weights for positive
and negative samples in the contrastive loss function  without considering differences in item popularity. However  in real-world
recommendation datasets  the impact of items varies due to dataset characteristics and interaction distributions. Neglecting this
aspect could lead to suboptimal results and exacerbate representation separation.
We propose to identify different influences by re-weighting different popularity items. To this end  we introduce re-weighting
different positive and negative samples to mitigate representation separation from a popularity-centric perspective. We incorporate
this approach into contrastive learning to better optimize the consistency of representations. Specifically  we aim to reduce the risk
of pushing items with varying popularity further apart. For example  when using a popular item as a positive sample  our goal is
to avoid pushing unpopular items too far away. Thus  we introduce two hyperparameters to control the weights when items are
considered positive and negative samples.
To ensure balanced and equitable representations of items within our model  we first propose a dynamic strategy to categorize items
into popular and unpopular groups for each mini-batch. Instead of relying on a fixed global threshold  which often leads to the
overrepresentation of popular items across various batches  we implement a hyperparameter x. This hyperparameter readjusts the
classification of items within the current batch. By adjusting the hyperparameter x  we maintain a balance between different item
popularity levels. This enhances the model’s ability to generalize across diverse item sets by accurately reflecting the popularity
distribution in the current training context. Specifically  we denote the set of items within each batch as IB. And then we divide IB
into a popular group Ipopand an unpopular group Iunpop based on their respective popularity levels  classifying the top x%of items
asIpop:
IB=Ipop∪Iunpop  ∀i∈Ipop∧j∈Iunpop   p(i)> p(j)  (2)
where Ipop∈IBandIunpop∈IBare disjoint  with Ipopconsisting of the top x%of items in the batch. In this work  we dynamically
divided items into popular and unpopular groups within each mini-batch based on their popularity  assigning the top 50% as popular
items and the bottom 50% as unpopular items. This radio not only ensures equal representation of both groups in our contrastive
learning but also allows items to be classified adaptively based on the batch’s current composition.
After that  we use InfoNCE to optimize the uniformity of item representations. Unlike traditional CL-based methods  we calculate
the loss for different item groups. Specifically  we introduce the hyperparameter αto control the positive sample weights between
popular and unpopular items  adapting to varying item distributions in different datasets:
2
LCL
item=α×LCL
pop+ (1−α)×LCL
unpop   (3)
where LCL
poprepresents the contrastive loss when popular items are considered as positive samples  and LCL
unpop represents the
contrastive loss when unpopular items are considered as positive samples. The value of αranges from 0 to 1  where α= 0means
exclusive emphasis on the loss of unpopular items LCL
unpop   and α= 1 means exclusive emphasis on the loss of popular items
pop. By adjusting α  we can effectively balance the impact of positive samples from both popular and unpopular items  allowing
adaptability to varying item distributions in different datasets.
Following this  we fine-tune the weighting of negative samples in the contrastive learning framework using the hyperparameter β.
This parameter controls how samples from different popularity groups contribute as negative samples. Specifically  we prioritize
re-weighting items with popularity opposite to the positive samples  mitigating the risk of excessively pushing negative samples
away and reducing representation separation. Simultaneously  this approach ensures the optimization of intra-group consistency. For
instance  when dealing with popular items as positive samples  we separately calculate the impact of popular and unpopular items
as negative samples. The hyperparameter βis then used to control the degree to which unpopular items are pushed away. This is
formalized as follows:
L′
pop=X
i∈Ipoplogexp(h′
ihi/τ)P
j∈Ipopexp(h′
ihj/τ) +βP
j∈Iunpopexp(h′
ihj/τ)  (4)
similarly  the contrastive loss for unpopular items is defined as:
unpop =X
i∈Iunpoplogexp(h′
ihj/τ)  (5)
where the parameter βranges from 0 to 1  controlling the negative sample weighting in the contrastive loss. When β= 0  it means
that only intra-group uniformity optimization is performed. Conversely  when β= 1  it means equal treatment of both popular and
unpopular items in terms of their impact on positive samples. The setting of βallows for a flexible adjustment between prioritizing
intra-group uniformity and considering the impact of different popularity levels in the training. We prefer to push away items
within the same group to optimize uniformity. This setup helps prevent over-optimizing the uniformity of different groups  thereby
mitigating representation separation.
The final re-weighting contrastive objective is the weighted sum of the user objective and the item objective:
LCL=1
2×(LCL
item+LCL
user). (6)
In this way  we not only achieved consistency in representation but also reduced the risk of further separating items with similar
characteristics into different representation spaces  thereby alleviating the issue of representation separation caused by popularity
bias.
2.3 Model Optimization
To reduce popularity bias in collaborative filtering tasks  we employ a multi-task training strategy to jointly optimize the classic
recommendation loss ( LREC )  supervised alignment loss ( LSA)  and re-weighting contrast loss ( LCL).
L=LREC+λ1LSA+λ2LCL+λ3||Θ||2  (7)
where Θis the set of model parameters in LREC as we do not introduce additional parameters  λ1andλ2are hyperparameters that
control the strengths of the popularity-aware supervised alignment loss and the re-weighting contrastive learning loss respectively
andλ3is the L2regularization coefficient. After completing the model training process  we use the dot product to predict unknown
preferences for recommendations.
3 Experiments
In this section  we assess the efficacy of PAAC through comprehensive experiments  aiming to address the following research
questions:
• How does PAAC compare to existing debiasing methods?
• How do different designed components play roles in our proposed PAAC?
3
• How does PAAC alleviate the popularity bias?
• How do different hyper-parameters affect the PAAC recommendation performance?
3.1 Experiments Settings
3.1.1 Datasets
In our experiments  we use three widely public datasets: Amazon-book  Yelp2018  and Gowalla. We retained users and items with a
minimum of 10 interactions.
3.1.2 Baselines and Evaluation Metrics
We implement the state-of-the-art LightGCN to instantiate PAAC  aiming to investigate how it alleviates popularity bias. We
compare PAAC with several debiased baselines  including re-weighting-based models  decorrelation-based models  and contrastive
learning-based models.
We utilize three widely used metrics  namely Recall@K  HR@K  and NDCG@K  to evaluate the performance of Top-K recommen-
dation. Recall@K and HR@K assess the number of target items retrieved in the recommendation results  emphasizing coverage. In
contrast  NDCG@K evaluates the positions of target items in the ranking list  with a focus on their positions in the list. We use
the full ranking strategy  considering all non-interacted items as candidate items to avoid selection bias during the test stage. We
repeated each experiment five times with different random seeds and reported the average scores.
3.2 Overall Performance
As shown in Table 1  we compare our model with several baselines across three datasets. The best performance for each metric
is highlighted in bold  while the second best is underlined. Our model consistently outperforms all compared methods across all
metrics in every dataset.
•Our proposed model PAAC consistently outperforms all baselines and significantly mitigates the popularity bias. Specif-
ically  PAAC enhances LightGCN  achieving improvements of 282.65%  180.79%  and 82.89% in NDCG@20 on the
Yelp2018  Gowalla  and Amazon-Book datasets  respectively. Compared to the strongest baselines  PAAC delivers better
performance. The most significant improvements are observed on Yelp2018  where our model achieves an 8.70% increase
in Recall@20  a 10.81% increase in HR@20  and a 30.2% increase in NDCG@20. This improvement can be attributed
to our use of popularity-aware supervised alignment to enhance the representation of less popular items and re-weighted
contrastive learning to address representation separation from a popularity-centric perspective.
•The performance improvements of PAAC are smaller on sparser datasets. For example  on the Gowalla dataset  the
improvements in Recall@20  HR@20  and NDCG@20 are 3.18%  5.85%  and 5.47%  respectively. This may be because
in sparser datasets like Gowalla  even popular items are not well-represented due to lower data density. Aligning unpopular
items with these poorly represented popular items can introduce noise into the model. Therefore  the benefits of using
supervisory signals for unpopular items may be reduced in very sparse environments  leading to smaller performance
improvements.
•Regarding the baselines for mitigating popularity bias  the improvement of some is relatively limited compared to the
backbone model (LightGCN) and even performs worse in some cases. This may be because some are specifically designed
for traditional data-splitting scenarios  where the test set still follows a long-tail distribution  leading to poor generalization.
Some mitigate popularity bias by excluding item popularity information. Others use invariant learning to remove popularity
information at the representation level  generally performing better than the formers. This shows the importance of
addressing popularity bias at the representation level. Some outperform the other baselines  emphasizing the necessary to
improve item representation consistency for mitigating popularity bias.
•Different metrics across various datasets show varying improvements in model performance. This suggests that different
debiasing methods may need distinct optimization strategies for models. Additionally  we observe varying effects of PAAC
across different datasets. This difference could be due to the sparser nature of the Gowalla dataset. Conversely  our model
can directly provide supervisory signals for unpopular items and conduct intra-group optimization  consistently maintaining
optimal performance across all metrics on the three datasets.
3.3 Ablation Study
To better understand the effectiveness of each component in PAAC  we conduct ablation studies on three datasets. Table 2 presents a
comparison between PAAC and its variants on recommendation performance. Specifically  PAAC-w/o P refers to the variant where
the re-weighting contrastive loss of popular items is removed  focusing instead on optimizing the consistency of representations for
unpopular items. Similarly  PAAC-w/o U denotes the removal of the re-weighting contrastive loss for unpopular items. PAAC-w/o
A refers to the variant without the popularity-aware supervised alignment loss. It’s worth noting that PAAC-w/o A differs from
4
Table 1: Performance comparison on three public datasets with K = 20. The best performance is indicated in bold  while the
second-best performance is underlined. The superscripts * indicate p ≤0.05 for the paired t-test of PAAC vs. the best baseline (the
relative improvements are denoted as Imp.).
!ModelYelp2018 Gowalla Amazon-book
Recall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20
MF 0.0050 0.0109 0.0093 0.0343 0.0422 0.0280 0.0370 0.0388 0.0270
LightGCN 0.0048 0.0111 0.0098 0.0380 0.0468 0.0302 0.0421 0.0439 0.0304
IPS 0.0104 0.0183 0.0158 0.0562 0.0670 0.0444 0.0488 0.0510 0.0365
MACR 0.0402 0.0312 0.0265 0.0908 0.1086 0.0600 0.0515 0.0609 0.0487
α-Adjnorm 0.0053 0.0088 0.0080 0.0328 0.0409 0.0267 0.0422 0.0450 0.0264
InvCF 0.0444 0.0344 0.0291 0.1001 0.1202 0.0662 0.0562 0.0665 0.0515
Adap- τ 0.0450 0.0497 0.0341 0.1182 0.1248 0.0794 0.0641 0.0678 0.0511
SimGCL 0.0449 0.0518 0.0345 0.1194 0.1228 0.0804 0.0628 0.0648 0.0525
PAAC 0.0494* 0.0574* 0.0375* 0.1232* 0.1321* 0.0848* 0.0701* 0.0724* 0.0556*
Imp. +9.78 % +10.81% +8.70% +3.18% +5.85% +5.47% +9.36% +6.78% 5.90%
SimGCL in that we split the contrastive loss on the item side  LCL
item  into two distinct losses: LCL
popandLCL
unpop . This approach
allows us to separately address the consistency of popular and unpopular item representations  thereby providing a more detailed
analysis of the impact of each component on the overall performance.
From Table 2  we observe that PAAC-w/o A outperforms SimGCL in most cases. This validates that re-weighting the importance of
popular and unpopular items can effectively improve the model’s performance in alleviating popularity bias. It also demonstrates the
effectiveness of using supervision signals from popular items to enhance the representations of unpopular items  providing more
opportunities for future research on mitigating popularity bias. Moreover  compared with PAAC-w/o U  PAAC-w/o P results in much
worse performance. This confirms the importance of re-weighting popular items in contrastive learning for mitigating popularity
bias. Finally  PAAC consistently outperforms the three variants  demonstrating the effectiveness of combining supervised alignment
and re-weighting contrastive learning. Based on the above analysis  we conclude that leveraging supervisory signals from popular
item representations can better optimize representations for unpopular items  and re-weighting contrastive learning allows the model
to focus on more informative or critical samples  thereby improving overall performance. All the proposed modules significantly
contribute to alleviating popularity bias.
Table 2: Ablation study of PAAC  highlighting the best-performing model on each dataset and metrics in bold. Specifically
PAAC-w/o P removes the re-weighting contrastive loss of popular items  PAAC-w/o U eliminates the re-weighting contrastive loss
of unpopular items  and PAAC-w/o A omits the popularity-aware supervised alignment loss.
PAAC-w/o P 0.0443 0.0536 0.0340 0.1098 0.1191 0.0750 0.0616 0.0639 0.0458
PAAC-w/o U 0.0462 0.0545 0.0358 0.1120 0.1179 0.0752 0.0594 0.0617 0.0464
PAAC-w/o A 0.0466 0.0547 0.0360 0.1195 0.1260 0.0815 0.0687 0.0711 0.0536
3.4 Debias Ability
To further verify the effectiveness of PAAC in alleviating popularity bias  we conduct a comprehensive analysis focusing on the
recommendation performance across different popularity item groups. Specifically  20% of the most popular items are labeled
’Popular’  and the rest are labeled ’Unpopular’. We compare the performance of PAAC with LightGCN  IPS  MACR  and SimGCL
using the NDCG@20 metric across different popularity groups. We use ∆to denote the accuracy gap between the two groups. We
draw the following","hinders the precise comprehension of user preferences  thereby diminishing the variety of recommendations. Furthermore  popularity
bias can worsen the Matthew effect  where items that are already popular gain even more popularity because they are recommended
more frequently.
Two significant challenges are presented when mitigating popularity bias in recommendation systems. The first challenge is the
inadequate representation of unpopular items during training  which results in overfitting and limited generalization ability. The
second challenge  known as representation separation  happens when popular and unpopular items are categorized into distinct
semantic spaces  thereby intensifying the bias and diminishing the precision of recommendations.
2","•Improving the performance of unpopular items is crucial for enhancing overall model performance. Specially  on the
Yelp2018 dataset  PAAC shows reduced accuracy in recommending popular items  with a notable decrease of 20.14%
compared to SimGCL. However  despite this decrease  the overall recommendation accuracy surpasses that of SimGCL
by 11.94%  primarily due to a 6.81% improvement in recommending unpopular items. This improvement highlights the
importance of better recommendations for unpopular items and emphasizes their crucial role in enhancing overall model
performance.
5
•Our proposed PAAC significantly enhances the recommendation performance for unpopular items. Specifically  we observe
an improvement of 8.94% and 7.30% in NDCG@20 relative to SimGCL on the Gowalla and Yelp2018 datasets  respectively.
This improvement is due to the popularity-aware alignment method  which uses supervisory signals from popular items to
improve the representations of unpopular items.
•PAAC has successfully narrowed the accuracy gap between different item groups. Specifically  PAAC achieved the smallest
gap  reducing the NDCG@20 accuracy gap by 34.18% and 87.50% on the Gowalla and Yelp2018 datasets  respectively.
This indicates that our method treats items from different groups fairly  effectively alleviating the impact of popularity
bias. This success can be attributed to our re-weighted contrast module  which addresses representation separation from a
popularity-centric perspective  resulting in more consistent recommendation results across different groups.
3.5 Hyperparameter Sensitivities
In this section  we analyze the impact of hyperparameters in PAAC. Firstly  we investigate the influence of λ1andλ2  which
respectively control the impact of the popularity-aware supervised alignment and re-weighting contrast loss. Additionally  in the
re-weighting contrastive loss  we introduce two hyperparameters  αandβ  to control the re-weighting of different popularity items
as positive and negative samples. Finally  we explore the impact of the grouping ratio xon the model’s performance.
3.5.1 Effect of λ1andλ2
As formulated in Eq. (11)  λ1controls the extent of providing additional supervisory signals for unpopular items  while λ2controls
the extent of optimizing representation consistency. Horizontally  with the increase in λ2  the performance initially increases and
then decreases. This indicates that appropriate re-weighting contrastive loss effectively enhances the consistency of representation
distributions  mitigating popularity bias. However  overly strong contrastive loss may lead the model to neglect recommendation
accuracy. Vertically  as λ1increases  the performance also initially increases and then decreases. This suggests that suitable
alignment can provide beneficial supervisory signals for unpopular items  while too strong an alignment may introduce more noise
from popular items to unpopular ones  thereby impacting recommendation performance.
3.5.2 Effect of re-weighting coefficient αandβ
To mitigate representation separation due to imbalanced positive and negative sampling  we introduce two hyperparameters into the
contrastive loss. Specifically  αcontrols the weight difference between positive samples from popular and unpopular items  while β
controls the influence of different popularity items as negative samples.
In our experiments  while keeping other hyperparameters constant  we search αandβwithin the range {0  0.2  0.4  0.6  0.8  1}. As
αandβincrease  performance initially improves and then declines. The optimal hyperparameters for the Yelp2018 and Gowalla
datasets are α= 0.8 β= 0.6andα= 0.2 β= 0.2  respectively. This may be attributed to the characteristics of the datasets. The
Yelp2018 dataset  with a higher average interaction frequency per item  benefits more from a higher weight αfor popular items as
positive samples. Conversely  the Gowalla dataset  being relatively sparse  prefers a smaller α. This indicates the importance of
considering dataset characteristics when adjusting the contributions of popular and unpopular items to the model.
Notably  αandβare not highly sensitive within the range [0  1]  performing well across a broad spectrum. Performance exceeds the
baseline regardless of βvalues when other parameters are optimal. Additionally  αvalues from [0.4  1.0] on the Yelp2018 dataset
and [0.2  0.8] on the Gowalla dataset surpass the baseline  indicating less need for precise tuning. Thus  αandβachieve optimal
performance without meticulous adjustments  focusing on weight coefficients to maintain model efficacy.
3.5.3 Effect of grouping ratio x
To investigate the impact of different grouping ratios on recommendation performance  we developed a flexible classification
method for items within each mini-batch based on their popularity. Instead of adopting a fixed global threshold  which tends to
overrepresent popular items in some mini-batches  our approach dynamically divides items in each mini-batch into popular and
unpopular categories. Specifically  the top x%of items are classified as popular and the remaining (100 - x)% as unpopular  with x
varying. This strategy prevents the overrepresentation typical in fixed distribution models  which could skew the learning process
and degrade performance. To quantify the effects of these varying ratios  we examined various division ratios for popular items
including 20%  40%  60%  and 80%  as shown in Table 3. The preliminary results indicate that both extremely low and high ratios
negatively affect model performance  thereby underscoring the superiority of our dynamic data partitioning approach. Moreover
within the 40%-60% range  our model’s performance remained consistently robust  further validating the effectiveness of PAAC.
6
Table 3: Performance comparison across varying popular item ratios x on metrics.
!RatioYelp2018 Gowalla
Recall@20 HR@20 NDCG@20 Recall@20 HR@20 NDCG@20
20% 0.0467 0.0555 0.0361 0.1232 0.1319 0.0845
40% 0.0505 0.0581 0.0378 0.1239 0.1325 0.0848
50% 0.0494 0.0574 0.0375 0.1232 0.1321 0.0848
60% 0.0492 0.0569 0.0370 0.1225 0.1314 0.0843
80% 0.0467 0.0545 0.0350 0.1176 0.1270 0.0818
4 Related Work
4.1 Popularity Bias in Recommendation
Popularity bias is a prevalent problem in recommender systems  where unpopular items in the training dataset are seldom recom-
mended. Numerous techniques have been suggested to examine and decrease performance variations between popular and unpopular
items. These techniques can be broadly divided into three categories.
•Re-weighting-based methods aim to increase the training weight or scores for unpopular items  redirecting focus away
from popular items during training or prediction. For instance  IPS adds compensation to unpopular items and adjusts
the prediction of the user-item preference matrix  resulting in higher preference scores and improving rankings for
unpopular items. α-AdjNorm enhances the focus on unpopular items by controlling the normalization strength during the
neighborhood aggregation process in GCN-based models.
•Decorrelation-based methods aim to effectively remove the correlations between item representations (or prediction scores)
and popularity. For instance  MACR uses counterfactual reasoning to eliminate the direct impact of popularity on item
outcomes. In contrast  InvCF operates on the principle that item representations remain invariant to changes in popularity
semantics  filtering out unstable or outdated popularity characteristics to learn unbiased representations.
•Contrastive-learning-based methods aim to achieve overall uniformity in item representations using InfoNCE  preserving
more inherent characteristics of items to mitigate popularity bias. This approach has been demonstrated as a state-of-the-art
method for alleviating popularity bias. It employs data augmentation techniques such as graph augmentation or feature
augmentation to generate different views  maximizing positive pair consistency and minimizing negative pair consistency
to promote more uniform representations. Specifically  Adap- τadjusts user/item embeddings to specific values  while
SimGCL integrates InfoNCE loss to enhance representation uniformity and alleviate popularity bias.
4.2 Representation Learning for CF
Representation learning is crucial in recommendation systems  especially in modern collaborative filtering (CF) techniques. It
creates personalized embeddings that capture user preferences and item characteristics. The quality of these representations critically
determines a recommender system’s effectiveness by precisely capturing the interplay between user interests and item features.
Recent studies emphasize two fundamental principles in representation learning: alignment and uniformity. The alignment principle
ensures that embeddings of similar or related items (or users) are closely clustered together  improving the system’s ability to
recommend items that align with a user’s interests. This principle is crucial when accurately reflecting user preferences through
corresponding item characteristics. Conversely  the uniformity principle ensures a balanced distribution of all embeddings across the
representation space. This approach prevents the over-concentration of embeddings in specific areas  enhancing recommendation
diversity and improving generalization to unseen data.
In this work  we focus on aligning the representations of popular and unpopular items interacted with by the same user and re-
weighting uniformity to mitigate representation separation. Our model PAAC uniquely addresses popularity bias by combining group
alignment and contrastive learning  a first in the field. Unlike previous works that align positive user-item pairs or contrastive pairs
PAAC directly aligns popular and unpopular items  leveraging the rich information of popular items to enhance the representations
of unpopular items and reduce overfitting. Additionally  we introduce targeted re-weighting from a popularity-centric perspective to
achieve a more balanced representation.
5 Conclusion
In this study  we have examined popularity bias and put forward PAAC as a method to lessen its impact. We postulated that items
engaged with by the same user exhibit common traits  and we utilized this insight to coordinate the representations of both popular
and unpopular items via a popularity-conscious supervised alignment method. This strategy furnished additional supervisory data for
less popular items. It is important to note that our concept of aligning and categorizing items according to user-specific preferences
introduces a fresh perspective on alignment. Moreover  we tackled the problem of representation separation seen in current CL-based
7
models by incorporating two hyperparameters to regulate the influence of items with varying popularity levels when considered
as positive and negative samples. This method refined the uniformity of representations and successfully reduced separation. We
validated our method  PAAC  on three publicly available datasets  demonstrating its effectiveness and underlying rationale.
In the future  we will explore deeper alignment and contrast adjustments tailored to specific tasks to further mitigate popularity
bias. We aim to investigate the synergies between alignment and contrast and extend our approach to address other biases in
recommendation systems.
Acknowledgments
This work was supported in part by grants from the National Key Research and Development Program of China  the National Natural
Science Foundation of China  the Fundamental Research Funds for the Central Universities  and Quan Cheng Laboratory.
8
CollaborativeFiltering(CF)oftenencounterssubstantialdifficultieswithpopularitybiasbecauseoftheskewed
distributionofitemsinreal-worlddatasets. Thistendencycreatesanotabledifferenceinaccuracybetweenitems
thatarepopularandthosethatarenot. Thisdiscrepancyimpedestheaccuratecomprehensionofuserpreferences
andintensifiestheMattheweffectwithinrecommendationsystems. Tocounterpopularitybias currentmethods
concentrateonhighlightinglesspopularitemsorondifferentiatingthecorrelationbetweenitemrepresentations
issues: firstly theextractionofsharedsupervisorysignalsfrompopularitemstoenhancetherepresentationsof
lesspopularitems andsecondly thereductionofrepresentationseparationcausedbypopularitybias. Inthis
study wepresentanempiricalexaminationofpopularitybiasandintroduceamethodcalledPopularity-Aware
AlignmentandContrast(PAAC)totacklethesetwoproblems. Specifically weutilizethecommonsupervisory
signalsfoundinpopularitemrepresentationsandintroduceaninnovativepopularity-awaresupervisedalignment
moduletoimprovethelearningofrepresentationsforunpopularitems. Furthermore weproposeadjustingthe
weightsinthecontrastivelearninglosstodecreasetheseparationofrepresentationsbyfocusingonpopularity.
WeconfirmtheefficacyandlogicofPAACinreducingpopularitybiasthroughthoroughexperimentsonthree
real-worlddatasets.
Contemporaryrecommendersystemsareessentialinreducinginformationoverload. Personalizedrecommendationsfrequently
learnuserpreferencesanditemattributesbymatchingtherepresentationsofuserswiththeitemstheyengagewith. Despitetheir
accuracybetweenitemsthatarepopularandthosethatarenot. Popularitybiasoccursbecausetherearelimitedsupervisorysignals
foritemsthatarenotpopular whichresultsinoverfittingduringthetrainingphaseanddecreasedeffectivenessonthetestset. This
hinderstheprecisecomprehensionofuserpreferences therebydiminishingthevarietyofrecommendations. Furthermore popularity
biascanworsentheMattheweffect whereitemsthatarealreadypopulargainevenmorepopularitybecausetheyarerecommended
morefrequently.
Twosignificantchallengesarepresentedwhenmitigatingpopularitybiasinrecommendationsystems. Thefirstchallengeisthe
inadequaterepresentationofunpopularitemsduringtraining whichresultsinoverfittingandlimitedgeneralizationability. The
secondchallenge knownasrepresentationseparation happenswhenpopularandunpopularitemsarecategorizedintodistinct
semanticspaces therebyintensifyingthebiasanddiminishingtheprecisionofrecommendations.
Toovercomethecurrentdifficultiesinreducingpopularitybias weintroducethePopularity-AwareAlignmentandContrast(PAAC)
method. Weutilizethecommonsupervisorysignalspresentinpopularitemrepresentationstodirectthelearningofunpopular
representations andwepresentapopularity-awaresupervisedalignmentmodule. Moreover weincorporateare-weightingsystem
inthecontrastivelearningmoduletodealwithrepresentationseparationbyconsideringpopularity.
2.1 SupervisedAlignmentModule
Duringthetrainingprocess thealignmentofrepresentationsusuallyemphasizesusersanditemsthathaveinteracted oftencausing
itemstobeclosertointeractedusersthannon-interactedonesintherepresentationspace. However becauseunpopularitemshave
limitedinteractions theyareusuallymodeledbasedonasmallgroupofusers. Thislimitedfocuscanresultinoverfitting asthe
representationsofunpopularitemsmightnotfullycapturetheirfeatures.
Thedisparityinthequantityofsupervisorysignalsisessentialforlearningrepresentationsofbothpopularandunpopularitems.
learningtheirrepresentations. Ontheotherhand unpopularitems whichhavealimitednumberofusersprovidingsupervision are
moresusceptibletooverfitting. Thisisbecausethereisinsufficientrepresentationlearningforunpopularitems emphasizingthe
effectofsupervisorysignaldistributiononthequalityofrepresentation. Intuitively itemsinteractedwithbythesameuserhave
somesimilarcharacteristics. Inthissection weutilizecommonsupervisorysignalsinpopularitemrepresentationsandsuggesta
popularity-awaresupervisedalignmentmethodtoimprovetherepresentationsofunpopularitems.
Weinitiallyfilteritemswithsimilarcharacteristicsbasedontheuser’sinterests. Foranyuser wedefinethesetofitemstheyinteract
with. Wecountthefrequencyofeachitemappearinginthetrainingdatasetasitspopularity. Subsequently wegroupitemsbasedon
theirrelativepopularity. Wedivideitemsintotwogroups: thepopularitemgroupandtheunpopularitemgroup. Thepopularityof
eachiteminthepopulargroupishigherthanthatofanyitemintheunpopulargroup. Thisindicatesthatpopularitemsreceivemore
supervisoryinformationthanunpopularitems resultinginpoorerrecommendationperformanceforunpopularitems.
Totackletheissueofinsufficientrepresentationlearningforunpopularitems weutilizetheconceptthatitemsinteractedwithbythe
sameusersharesomesimilarcharacteristics. Specifically weusesimilarsupervisorysignalsinpopularitemrepresentationsto
improvetherepresentationsofunpopularitems. Wealigntherepresentationsofitemstoprovidemoresupervisoryinformationto
unpopularitemsandimprovetheirrepresentation asfollows:
(cid:88) 1 (cid:88)
L = ||f(i)−f(j)||   (1)
SA |I | 2
u
u∈U i∈Iu  j∈Iu
pop unpop
wheref(·)isarecommendationencoderandh =f(i). Byefficientlyusingtheinherentinformationinthedata weprovidemore
i
supervisorysignalsforunpopularitemswithoutintroducingadditionalsideinformation. Thismoduleenhancestherepresentationof
unpopularitems mitigatingtheoverfittingissue.
2.2 Re-weightingContrastModule
Recentresearchhasindicatedthatpopularitybiasfrequentlyleadstoanoticeableseparationintherepresentationofitemembeddings.
Althoughmethodsbasedoncontrastivelearningaimtoenhanceoveralluniformitybydistancingnegativesamples theircurrent
samplingmethodsmightunintentionallyworsenthisseparation. Whennegativesamplesfollowthepopularitydistribution which
isdominatedbypopularitems prioritizingunpopularitemsaspositivesampleswidensthegapbetweenpopularandunpopular
itemsintherepresentationspace. Conversely whennegativesamplesfollowauniformdistribution focusingonpopularitems
separatesthemfrommostunpopularones thusworseningtherepresentationgap. Existingstudiesusethesameweightsforpositive
andnegativesamplesinthecontrastivelossfunction withoutconsideringdifferencesinitempopularity. However inreal-world
recommendationdatasets theimpactofitemsvariesduetodatasetcharacteristicsandinteractiondistributions. Neglectingthis
aspectcouldleadtosuboptimalresultsandexacerbaterepresentationseparation.
differentpositiveandnegativesamplestomitigaterepresentationseparationfromapopularity-centricperspective. Weincorporate
thisapproachintocontrastivelearningtobetteroptimizetheconsistencyofrepresentations. Specifically weaimtoreducetherisk
ofpushingitemswithvaryingpopularityfurtherapart. Forexample whenusingapopularitemasapositivesample ourgoalis
toavoidpushingunpopularitemstoofaraway. Thus weintroducetwohyperparameterstocontroltheweightswhenitemsare
consideredpositiveandnegativesamples.
Toensurebalancedandequitablerepresentationsofitemswithinourmodel wefirstproposeadynamicstrategytocategorizeitems
intopopularandunpopulargroupsforeachmini-batch. Insteadofrelyingonafixedglobalthreshold whichoftenleadstothe
overrepresentationofpopularitemsacrossvariousbatches weimplementahyperparameterx. Thishyperparameterreadjuststhe
classificationofitemswithinthecurrentbatch. Byadjustingthehyperparameterx wemaintainabalancebetweendifferentitem
popularitylevels. Thisenhancesthemodel’sabilitytogeneralizeacrossdiverseitemsetsbyaccuratelyreflectingthepopularity
distributioninthecurrenttrainingcontext. Specifically wedenotethesetofitemswithineachbatchasI . AndthenwedivideI
B B
intoapopulargroupI andanunpopulargroupI basedontheirrespectivepopularitylevels classifyingthetopx%ofitems
asI :
pop
I =I ∪I  ∀i∈I ∧j ∈I  p(i)>p(j)  (2)
B pop unpop pop unpop
whereI ∈I andI ∈I aredisjoint withI consistingofthetopx%ofitemsinthebatch.Inthiswork wedynamically
pop B unpop B pop
divideditemsintopopularandunpopulargroupswithineachmini-batchbasedontheirpopularity assigningthetop50%aspopular
itemsandthebottom50%asunpopularitems. Thisradionotonlyensuresequalrepresentationofbothgroupsinourcontrastive
learningbutalsoallowsitemstobeclassifiedadaptivelybasedonthebatch’scurrentcomposition.
Afterthat weuseInfoNCEtooptimizetheuniformityofitemrepresentations. UnliketraditionalCL-basedmethods wecalculate
thelossfordifferentitemgroups. Specifically weintroducethehyperparameterαtocontrolthepositivesampleweightsbetween
popularandunpopularitems adaptingtovaryingitemdistributionsindifferentdatasets:
LCL =α×LCL +(1−α)×LCL   (3)
item pop unpop
where LCL represents the contrastive loss when popular items are considered as positive samples  and LCL represents the
contrastivelosswhenunpopularitemsareconsideredaspositivesamples. Thevalueofαrangesfrom0to1 whereα=0means
exclusive emphasis on the loss of unpopular items LCL   and α = 1 means exclusive emphasis on the loss of popular items
unpop
LCL. Byadjustingα wecaneffectivelybalancetheimpactofpositivesamplesfrombothpopularandunpopularitems allowing
adaptabilitytovaryingitemdistributionsindifferentdatasets.
Followingthis wefine-tunetheweightingofnegativesamplesinthecontrastivelearningframeworkusingthehyperparameterβ.
Thisparametercontrolshowsamplesfromdifferentpopularitygroupscontributeasnegativesamples. Specifically weprioritize
re-weightingitemswithpopularityoppositetothepositivesamples mitigatingtheriskofexcessivelypushingnegativesamples
awayandreducingrepresentationseparation. Simultaneously thisapproachensurestheoptimizationofintra-groupconsistency. For
instance whendealingwithpopularitemsaspositivesamples weseparatelycalculatetheimpactofpopularandunpopularitems
asnegativesamples. Thehyperparameterβ isthenusedtocontrolthedegreetowhichunpopularitemsarepushedaway. Thisis
formalizedasfollows:
L′ = (cid:88) log exp(h′ihi/τ)   (4)
pop (cid:80) exp(h′h /τ)+β(cid:80) exp(h′h /τ)
i∈Ipop j∈Ipop i j j∈Iunpop i j
similarly thecontrastivelossforunpopularitemsisdefinedas:
L′ = (cid:88) log exp(h′ihi/τ)   (5)
unpop (cid:80) exp(h′h /τ)+β(cid:80) exp(h′h /τ)
i∈Iunpop j∈Iunpop i j j∈Ipop i j
wheretheparameterβ rangesfrom0to1 controllingthenegativesampleweightinginthecontrastiveloss. Whenβ =0 itmeans
thatonlyintra-groupuniformityoptimizationisperformed. Conversely whenβ =1 itmeansequaltreatmentofbothpopularand
unpopularitemsintermsoftheirimpactonpositivesamples. Thesettingofβ allowsforaflexibleadjustmentbetweenprioritizing
withinthesamegrouptooptimizeuniformity. Thissetuphelpspreventover-optimizingtheuniformityofdifferentgroups thereby
mitigatingrepresentationseparation.
Thefinalre-weightingcontrastiveobjectiveistheweightedsumoftheuserobjectiveandtheitemobjective:
1
L = ×(LCL +LCL ). (6)
CL 2 item user
Inthisway wenotonlyachievedconsistencyinrepresentationbutalsoreducedtheriskoffurtherseparatingitemswithsimilar
characteristicsintodifferentrepresentationspaces therebyalleviatingtheissueofrepresentationseparationcausedbypopularity
2.3 ModelOptimization
Toreducepopularitybiasincollaborativefilteringtasks weemployamulti-tasktrainingstrategytojointlyoptimizetheclassic
recommendationloss(L ) supervisedalignmentloss(L ) andre-weightingcontrastloss(L ).
REC SA CL
L=L +λ L +λ L +λ ||Θ||2  (7)
REC 1 SA 2 CL 3
whereΘisthesetofmodelparametersinL aswedonotintroduceadditionalparameters λ andλ arehyperparametersthat
REC 1 2
controlthestrengthsofthepopularity-awaresupervisedalignmentlossandthere-weightingcontrastivelearninglossrespectively
andλ istheL regularizationcoefficient. Aftercompletingthemodeltrainingprocess weusethedotproducttopredictunknown
3 2
preferencesforrecommendations.
• HowdoesPAACcomparetoexistingdebiasingmethods?
• HowdodifferentdesignedcomponentsplayrolesinourproposedPAAC?
• HowdoesPAACalleviatethepopularitybias?
• Howdodifferenthyper-parametersaffectthePAACrecommendationperformance?
3.1 ExperimentsSettings
Inourexperiments weusethreewidelypublicdatasets: Amazon-book Yelp2018 andGowalla. Weretainedusersanditemswitha
minimumof10interactions.
3.1.2 BaselinesandEvaluationMetrics
comparePAACwithseveraldebiasedbaselines includingre-weighting-basedmodels decorrelation-basedmodels andcontrastive
learning-basedmodels.
Weutilizethreewidelyusedmetrics namelyRecall@K HR@K andNDCG@K toevaluatetheperformanceofTop-Krecommen-
dation. Recall@KandHR@Kassessthenumberoftargetitemsretrievedintherecommendationresults emphasizingcoverage. In
contrast NDCG@Kevaluatesthepositionsoftargetitemsintherankinglist withafocusontheirpositionsinthelist. Weuse
thefullrankingstrategy consideringallnon-interacteditemsascandidateitemstoavoidselectionbiasduringtheteststage. We
repeatedeachexperimentfivetimeswithdifferentrandomseedsandreportedtheaveragescores.
3.2 OverallPerformance
AsshowninTable1 wecompareourmodelwithseveralbaselinesacrossthreedatasets. Thebestperformanceforeachmetric
ishighlightedinbold whilethesecondbestisunderlined. Ourmodelconsistentlyoutperformsallcomparedmethodsacrossall
metricsineverydataset.
• OurproposedmodelPAACconsistentlyoutperformsallbaselinesandsignificantlymitigatesthepopularitybias. Specif-
ically PAACenhancesLightGCN achievingimprovementsof282.65% 180.79% and82.89%inNDCG@20onthe
Yelp2018 Gowalla andAmazon-Bookdatasets respectively. Comparedtothestrongestbaselines PAACdeliversbetter
performance. ThemostsignificantimprovementsareobservedonYelp2018 whereourmodelachievesan8.70%increase
inRecall@20 a10.81%increaseinHR@20 anda30.2%increaseinNDCG@20. Thisimprovementcanbeattributed
toouruseofpopularity-awaresupervisedalignmenttoenhancetherepresentationoflesspopularitemsandre-weighted
contrastivelearningtoaddressrepresentationseparationfromapopularity-centricperspective.
• The performance improvements of PAAC are smaller on sparser datasets. For example  on the Gowalla dataset  the
improvementsinRecall@20 HR@20 andNDCG@20are3.18% 5.85% and5.47% respectively. Thismaybebecause
insparserdatasetslikeGowalla evenpopularitemsarenotwell-representedduetolowerdatadensity. Aligningunpopular
itemswiththesepoorlyrepresentedpopularitemscanintroducenoiseintothemodel. Therefore thebenefitsofusing
supervisorysignalsforunpopularitemsmaybereducedinverysparseenvironments  leadingtosmallerperformance
• Regardingthebaselinesformitigatingpopularitybias theimprovementofsomeisrelativelylimitedcomparedtothe
backbonemodel(LightGCN)andevenperformsworseinsomecases. Thismaybebecausesomearespecificallydesigned
fortraditionaldata-splittingscenarios wherethetestsetstillfollowsalong-taildistribution leadingtopoorgeneralization.
Somemitigatepopularitybiasbyexcludingitempopularityinformation. Othersuseinvariantlearningtoremovepopularity
addressingpopularitybiasattherepresentationlevel. Someoutperformtheotherbaselines emphasizingthenecessaryto
improveitemrepresentationconsistencyformitigatingpopularitybias.
• Differentmetricsacrossvariousdatasetsshowvaryingimprovementsinmodelperformance. Thissuggeststhatdifferent
debiasingmethodsmayneeddistinctoptimizationstrategiesformodels. Additionally weobservevaryingeffectsofPAAC
acrossdifferentdatasets. ThisdifferencecouldbeduetothesparsernatureoftheGowalladataset. Conversely ourmodel
candirectlyprovidesupervisorysignalsforunpopularitemsandconductintra-groupoptimization consistentlymaintaining
optimalperformanceacrossallmetricsonthethreedatasets.
3.3 AblationStudy
TobetterunderstandtheeffectivenessofeachcomponentinPAAC weconductablationstudiesonthreedatasets. Table2presentsa
comparisonbetweenPAACanditsvariantsonrecommendationperformance. Specifically PAAC-w/oPreferstothevariantwhere
there-weightingcontrastivelossofpopularitemsisremoved focusinginsteadonoptimizingtheconsistencyofrepresentationsfor
unpopularitems. Similarly PAAC-w/oUdenotestheremovalofthere-weightingcontrastivelossforunpopularitems. PAAC-w/o
Areferstothevariantwithoutthepopularity-awaresupervisedalignmentloss. It’sworthnotingthatPAAC-w/oAdiffersfrom
second-bestperformanceisunderlined. Thesuperscripts*indicatep≤0.05forthepairedt-testofPAACvs. thebestbaseline(the
relativeimprovementsaredenotedasImp.).
Yelp2018 Gowalla Amazon-book
Model
!MACR 0.0402 0.0312 0.0265 0.0908 0.1086 0.0600 0.0515 0.0609 0.0487
Adap-τ 0.0450 0.0497 0.0341 0.1182 0.1248 0.0794 0.0641 0.0678 0.0511
Imp. +9.78% +10.81% +8.70% +3.18% +5.85% +5.47% +9.36% +6.78% 5.90%
SimGCLinthatwesplitthecontrastivelossontheitemside LCL  intotwodistinctlosses: LCL andLCL . Thisapproach
allowsustoseparatelyaddresstheconsistencyofpopularandunpopularitemrepresentations therebyprovidingamoredetailed
analysisoftheimpactofeachcomponentontheoverallperformance.
FromTable2 weobservethatPAAC-w/oAoutperformsSimGCLinmostcases. Thisvalidatesthatre-weightingtheimportanceof
popularandunpopularitemscaneffectivelyimprovethemodel’sperformanceinalleviatingpopularitybias. Italsodemonstratesthe
effectivenessofusingsupervisionsignalsfrompopularitemstoenhancetherepresentationsofunpopularitems providingmore
opportunitiesforfutureresearchonmitigatingpopularitybias. Moreover comparedwithPAAC-w/oU PAAC-w/oPresultsinmuch
worseperformance. Thisconfirmstheimportanceofre-weightingpopularitemsincontrastivelearningformitigatingpopularity
bias. Finally PAACconsistentlyoutperformsthethreevariants demonstratingtheeffectivenessofcombiningsupervisedalignment
andre-weightingcontrastivelearning. Basedontheaboveanalysis weconcludethatleveragingsupervisorysignalsfrompopular
itemrepresentationscanbetteroptimizerepresentationsforunpopularitems andre-weightingcontrastivelearningallowsthemodel
tofocusonmoreinformativeorcriticalsamples therebyimprovingoverallperformance. Alltheproposedmodulessignificantly
contributetoalleviatingpopularitybias.
PAAC-w/oPremovesthere-weightingcontrastivelossofpopularitems PAAC-w/oUeliminatesthere-weightingcontrastiveloss
ofunpopularitems andPAAC-w/oAomitsthepopularity-awaresupervisedalignmentloss.
!
PAAC-w/oP 0.0443 0.0536 0.0340 0.1098 0.1191 0.0750 0.0616 0.0639 0.0458
PAAC-w/oU 0.0462 0.0545 0.0358 0.1120 0.1179 0.0752 0.0594 0.0617 0.0464
PAAC-w/oA 0.0466 0.0547 0.0360 0.1195 0.1260 0.0815 0.0687 0.0711 0.0536
3.4 DebiasAbility
TofurtherverifytheeffectivenessofPAACinalleviatingpopularitybias weconductacomprehensiveanalysisfocusingonthe
recommendationperformanceacrossdifferentpopularityitemgroups. Specifically 20%ofthemostpopularitemsarelabeled
’Popular’ andtherestarelabeled’Unpopular’. WecomparetheperformanceofPAACwithLightGCN IPS MACR andSimGCL
usingtheNDCG@20metricacrossdifferentpopularitygroups. Weuse∆todenotetheaccuracygapbetweenthetwogroups. We
drawthefollowingconclusions:
• Improving theperformanceof unpopularitems iscrucialfor enhancingoverallmodelperformance. Specially  on the
Yelp2018dataset PAACshowsreducedaccuracyinrecommendingpopularitems withanotabledecreaseof20.14%
comparedtoSimGCL.However despitethisdecrease theoverallrecommendationaccuracysurpassesthatofSimGCL
by11.94% primarilyduetoa6.81%improvementinrecommendingunpopularitems. Thisimprovementhighlightsthe
importanceofbetterrecommendationsforunpopularitemsandemphasizestheircrucialroleinenhancingoverallmodel
• OurproposedPAACsignificantlyenhancestherecommendationperformanceforunpopularitems. Specifically weobserve
animprovementof8.94%and7.30%inNDCG@20relativetoSimGCLontheGowallaandYelp2018datasets respectively.
Thisimprovementisduetothepopularity-awarealignmentmethod whichusessupervisorysignalsfrompopularitemsto
improvetherepresentationsofunpopularitems.
• PAAChassuccessfullynarrowedtheaccuracygapbetweendifferentitemgroups. Specifically PAACachievedthesmallest
gap reducingtheNDCG@20accuracygapby34.18%and87.50%ontheGowallaandYelp2018datasets respectively.
Thisindicatesthatourmethodtreatsitemsfromdifferentgroupsfairly effectivelyalleviatingtheimpactofpopularity
bias. Thissuccesscanbeattributedtoourre-weightedcontrastmodule whichaddressesrepresentationseparationfroma
popularity-centricperspective resultinginmoreconsistentrecommendationresultsacrossdifferentgroups.
3.5 HyperparameterSensitivities
In this section  we analyze the impact of hyperparameters in PAAC. Firstly  we investigate the influence of λ and λ   which
1 2
respectivelycontroltheimpactofthepopularity-awaresupervisedalignmentandre-weightingcontrastloss. Additionally inthe
re-weightingcontrastiveloss weintroducetwohyperparameters αandβ tocontrolthere-weightingofdifferentpopularityitems
aspositiveandnegativesamples. Finally weexploretheimpactofthegroupingratioxonthemodel’sperformance.
3.5.1 Effectofλ andλ
AsformulatedinEq. (11) λ controlstheextentofprovidingadditionalsupervisorysignalsforunpopularitems whileλ controls
theextentofoptimizingrepresentationconsistency. Horizontally withtheincreaseinλ  theperformanceinitiallyincreasesand
thendecreases. Thisindicatesthatappropriatere-weightingcontrastivelosseffectivelyenhancestheconsistencyofrepresentation
distributions mitigatingpopularitybias. However overlystrongcontrastivelossmayleadthemodeltoneglectrecommendation
accuracy. Vertically  as λ increases  the performance also initially increases and then decreases. This suggests that suitable
alignmentcanprovidebeneficialsupervisorysignalsforunpopularitems whiletoostronganalignmentmayintroducemorenoise
frompopularitemstounpopularones therebyimpactingrecommendationperformance.
3.5.2 Effectofre-weightingcoefficientαandβ
Tomitigaterepresentationseparationduetoimbalancedpositiveandnegativesampling weintroducetwohyperparametersintothe
contrastiveloss. Specifically αcontrolstheweightdifferencebetweenpositivesamplesfrompopularandunpopularitems whileβ
controlstheinfluenceofdifferentpopularityitemsasnegativesamples.
Inourexperiments whilekeepingotherhyperparametersconstant wesearchαandβ withintherange{0 0.2 0.4 0.6 0.8 1}. As
αandβ increase performanceinitiallyimprovesandthendeclines. TheoptimalhyperparametersfortheYelp2018andGowalla
datasetsareα=0.8 β =0.6andα=0.2 β =0.2 respectively. Thismaybeattributedtothecharacteristicsofthedatasets. The
Yelp2018dataset withahigheraverageinteractionfrequencyperitem benefitsmorefromahigherweightαforpopularitemsas
positivesamples. Conversely theGowalladataset beingrelativelysparse prefersasmallerα. Thisindicatestheimportanceof
consideringdatasetcharacteristicswhenadjustingthecontributionsofpopularandunpopularitemstothemodel.
Notably αandβ arenothighlysensitivewithintherange[0 1] performingwellacrossabroadspectrum. Performanceexceedsthe
baselineregardlessofβ valueswhenotherparametersareoptimal. Additionally αvaluesfrom[0.4 1.0]ontheYelp2018dataset
and[0.2 0.8]ontheGowalladatasetsurpassthebaseline indicatinglessneedforprecisetuning. Thus αandβ achieveoptimal
performancewithoutmeticulousadjustments focusingonweightcoefficientstomaintainmodelefficacy.
3.5.3 Effectofgroupingratiox
methodforitemswithineachmini-batchbasedontheirpopularity. Insteadofadoptingafixedglobalthreshold whichtendsto
overrepresentpopularitemsinsomemini-batches ourapproachdynamicallydividesitemsineachmini-batchintopopularand
unpopularcategories. Specifically thetopx%ofitemsareclassifiedaspopularandtheremaining(100-x)%asunpopular withx
varying. Thisstrategypreventstheoverrepresentationtypicalinfixeddistributionmodels whichcouldskewthelearningprocess
anddegradeperformance. Toquantifytheeffectsofthesevaryingratios weexaminedvariousdivisionratiosforpopularitems
including20% 40% 60% and80% asshowninTable3. Thepreliminaryresultsindicatethatbothextremelylowandhighratios
negativelyaffectmodelperformance therebyunderscoringthesuperiorityofourdynamicdatapartitioningapproach. Moreover
withinthe40%-60%range ourmodel’sperformanceremainedconsistentlyrobust furthervalidatingtheeffectivenessofPAAC.
Table3: Performancecomparisonacrossvaryingpopularitemratiosxonmetrics.
Yelp2018 Gowalla
Ratio
4 RelatedWork
4.1 PopularityBiasinRecommendation
Popularitybiasisaprevalentprobleminrecommendersystems whereunpopularitemsinthetrainingdatasetareseldomrecom-
mended. Numeroustechniqueshavebeensuggestedtoexamineanddecreaseperformancevariationsbetweenpopularandunpopular
items. Thesetechniquescanbebroadlydividedintothreecategories.
• Re-weighting-basedmethodsaimtoincreasethetrainingweightorscoresforunpopularitems redirectingfocusaway
frompopularitemsduringtrainingorprediction. Forinstance IPSaddscompensationtounpopularitemsandadjusts
unpopularitems. α-AdjNormenhancesthefocusonunpopularitemsbycontrollingthenormalizationstrengthduringthe
neighborhoodaggregationprocessinGCN-basedmodels.
• Decorrelation-basedmethodsaimtoeffectivelyremovethecorrelationsbetweenitemrepresentations(orpredictionscores)
andpopularity. Forinstance MACRusescounterfactualreasoningtoeliminatethedirectimpactofpopularityonitem
outcomes. Incontrast InvCFoperatesontheprinciplethatitemrepresentationsremaininvarianttochangesinpopularity
semantics filteringoutunstableoroutdatedpopularitycharacteristicstolearnunbiasedrepresentations.
• Contrastive-learning-basedmethodsaimtoachieveoveralluniformityinitemrepresentationsusingInfoNCE preserving
moreinherentcharacteristicsofitemstomitigatepopularitybias. Thisapproachhasbeendemonstratedasastate-of-the-art
methodforalleviatingpopularitybias. Itemploysdataaugmentationtechniquessuchasgraphaugmentationorfeature
augmentationtogeneratedifferentviews maximizingpositivepairconsistencyandminimizingnegativepairconsistency
topromotemoreuniformrepresentations. Specifically Adap-τ adjustsuser/itemembeddingstospecificvalues while
SimGCLintegratesInfoNCElosstoenhancerepresentationuniformityandalleviatepopularitybias.
4.2 RepresentationLearningforCF
Representationlearningiscrucialinrecommendationsystems  especiallyinmoderncollaborativefiltering(CF)techniques. It
createspersonalizedembeddingsthatcaptureuserpreferencesanditemcharacteristics. Thequalityoftheserepresentationscritically
determinesarecommendersystem’seffectivenessbypreciselycapturingtheinterplaybetweenuserinterestsanditemfeatures.
Recentstudiesemphasizetwofundamentalprinciplesinrepresentationlearning: alignmentanduniformity. Thealignmentprinciple
recommenditemsthatalignwithauser’sinterests. Thisprincipleiscrucialwhenaccuratelyreflectinguserpreferencesthrough
correspondingitemcharacteristics. Conversely theuniformityprincipleensuresabalanceddistributionofallembeddingsacrossthe
representationspace. Thisapproachpreventstheover-concentrationofembeddingsinspecificareas enhancingrecommendation
diversityandimprovinggeneralizationtounseendata.
Inthiswork wefocusonaligningtherepresentationsofpopularandunpopularitemsinteractedwithbythesameuserandre-
weightinguniformitytomitigaterepresentationseparation.OurmodelPAACuniquelyaddressespopularitybiasbycombininggroup
alignmentandcontrastivelearning afirstinthefield. Unlikepreviousworksthatalignpositiveuser-itempairsorcontrastivepairs
PAACdirectlyalignspopularandunpopularitems leveragingtherichinformationofpopularitemstoenhancetherepresentations
ofunpopularitemsandreduceoverfitting. Additionally weintroducetargetedre-weightingfromapopularity-centricperspectiveto
achieveamorebalancedrepresentation.
Inthisstudy wehaveexaminedpopularitybiasandputforwardPAACasamethodtolessenitsimpact. Wepostulatedthatitems
engagedwithbythesameuserexhibitcommontraits andweutilizedthisinsighttocoordinatetherepresentationsofbothpopular
andunpopularitemsviaapopularity-conscioussupervisedalignmentmethod. Thisstrategyfurnishedadditionalsupervisorydatafor
lesspopularitems. Itisimportanttonotethatourconceptofaligningandcategorizingitemsaccordingtouser-specificpreferences
introducesafreshperspectiveonalignment. Moreover wetackledtheproblemofrepresentationseparationseenincurrentCL-based
modelsbyincorporatingtwohyperparameterstoregulatetheinfluenceofitemswithvaryingpopularitylevelswhenconsidered
aspositiveandnegativesamples. Thismethodrefinedtheuniformityofrepresentationsandsuccessfullyreducedseparation. We
validatedourmethod PAAC onthreepubliclyavailabledatasets demonstratingitseffectivenessandunderlyingrationale.
Inthefuture wewillexploredeeperalignmentandcontrastadjustmentstailoredtospecifictaskstofurthermitigatepopularity
recommendationsystems.
ThisworkwassupportedinpartbygrantsfromtheNationalKeyResearchandDevelopmentProgramofChina theNationalNatural
ScienceFoundationofChina theFundamentalResearchFundsfortheCentralUniversities andQuanChengLaboratory.
Introduction
Methodology
2.1
Supervised Alignment Module
LSA =
�
u∈U
|Iu|
i∈Iu
pop j∈Iu
||f(i) − f(j)||2
(1)
where f(·) is a recommendation encoder and hi = f(i). By efficiently using the inherent information in the data  we provide more
2.2
Re-weighting Contrast Module
into a popular group Ipop and an unpopular group Iunpop based on their respective popularity levels  classifying the top x% of items
as Ipop:
IB = Ipop ∪ Iunpop  ∀i ∈ Ipop ∧ j ∈ Iunpop  p(i) > p(j)
(2)
where Ipop ∈ IB and Iunpop ∈ IB are disjoint  with Ipop consisting of the top x% of items in the batch. In this work  we dynamically
the loss for different item groups. Specifically  we introduce the hyperparameter α to control the positive sample weights between
item = α × LCL
pop + (1 − α) × LCL
(3)
pop represents the contrastive loss when popular items are considered as positive samples  and LCL
contrastive loss when unpopular items are considered as positive samples. The value of α ranges from 0 to 1  where α = 0 means
unpop  and α = 1 means exclusive emphasis on the loss of popular items
as negative samples. The hyperparameter β is then used to control the degree to which unpopular items are pushed away. This is
L
′
pop =
i∈Ipop
log
exp(h
ihi/τ)
j∈Ipop exp(h
ihj/τ) + β �
j∈Iunpop exp(h
ihj/τ)
(4)
unpop =
i∈Iunpop
(5)
where the parameter β ranges from 0 to 1  controlling the negative sample weighting in the contrastive loss. When β = 0  it means
that only intra-group uniformity optimization is performed. Conversely  when β = 1  it means equal treatment of both popular and
unpopular items in terms of their impact on positive samples. The setting of β allows for a flexible adjustment between prioritizing
LCL = 1
2 × (LCL
item + LCL
user).
(6)
2.3
Model Optimization
recommendation loss (LREC)  supervised alignment loss (LSA)  and re-weighting contrast loss (LCL).
L = LREC + λ1LSA + λ2LCL + λ3||Θ||2
(7)
where Θ is the set of model parameters in LREC as we do not introduce additional parameters  λ1 and λ2 are hyperparameters that
and λ3 is the L2 regularization coefficient. After completing the model training process  we use the dot product to predict unknown
Experiments
3.1
Experiments Settings
3.1.1
Datasets
3.1.2
Baselines and Evaluation Metrics
3.2
Overall Performance
• Our proposed model PAAC consistently outperforms all baselines and significantly mitigates the popularity bias. Specif-
• Regarding the baselines for mitigating popularity bias  the improvement of some is relatively limited compared to the
• Different metrics across various datasets show varying improvements in model performance. This suggests that different
3.3
Ablation Study
second-best performance is underlined. The superscripts * indicate p ≤ 0.05 for the paired t-test of PAAC vs. the best baseline (the
Yelp2018
Gowalla
Amazon-book
Recall@20
HR@20
NDCG@20
MF
0.0050
0.0109
0.0093
0.0343
0.0422
0.0280
0.0370
0.0388
0.0270
LightGCN
0.0048
0.0111
0.0098
0.0380
0.0468
0.0302
0.0421
0.0439
0.0304
IPS
0.0104
0.0183
0.0158
0.0562
0.0670
0.0444
0.0488
0.0510
0.0365
MACR
0.0402
0.0312
0.0265
0.0908
0.1086
0.0600
0.0515
0.0609
0.0487
α-Adjnorm
0.0053
0.0088
0.0080
0.0328
0.0409
0.0267
0.0450
0.0264
InvCF
0.0344
0.0291
0.1001
0.1202
0.0662
0.0665
Adap-τ
0.0497
0.0341
0.1182
0.1248
0.0794
0.0641
0.0678
0.0511
SimGCL
0.0449
0.0518
0.0345
0.1194
0.1228
0.0804
0.0628
0.0648
0.0525
PAAC
0.0494*
0.0574*
0.0375*
0.1232*
0.1321*
0.0848*
0.0701*
0.0724*
0.0556*
Imp.
+9.78 %
+10.81%
+8.70%
+3.18%
+5.85%
+5.47%
+9.36%
+6.78%
5.90%
pop and LCL
unpop. This approach
PAAC-w/o P
0.0443
0.0536
0.0340
0.1098
0.1191
0.0750
0.0616
0.0639
0.0458
PAAC-w/o U
0.0462
0.0545
0.0358
0.1120
0.1179
0.0752
0.0594
0.0617
0.0464
PAAC-w/o A
0.0466
0.0547
0.0360
0.1195
0.1260
0.0815
0.0687
0.0711
3.4
Debias Ability
using the NDCG@20 metric across different popularity groups. We use ∆ to denote the accuracy gap between the two groups. We
• Improving the performance of unpopular items is crucial for enhancing overall model performance. Specially  on the
• Our proposed PAAC significantly enhances the recommendation performance for unpopular items. Specifically  we observe
• PAAC has successfully narrowed the accuracy gap between different item groups. Specifically  PAAC achieved the smallest
3.5
Hyperparameter Sensitivities
In this section  we analyze the impact of hyperparameters in PAAC. Firstly  we investigate the influence of λ1 and λ2  which
re-weighting contrastive loss  we introduce two hyperparameters  α and β  to control the re-weighting of different popularity items
as positive and negative samples. Finally  we explore the impact of the grouping ratio x on the model’s performance.
3.5.1
Effect of λ1 and λ2
As formulated in Eq. (11)  λ1 controls the extent of providing additional supervisory signals for unpopular items  while λ2 controls
accuracy. Vertically  as λ1 increases  the performance also initially increases and then decreases. This suggests that suitable
3.5.2
Effect of re-weighting coefficient α and β
contrastive loss. Specifically  α controls the weight difference between positive samples from popular and unpopular items  while β
In our experiments  while keeping other hyperparameters constant  we search α and β within the range {0  0.2  0.4  0.6  0.8  1}. As
α and β increase  performance initially improves and then declines. The optimal hyperparameters for the Yelp2018 and Gowalla
datasets are α = 0.8  β = 0.6 and α = 0.2  β = 0.2  respectively. This may be attributed to the characteristics of the datasets. The
Yelp2018 dataset  with a higher average interaction frequency per item  benefits more from a higher weight α for popular items as
Notably  α and β are not highly sensitive within the range [0  1]  performing well across a broad spectrum. Performance exceeds the
baseline regardless of β values when other parameters are optimal. Additionally  α values from [0.4  1.0] on the Yelp2018 dataset
and [0.2  0.8] on the Gowalla dataset surpass the baseline  indicating less need for precise tuning. Thus  α and β achieve optimal
3.5.3
Effect of grouping ratio x
unpopular categories. Specifically  the top x% of items are classified as popular and the remaining (100 - x)% as unpopular  with x
20%
0.0467
0.0555
0.0361
0.1232
0.1319
0.0845
40%
0.0505
0.0581
0.0378
0.1239
0.1325
0.0848
50%
0.0494
0.0574
0.0375
0.1321
60%
0.0492
0.0569
0.1225
0.1314
0.0843
80%
0.0350
0.1176
0.1270
0.0818
Related Work
4.1
Popularity Bias in Recommendation
• Re-weighting-based methods aim to increase the training weight or scores for unpopular items  redirecting focus away
• Decorrelation-based methods aim to effectively remove the correlations between item representations (or prediction scores)
• Contrastive-learning-based methods aim to achieve overall uniformity in item representations using InfoNCE  preserving
to promote more uniform representations. Specifically  Adap-τ adjusts user/item embeddings to specific values  while
4.2
Representation Learning for CF
Conclusion"
R030,0,,"on a social media platform Twitter in any given time interval
is usually quite low. However  there is a subset of users whose
volume of posts is much higher than the median. In this paper
we investigate the content diversity and the social neighborhood
of these extreme users and others. We deﬁne a metric called
“interest narrowness”  and identify that a subset of extreme users
termed anomalous users  write posts with very low topic diversity
including posts with no text content. Using a few interaction pat-
terns we show that anomalous groups have the strongest within-
group interactions  compared to their interaction with others.
Further  they exhibit different information sharing behaviors
with other anomalous users compared to non-anomalous extreme
tweeters.
Index Terms —Twitter  Social Media  user characterization
network analysis  content diversity  behavior analysis
I. I NTRODUCTION
Social Media is part of our daily lives  and increasingly
more people are actively participating in various social media
platforms. It was recently reported [10] that Twitter has 126
million daily users  with an estimated annual growth rate of
about 9%. It is estimated that roughly 46% of Twitter users
are on the platform daily. In this paper  we investigate the
following questions: What types of users tweet an enormous
amount and what do they talk about?
The intuition behind this paper comes from the observation
that we can characterize users’ tweeting behavior based on the
volume and the content diversity of their tweets.
We ﬁrst consider tweet volumes for individual users. How
often do people tweet? Based on our data set of over 1.5
billion tweets  we observe that over any arbitrary time interval
the number of tweets by a user follows a power law type of
distribution (see Fig. 1(a) for a typical distribution for one
month) – most users post very few tweets while only a few
users write considerably more tweets. In the empirical result
shown in Fig. 1(a)  only 20% of the users posted more than 24
tweets in July 2017. We use the term extreme tweeters (ETT)
for users who tweet more frequently than an average user in
any given time interval (we give a more precise deﬁnition
later).
A different stratiﬁcation of users can be created based on the
content diversity (c-diversity) of their posts. Intuitively  a user
who is interested in many topics will have a higher content
diversity than a user with a narrow range of interests (e.g.  only
This works is partly funded by NSF Grant 1738411football). One simple  but rough measure of c-diversity is the
number of distinct words (not counting mentions) used by a
user over all their posts in a given period of time. Fig. 1(b)
shows a typical plot of the c-diversity of users as a function
of the number of tweets they sent. In general  users who tweet
more tend to have higher c-diversity. Clustering the frequency
distribution reveals three different clusters corresponding to (a)
users who tweet less and and use fewer distinct words (blue
cluster)  (b) users who tweet more and have higher content
diversity (greenish yellow cluster)  and (c) the small number
of users who tweet more and yet have low content diversity
(red cluster).
In this case study  we explore the tweeting behavior as
well as the social network of ETT  who constitute groups
(b) and (c) above. We are particularly interested in group
(c) because at ﬁrst glance  their high-tweet-rate  low-diversity
behavior is anomalous and appears somewhat counterintuitive.
In the light of this exploration  the paper makes the following
contributions:
1) We propose a novel way to classify users based on their
tweet rate and a new measure of verbosity called interest
narrowness .
2) We present an algorithm to detect the anomalous users.
3) We investigate the nature of the network relations of the
anomalous group and the other groups.
4) We show that the social interaction of anomalous and
non-anomalous groups vary with time  and the vigor-
ousness of the interaction intensiﬁes around events like
elections.
A. Related Work
The problem of user characterization in a social network
has been investigated by many research groups. We present a
few samples of these research efforts.
A 2016 survey [11] covers a wide bandwidth of different
approaches for user characterization. The behavioral properties
they report range from conscientiousness and extroversion
to privacy behavior  deceptive traits and response to social
attacks. In contrast to our approach  many of the reported
analyses studied in this paper are based on focused user
surveys.
Gabrowicz et al [5] take a bond-theory based approach and
distinguish between social user groups and topical user groups
in a network based on features like reciprocity  topicalityarXiv:1905.00567v1  [cs.SI]  2 May 2019
100101102
count of tweets per user in one month100101102103104count of users(a)
0 25 50 75 100 125 150 175 200
number of tweets in one month0200400600800100012001400number of distinct words
(b)
Fig. 1. (a) Frequency distribution of tweet count for users in a month. (b)
Frequency distribution of distinct words vs. number of tweets in a month.
and activity. Of these  topicality  deﬁned based on a metric
called “normalized entropy” measures how much the topics
of discussion vary within a group. The higher the entropy  the
greater is the variety of terms and  according to the theory  the
more social the group is. However  this measure considers the
words to be independent  which usually does not hold.
Similar to our notion of anomalous users  [8] investigates
the concept of “dedicators”  users who transmit information in
selected topic areas to the people in their egocentric networks.
The concept of dedication is determined by volume  engage-
ment  personal tendencies and topic weight  where personal
tendency includes the user’s topic diversity as measured by
Latent Dirichlet Allocation (LDA)  and engagement measures
the activity level of conversations.
Diversity of topics within text posts is also analyzed in
papers like [9] that perform LDA to compute topics and then
determine topic diversity across a users posts as the number
of distinct probable topics found across all of the users posts.
Closer to our application  Bail et al [2] used a POS-
tagged BOW model to analyze Facebook posts for politi-
cal content and derived a network of correlated concepts.
They demonstrated how some advocacy organizations produce
social media messages that inspire far-ranging conversation
among social media users. Interestingly  their network analysis
is based on the co-occurrence of concept terms from which
they extract interesting connection patterns that characterize
inﬂuence modalities for advocacy groups.
In contrast to all related work  ours is the ﬁrst attempt  to
our knowledge  that speciﬁcally analyzes the posting behavior
and interaction patterns of extreme tweeters. Although we use
Twitter as our example social media platform  our method is
equally valid for any other platform that exhibits high-volumepostings and vigorous interactions.
II. T HESETTING
A. User Behavior Classiﬁcation
We now formalize the intuitive user behavior classiﬁcation
scheme presented in Section I.
1) Classiﬁcation based on Tweet Volume: Assume that
Wis the time window of observation (e.g.  24 months)
andWis the minimal analytic interval (MAI)  i.e.
a minimum time-interval (e.g.  1 week) over which data is
collected in order to perform any user behavior analysis. We
denote consecutive MAIs as 1;2:::nwithinW.
Deﬁnition 1 (ETT Behavior): LetUbe the set of users  and
Tu(i)be the number of posts by a user u2Uini-th MAI
i  then we say an user u0exhibits ETT behavior in iif
Tu0(i)E
Tu
i
+q
Var
where0is an
arbitrary constant to control the selectivity.
Deﬁnition 2 (ETT Interval): If a useruexhibits ETT
behavior in MAI fu
i;u
j:::u
lg  then the ETT interval
of useruis deﬁned as the concatenation of consecutive
subintervals fromfu
kg.
Often a single user would not have a single continuous
period of hyperactivity  but several ETT intervals within an
observation window. We can use both the longest ETT interval
(LETTI)  and the total ETT interval (TETTI) as measures of
a user’s sustained ETT behavior. In this paper  we simply
classify users with no associated ETT intervals as regular
users   and users with at least one ETT interval as ETT users .
2) Classiﬁcation based on Content Diversity: In Section I
we used the number of distinct words as a rough measure of
c-diversity for a user. However  it is not a suitable measure
of c-diversity because it actually measures the vocabulary
diversity of a user. While a low-vocabulary user will have
lower c-diversity than a high-vocabulary user  the measure
cannot well distinguish between the c-diversities of two users
with comparable vocabulary sizes. Secondly  just the raw count
of words does not capture the thematic diversity of a user.
Although two users have comparable vocabulary sizes  one
may cover more themes than the other. In Section III  we
develop a new measure called Interest Narrowness by taking
a topic model type approach.
III. M EASURING INTEREST NARROWNESS
We use the bag-of-words and singular value decomposition
(SVD) techniques to develop the Interest Narrowness measure.
For a useru  we construct its text matrix Muby adopting the
bag-of-words model on all tweets of uduring a certain period
of time. Let pbe the tweet count of uandqbe the number of
distinct words (except stop words and URLs) over all tweets
ofu. Apparently  Muis of dimension pq  and without loss
of generality  we assume that pq. Apply SVD to Mu:
Mu=UuV; (1)
where UandVareppandqqunitary matrices respectively
anduis a matrix where the diagonal entries u
i=u
i;i
are singular values. Equivalently  Mucan be rewritten by a
weighted sum of pseparable matrices:
Mu=X
iAu
i=X
iu
iUi
Vi; (2)
where UiandViare theithcolumns of UandVrespectively
and
refers to the outer product. Based on Eq. (2)  we deﬁne
the contribution of jthseparable matrix Au
jas follows
cu
j=u
j2
P
iu2
i: (3)
1) Exact SVD Based Measure (EM): Assume that separable
matrices Au
1;;Au
pare sorted in descending order by their
corresponding singular values. Given a threshold d2[0;1]
letKdenote the minimum value of ksuch that
kX
j=1cu
j=Pk
j=1u2
jP
id:
ThusKis the minimum number of ksuch that the top- k
separable matrices can explain d100% of matrix Mu. In
other words  the ﬁrst Kcolumns of Vcan represent most of
the topics of u’s tweets. Naturally  the interest narrowness of
userucan be deﬁned as
u= 1 K
p: (4)
Notably  the bottleneck of the exact measurement is the
computation of SVD on text matrix Mu  which takes time
O(min(p2q;pq2)).
2) Randomized SVD based Measure (RM): Evaluation for
the ﬁrst measure for all ETT users is expensive since both
the size of the text matrix and the number of ETT users can
be large. To speed up the computation  we can approximate
matrix decomposition by using the randomized SVD [6] where
only partial singular values are computed. For a user uwith
text matrix Muof dimension pq  it can be approximated
by
MueUeueV; (5)
whereeUis ofpk eVis ofkqandeuis ofkkwhere
k<min(p;q). The interest narrowness is then given by
u=Pk
j=1eju2
i=Pk
jjMujj2
F; (6)
wherejjMujjFis the Frobenius norm of matrix Muand
F=Pp
i=1Pq
j=1jmu
ijj2=P
i=1u2
i. The com-
monly used implementation of randomized SVD takes time
O(pqlogk+ (p+q)k2)[3]. Notice that kserves as a hyper-
parameter in the computation of narrowness. In our experi-
ments we have noticed that setting ktomax(10;p=10)can
well represent the c-diversity of all tweets. For this ﬁxed
setting ofk  a largestands for relatively narrow topic
interests.
Given any time interval I= [ts;te]  we can deﬁne anoma-
lous users (shown as red cluster in Fig. 1(b)) as a user withETT behavior and narrow topic interests during I. Algorithm 1
provides the framework to detect anomalous users based on
the deﬁnition.
Algorithm 1: Anomalous User Detection
Input: a list of triples:T=f(T:u;T:text;T:time )g  a ﬁxed time
period:I  and hyper-parameters: and
Output: a list of anomalous users: A
/*Step 1: Find ETTs */
1T0 f(T:u;T:text )jT2T;T:time2Ig;
2group by users to get per user tweet count and tweet corpus as
C f (C:u;C:count;C:all texts )g;
3ETT fC:ujC2C;C:countE[C:count ] +p
Var[C:count )];
/*Step 2: Calculate Narrowness */
4N max(fC:countjC2Cg);
5get the maximum number of distict words for individual user
D max(fC:distinctwordjC2Cg);
6H fg ;
7ifNDMthen
8 foru2ETT do
9 use EM (Eq. (4)) to calculate interest narrowness nrw foru;
10H.add((u;nrw ));
11else
12 foru2ETT do
13 use RM (Eq. (6)) to calculate interest narrowness nrw foru;
14H.add((u;nrw ));
/*Step 3: Find Anomalous Users */
15A fH:ujH2H;H:nrwE[H:nrw] +p
Var[H:nrw)];
Algorithm 1 takes a list of (user;text;time )triples as input
and outputs all users with ETT behavior and high interest
narrowness. The algorithm has two hyper-parameters  and
  which jointly control the selectivity of anomalous users.
Intuitively  the larger andare  the more strict the criteria
of anomalous users will be. The algorithm consists of two
steps. Given a time period I  lines 1-3 constitute step 1 where
all users who exhibit ETT behavior in Iare found based on
Deﬁnition 1. For each of these users  lines 4-14 calculate
its interest narrowness. Notably  we set a threshold Mto
decide whether or not to apply the randomized SVD based
measure (RM). If the possible largest size of text matrix  i.e.
ND  is smaller than M  the exact SVD based measure
(EM) is adopted; otherwise  RM is performed for efﬁciency
consideration. In our setting  Mis set to be 2;0005;000
which is experimentally demonstrated to reach a balance
between efﬁciency and effectiveness. Finally  line 15 ﬁnds
out users from ETT with high interest narrowness values as
anomalous users.
The complexity of Algorithm 1 comes primarily from the
computation of SVD for each user in ETT . If the exact
version of SVD is adopted  the total time complexity is
O(jETTjmin(ND2;DN2))  whereNandDrepresent the
maximum tweet count and the maximum number of distinct
words  respectively. Otherwise  the total complexity should be
O(jETTj[NDlogk+ (N+D)k2]).
We choose the randomized SVD based algorithm rather
than the more standard LDA technique [8]  [9] based on
the following considerations. First  the number of potential
topics is hard to set  especially in our setting where the
topics are computed per user and we have potentially many
users. Second  when the tweet count of one user is not
big enough  SVD gives more meaningful","[ (c) because at ﬁrst glance  their high-tweet-rate  low-diversity
behaviorisanomalousandappearssomewhatcounterintuitive.
1 more people are actively participating in various social media
v In the light of this exploration  the paper makes the following
7 contributions:
6
5 about 9%. It is estimated that roughly 46% of Twitter users
0 are on the platform daily. In this paper  we investigate the
0 following questions: What types of users tweet an enormous narrowness.
. 2) We present an algorithm to detect the anomalous users.
5 amount and what do they talk about?
0 The intuition behind this paper comes from the observation 3) We investigate the nature of the network relations of the
9 anomalous group and the other groups.
thatwecancharacterizeusers’tweetingbehaviorbasedonthe
1 4) We show that the social interaction of anomalous and
v: We ﬁrst consider tweet volumes for individual users. How non-anomalous groups vary with time  and the vigor-
i ousness of the interaction intensiﬁes around events like
X often do people tweet? Based on our data set of over 1.5
billiontweets weobservethatoveranyarbitrarytimeinterval
r
a the number of tweets by a user follows a power law type of
showninFig.1(a) only20%oftheuserspostedmorethan24
approachesforusercharacterization.Thebehavioralproperties
Adifferentstratiﬁcationofuserscanbecreatedbasedonthe
diversitythanauserwithanarrowrangeofinterests(e.g. only
distinguishbetweensocialusergroupsandtopicalusergroups
ThisworksispartlyfundedbyNSFGrant1738411 in a network based on features like reciprocity  topicality
postings and vigorous interactions.
104
II. THESETTING
ers103 A. User Behavior Classiﬁcation
us
of 102 We now formalize the intuitive user behavior classiﬁcation
nt
u scheme presented in Section I.
o
c101
100 W is the time window of observation (e.g.  24 months)
100 101 102 and ∆ (cid:28) W is the minimal analytic interval (MAI)  i.e.
count of tweets per user in one month
(a) a minimum time-interval (e.g.  1 week) over which data is
denote consecutive MAIs as ∆  ∆ ...∆ within W.
1 2 n
1400
Deﬁnition 1 (ETT Behavior): Let U be the set of users  and
ber of distinct words11468020000000000 Ta∆Truubi (cid:48)(i(t∆tr∆haieri)yn)bcw≥eoenthEssetaa(cid:2)ynnTtu∆aumtnio(cid:3)buce+sorenroδtfru·op(cid:48)(cid:113)loetsxhVthesaibsbreyi(cid:2)lteTsac∆uEtuiivsT(cid:3)ei tTryw.ubhe∈ehrUeavδiion≥ri-int0h∆iMsiAainfI
m
nu200 Deﬁnition 2 (ETT Interval): If a user u exhibits ETT
00 25 nu5m0ber7 5of tw10e0ets1 i2n5 on1e5 0mo1n7t5h 200 behavior in MAI {∆ui ∆uj ...∆ul}  then the ETT interval
of user u is deﬁned as the concatenation of consecutive
Fig. 1. (a) Frequency distribution of tweet count for users in a month. (b) subintervals from {∆u}.
k
Frequencydistributionofdistinctwordsvs.numberoftweetsinamonth. Often a single user would not have a single continuous
and activity. Of these  topicality  deﬁned based on a metric observationwindow.WecanuseboththelongestETTinterval
called “normalized entropy” measures how much the topics (LETTI)  and the total ETT interval (TETTI) as measures of
of discussion vary within a group. The higher the entropy  the a user’s sustained ETT behavior. In this paper  we simply
greateristhevarietyoftermsand accordingtothetheory the classify users with no associated ETT intervals as regular
more social the group is. However  this measure considers the users  and users with at least one ETT interval as ETT users.
words to be independent  which usually does not hold. 2) Classiﬁcation based on Content Diversity: In Section I
Similar to our notion of anomalous users  [8] investigates we used the number of distinct words as a rough measure of
theconceptof“dedicators” userswhotransmitinformationin c-diversity for a user. However  it is not a suitable measure
selectedtopicareastothepeopleintheiregocentricnetworks. of c-diversity because it actually measures the vocabulary
The concept of dedication is determined by volume  engage- diversity of a user. While a low-vocabulary user will have
ment  personal tendencies and topic weight  where personal lower c-diversity than a high-vocabulary user  the measure
tendency includes the user’s topic diversity as measured by cannot well distinguish between the c-diversities of two users
Latent Dirichlet Allocation (LDA)  and engagement measures withcomparablevocabularysizes.Secondly justtherawcount
the activity level of conversations. of words does not capture the thematic diversity of a user.
Diversity of topics within text posts is also analyzed in Although two users have comparable vocabulary sizes  one
papers like [9] that perform LDA to compute topics and then may cover more themes than the other. In Section III  we
determine topic diversity across a users posts as the number develop a new measure called Interest Narrowness by taking
of distinct probable topics found across all of the users posts. a topic model type approach.
III. MEASURINGINTERESTNARROWNESS
cal content and derived a network of correlated concepts. We use the bag-of-words and singular value decomposition
Theydemonstratedhowsomeadvocacyorganizationsproduce (SVD)techniquestodeveloptheInterestNarrownessmeasure.
social media messages that inspire far-ranging conversation For a user u  we construct its text matrix Mu by adopting the
amongsocialmediausers.Interestingly theirnetworkanalysis bag-of-words model on all tweets of u during a certain period
is based on the co-occurrence of concept terms from which of time. Let p be the tweet count of u and q be the number of
they extract interesting connection patterns that characterize distinct words (except stop words and URLs) over all tweets
inﬂuence modalities for advocacy groups. of u. Apparently  Mu is of dimension p×q  and without loss
of generality  we assume that p≤q. Apply SVD to Mu:
ourknowledge  thatspeciﬁcally analyzesthe postingbehavior Mu =UΣuV  (1)
and interaction patterns of extremetweeters. Although we use
Twitter as our example social media platform  our method is whereUandVarep×pandq×qunitarymatricesrespectively
equally valid for any other platform that exhibits high-volume and Σu is a matrix where the diagonal entries σu = Σu
i i i
are singular values. Equivalently  Mu can be rewritten by a ETTbehaviorandnarrowtopicinterestsduringI.Algorithm1
weighted sum of p separable matrices: provides the framework to detect anomalous users based on
(cid:88) (cid:88) the deﬁnition.
Mu = Au = σu·U ⊗V   (2)
i i i i
i i Algorithm 1: Anomalous User Detection
whereUiandViaretheithcolumnsofUandVrespectively  Input:alistoftriples:T ={(T.u T.text T.time)} aﬁxedtime
and⊗referstotheouterproduct.BasedonEq.(2) wedeﬁne period:I andhyper-parameters:δ andλ
the contribution of jth separable matrix Au as follows  Output:alistofanomaloususers:A
j /* Step 1: Find ETTs */
σu2 1 T(cid:48)←{(T.u T.text)|T ∈T T.time∈I};
cu = j . (3) 2 groupbyuserstogetperusertweetcountandtweetcorpusas
j (cid:80) σu2 C←{(C.u C.count C.all texts)};
i i (cid:112)
3 ETT ←{C.u|C∈C C.count≥E[C.count]+δ Var[C.count)];
1) ExactSVDBasedMeasure(EM): Assumethatseparable /* Step 2: Calculate Narrowness */
matrices Au ···  Au are sorted in descending order by their 4 N ←max({C.count|C∈C});
1 p 5 getthemaximumnumberofdistictwordsforindividualuser
corresponding singular values. Given a threshold d ∈ [0 1]
D←max({C.distinct word|C∈C});
let K denote the minimum value of k such that 6 H←{};
(cid:88)k cuj = (cid:80)(cid:80)kj=1σσu2ju2 ≥d. 789 ifNf×orDuus≤∈eEMEMTtTh(Eedqno.(4))tocalculateinterestnarrownessnrw foru;
j=1 i i 10 H.add((u nrw));
Thus K is the minimum number of k such that the top-k 11 else
separable matrices can explain d×100% of matrix Mu. In 12 foru∈ETT do
13 useRM(Eq.(6))tocalculateinterestnarrownessnrw foru;
other words  the ﬁrst K columns of V can represent most of 14 H.add((u nrw));
/* Step 3: Find Anomalous Users */
user u can be deﬁned as  (cid:112)
15 A←{H.u|H∈H H.nrw≥E[H.nrw]+λ Var[H.nrw)];
K
γu =1− . (4)
p
Algorithm1takesalistof(user text time)triplesasinput
Notably  the bottleneck of the exact measurement is the and outputs all users with ETT behavior and high interest
computation of SVD on text matrix Mu  which takes time narrowness. The algorithm has two hyper-parameters  δ and
O(min(p2q pq2)). λ  which jointly control the selectivity of anomalous users.
2) Randomized SVD based Measure (RM): Evaluation for Intuitively  the larger δ and λ are  the more strict the criteria
the ﬁrst measure for all ETT users is expensive since both of anomalous users will be. The algorithm consists of two
the size of the text matrix and the number of ETT users can steps. Given a time period I  lines 1-3 constitute step 1 where
be large. To speed up the computation  we can approximate all users who exhibit ETT behavior in I are found based on
matrixdecompositionbyusingtherandomizedSVD[6]where Deﬁnition 1. For each of these users  lines 4-14 calculate
only partial singular values are computed. For a user u with its interest narrowness. Notably  we set a threshold M to
text matrix Mu of dimension p×q  it can be approximated decide whether or not to apply the randomized SVD based
by measure (RM). If the possible largest size of text matrix  i.e.
Mu ≈U(cid:101)Σ(cid:101)uV(cid:101)  (5) N · D  is smaller than M  the exact SVD based measure
where U(cid:101) is of p×k  V(cid:101) is of k×q and Σ(cid:101)u is of k×k where consideration. In our setting  M is set to be 2 000×5 000
k <min(p q). The interest narrowness is then given by
(cid:80)k σ u2 (cid:80)k σ u2 between efﬁciency and effectiveness. Finally  line 15 ﬁnds
ηu = j=1 (cid:101)j = j=1 (cid:101)j   (6) out users from ETT with high interest narrowness values as
(cid:80) σu2 ||Mu||2
i i F anomalous users.
where ||Mu|| is the Frobenius norm of matrix Mu and The complexity of Algorithm 1 comes primarily from the
F
||Mu||2 = (cid:80)p (cid:80)q |mu|2 = (cid:80) σu2. The com- computation of SVD for each user in ETT. If the exact
F i=1 j=1 ij i=1 i
monly used implementation of randomized SVD takes time version of SVD is adopted  the total time complexity is
O(pqlogk+(p+q)k2) [3]. Notice that k serves as a hyper- O(|ETT|·min(ND2 DN2))  where N and D represent the
parameter in the computation of narrowness. In our experi- maximum tweet count and the maximum number of distinct
ments we have noticed that setting k to max(10 p/10) can words respectively.Otherwise thetotalcomplexityshouldbe
well represent the c-diversity of all tweets. For this ﬁxed O(|ETT|·[NDlogk+(N +D)k2]).
setting of k  a large η stands for relatively narrow topic We choose the randomized SVD based algorithm rather
interests. than the more standard LDA technique [8]  [9] based on
Given any time interval I =[t  t ]  we can deﬁne anoma- the following considerations. First  the number of potential
s e
lous users (shown as red cluster in Fig. 1(b)) as a user with topics is hard to set  especially in our setting where the
notonlydependsonthenumberoftopics butonwithin-topic
term-diversity as well. Type	I	 Type	II	 Type	III
IV. SOCIALNETWORKANALYSIS Fig.2. Threeconnectionpatternsaroundanomaloususers.
and all their ﬁrst neighbors. These three patterns correspond
conversationtopicsoftheirnetworkneighborhoods.Wewould In this subsection  we propose three measures to assess
like to investigate four questions about anomalous users: the connectivity and density of our connection patterns in
(Q3): Ifsomeanomaloususersinteractheavilywitheachother
1) CorenessDistribution: k-coreisastandardtechniquefor
[12].Werecallthatthecorenessofavertexinagraphiskifit
Theanomaloususersthemselvesconstituteaverysmallpart
belongstoak-core butnottoa(k+1)-core.Inthispaper we
Let GT be the complete social network within time in-
terval T. To capture the holistic connective characteristic of
followingsection wepresentafewsimpleconnectionpatterns
B. Three Simple Patterns GT satisfying the Type-I pattern  denoted by GT  which
1
In Fig. 2 we show three simple connection patterns involv- would include all anomalous users  and then calculate core
ingthethreeusercategories.Therearethreetypesofnodesin number of all users in GT to get the coreness distribution.
the pattern graph: red nodes represent anomalous users  blue As GT only includes the relations among anomalous users
nodes represent extreme tweeters and white nodes represent the coreness distribution of GT would give an answer to Q1
regular users. Edges of the graph are undirected because in from Section IV-A. If the distribution shows that a subset
this case study we are simply exploring the connections and of users have relatively high coreness  we can conclude that
not the direction of messages that ﬂow between user groups. these anomalous users interact intensively with each other.
The ﬁrst connection pattern (called Type-I hereafter)  de- Similarly tocapturehowintenselydoanomaloususersinteract
noted as G = (V  E )  shows only within-group triads with extreme tweeters (or regular users)  the largest subgraph
1 1 1
for anomalous users. The edges represent the mentions re- of GT satisfying Type-II pattern (resp. Type-III pattern)  say
lationship between two anomalous users: (v  v ) ∈ E if GT (or GT)  is constructed  and the coreness distribution of
i j 1 2 3
v mentions v or v mentions v during a certain time anomalous users in GT (or GT) is calculated and compared
i j j i 2 3
periodT.Thesecondconnectionpattern(Type-II) denotedas with that of GT.
G =(V  E ) extendsthetriadstoincludeﬁrstneighborsof 2) Measure of Collective Behavior: The coreness distribu-
2 2 2
anomalous users who are also extreme tweeters in the same tion depicts the holistic interaction intensity of all anomalous
time period. V is the set of anomalous users and extreme userswithdifferentcategoriesofusers whichanswersQ1and
tweeters who are mentioned by or mention an anomalous Q2. However  it is possible that an anomalous user may not
user  and edges E also include the mentions between these stronglyconnectwithotheranomaloususers.Wefocusonthe
non-anomalous ETT users. As for the third case  the pattern groupofstronglyconnectedanomaloususers(Q3) deﬁnedas
network (Type-III) is formed by considering anomalous users follows.
Deﬁnition 3 (Anomalous Group): Let GT be a subgraph of
A
GT satisfying
• Pattern Constraint: GTA satisﬁes Type-I pattern;
• Strongest Connection: GTA is a k-core where k is the
• Maximality: There does not exist another k-core satisfy-
ing Type-I pattern that is a supergraph of GT. Fig.3. Illustrationofgroupbehaviorpatterns.
ThenthenodesofGT sayA formoneanomalousgroupwith
A users interact with others collaboratively in this core. The
coreness kA =k.
1 diversityratioβ canbeinterpretedasthefractionofusersthat
Lemma 1: For any graph G satisfying Type-x connection
pattern  x = 1 2 3  any subgraph of G also satisﬁes Type-x
r and small β (r = 1 and β = 1/3). For groups with this
the maximal k-core of GT where k is the maximum coreness
1 mentionbehaviorandtheyonlyinteractwithafewusers.The
value of nodes in GT. In Type-II or Type-III connection
1 second pattern has large r and large β (r = 1 and β = 2).
users collaboratively. For the third pattern with small r and
suchinteractionpatterns wedeﬁnethecorenessofanomalous
large β (r = 1/3 and β = 2)  group members show diverse
Deﬁnition4(CorenessofAnomalousGroup):Forananoma-
lous group A  let GTAx be a subgraph of GT such that  V. EXPERIMENTRESULTS
• GTAx satisﬁes Type-x pattern where x can be 1  2  or 3. The primary data set for this case study is a collection
• The nodes of GTAx is a superset of A. of “politics related” tweets obtained by using the Twitter
• GTAx is a kxA-core where kxA is the largest degeneracy streaming API  using a set of keywords as a ﬁlter. The
value among all subgraphs of GT satisfying Type-x ﬁlteringkeywordsarenamesofpoliticians(membersoftheUS
pattern and including nodes A. Congress membersofthePresident’scabinet)andtopicsatthe
We say kA is the coreness of A in Type-x pattern. center of public debates (healthcare  tax reform
x
Obviously  kA and kA would be at least kA because Type- Russia  abortion...). A large majority of the collected
2 3 1
II or III is an extension of Type-I pattern. From Lemma 1  data of 1.6 billion tweets spans from 2016 till date. The
kA and kA can be calculated as min{C = {c |u ∈ A}}  experiments conducted in this section is on a subset of the
2 3 u
where c is the coreness of u in GT or GT  which are the political tweet data set. We chose the subset to capture a time
u 2 3
largestsubgraphsofGT satisfyingType-IIorType-IIIpatterns interval where the total number of messages were very high
respectively. with many conversations  as described below.
We extract the maximal kA-core and kA-core including
A from GT and GT respec2tively. These3cores could be A. Extreme Tweeters Analysis
2 3
consideredasagroupofuserswhohighlyinteractwithA.To To analyze ETT behaviors  the total time window of obser-
evaluate the behaviors of group A  we provide the following vation W is set as 12 months from Nov 14 2017 to Nov 14
measures. 2018 and the minimal analytic interval (MAI) ∆ is set as one
Deﬁnition5(CommonNeighborRatioandDiversityRatio): week.Wesetδ fromDeﬁnition1as1.5andgetalluserswith
Let A be an anomalous group  and U denote the set of other ETT behavior in each MAI illustrated in Fig. 4. The red line
nodes in the kA-core or kA-core of subgraph GT or GT. showsthetotalnumberofuserswhotweetatleastonceduring
2 3 2 3
Deﬁne the common neighbor ratio (CNR) as  each MAI  and the blue line shows the percentage of users
(cid:80) with ETT behavior. Before September 2018  there are only a
N
r = u∈U u  (7) few users with ETT behaviors  however  after September the
|A||U|
where N is the number of anomalous users in A who are ataroundthemidOctoberof2018.Togetareasonablenumber
u
connected to a user u. The diversity ratio (DR) is deﬁned as  of ETT users for our experiments  we select a 6-week time
|{u|u∈U N ≥r×|A|}|
β = u . (8) onNovember6 2018.Speciﬁcally ourexperimentsarebased
|A|
The common neighbor ratio r can be interpreted as the into 6 nearly equal time intervals.
fractionofanomaloususersthatcaninteractwithauserinthe The length of total ETT intervals (TETTI) for each user
core on an average. A large r implies that these anomalous is calculated and Fig. 5 shows the distribution of users
120000 10 Nov 1 - Nov 7
%)60 Oct 1 - Oct 7
number of users146800000000000000000 456789 percent of ETT(%) entage of users (23450000
20000 c
3 per10
start date of MAI 0
interest narrowness value
Fig. 4. Percentage of users with ETT behavior at each MAI. The largest
numberexceeds12000 over10%ofthenumberofusers. Fig.6. Extremetweetersdistributionoverinterestnarrowness
105
ers104 higher than 80% and dive into their tweets. Many of these
of us103 users have extremely high narrowness because they have a lot
ber 102 of“nulltexttweets”wheretheydonotwriteanyoftheirown
u wordsbutonlymentionothers.Weselectoutuserswithmore
n101
than80%nulltexttweetsandTableIIshowstheirtotaltweets
100 count and the number of users mentioned by them during a
0 1 2 3 4 5 6 7 8 9101112 14151617 20
length of TETTI certain time period.
Fig.5. DistributionofusersoverTETTIlength.
TABLEII
over TETTI lengths. Most users are regular users who never USERSWITHMORETHAN80%OFNULLTEXTTWEETS
TimePeriod UsersCount TweetsCount MentionsCounts
behaviors frequently. The largest TETTI length within the 54 Oct1-7 4 459 203
Oct8-15 9 906 1 158
Oct16-23 13 1 956 1 284
Oct24-31 5 2 459 1 058
B. Interest Narrowness Analysis Nov1-7 4 2 600 3 222
Nov8-14 4 725 129
hyper parameters  δ and λ  are set to 1.5 and 1 respectively. During Oct 8 - Nov 7  a handful of users contribute
Table I shows the fraction of ETT users and the fraction of many null text tweets and there are a large volume of users
anomalous users (AU in Table I) over ETT users during each mentioned by them. The situation becomes the most extreme
time period. duringNov1-7whenfourusershave2600tweetsintotal of
whichmorethan80%havenoself-writtenwords andmention
TABLEI
PERCENTAGESOFETTUSERSANDANOMALOUSUSERS
TimePeriod UsersCount ETT(%ofUsers) AU(%ofETT)
Oct1-7 52 262 7.54 8.15 Toanalyzethesocialconnectivitybetweenanomaloususers
Oct8-15 76 935 8.57 8.51
Oct16-23 89 810 10.31 9.38
Oct24-31 122 570 6.14 11.69 fromthecompletesocialnetworkthatsatisfyingType-I Type-
Nov1-7 96 858 7.36 11.03
Nov8-14 69 060 8.19 7.92
callthemType-I Type-II Type-IIIsocialnetworkinthewhole
1) Change of interest narrowness over different time peri- experiment section.
ods: We calculate the interest narrowness for every extreme 1) Type-I Social Network: We evaluate the distribution of
tweeter at each time period. Fig. 6 shows the distribution of anomalous users over coreness in Type-I social network at
extreme tweeters over interest narrowness value at two time differenttimeperiods andFig.7(a)showsthecomplementary
periods: Oct 1 - 7 and Nov 1 - 7. cumulative distribution function (CCDF) of anomalous users.
Theuserdistributionshiftstorightwhentimerangechanges In the ﬁrst three weeks of October  most users do not interact
from Oct 1 - 7 to Nov 1 - 7 and more extreme tweeters with each other and more than half of anomalous users have
concentrate on higher narrowness  i.e.  extreme tweeters have 0 coreness. However  when it is near the American midterm
narrower topic interests during Nov 1 - 7. election  i.e.  from Oct 24 - Nov 7  the red and purple lines
2) Analysis of users with extreme interest narrowness: decrease slowly at the beginning and go down quickly at
As the black dashed line in Fig. 6 shows  a few users have the tail  suggesting that users concentrate on high coreness.
extremely high interest narrowness  i.e.  only 10% of their There are only around 700 anomalous users at Nov 1 - 7
tweets count number of topics can well represent all their from Table I  while the largest coreness is 72  which implies
1.0 1.0
malous users)00..68 OOOONNooccccttttvv      181218  64  ----      --OONN  OOccoottccvv  tt71    71523431 malous users)00..68 (a) GroupI
CCDF (ano00..24 CCDF (ano00..24 OOOONoccccttttv     18121  64 ---     --OON  OOccottccv  tt71   752331
0.0 0.0
0 10 20 30 40 50 60 70 0 20 40 60 80 100 120 140
core number core number
(a) TypeINetwork (b) TypeIINetwork
Fig.7. CCDFofanomaloususersovercoreness.
(b) GroupII
2) Type-II and III Social Networks: We evaluate the core- Fig.9. SamplesubgraphssatisfyingType-IIconnectionpattern.
regular neighbors are included. Fig. 8 shows the CCDF of that many users interact intensively with each other. However
anomaloususersovercorenessinthreetypesofnetworksdur- after midterm election  the lines (brown lines) shift to left and
ingNov1-7.Whenincludingﬁrstneighbors thecorenessof social networks tend to become more silent  but users still
anomalous users increases dramatically  suggesting that many have more interactions than normal days  e.g.  Oct 1 - 7. In
anomaloususershavestrongconnectionswiththeirneighbors. a word  the interaction intensity between users changes over
The yellow line reaches bottom before coreness 75  but there time  especially during important real world events.
largerthan75inType-IIandType-IIInetworksrespectively.In
addition thelargestcorenessincreasesfrom72toaround140 For ﬁrst three weeks of our observation window  there
and 200. However  from Table I  the number of anomalous is no anomalous group formed because anomalous users do
users is only around 11% of ETTs and less than 0.8% of not interact heavily during these time intervals. We ﬁnd an
regular users at Nov 1 - 7. Considering the sizes of these anomalous group in the other three time periods. Table III
three user categories  the increase of coreness of anomalous presents their group coreness of Type-I connection pattern
users is of less signiﬁcance. Thus  we conclude that many (denoted as Coreness 1) and of Type-II pattern (denoted as
anomalous users indeed have strong connections with some Coreness 2)  and their common neighbor ratio and diversity
extreme tweeters or regular users  however  from a holistic ratio in Type-II connection pattern.
viewofthreecategories interactionsamongstanomaloususers
themselves are stronger than those between anomalous users TABLEIII
with ETT users or regular users. ANOMALOUSGROUPS
3) Change of Social Network over Time: To analyze the TimePeriod Coreness1 Coreness2 CNR DR
dynamic pattern of social networks  we also plot the CCDF Oct24-31 56 143 0.37 1.59
Nov1-7 72 144 0.40 0.89
Nov8-14 13 37 0.21 4.62
observationperiods.AsshowninFig.7 bothType-IandType-
II networks show the similar trend of changes. At the ﬁrst During Nov 1 - 7  as the relatively high CNR and low DR
three time periods  users do not involve in dense interaction values suggest  the anomalous users in this group (Group I)
witheachotherandthelinesdecreaserapidlyatthebeginning tendtohavesimilarmentionbehaviorsandtheydonotinteract
andreach0before15coreness.However duringOct24-Nov with many people comparing to their group size. Instead
7  the lines suddenly and dramatically shift to right and they during Nov 8 - 14  anomalous users in the group (Group II)
decreaseslowlyatﬁrstbutmuchfasteratthetail whichmeans tend to have diverse behaviors  i.e.  they do not interact with
molous users)001...680 TTTyyypppeee---III II Is sosooccicaiaial l ln nneeettwtwwooorrkrkk ptwhahettoearncnoomnfonarelcothtueswsegitrhtowuaoptglaernaodsutpbosl.unTeehnaenoodremedsandlooeundsoestuesdeeerxn.toretemuesetwrsefertoemrs
CDF (ano00..24 tioTnhbeethwavoioanrso.mWaleoucsolglerocutpasllshhoawshtcaogmspulseetedlybydifmfeermenbtemrseno-f
C each group to mention i) regular users (regular hashtags)  ii)
0.0
0 25 50 75 100 125 150 175 200 non-anomalousETT(ETThashtags)andiii)eachotherwithin
core number
Fig.8. CCDFofanomaloususersinthreenetworksduringNov1-7
VI.","interest narrowness  and user stratiﬁcation.
REFERENCES
[1] J. I. Alvarez-Hamelin  L. Dall’Asta  A. Barrat  and A. Vespignani.
Large scale networks ﬁngerprinting and visualization using the k-core
decomposition. In Adv. in Neural Info. Proc. Syst.   pages 41–50  2006.
[2] C. A. Bail. Combining natural language processing and network analysis
to examine how advocacy organizations stimulate conversation on social
media. Proc. of the National Academy of Sciences   113(42):11823–
11828  2016.
[3] E. J. Cand `es and B. Recht. Exact matrix completion via convex
optimization. Found. of Comput. Mathematics   9(6):717  2009.
[4] S. N. Dorogovtsev  A. V . Goltsev  and J. F. F. Mendes. K-core
organization of complex networks. Phys. Rev. Letters   96(4):40601
2006.
[5] P. A. Grabowicz  L. M. Aiello  V . M. Eguiluz  and A. Jaimes. Distin-
guishing topical and social groups based on common identity and bond
theory. In Proc. of the 6th ACM Int. Conf. on Web search and data
mining   pages 627–636. ACM  2013.
[6] N. Halko  P.-G. Martinsson  and J. A. Tropp. Finding structure
with randomness: Probabilistic algorithms for constructing approximate
matrix decompositions. SIAM review   53(2):217–288  2011.
[7] Q. Huang  V . K. Singh  and P. K. Atrey. Cyber bullying detection using
social and textual analysis. In Proc. of the 3rd Int. Workshop on Socially-
Aware Multimedia   pages 3–6. ACM  2014.
[8] J. Jang and S.-H. Myaeng. Discovering dedicators with topic-based
semantic social networks. In Proc. of the 7th Int. AAAI Conf. on Weblogs
and Social Media   2013.
[9] F. T. O’Donovan  C. Fournelle  S. Gafﬁgan  O. Brdiczka  J. Shen
J. Liu  and K. E. Moore. Characterizing user behavior and information
propagation on a social multimedia network. In 2013 IEEE Int. Conf.
on Multimedia and Expo Workshops   pages 1–6. IEEE  2013.
[10] H. Shaban. Twitter reveals its daily active user numbers for the ﬁrst
time. Washington Post   Feb. 2019.
[11] T. Tuna  E. Akbas  A. Aksoy  M. A. Canbaz  U. Karabiyik  B. Gonen
and R. Aygun. User characterization for online social networks. Social
Network Analysis and Mining   6(1):104  2016.
[12] J. Ugander  L. Backstrom  C. Marlow  and J. Kleinberg. Structural
diversity in social contagion. Proc. of the National Academy of Sciences
109(16):5962–5966  2012.
Xiuwen Zheng Amarnath Gupta
San Diego Supercomputer Center San Diego Supercomputer Center
University of California San Diego University of California San Diego
La Jolla  CA 92093 La Jolla  CA 92093
Email: xiz675@eng.ucsd.edu Email: a1gupta@ucsd.edu
Abstract—Thenumberofpostsmadebyasingleuseraccount football). One simple  but rough measure of c-diversity is the
on a social media platform Twitter in any given time interval number of distinct words (not counting mentions) used by a
9 user over all their posts in a given period of time. Fig. 1(b)
1 shows a typical plot of the c-diversity of users as a function
0 of these extreme users and others. We deﬁne a metric called ofthenumberoftweetstheysent.Ingeneral userswhotweet
2
“interestnarrowness” andidentifythatasubsetofextremeusers  more tend to have higher c-diversity. Clustering the frequency
y termedanomaloususers writepostswithverylowtopicdiversity  distributionrevealsthreedifferentclusterscorrespondingto(a)
a includingpostswithnotextcontent.Usingafewinteractionpat-
M terns we show that anomalous groups have the strongest within-
2  Further  they exhibit different information sharing behaviors diversity (greenish yellow cluster)  and (c) the small number
withotheranomaloususerscomparedtonon-anomalousextreme of users who tweet more and yet have low content diversity
] tweeters. (red cluster).
I Index Terms—Twitter  Social Media  user characterization
S In this case study  we explore the tweeting behavior as
. well as the social network of ETT  who constitute groups
s
c I.","because the quality of LDA-produced topics is satisfactory
when the training set is large. Finally  the interest narrowness
not only depends on the number of topics  but on within-topic
term-diversity as well.
IV. S OCIAL NETWORK ANALYSIS
A. Rationale
So far  we have focused on the ETT behavior of users
and identiﬁed narrow-interest users with ETT behavior. We
now explore the social network around these users to study
their interaction patterns with other users  as well as the
conversation topics of their network neighborhoods. We would
like to investigate four questions about anomalous users:
(Q1): Do anomalous users interact heavily with each other?
(Q2): Do anomalous users interact heavily with extreme tweet-
ers or regular users?
(Q3): If some anomalous users interact heavily with each other
what is their behavior pattern as a group?
(Q4): Do anomalous users have different behaviors during
different time periods  especially around major public-
opinion-inciting events?
The anomalous users themselves constitute a very small part
of the larger social network that is induced by various forms
of communication (reply  retweets  mentions  etc.) between
any pair of users. We seek to identify connection patterns
in network neighborhoods of the users of interest. In the
following section  we present a few simple connection patterns
amongst the anomalous users  the non-anomalous users with
ETT behavior and regular users  and Section V shows that
these patterns sufﬁce to bring out some distinctive character-
istics of anomalous users.
B. Three Simple Patterns
In Fig. 2 we show three simple connection patterns involv-
ing the three user categories. There are three types of nodes in
the pattern graph: red nodes represent anomalous users  blue
nodes represent extreme tweeters and white nodes represent
regular users. Edges of the graph are undirected because in
this case study we are simply exploring the connections and
not the direction of messages that ﬂow between user groups.
The ﬁrst connection pattern (called Type-I hereafter)  de-
noted asG1= (V1;E1)  shows only within-group triads
for anomalous users. The edges represent the mentions re-
lationship between two anomalous users: (vi;vj)2E1if
vimentionsvjorvjmentionsviduring a certain time
periodT. The second connection pattern (Type-II)  denoted as
G2= (V2;E2)  extends the triads to include ﬁrst neighbors of
anomalous users who are also extreme tweeters in the same
time period. V2is the set of anomalous users and extreme
tweeters who are mentioned by or mention an anomalous
user  and edges E2also include the mentions between these
non-anomalous ETT users. As for the third case  the pattern
network (Type-III) is formed by considering anomalous users
Type	I	Type	II	Type	III	Fig. 2. Three connection patterns around anomalous users.
andalltheir ﬁrst neighbors. These three patterns correspond
to the questions Q1 and Q2 posed in Section IV-A.
C. Metrics
In this subsection  we propose three measures to assess
the connectivity and density of our connection patterns in
the network during any time interval. With these measures
we expect to evaluate the interaction intensity of anomalous
users and their neighbors (Q1 and Q2)  and capture the group
behavior pattern (Q3) of the anomalous user group and their
immediate neighborhood.
1) Coreness Distribution: k-core is a standard technique for
tasks like dense region detection [1]  [4] and network feature
extraction for different kinds of social network analysis [7]
[12]. We recall that the coreness of a vertex in a graph is kif it
belongs to a k-core  but not to a (k+1)-core. In this paper  we
use the terms “coreness” and “core number” interchangeably.
LetGTbe the complete social network within time in-
tervalT. To capture the holistic connective characteristic of
anomalous users and their related users in GT  we introduce
the concept of “coreness distribution”  which is the distri-
bution of coreness of nodes in the largest subgraph of GT
satisfying a certain connection pattern (i.e.  Type-I  Type-II
or Type-III). Speciﬁcally  to measure the connection intensity
among anomalous users  we extract the largest subgraph of
GTsatisfying the Type-I pattern  denoted by GT
1  which
would include all anomalous users  and then calculate core
number of all users in GT
1to get the coreness distribution.
AsGT
1only includes the relations among anomalous users
the coreness distribution of GT
1would give an answer to Q1
from Section IV-A. If the distribution shows that a subset
of users have relatively high coreness  we can conclude that
these anomalous users interact intensively with each other.
Similarly  to capture how intensely do anomalous users interact
with extreme tweeters (or regular users)  the largest subgraph
ofGTsatisfying Type-II pattern (resp. Type-III pattern)  say
GT
2(orGT
3)  is constructed  and the coreness distribution of
anomalous users in GT
3) is calculated and compared
with that of GT
1.
2) Measure of Collective Behavior: The coreness distribu-
tion depicts the holistic interaction intensity of all anomalous
users with different categories of users  which answers Q1 and
Q2. However  it is possible that an anomalous user may not
strongly connect with other anomalous users. We focus on the
group of strongly connected anomalous users (Q3)  deﬁned as
follows.
Deﬁnition 3 (Anomalous Group): LetGT
Abe a subgraph of
GTsatisfying
Pattern Constraint: GT
Asatisﬁes Type-I pattern;
Strongest Connection: GT
Ais ak-core where kis the
largest degeneracy value among all subragphs of GT
satisfying Type-I pattern;
Maximality: There does not exist another k-core satisfy-
ing Type-I pattern that is a supergraph of GT
A.
Then the nodes of GT
A  sayA  form one anomalous group with
corenesskA
1=k.
Lemma 1: For any graph Gsatisfying Type- xconnection
pattern x= 1;2;3  any subgraph of Galso satisﬁes Type- x
pattern.
Based on Lemma 1  an anomalous group is  by deﬁnition
the maximal k-core ofGT
1wherekis the maximum coreness
value of nodes in GT
1. In Type-II or Type-III connection
patterns  users in an anomalous group may act collaboratively
(i.e.  they mention similar users)  or they may have diverse
behaviors (i.e.  they interact with different users). To measure
such interaction patterns  we deﬁne the coreness of anomalous
group and then present two metrics.
Deﬁnition 4 (Coreness of Anomalous Group): For an anoma-
lous groupA  letGT
Axbe a subgraph of GTsuch that
GT
Axsatisﬁes Type- xpattern where xcan be 1  2  or 3.
The nodes of GT
Axis a superset ofA.
Axis akA
x-core where kA
xis the largest degeneracy
value among all subgraphs of GTsatisfying Type- x
pattern and including nodes A.
We saykA
xis the coreness of Ain Type-xpattern.
Obviously kA
2andkA
3would be at least kA
1because Type-
II or III is an extension of Type-I pattern. From Lemma 1
kA
3can be calculated as minfC=fcuju2 Agg
wherecuis the coreness of uinGT
2orGT
3  which are the
largest subgraphs of GTsatisfying Type-II or Type-III patterns
respectively.
We extract the maximal kA
2-core andkA
3-core including
AfromGT
2andGT
3respectively. These cores could be
considered as a group of users who highly interact with A. To
evaluate the behaviors of group A  we provide the following
measures.
Deﬁnition 5 (Common Neighbor Ratio and Diversity Ratio):
LetAbe an anomalous group  and Udenote the set of other
nodes in the kA
2-core orkA
3-core of subgraph GT
3.
Deﬁne the common neighbor ratio (CNR) as
r=P
u2UNu
jAjjUj; (7)
whereNuis the number of anomalous users in Awho are
connected to a user u. The diversity ratio (DR) is deﬁned as
=jfuju2U;NurjAjgj
jAj: (8)
The common neighbor ratio rcan be interpreted as the
fraction of anomalous users that can interact with a user in the
core on an average. A large rimplies that these anomalous
Fig. 3. Illustration of group behavior patterns.
users interact with others collaboratively in this core. The
diversity ratio can be interpreted as the fraction of users that
have interaction with the anomalous group. Fig. 3 illustrates
three typical patterns. The ﬁrst one shows a pattern with large
rand small(r= 1 and= 1=3). For groups with this
pattern  group members  i.e.  anomalous users  have similar
mention behavior and they only interact with a few users. The
second pattern has large rand large(r= 1 and= 2).
For groups with this pattern  group members have similar
behavior  in addition  they interact with a large number of
users collaboratively. For the third pattern with small rand
large(r= 1=3and= 2)  group members show diverse
mention behaviors.
V. E XPERIMENT RESULTS
The primary data set for this case study is a collection
of “politics related” tweets obtained by using the Twitter
streaming API  using a set of keywords as a ﬁlter. The
ﬁltering keywords are names of politicians (members of the US
Congress  members of the President’s cabinet) and topics at the
center of public debates ( healthcare  tax reform
Russia  abortion :::). A large majority of the collected
data of 1.6 billion tweets spans from 2016 till date. The
experiments conducted in this section is on a subset of the
political tweet data set. We chose the subset to capture a time
interval where the total number of messages were very high
with many conversations  as described below.
A. Extreme Tweeters Analysis
To analyze ETT behaviors  the total time window of obser-
vationWis set as 12 months from Nov 14 2017 to Nov 14
2018 and the minimal analytic interval (MAI) is set as one
week. We set from Deﬁnition 1 as 1.5 and get all users with
ETT behavior in each MAI illustrated in Fig. 4. The red line
shows the total number of users who tweet at least once during
each MAI  and the blue line shows the percentage of users
with ETT behavior. Before September 2018  there are only a
few users with ETT behaviors  however  after September the
fraction of ETT users grows signiﬁcantly  and reaches a peak
at around the mid October of 2018. To get a reasonable number
of ETT users for our experiments  we select a 6-week time
period around the 2018 United States mid-term elections held
on November 6  2018. Speciﬁcally  our experiments are based
on tweets from October 1 to November 14  2018  partitioned
into 6 nearly equal time intervals.
The length of total ETT intervals (TETTI) for each user
is calculated and Fig. 5 shows the distribution of users
17-11-14 18-04-03 18-10-01
start date of MAI20000400006000080000100000120000number of users
345678910
percent of ETT(%)Fig. 4. Percentage of users with ETT behavior at each MAI. The largest
number exceeds 12000  over 10% of the number of users.
0123456789101112 14151617 20
length of TETTI100101102103104105number of users
Fig. 5. Distribution of users over TETTI length.
over TETTI lengths. Most users are regular users who never
have ETT behavior. Only a few number of users have ETT
behaviors frequently. The largest TETTI length within the 54
observation MAIs is 20.
B. Interest Narrowness Analysis
Anomalous users are selected out for each time period by
following the framework illustrated in Algorithm 1. The two
hyper parameters  and  are set to 1.5 and 1 respectively.
Table I shows the fraction of ETT users and the fraction of
anomalous users (AU in Table I) over ETT users during each
time period.
TABLE I
PERCENTAGES OF ETT USERS AND ANOMALOUS USERS
Time Period Users Count ETT (% of Users) AU (% of ETT)
Oct 1 - 7 52 262 7.54 8.15
Oct 8 - 15 76 935 8.57 8.51
Oct 16 - 23 89 810 10.31 9.38
Oct 24 - 31 122 570 6.14 11.69
Nov 1 - 7 96 858 7.36 11.03
Nov 8 - 14 69 060 8.19 7.92
1) Change of interest narrowness over different time peri-
ods: We calculate the interest narrowness for every extreme
tweeter at each time period. Fig. 6 shows the distribution of
extreme tweeters over interest narrowness value at two time
periods: Oct 1 - 7 and Nov 1 - 7.
The user distribution shifts to right when time range changes
from Oct 1 - 7 to Nov 1 - 7 and more extreme tweeters
concentrate on higher narrowness  i.e.  extreme tweeters have
narrower topic interests during Nov 1 - 7.
2) Analysis of users with extreme interest narrowness:
As the black dashed line in Fig. 6 shows  a few users have
extremely high interest narrowness  i.e.  only 10% of their
tweets count number of topics can well represent all their
0.0 0.2 0.4 0.6 0.8 1.0
interest narrowness value0102030405060percentage of users (%)Nov 1 - Nov 7
Oct 1 - Oct 7Fig. 6. Extreme tweeters distribution over interest narrowness
tweets. In order to understand their tweeting behaviors  for
each time period  we select users with interest narrowness
higher than 80% and dive into their tweets. Many of these
users have extremely high narrowness because they have a lot
of “null text tweets” where they do not write any of their own
words but only mention others. We select out users with more
than80% null text tweets and Table II shows their total tweets
count and the number of users mentioned by them during a
certain time period.
TABLE II
USERS WITH MORE THAN 80% OF NULL TEXT TWEETS
Time Period Users Count Tweets Count Mentions Counts
Oct 1 - 7 4 459 203
Oct 8 - 15 9 906 1 158
Oct 16 - 23 13 1 956 1 284
Oct 24 - 31 5 2 459 1 058
Nov 1 - 7 4 2 600 3 222
Nov 8 - 14 4 725 129
During Oct 8 - Nov 7  a handful of users contribute
many null text tweets and there are a large volume of users
mentioned by them. The situation becomes the most extreme
during Nov 1 - 7 when four users have 2600 tweets in total  of
which more than 80% have no self-written words  and mention
more than 3000 distinct users.
C. User Distribution over Coreness
To analyze the social connectivity between anomalous users
and other user categories  we construct the largest subgraphs
from the complete social network that satisfying Type-I  Type-
II and Type-III connection patterns for each time period. We
call them Type-I  Type-II  Type-III social network in the whole
experiment section.
1) Type-I Social Network: We evaluate the distribution of
anomalous users over coreness in Type-I social network at
different time periods  and Fig. 7(a) shows the complementary
cumulative distribution function (CCDF) of anomalous users.
In the ﬁrst three weeks of October  most users do not interact
with each other and more than half of anomalous users have
0 coreness. However  when it is near the American midterm
election  i.e.  from Oct 24 - Nov 7  the red and purple lines
decrease slowly at the beginning and go down quickly at
the tail  suggesting that users concentrate on high coreness.
There are only around 700 anomalous users at Nov 1 - 7
from Table I  while the largest coreness is 72  which implies
extremely strong connections amongst them. However  after
the election  the largest coreness dropped to 13  which means
that many anomalous users leave hot interaction with others.
In addition  the distribution of users tends to be more uniform
since the brown line decreases smoothly.
0 10 20 30 40 50 60 70
core number0.00.20.40.60.81.0CCDF (anomalous users)Oct 1 - Oct 7
Oct 8 - Oct 15
Oct 16 - Oct 23
Oct 24 - Oct 31
Nov 1 - Nov 7
Nov 8 - Nov 14
(a) Type I Network
0 20 40 60 80 100 120 140
Nov 8 - Nov 14 (b) Type II Network
Fig. 7. CCDF of anomalous users over coreness.
2) Type-II and III Social Networks: We evaluate the core-
ness distribution of anomalous users when their ETT- or
regular neighbors are included. Fig. 8 shows the CCDF of
anomalous users over coreness in three types of networks dur-
ing Nov 1 - 7. When including ﬁrst neighbors  the coreness of
anomalous users increases dramatically  suggesting that many
anomalous users have strong connections with their neighbors.
The yellow line reaches bottom before coreness 75  but there
are around 70% and 80% of anomalous users with coreness
larger than 75 in Type-II and Type-III networks respectively. In
addition  the largest coreness increases from 72 to around 140
and 200. However  from Table I  the number of anomalous
users is only around 11% of ETTs and less than 0.8% of
regular users at Nov 1 - 7. Considering the sizes of these
three user categories  the increase of coreness of anomalous
users is of less signiﬁcance. Thus  we conclude that many
anomalous users indeed have strong connections with some
extreme tweeters or regular users  however  from a holistic
view of three categories  interactions amongst anomalous users
themselves are stronger than those between anomalous users
with ETT users or regular users.
3) Change of Social Network over Time: To analyze the
dynamic pattern of social networks  we also plot the CCDF
of anomalous users for Type-II social networks during all
observation periods. As shown in Fig. 7  both Type-I and Type-
II networks show the similar trend of changes. At the ﬁrst
three time periods  users do not involve in dense interaction
with each other and the lines decrease rapidly at the beginning
and reach 0 before 15 coreness. However  during Oct 24 - Nov
7  the lines suddenly and dramatically shift to right and they
decrease slowly at ﬁrst but much faster at the tail  which means
core number0.00.20.40.60.81.0CCDF (anomolous users)Type-I social network
Type-II social network
Type-III social network
Fig. 8. CCDF of anomalous users in three networks during Nov 1 - 7
(a) Group I
(b) Group II
Fig. 9. Sample subgraphs satisfying Type-II connection pattern.
that many users interact intensively with each other. However
after midterm election  the lines (brown lines) shift to left and
social networks tend to become more silent  but users still
have more interactions than normal days  e.g.  Oct 1 - 7. In
a word  the interaction intensity between users changes over
time  especially during important real world events.
D. Anomalous Group Behavior Pattern
For ﬁrst three weeks of our observation window  there
is no anomalous group formed because anomalous users do
not interact heavily during these time intervals. We ﬁnd an
anomalous group in the other three time periods. Table III
presents their group coreness of Type-I connection pattern
(denoted as Coreness 1) and of Type-II pattern (denoted as
Coreness 2)  and their common neighbor ratio and diversity
ratio in Type-II connection pattern.
TABLE III
ANOMALOUS GROUPS
Time Period Coreness 1 Coreness 2 CNR DR
Oct 24 - 31 56 143 0.37 1.59
Nov 1 - 7 72 144 0.40 0.89
Nov 8 - 14 13 37 0.21 4.62
During Nov 1 - 7  as the relatively high CNR and low DR
values suggest  the anomalous users in this group (Group I)
tend to have similar mention behaviors and they do not interact
with many people comparing to their group size. Instead
during Nov 8 - 14  anomalous users in the group (Group II)
tend to have diverse behaviors  i.e.  they do not interact with
others collaboratively and they mention a lot of people. Fig. 9
shows two sample subgraphs satisfying Type-II connection
pattern for these two groups. The red nodes denote users from
the anomalous group and blue nodes denote extreme tweeters
who connect with at least one anomalous user.
The two anomalous groups show completely different men-
tion behaviors. We collect all hashtags used by members of
each group to mention i) regular users (regular hashtags)  ii)
non-anomalous ETT (ETT hashtags) and iii) each other within
RedTsunami Democrats FakeNews ElectionEveRedWave
DJTrumplicansFakeNews
UnitedVoteRed0.00.20.40.60.81.01.21.4hashtag count percent (%)group hashtages
ETT hashtags
regular hashtags(a) Group I
UnitedVoteRed024681012hashtag count percent (%)group hashtages
regular hashtags (b) Group II
Fig. 10. Hashtag histogram for anomalous group.
group (group hashtags). Fig. 10 shows the count percentages
of a same subset of hashtags for the two groups. We calculate
the correlation coefﬁcient between counts of group hashtags
and ETT hashtags ( coef1 ) and that between counts of group
hashtags and regular hashtags ( coef2 ) as shown in Table IV.
Besides  the standard deviations ( stdev ) for group hashtags
ETT hashtags and regular hashtags distributions ( stdev1
stdev2  stdev3 respectively in Table IV) are calculated
for each group. Note that the stdev presented in this section
is standardized by the percentage of hashtag counts.
TABLE IV
STATISTICS OF HASHTAG DISTRIBUTIONS
coef1 coef2 stdev1 stdev2 stdev3
Group I 0.81 0.77 0.004 0.003 0.002
Group II 0.31 0.10 0.024 0.008 0.004
For Group I  it has similar intra-group and inter-group
hashtag usage behaviors as two correlation coefﬁcients indi-
cate  and all the three distributions have very low stdev
indicating a uniform use of hashtags. For Group II  however
the low coefﬁcients suggest that this group may have different
hashtag uses when they mention amongst group members
versus mention other categories. In addition  the distribution of
group hashtags (blue bars in Fig. 10) has much higher stdev
than the other two distributions (yellow and green bars in
Fig. 10). It indicates that some hashtags (e.g.  DJTrumplicans
or UnitedV oteRed) are used disproportionately often when
members of Group II interact amongst themselves. However
when they mention other categories of users  their hashtag
distribution is more uniform  i.e.  the conversation covers
broader topics.
During the ﬁrst three time periods in our observation
window  only one or two users in Group II are ETT users.
However  since Oct 24  most members of Group II show
sustained hyperactivity and high interest narrowness  i.e.  they
are anomalous users during the other three time periods.
Table V presents their group coreness in Type-I connection
pattern and the stdev of three hashtag distributions. For each
time period  this group shows highly-skewed hashtag usage
within group  but broader interests outside it.
TABLE V
BEHAVIORS OF GROUP II D URING DIFFERENT TIMEPERIODS
Time Period Coreness stdev1 stdev2 stdev3
Oct 24 - 31 54 0.040 0.004 0.005
Nov 1 - 7 62 0.015 0.003 0.003VI. C ONCLUSION
Our experimental results conﬁrm that our user stratiﬁcation
strategy successfully brings out the unusual behavior of the
extreme tweeters. The strategy itself is fairly generic and can
be applied to any social media platform. In terms of results
we ﬁnd that the highly-connected group we call “anomalous”
indeed exhibits a “clannish” behavior because of their strong
and narrowly-scoped within-group interactions that markedly
differs from their across-group behavior. The fact that their
“reach” increases with time  especially leading up to the
US mid-term elections  suggests that the group has strong
political beliefs and forms a strong trust network of its own
that establishes contact with a high number of users but
carefully controls content outside the group. Similarly  the
“anomalous” aspect of their behavior dramatically comes into
play two weeks before the elections  suggesting a motivation
to reinforce each others’ beliefs and perhaps an intention to
inﬂuence others in their mention network.
Our future work will elaborate this case study to uncover
other characteristics of these user groups and their interactions.
We will conduct a larger comparative study across more
diverse time periods  a larger cross-section of users  and
user behavior in different hashtag communities. We will also
investigate more computationally efﬁcient","1.4 gETroTu hpa hsahstahgtasges 12 gETroTu hpa hsahstahgtasges Our experimental results conﬁrm that our user stratiﬁcation
hashtag count percent (%)00011.....46802 regular hashtags hashtag count percent (%)14680 regular hashtags sebtxertarateepmgpyelietsdwucetcoeetesarsnsfy.ulTslyohcebiasrlitnrmagtseegdoyiuatitpstlehaletffoiusrnmfua.siurIlnayltgebreemnheasrvioicofrarneodsfuctlahtsne
0.2 2 we ﬁnd that the highly-connected group we call “anomalous”
0.0 RedTsunamDiemocratsFakeNewsElectionEveRedWaveDJTrumplicanFsakeNewUsnitedVoteRed 0 RedTsunamDiemocratsFakeNewsElectionEveRedWaveDJTrumplicanFsakeNewUsnitedVoteRed indeed exhibits a “clannish” behavior because of their strong
(a) GroupI (b) GroupII
Fig.10. Hashtaghistogramforanomalousgroup. “reach” increases with time  especially leading up to the
and ETT hashtags (coef1) and that between counts of group
hashtagsandregularhashtags(coef2)asshowninTableIV.
Besides  the standard deviations (stdev) for group hashtags
ETT hashtags and regular hashtags distributions (stdev1
stdev2  stdev3 respectively in Table IV) are calculated
othercharacteristicsoftheseusergroupsandtheirinteractions.
TABLEIV
STATISTICSOFHASHTAGDISTRIBUTIONS
coef1 coef2 stdev1 stdev2 stdev3 investigate more computationally efﬁcient methods of ﬁnding
GroupI 0.81 0.77 0.004 0.003 0.002
GroupII 0.31 0.10 0.024 0.008 0.004
For Group I  it has similar intra-group and inter-group REFERENCES
hashtag usage behaviors as two correlation coefﬁcients indi- [1] J. I. Alvarez-Hamelin  L. Dall’Asta  A. Barrat  and A. Vespignani.
cate  and all the three distributions have very low stdev  Large scale networks ﬁngerprinting and visualization using the k-core
decomposition. InAdv.inNeuralInfo.Proc.Syst. pages41–50 2006.
[2] C.A.Bail.Combiningnaturallanguageprocessingandnetworkanalysis
thelowcoefﬁcientssuggestthatthisgroupmayhavedifferent toexaminehowadvocacyorganizationsstimulateconversationonsocial
hashtag uses when they mention amongst group members media. Proc. of the National Academy of Sciences  113(42):11823–
11828 2016.
versusmentionothercategories.Inaddition thedistributionof
[3] E. J. Cande`s and B. Recht. Exact matrix completion via convex
grouphashtags(bluebarsinFig.10)hasmuchhigherstdev optimization. Found.ofComput.Mathematics 9(6):717 2009.
than the other two distributions (yellow and green bars in [4] S. N. Dorogovtsev  A. V. Goltsev  and J. F. F. Mendes. K-core
organization of complex networks. Phys. Rev. Letters  96(4):40601
or UnitedVoteRed) are used disproportionately often when [5] P.A.Grabowicz L.M.Aiello V.M.Eguiluz andA.Jaimes. Distin-
members of Group II interact amongst themselves. However  guishingtopicalandsocialgroupsbasedoncommonidentityandbond
mining pages627–636.ACM 2013.
distribution is more uniform  i.e.  the conversation covers [6] N. Halko  P.-G. Martinsson  and J. A. Tropp. Finding structure
broader topics. withrandomness:Probabilisticalgorithmsforconstructingapproximate
matrixdecompositions. SIAMreview 53(2):217–288 2011.
[7] Q.Huang V.K.Singh andP.K.Atrey. Cyberbullyingdetectionusing
window  only one or two users in Group II are ETT users. socialandtextualanalysis.InProc.ofthe3rdInt.WorkshoponSocially-
However  since Oct 24  most members of Group II show AwareMultimedia pages3–6.ACM 2014.
sustainedhyperactivityandhighinterestnarrowness i.e. they
semanticsocialnetworks.InProc.ofthe7thInt.AAAIConf.onWeblogs
are anomalous users during the other three time periods. andSocialMedia 2013.
Table V presents their group coreness in Type-I connection [9] F. T. O’Donovan  C. Fournelle  S. Gafﬁgan  O. Brdiczka  J. Shen
J.Liu andK.E.Moore. Characterizinguserbehaviorandinformation
patternandthestdevofthreehashtagdistributions.Foreach
time period  this group shows highly-skewed hashtag usage onMultimediaandExpoWorkshops pages1–6.IEEE 2013.
within group  but broader interests outside it. [10] H. Shaban. Twitter reveals its daily active user numbers for the ﬁrst
time. WashingtonPost Feb.2019.
[11] T.Tuna E.Akbas A.Aksoy M.A.Canbaz U.Karabiyik B.Gonen
TABLEV andR.Aygun. Usercharacterizationforonlinesocialnetworks. Social
BEHAVIORSOFGROUPIIDURINGDIFFERENTTIMEPERIODS NetworkAnalysisandMining 6(1):104 2016.
TimePeriod Coreness stdev1 stdev2 stdev3 diversityinsocialcontagion.Proc.oftheNationalAcademyofSciences
Oct24-31 54 0.040 0.004 0.005 109(16):5962–5966 2012.
Nov1-7 62 0.015 0.003 0.003
Email: xiz675@eng.ucsd.edu
Amarnath Gupta
Abstract—The number of posts made by a single user account
Index Terms—Twitter  Social Media  user characterization
I. INTRODUCTION
This works is partly funded by NSF Grant 1738411
football). One simple  but rough measure of c-diversity is the
narrowness.
in a network based on features like reciprocity  topicality
arXiv:1905.00567v1  [cs.SI]  2 May 2019
100
101
102
103
count of users
(a)
0
25
50
75
125
150
175
200
number of tweets in one month
400
600
800
1000
1200
number of distinct words
Fig. 1.
(a) Frequency distribution of tweet count for users in a month. (b)
equally valid for any other platform that exhibits high-volume
II. THE SETTING
W is the time window of observation (e.g.  24 months)
and ∆ ≪ W is the minimal analytic interval (MAI)  i.e.
denote consecutive MAIs as ∆1  ∆2 . . . ∆n within W.
T u(∆i) be the number of posts by a user u ∈ U in i-th MAI
∆i  then we say an user u′ exhibits ETT behavior in ∆i if
T u′(∆i) ≥ E
�
T u
∆i
+ δ ·
Var
where δ ≥ 0 is an
Deﬁnition 2 (ETT Interval): If a user u exhibits ETT
behavior in MAI {∆u
i   ∆u
j . . . ∆u
l }  then the ETT interval
subintervals from {∆u
k}.
users  and users with at least one ETT interval as ETT users.
III. MEASURING INTEREST NARROWNESS
For a user u  we construct its text matrix Mu by adopting the
bag-of-words model on all tweets of u during a certain period
of time. Let p be the tweet count of u and q be the number of
of u. Apparently  Mu is of dimension p × q  and without loss
of generality  we assume that p ≤ q. Apply SVD to Mu:
Mu = UΣuV
(1)
where U and V are p×p and q×q unitary matrices respectively
and Σu is a matrix where the diagonal entries σu
i = Σu
i i
are singular values. Equivalently  Mu can be rewritten by a
weighted sum of p separable matrices:
Mu =
i
Au
i =
σu
i · Ui ⊗ Vi
(2)
where Ui and Vi are the ith columns of U and V respectively
and ⊗ refers to the outer product. Based on Eq. (2)  we deﬁne
the contribution of jth separable matrix Au
j as follows
j =
j
i σu2
.
(3)
1  · · ·   Au
p are sorted in descending order by their
corresponding singular values. Given a threshold d ∈ [0  1]
let K denote the minimum value of k such that
j=1
�k
j=1 σu2
≥ d.
Thus K is the minimum number of k such that the top-k
separable matrices can explain d × 100% of matrix Mu. In
other words  the ﬁrst K columns of V can represent most of
user u can be deﬁned as
γu = 1 − K
p .
(4)
O(min(p2q  pq2)).
only partial singular values are computed. For a user u with
text matrix Mu of dimension p × q  it can be approximated
Mu ≈ �U�Σu �V
(5)
where �U is of p × k  �V is of k × q and �Σu is of k × k where
k < min(p  q). The interest narrowness is then given by
ηu =
j=1 �σj
u2
=
||Mu||2
(6)
where ||Mu||F is the Frobenius norm of matrix Mu and
�p
i=1
�q
j=1 |mu
ij|2
i=1 σu2
i . The com-
O(pq log k + (p + q)k2) [3]. Notice that k serves as a hyper-
ments we have noticed that setting k to max(10  p/10) can
setting of k  a large η stands for relatively narrow topic
Given any time interval I = [ts  te]  we can deﬁne anoma-
lous users (shown as red cluster in Fig. 1(b)) as a user with
ETT behavior and narrow topic interests during I. Algorithm 1
Input: a list of triples: T = {(T.u  T.text  T.time)}  a ﬁxed time
period: I  and hyper-parameters: δ and λ
/* Step 1: Find ETTs
*/
1 T ′ ← {(T.u  T.text)|T ∈ T   T.time ∈ I};
2 group by users to get per user tweet count and tweet corpus as
C ← {(C.u  C.count  C.all texts)};
3 ETT ← {C.u|C ∈ C  C.count ≥ E [C.count] + δ
Var [C.count)];
/* Step 2: Calculate Narrowness
4 N ← max({C.count|C ∈ C});
5 get the maximum number of distict words for individual user
D ← max({C.distinct word|C ∈ C});
6 H ← {};
7 if N × D ≤ M then
8
for u ∈ ETT do
9
use EM (Eq. (4)) to calculate interest narrowness nrw for u;
10
H.add((u  nrw));
11 else
12
13
use RM (Eq. (6)) to calculate interest narrowness nrw for u;
14
/* Step 3: Find Anomalous Users
15 A ← {H.u|H ∈ H  H.nrw ≥ E [H.nrw] + λ
Var [H.nrw)];
Algorithm 1 takes a list of (user  text  time) triples as input
narrowness. The algorithm has two hyper-parameters  δ and
λ  which jointly control the selectivity of anomalous users.
Intuitively  the larger δ and λ are  the more strict the criteria
all users who exhibit ETT behavior in I are found based on
its interest narrowness. Notably  we set a threshold M to
N · D  is smaller than M  the exact SVD based measure
consideration. In our setting  M is set to be 2  000 × 5  000
computation of SVD for each user in ETT. If the exact
O(|ETT| · min(ND2  DN 2))  where N and D represent the
O(|ETT| · [ND log k + (N + D)k2]).
IV. SOCIAL NETWORK ANALYSIS
noted as G1 = (V1  E1)  shows only within-group triads
lationship between two anomalous users: (vi  vj) ∈ E1 if
vi mentions vj or vj mentions vi during a certain time
period T. The second connection pattern (Type-II)  denoted as
G2 = (V2  E2)  extends the triads to include ﬁrst neighbors of
time period. V2 is the set of anomalous users and extreme
user  and edges E2 also include the mentions between these
Type%I%
Type%II%
Type%III%
Fig. 2. Three connection patterns around anomalous users.
[12]. We recall that the coreness of a vertex in a graph is k if it
anomalous users and their related users in GT   we introduce
GT satisfying the Type-I pattern  denoted by GT
1   which
1 to get the coreness distribution.
As GT
1 only includes the relations among anomalous users
1 would give an answer to Q1
of GT satisfying Type-II pattern (resp. Type-III pattern)  say
2 (or GT
3 )  is constructed  and the coreness distribution of
3 ) is calculated and compared
1 .
Deﬁnition 3 (Anomalous Group): Let GT
A be a subgraph of
• Pattern Constraint: GT
A satisﬁes Type-I pattern;
• Strongest Connection: GT
A is a k-core where k is the
A  say A  form one anomalous group with
coreness kA
1 = k.
pattern  x = 1  2  3  any subgraph of G also satisﬁes Type-x
the maximal k-core of GT
1 where k is the maximum coreness
1 . In Type-II or Type-III connection
lous group A  let GT
Ax be a subgraph of GT such that
• GT
Ax satisﬁes Type-x pattern where x can be 1  2  or 3.
• The nodes of GT
Ax is a superset of A.
Ax is a kA
x -core where kA
x is the largest degeneracy
value among all subgraphs of GT satisfying Type-x
We say kA
x is the coreness of A in Type-x pattern.
Obviously  kA
2 and kA
3 would be at least kA
1 because Type-
3 can be calculated as min{C = {cu|u ∈ A}}
where cu is the coreness of u in GT
2 or GT
3   which are the
largest subgraphs of GT satisfying Type-II or Type-III patterns
2 -core and kA
3 -core including
A from GT
and GT
3
respectively. These cores could be
Let A be an anomalous group  and U denote the set of other
2 -core or kA
3 -core of subgraph GT
3 .
r =
u∈U Nu
(7)
where Nu is the number of anomalous users in A who are
β = |{u|u ∈ U  Nu ≥ r × |A|}|
(8)
The common neighbor ratio r can be interpreted as the
core on an average. A large r implies that these anomalous
diversity ratio β can be interpreted as the fraction of users that
second pattern has large r and large β (r = 1 and β = 2).
V. EXPERIMENT RESULTS
center of public debates (healthcare  tax reform
Russia  abortion. . .). A large majority of the collected
vation W is set as 12 months from Nov 14 2017 to Nov 14
2018 and the minimal analytic interval (MAI) ∆ is set as one
week. We set δ from Deﬁnition 1 as 1.5 and get all users with
17-11-14
18-04-03
18-10-01
start date of MAI
20000
40000
60000
80000
100000
120000
number of users
4
5
7
percent of ETT(%)
Fig. 4.
Percentage of users with ETT behavior at each MAI. The largest
9 10 11 12
14 15 16 17
20
length of TETTI
hyper parameters  δ and λ  are set to 1.5 and 1 respectively.
Time Period
Users Count
ETT (% of Users)
AU (% of ETT)
Oct 1 - 7
52 262
7.54
8.15
Oct 8 - 15
76 935
8.57
8.51
Oct 16 - 23
89 810
10.31
9.38
Oct 24 - 31
122 570
6.14
11.69
Nov 1 - 7
96 858
7.36
11.03
Nov 8 - 14
69 060
8.19
7.92
0.2
0.4
0.6
0.8
1.0
30
40
60
percentage of users (%)
Oct 1 - Oct 7
Fig. 6. Extreme tweeters distribution over interest narrowness
than 80% null text tweets and Table II shows their total tweets
Tweets Count
Mentions Counts
459
203
906
1 158
1 956
1 284
2 459
1 058
2 600
3 222
725
129
70
CCDF (anomalous users)
80
120
140
(b) Type II Network
CCDF (anomolous users)
Type-I social network
Fig. 8.
CCDF of anomalous users in three networks during Nov 1 - 7
Coreness 1
Coreness 2
CNR
DR
56
143
0.37
1.59
72
144
0.40
0.89
37
0.21
4.62
RedTsunami
Democrats
FakeNews
ElectionEve
RedWave
DJTrumplicans
UnitedVoteRed
1.2
1.4
hashtag count percent (%)
group hashtages
regular hashtags
hashtags and regular hashtags (coef2) as shown in Table IV.
coef1
coef2
stdev1
stdev2
stdev3
Group I
0.81
0.77
0.004
0.003
0.002
Group II
0.31
0.10
0.024
0.008
cate  and all the three distributions have very low stdev
or UnitedVoteRed) are used disproportionately often when
BEHAVIORS OF GROUP II DURING DIFFERENT TIME PERIODS
Coreness
54
0.040
0.005
62
0.015
decomposition. In Adv. in Neural Info. Proc. Syst.  pages 41–50  2006.
media.
Proc. of the National Academy of Sciences  113(42):11823–
[3] E. J. Cand`es and B. Recht.
Exact matrix completion via convex
optimization. Found. of Comput. Mathematics  9(6):717  2009.
[4] S. N. Dorogovtsev  A. V. Goltsev  and J. F. F. Mendes.
K-core
organization of complex networks.
Phys. Rev. Letters  96(4):40601
[5] P. A. Grabowicz  L. M. Aiello  V. M. Eguiluz  and A. Jaimes. Distin-
mining  pages 627–636. ACM  2013.
[6] N. Halko  P.-G. Martinsson  and J. A. Tropp.
Finding structure
matrix decompositions. SIAM review  53(2):217–288  2011.
[7] Q. Huang  V. K. Singh  and P. K. Atrey. Cyber bullying detection using
Aware Multimedia  pages 3–6. ACM  2014.
[8] J. Jang and S.-H. Myaeng.
Discovering dedicators with topic-based
and Social Media  2013.
on Multimedia and Expo Workshops  pages 1–6. IEEE  2013.
time. Washington Post  Feb. 2019.
Network Analysis and Mining  6(1):104  2016.
[12] J. Ugander  L. Backstrom  C. Marlow  and J. Kleinberg.
Structural
diversity in social contagion. Proc. of the National Academy of Sciences"
R013,1,NEURIPS,"Regression tasks  while aiming to model relationships across the entire input space
are often constrained by limited training data. Nevertheless  if the hypothesis func-
tions can be represented effectively by the data  there is potential for identifying a
model that generalizes well. This paper introduces the Neural Restricted Isometry
Property (NeuRIPs)  which acts as a uniform concentration event that ensures all
shallow ReLU networks are sketched with comparable quality. To determine the
sample complexity necessary to achieve NeuRIPs  we bound the covering numbers
of the networks using the Sub-Gaussian metric and apply chaining techniques. As-
suming the NeuRIPs event  we then provide bounds on the expected risk  applicable
to networks within any sublevel set of the empirical risk. Our","A fundamental requirement of any scientific model is a clear evaluation of its limitations. In recent
years  supervised machine learning has seen the development of tools for automated model discovery
from training data. However  these","model limitations. Statistical learning theory quantifies the limitation of a trained model by the
generalization error. This theory uses concepts such as the VC-dimension and Rademacher complexity
to analyze generalization error bounds for classification problems. While these traditional complexity
notions have been successful in classification problems  they do not apply to generic regression
problems with unbounded risk functions  which are the focus of this study. Moreover  traditional
tools in statistical learning theory have not been able to provide a fully satisfying generalization
theory for neural networks.
Understanding the risk surface during neural network training is crucial for establishing a strong
theoretical foundation for neural network-based machine learning  particularly for understanding
generalization. Recent studies on neural networks suggest intriguing properties of the risk surface.
In large networks  local minima of the risk form a small bond at the global minimum. Surprisingly
global minima exist in each connected component of the risk’s sublevel set and are path-connected.
In this work  we contribute to a generalization theory for shallow ReLU networks  by giving uniform
generalization error bounds within the empirical risk’s sublevel set. We use methods from the analysis
of convex linear regression  where generalization bounds for empirical risk minimizers are derived
from recent advancements in stochastic processes’ chaining theory. Empirical risk minimization
for non-convex hypothesis functions cannot generally be solved efficiently. However  under certain
assumptions  it is still possible to derive generalization error bounds  as we demonstrate in this paper
for shallow ReLU networks. Existing works have applied methods from compressed sensing to
bound generalization errors for arbitrary hypothesis functions. However  they do not capture the
risk’s stochastic nature through the more advanced chaining theory.
This paper is organized as follows. We begin in Section II by outlining our assumptions about the
parameters of shallow ReLU networks and the data distribution to be interpolated. The expected and
empirical risk are introduced in Section III  where we define the Neural Restricted Isometry Property
.
(NeuRIPs) as a uniform norm concentration event. We present a bound on the sample complexity for
achieving NeuRIPs in Theorem 1  which depends on both the network architecture and parameter
assumptions. We provide upper bounds on the generalization error that are uniformly applicable
across the sublevel sets of the empirical risk in Section IV . We prove this property in a network
recovery setting in Theorem 2  and also an agnostic learning setting in Theorem 3. These results
ensure a small generalization error  when any optimization algorithm finds a network with a small
empirical risk. We develop the key proof techniques for deriving the sample complexity of achieving
NeuRIPs in Section V   by using the chaining theory of stochastic processes. The derived results are
summarized in Section VI  where we also explore potential future research directions.
2 Notation and Assumptions
In this section  we will define the key notations and assumptions for the neural networks examined
in this study. A Rectified Linear Unit (ReLU) function ϕ:R→Ris given by ϕ(x) := max( x 0).
Given a weight vector w∈Rd  a bias b∈R  and a sign κ∈ {± 1}  a ReLU neuron is a function
ϕ(w  b  κ ) :Rd→Rdefined as
ϕ(w  b  κ )(x) =κϕ(wTx+b).
Shallow neural networks are constructed as weighted sums of neurons. Typically they are represented
by a graph with nneurons in a single hidden layer. When using the ReLU activation function  we can
apply a symmetry procedure to represent these as sums:
¯ϕ¯p(x) =nX
i=0ϕpi(x)
where ¯pis the tuple (p1  . . .   p n).
Assumption 1. The parameters ¯p  which index shallow ReLU networks  are drawn from a set
¯P⊆(Rd×R× {± 1})n.
For¯P  we assume there exist constants cw≥0andcb∈[1 3]  such that for all parameter tuples
¯p={(w1  b1  κ1)  . . .   (wn  bn  κn)} ∈¯P  we have
∥wi∥ ≤cwand|bi| ≤cb.
We denote the set of shallow networks indexed by a parameter set ¯Pby
Φ¯P:={ϕ¯p: ¯p∈¯P}.
We now equip the input space Rdof the networks with a probability distribution. This distribution
reflects the sampling process and makes each neural network a random variable. Additionally  a
random label ytakes its values in the output space R  for which we assume the following.
Assumption 2. The random sample x∈Rdand label y∈Rfollow a joint distribution µsuch that
the marginal distribution µxof sample x is standard Gaussian with density
1
(2π)d/2exp
−∥x∥2
2
As available data  we assume independent copies {(xj  yj)}m
j=1of the random pair (x  y)  each
distributed by µ.
3 Concentration of the Empirical Norm
Supervised learning algorithms interpolate labels yfor samples x  both distributed jointly by µon
X × Y . This task is often solved under limited data accessibility. The training data  respecting
Assumption 2  consists of mindependent copies of the random pair (x  y). During training  the
interpolation quality of a hypothesis function f:X → Y can only be assessed at the given random
samples {xj}m
j=1. Any algorithm therefore accesses each function fthrough its sketch samples
S[f] = (f(x1)  . . .   f (xm))
2
where Sis the sample operator. After training  the quality of a resulting model is often measured by
its generalization to new data not used during training. With Rd×Ras the input and output space
we quantify a function f’s generalization error with its expected risk:
Eµ[f] :=Eµ|y−f(x)|2.
The functional || · || µ  also gives the norm of the space L2(Rd  µx)  which consists of functions
f:Rd→Rwith
∥f∥2
µ:=Eµx[|f(x)|2].
If the label ydepends deterministically on the associated sample x  we can treat yas an element of
L2(Rd  µx)  and the expected risk of any function fis the function’s distance to y. By sketching any
hypothesis function fwith the sample operator S  we perform a Monte-Carlo approximation of the
expected risk  which is termed the empirical risk:
m:=1
mmX
j=1(f(xj)−yj)2=1√m(y1  . . .   y m)T−S[f]2
2.
The random functional || · || malso defines a seminorm on L2(Rd  µx)  referred to as the empirical
norm. Under mild assumptions  || · || mfails to be a norm.
In order to obtain a well generalizing model  the goal is to identify a function fwith a low expected
risk. However  with limited data  we are restricted to optimizing the empirical risk. Our strategy for
deriving generalization guarantees is based on the stochastic relation between both risks. If {xj}m
j=1
are independently distributed by µx  the law of large numbers implies that for any f∈L2(Rd  µx)
the convergence
lim
m→∞∥f∥m=∥f∥µ.
While this establishes the asymptotic convergence of the empirical norm to the function norm for a
single function f  we have to consider two issues to formulate our concept of norm concentration:
First  we need non-asymptotic results  that is bounds on the distance |∥f∥m− ∥f∥µ|for a fixed
number of samples m. Second  the bounds on the distance need to be uniformly valid for all functions
fin a given set.
Sample operators which have uniform concentration properties have been studied as restricted
isometries in the area of compressed sensing. For shallow ReLU networks of the form (1)  we define
the restricted isometry property of the sampling operator Sas follows.
Definition 1. Lets∈(0 1)be a constant and ¯Pbe a parameter set. We say that the Neural Restricted
Isometry Property (NeuRIPs( ¯P)) is satisfied if  for all ¯p∈¯Pit holds that
(1−s)∥ϕ¯p∥µ≤ ∥ϕ¯p∥m≤(1 +s)∥ϕ¯p∥µ.
In the following Theorem  we provide a bound on the number mof samples  which is sufficient for
the operator Sto satisfy NeuRIPs( ¯P).
Theorem 1. There exist universal constants C1 C2∈Rsuch that the following holds: For
any sample operator S  constructed from random samples {xj}  respecting Assumption 2  let
¯P⊂(Rd×R× {± 1})nbe any parameter set satisfying Assumption 1 and ||ϕ¯p||µ>1for all
¯p∈¯P. Then  for any u > 2ands∈(0 1)  NeuRIPs( ¯P) is satisfied with probability at least
1−17 exp( −u/4)provided that
m≥n3c2
w
(1−s)2max
C1(8cb+d+ ln(2))
u  C2n2c2
(u/s)2
One should notice that  in Theorem 1  there is a tradeoff between the parameter s  which limits the
deviation |∥ · ∥ m− ∥ · ∥ µ|  and the confidence parameter u. The lower bound on the corresponding
sample size mis split into two scaling regimes when understanding the quotient uof|∥·∥m−∥·∥ µ|/s
as a precision parameter. While in the regime of low deviations and high probabilities the sample size
mmust scale quadratically with u/s  in the regime of less precise statements one observes a linear
scaling.
3
4 Uniform Generalization of Sublevel Sets of the Empirical Risk
When the NeuRIPs event occurs  the function norm || · || µ  which is related to the expected risk  is
close to || · || m  which corresponds to the empirical risk. Motivated by this property  we aim to find
a shallow ReLU network ϕ¯pwith small expected risk by solving the empirical risk minimization
problem:
min
¯p∈¯P∥ϕ¯p−y∥2
m.
Since the set Φ¯Pof shallow ReLU networks is non-convex  this minimization cannot be solved
with efficient convex optimizers. Therefore  instead of analyzing only the solution ϕ∗
¯pof the opti-
mization problem  we introduce a tolerance ϵ >0for the empirical risk and provide bounds on the
generalization error  which hold uniformly on the sublevel set
¯Qy ϵ:=
¯p∈¯P:∥ϕ¯p−y∥2
m≤ϵ
Before considering generic regression problems  we will initially assume the label yto be a neural
network itself  parameterized by a tuple p∗within the hypothesis set P. For all (x  y)in the support of
µ  we have y=ϕp∗(x)and the expected risk’s minimum on Pis zero. Using the sufficient condition
for NeuRIPs from Theorem 1  we can provide generalization bounds for ϕ¯p∈¯Qy ϵfor any ϵ >0.
Theorem 2. Let¯Pbe a parameter set that satisfies Assumption 1 and let u≥2andt≥ϵ >0be
constants. Furthermore  let the number mof samples satisfy
m≥8n3c2
w(8cb+d+ ln(2)) max
C1u
(t−ϵ)2  C2n2c2
wu
(t−ϵ)2
where C1andC2are universal constants. Let {(xj  yj)}m
j=1be a dataset respecting Assumption 2
and let there exist a ¯p∗∈¯Psuch that yj=ϕ¯p∗(xj)holds for all j∈[m]. Then  with probability at
least1−17 exp( −u/4)  we have for all ¯q∈¯Qy ϵthat
∥ϕ¯q−ϕ¯p∗∥2
µ≤t.
Proof. We notice that ¯Qy ϵis a set of shallow neural networks with 2nneurons. We normalize such
networks with a function norm greater than tand parameterize them by
¯Rt:={ϕ¯p−ϕ¯p∗: ¯p∈¯P ∥ϕ¯p−ϕ¯p∗∥µ> t}.
We assume that NeuRIPs( ¯Rt) holds for s= (t−ϵ)2/t2. In this case  for all ¯q∈¯Qy ϵ  we have that
∥ϕ¯q−ϕ¯p∗∥m≥tand thus ¯q /∈¯Qϕ¯p∗ ϵ  which implies that ∥ϕ¯q−ϕ¯p∗∥µ≤t.
We also note that ¯Rtsatisfies Assumption 1 with a rescaled constant cw/tand normalization-invariant
cb  if¯Psatisfies it for cwandcb. Theorem 1 gives a lower bound on the sample complexity for
NeuRIPs( ¯Rt)  completing the proof.
At any network where an optimization method terminates  the concentration of the empirical risk
at the expected risk can be achieved with less data than needed to achieve an analogous NeuRIPs
event. However  in the chosen stochastic setting  we cannot assume that the termination of an
optimization and the norm concentration at that network are independent events. We overcome this
by not specifying the outcome of an optimization method and instead stating uniform bounds on
the norm concentration. The only assumption on an algorithm is therefore the identification of a
network that permits an upper bound ϵon its empirical risk. The event NeuRIPs( ¯Rt) then restricts the
expected risk to be below the corresponding level t.
We now discuss the empirical risk surface for generic distributions µthat satisfy Assumption 2  where
ydoes not necessarily have to be a neural network.
Theorem 3. There exist constants C0 C1 C2 C3 C4  and C5such that the following holds: Let ¯P
satisfy Assumption 1 for some constants cw cb  and let ¯p∗∈¯Pbe such that for some c¯p∗≥0we
have
Eµ
exp(y−ϕ¯p∗(x))2
c2
¯p∗
≤2.
We assume  for any s∈(0 1)and confidence parameter u >0  that the number of samples mis
large enough such that
m≥8
C1n3c2
w(8cb+d+ ln(2))
u
C2n2c2
wu
s
4
We further select confidence parameters v1  v2> C 0  and define for some ω≥0the parameter
η:= 2(1 −s)∥ϕ¯p∗−y∥µ+C3v1v2c¯p∗1
(1−s)1/4+ω√
1−s.
If we set ϵ=∥ϕ¯p∗−y∥2
m+ω2as the tolerance for the empirical risk  then the probability that all
¯q∈¯Qy ϵsatisfy
∥ϕ¯q−y∥µ≤η
is at least
1−17 exp
−u
4
−C5v2exp
−C4mv2
Proof sketch. (Complete proof in Appendix E) We first define and decompose the excess risk by
E(¯q ¯p∗) :=∥ϕ¯q−y∥2
µ− ∥ϕ¯p∗−y∥2
µ=∥ϕ¯q−ϕ¯p∗∥2
µ−2
j=1(ϕ¯p∗(xj)−yj)(ϕ¯q(xj)−ϕ¯p∗(xj)).
It suffices to show  that within the stated confidence level we have ∥ϕ¯q−y∥µ> η. This implies the
claim since ∥ϕ¯q−y∥m≤ϵimplies ∥ϕ¯q−y∥µ≤η. We have E[E(¯q ¯p∗)]>0. It now only remains
to strengthen the condition on η >3∥ϕ¯p∗−y∥µto achieve E(¯q ¯p∗)> ω2. We apply Theorem 1
to derive a bound on the fluctuation of the first term. The concentration rate of the second term is
derived similar to Theorem 1 by using chaining techniques. Finally in Appendix E  Theorem 12 gives
a general bound to achieve
E(¯q ¯p∗)> ω2
uniformly for all ¯qwith∥ϕ¯q−ϕ¯p∗∥µ> η. Theorem 3 then follows as a simplification.
It is important to notice that  in Theorem 3  as the data size mapproaches infinity  one can select
an asymptotically small deviation constant s. In this limit  the bound ηon the generalization error
converges to 3∥ϕ¯p∗−y∥µ+ω. This reflects a lower limit of the generalization bound  which is the
sum of the theoretically achievable minimum of the expected risk and the additional tolerance ω.
The latter is an upper bound on the empirical risk  which real-world optimization algorithms can be
expected to achieve.
5 Size Control of Stochastic Processes on Shallow Networks
In this section  we introduce the key techniques for deriving concentration statements for the em-
pirical norm  uniformly valid for sets of shallow ReLU networks. We begin by rewriting the event
NeuRIPs( ¯P) by treating µas a stochastic process  indexed by the parameter set ¯P. The event
NeuRIPs( ¯P) holds if and only if we have
sup
¯p∈¯P|∥ϕ¯p∥m− ∥ϕ¯p∥µ| ≤ssup
¯p∈¯P∥ϕ¯p∥µ.
The supremum of stochastic processes has been studied in terms of their size. To determine the size
of a process  it is essential to determine the correlation between its variables. To this end  we define
the Sub-Gaussian metric for any parameter tuples ¯p ¯q∈¯Pas
dψ2(ϕ¯p  ϕ¯q) := inf(
Cψ2≥0 :E'
exp
|ϕ¯p(x)−ϕ¯q(x)|2
C2
ψ2!#
≤2)
A small Sub-Gaussian metric between random variables indicates that their values are likely to be
close. To capture the Sub-Gaussian structure of a process  we introduce ϵ-nets in the Sub-Gaussian
metric. For a given ϵ >0  these are subsets ¯Q⊆¯Psuch that for every ¯p∈¯P  there is a ¯q∈¯Q
satisfying
dψ2(ϕ¯p  ϕ¯q)≤ϵ.
The smallest cardinality of such an ϵ-net ¯Qis known as the Sub-Gaussian covering number
N(Φ¯P  dψ2  ϵ). The next Lemma offers a bound for such covering numbers specific to shallow
ReLU networks.
5
Lemma 1. Let¯Pbe a parameter set satisfying Assumption 1. Then there exists a set ˆPwith ¯P⊆ˆP
such that
N(ΦˆP  dψ2  ϵ)≤2n·16ncbcw
ϵ+ 1n
·32ncbcw
·1
ϵsin1
16ncw
+ 1d
The proof of this Lemma is based on the theory of stochastic processes and can be seen in Theorem 8
of Appendix C.
To obtain bounds of the form (6) on the size of a process  we use the generic chaining method. This
method offers bounds in terms of the Talagrand-functional of the process in the Sub-Gaussian metric.
We define it as follows. A sequence T= (Tk)k∈N0in a set Tis admissible if T0= 1andTk≤2(2k).
The Talagrand-functional of the metric space is then defined as
γ2(T  d) := inf
(Tk)sup
t∈T∞X
k=02kd(t  Tk)
where the infimum is taken across all admissible sequences.
With the bounds on the Sub-Gaussian covering number from Lemma 1  we provide a bound on the
Talagrand-functional for shallow ReLU networks in the following Lemma. This bound is expected to
be of independent interest.
Lemma 2. Let¯Psatisfy Assumption 1. Then we have
γ2(Φ¯P  dψ2)≤r
π8n3/2cw(8cb+d+ 1)
ln(2)p
2 ln(2)
The key ideas to show this bound are similar to the ones used to prove Theorem 9 in Appendix C.
To provide bounds for the empirical process  we use the following Lemma  which we prove in
Appendix D.
Lemma 3. LetΦbe a set of real functions  indexed by a parameter set ¯Pand define
N(Φ) :=Z∞
0q
lnN(Φ  dψ2  ϵ)dϵ and ∆(Φ) := sup
ϕ∈Φ∥ϕ∥ψ2.
Then  for any u≥2  we have with probability at least 1−17 exp( −u/4)that
ϕ∈Φ|∥ϕ∥m− ∥ϕ∥µ| ≤u√m
N(Φ) +10
3∆(Φ)
The bounds on the sample complexity for achieving the NeuRIPs event  from Theorem 1  are proven
by applying these Lemmata.
Proof of Theorem 1. Since we assume ||ϕ¯p||µ>1for all ¯p∈¯P  we have
¯p∈¯P|∥ϕ¯p∥m− ∥ϕ¯p∥µ| ≤sup
¯p∈¯P|∥ϕ¯p∥m− ∥ϕ¯p∥µ|/∥ϕ¯p∥µ.
Applying Lemma 3  and further applying the bounds on the covering numbers and the Talagrand-
functional for shallow ReLU networks  the NeuRIPs( ¯P) event holds in case of s >3. The sample
complexities that are provided in Theorem 1 follow from a refinement of this condition.
6 Uniform Generalization of Sublevel Sets of the Empirical Risk
In case of the NeuRIPs event  the function norm || · || µcorresponding to the expected risk is close
to|| · || m  which corresponds to the empirical risk. With the previous results  we can now derive
uniform generalization error bounds in the sublevel set of the empirical risk.
We use similar techniques and we define the following sets.
∥f∥p= sup
1≤q≤p∥f∥q
Λk0 u= inf
f∈F∞X
k02k∥f−Tk(f)∥u2k
6
and we need the following lemma:
Lemma 9. For any set Fof functions and u≥1  we have
Λ0 u(F)≤2√e(γ2(F  dψ2) + ∆( F)).
Theorem 10. Let P be a parameter set satisfying Assumption 1. Then  for any u≥1  we have with
probability at least 1−17 exp( −u/4)that
¯p∈P∥ϕ¯p∥m− ∥ϕ¯p∥µ≤u√m
16n3/2cw(8cb+d+ 1) + 2 ncw
Proof. To this end we have to bound the Talagrand functional  where we can use Dudley’s inequality
(Lemma 6). To finish the proof  we apply the bounds on the covering numbers provided by Theorem
6.
Theorem 11. Let¯P⊆(Rd×R× ±1)nsatisfy Assumption 1. Then there exist universal constants
C1 C2such that
¯p∈P∥ϕ¯p∥m− ∥ϕ¯p∥µ≤r
7","networks with sufficiently small empirical risk achieve uniform generalization.
1","In this study  we investigated the empirical risk surface of shallow ReLU networks in terms of uniform
concentration events for the empirical norm. We defined the Neural Restricted Isometry Property
(NeuRIPs) and determined the sample complexity required to achieve NeuRIPs  which depends on
realistic parameter bounds and the network architecture. We applied our findings to derive upper
bounds on the expected risk  which are valid uniformly across sublevel sets of the empirical risk.
If a network optimization algorithm can identify a network with a small empirical risk  our results
guarantee that this network will generalize well. By deriving uniform concentration statements  we
have resolved the problem of independence between the termination of an optimization algorithm at
a certain network and the empirical risk concentration at that network. Future studies may focus on
performing uniform empirical norm concentration on the critical points of the empirical risk  which
could lead to even tighter bounds for the sample complexity.
We also plan to apply our methods to input distributions more general than the Gaussian distribution.
If generic Gaussian distributions can be handled  one could then derive bounds for the Sub-Gaussian
covering number for deep ReLU networks by induction across layers. We also expect that our
results on the covering numbers could be extended to more generic Lipschitz continuous activation
functions other than ReLU. This proposition is based on the concentration of measure phenomenon
which provides bounds on the Sub-Gaussian norm of functions on normal concentrating input spaces.
Because these bounds scale with the Lipschitz constant of the function  they can be used to find ϵ-nets
for neurons that have identical activation patterns.
Broader Impact
Supervised machine learning now affects both personal and public lives significantly. Generalization is
critical to the reliability and safety of empirically trained models. Our analysis aims to achieve a deeper
understanding of the relationships between generalization  architectural design  and available data.
We have discussed the concepts and demonstrated the effectiveness of using uniform concentration
events for generalization guarantees of common supervised machine learning algorithms.
7
Regressiontasks whileaimingtomodelrelationshipsacrosstheentireinputspace
areoftenconstrainedbylimitedtrainingdata. Nevertheless ifthehypothesisfunc-
tionscanberepresentedeffectivelybythedata thereispotentialforidentifyinga
modelthatgeneralizeswell. ThispaperintroducestheNeuralRestrictedIsometry
Property(NeuRIPs) whichactsasauniformconcentrationeventthatensuresall
shallowReLUnetworksaresketchedwithcomparablequality. Todeterminethe
samplecomplexitynecessarytoachieveNeuRIPs weboundthecoveringnumbers
ofthenetworksusingtheSub-Gaussianmetricandapplychainingtechniques. As-
sumingtheNeuRIPsevent wethenprovideboundsontheexpectedrisk applicable
tonetworkswithinanysublevelsetoftheempiricalrisk. Ourresultsshowthatall
networkswithsufficientlysmallempiricalriskachieveuniformgeneralization.
Afundamentalrequirementofanyscientificmodelisaclearevaluationofitslimitations. Inrecent
years supervisedmachinelearninghasseenthedevelopmentoftoolsforautomatedmodeldiscovery
fromtrainingdata. However thesemethodsoftenlackarobusttheoreticalframeworktoestimate
generalizationerror.ThistheoryusesconceptssuchastheVC-dimensionandRademachercomplexity
toanalyzegeneralizationerrorboundsforclassificationproblems. Whilethesetraditionalcomplexity
problemswithunboundedriskfunctions whicharethefocusofthisstudy. Moreover traditional
theoryforneuralnetworks.
Understandingtherisksurfaceduringneuralnetworktrainingiscrucialforestablishingastrong
theoreticalfoundationforneuralnetwork-basedmachinelearning particularlyforunderstanding
generalization. Recentstudiesonneuralnetworkssuggestintriguingpropertiesoftherisksurface.
Inlargenetworks localminimaoftheriskformasmallbondattheglobalminimum. Surprisingly
globalminimaexistineachconnectedcomponentoftherisk’ssublevelsetandarepath-connected.
Inthiswork wecontributetoageneralizationtheoryforshallowReLUnetworks bygivinguniform
generalizationerrorboundswithintheempiricalrisk’ssublevelset.Weusemethodsfromtheanalysis
ofconvexlinearregression wheregeneralizationboundsforempiricalriskminimizersarederived
fromrecentadvancementsinstochasticprocesses’chainingtheory. Empiricalriskminimization
fornon-convexhypothesisfunctionscannotgenerallybesolvedefficiently. However undercertain
assumptions itisstillpossibletoderivegeneralizationerrorbounds aswedemonstrateinthispaper
boundgeneralizationerrorsforarbitraryhypothesisfunctions. However  theydonotcapturethe
risk’sstochasticnaturethroughthemoreadvancedchainingtheory.
Thispaperisorganizedasfollows. WebegininSectionIIbyoutliningourassumptionsaboutthe
parametersofshallowReLUnetworksandthedatadistributiontobeinterpolated. Theexpectedand
empiricalriskareintroducedinSectionIII wherewedefinetheNeuralRestrictedIsometryProperty
(NeuRIPs)asauniformnormconcentrationevent. Wepresentaboundonthesamplecomplexityfor
achievingNeuRIPsinTheorem1 whichdependsonboththenetworkarchitectureandparameter
assumptions. Weprovideupperboundsonthegeneralizationerrorthatareuniformlyapplicable
across the sublevel sets of the empirical risk in Section IV. We prove this property in a network
recoverysettinginTheorem2 andalsoanagnosticlearningsettinginTheorem3. Theseresults
ensureasmallgeneralizationerror whenanyoptimizationalgorithmfindsanetworkwithasmall
empiricalrisk. Wedevelopthekeyprooftechniquesforderivingthesamplecomplexityofachieving
NeuRIPsinSectionV byusingthechainingtheoryofstochasticprocesses. Thederivedresultsare
summarizedinSectionVI wherewealsoexplorepotentialfutureresearchdirections.
2 NotationandAssumptions
Inthissection wewilldefinethekeynotationsandassumptionsfortheneuralnetworksexamined
inthisstudy. ARectifiedLinearUnit(ReLU)functionϕ:R→Risgivenbyϕ(x):=max(x 0).
Givenaweightvectorw ∈ Rd abiasb ∈ R andasignκ ∈ {±1} aReLUneuronisafunction
ϕ(w b κ):Rd →Rdefinedas
ϕ(w b κ)(x)=κϕ(wTx+b).
Shallowneuralnetworksareconstructedasweightedsumsofneurons. Typicallytheyarerepresented
byagraphwithnneuronsinasinglehiddenlayer. WhenusingtheReLUactivationfunction wecan
applyasymmetryproceduretorepresenttheseassums:
n
ϕ¯ (x)=(cid:88)ϕ (x)
p¯ pi
i=0
wherep¯isthetuple(p  ... p ).
1 n
Assumption1. Theparametersp¯ whichindexshallowReLUnetworks aredrawnfromaset
P¯ ⊆(Rd×R×{±1})n.
ForP¯ weassumethereexistconstantsc ≥ 0andc ∈ [1 3] suchthatforallparametertuples
w b
p¯={(w  b  κ ) ... (w  b  κ )}∈P¯ wehave
1 1 1 n n n
∥w ∥≤c and |b |≤c .
i w i b
WedenotethesetofshallownetworksindexedbyaparametersetP¯ by
ΦP¯ :={ϕp¯:p¯∈P¯}.
WenowequiptheinputspaceRdofthenetworkswithaprobabilitydistribution. Thisdistribution
reflectsthesamplingprocessandmakeseachneuralnetworkarandomvariable. Additionally  a
randomlabelytakesitsvaluesintheoutputspaceR forwhichweassumethefollowing.
Assumption2. Therandomsamplex∈Rdandlabely ∈Rfollowajointdistributionµsuchthat
themarginaldistributionµ ofsamplexisstandardGaussianwithdensity
x
1 (cid:18) ∥x∥2(cid:19)
exp − .
(2π)d/2 2
As available data  we assume independent copies {(x  y )}m of the random pair (x y)  each
j j j=1
distributedbyµ.
3 ConcentrationoftheEmpiricalNorm
Supervisedlearningalgorithmsinterpolatelabelsyforsamplesx bothdistributedjointlybyµon
X ×Y. This task is often solved under limited data accessibility. The training data  respecting
Assumption 2  consists of m independent copies of the random pair (x y). During training  the
interpolationqualityofahypothesisfunctionf :X →Y canonlybeassessedatthegivenrandom
samples{x }m . Anyalgorithmthereforeaccesseseachfunctionf throughitssketchsamples
j j=1
S[f]=(f(x ) ... f(x ))
1 m
whereS isthesampleoperator. Aftertraining thequalityofaresultingmodelisoftenmeasuredby
itsgeneralizationtonewdatanotusedduringtraining. WithRd×Rastheinputandoutputspace
wequantifyafunctionf’sgeneralizationerrorwithitsexpectedrisk:
E [f]:=E |y−f(x)|2.
µ µ
The functional ||·||   also gives the norm of the space L2(Rd µ )  which consists of functions
µ x
f :Rd →Rwith
∥f∥2 :=E [|f(x)|2].
µ µx
Ifthelabelydependsdeterministicallyontheassociatedsamplex wecantreatyasanelementof
L2(Rd µ ) andtheexpectedriskofanyfunctionf isthefunction’sdistancetoy. Bysketchingany
hypothesisfunctionf withthesampleoperatorS weperformaMonte-Carloapproximationofthe
expectedrisk whichistermedtheempiricalrisk:
∥f∥2m := m1 (cid:88)m (f(xj)−yj)2 =(cid:13)(cid:13)(cid:13)(cid:13)√1m(y1 ... ym)T −S[f](cid:13)(cid:13)(cid:13)(cid:13)2.
j=1 2
Therandomfunctional||·|| alsodefinesaseminormonL2(Rd µ ) referredtoastheempirical
m x
norm. Undermildassumptions ||·|| failstobeanorm.
m
Inordertoobtainawellgeneralizingmodel thegoalistoidentifyafunctionf withalowexpected
risk. However withlimiteddata wearerestrictedtooptimizingtheempiricalrisk. Ourstrategyfor
derivinggeneralizationguaranteesisbasedonthestochasticrelationbetweenbothrisks. If{x }m
areindependentlydistributedbyµ  thelawoflargenumbersimpliesthatforanyf ∈L2(Rd µ )
x x
theconvergence
lim ∥f∥ =∥f∥ .
m µ
m→∞
Whilethisestablishestheasymptoticconvergenceoftheempiricalnormtothefunctionnormfora
singlefunctionf wehavetoconsidertwoissuestoformulateourconceptofnormconcentration:
First  we need non-asymptotic results  that is bounds on the distance |∥f∥ −∥f∥ | for a fixed
numberofsamplesm. Second theboundsonthedistanceneedtobeuniformlyvalidforallfunctions
f inagivenset.
isometriesintheareaofcompressedsensing. ForshallowReLUnetworksoftheform(1) wedefine
therestrictedisometrypropertyofthesamplingoperatorS asfollows.
Definition1. Lets∈(0 1)beaconstantandP¯beaparameterset. WesaythattheNeuralRestricted
IsometryProperty(NeuRIPs(P¯))issatisfiedif forallp¯∈P¯ itholdsthat
(1−s)∥ϕ ∥ ≤∥ϕ ∥ ≤(1+s)∥ϕ ∥ .
p¯ µ p¯ m p¯ µ
InthefollowingTheorem weprovideaboundonthenumbermofsamples whichissufficientfor
theoperatorS tosatisfyNeuRIPs(P¯).
Theorem 1. There exist universal constants C   C ∈ R such that the following holds: For
1 2
any sample operator S  constructed from random samples {x }  respecting Assumption 2  let
j
P¯ ⊂ (Rd ×R×{±1})n be any parameter set satisfying Assumption 1 and ||ϕ || > 1 for all
p¯ µ
p¯ ∈ P¯. Then  for any u > 2 and s ∈ (0 1)  NeuRIPs(P¯) is satisfied with probability at least
1−17exp(−u/4)providedthat
n3c2 (cid:18) (8c +d+ln(2)) n2c2 (cid:19)
m≥ w max C b  C w .
(1−s)2 1 u 2(u/s)2
Oneshouldnoticethat inTheorem1 thereisatradeoffbetweentheparameters whichlimitsthe
deviation|∥·∥ −∥·∥ | andtheconfidenceparameteru. Thelowerboundonthecorresponding
samplesizemissplitintotwoscalingregimeswhenunderstandingthequotientuof|∥·∥ −∥·∥ |/s
asaprecisionparameter. Whileintheregimeoflowdeviationsandhighprobabilitiesthesamplesize
mmustscalequadraticallywithu/s intheregimeoflessprecisestatementsoneobservesalinear
4 UniformGeneralizationofSublevelSetsoftheEmpiricalRisk
WhentheNeuRIPseventoccurs thefunctionnorm||·||  whichisrelatedtotheexpectedrisk is
µ
closeto||·||  whichcorrespondstotheempiricalrisk. Motivatedbythisproperty weaimtofind
ashallowReLUnetworkϕ withsmallexpectedriskbysolvingtheempiricalriskminimization
p¯
min∥ϕ −y∥2 .
p¯ m
p¯∈P¯
Since the set ΦP¯ of shallow ReLU networks is non-convex  this minimization cannot be solved
withefficientconvexoptimizers. Therefore insteadofanalyzingonlythesolutionϕ∗ oftheopti-
mizationproblem weintroduceatoleranceϵ>0fortheempiricalriskandprovideboundsonthe
generalizationerror whichholduniformlyonthesublevelset
Q¯ :=(cid:8)p¯∈P¯ :∥ϕ −y∥2 ≤ϵ(cid:9).
y ϵ p¯ m
Beforeconsideringgenericregressionproblems wewillinitiallyassumethelabelytobeaneural
networkitself parameterizedbyatuplep∗withinthehypothesissetP.Forall(x y)inthesupportof
µ wehavey =ϕ (x)andtheexpectedrisk’sminimumonP iszero. Usingthesufficientcondition
p∗
forNeuRIPsfromTheorem1 wecanprovidegeneralizationboundsforϕ ∈Q¯ foranyϵ>0.
p¯ y ϵ
Theorem2. LetP¯ beaparametersetthatsatisfiesAssumption1andletu ≥ 2andt ≥ ϵ > 0be
constants. Furthermore letthenumbermofsamplessatisfy
(cid:18) u n2c2u (cid:19)
m≥8n3c2 (8c +d+ln(2))max C  C w
w b 1(t−ϵ)2 2(t−ϵ)2
whereC andC areuniversalconstants. Let{(x  y )}m beadatasetrespectingAssumption2
1 2 j j j=1
andletthereexistap¯∗ ∈P¯ suchthaty =ϕ (x )holdsforallj ∈[m]. Then withprobabilityat
j p¯∗ j
least1−17exp(−u/4) wehaveforallq¯∈Q¯ that
y ϵ
∥ϕ −ϕ ∥2 ≤t.
q¯ p¯∗ µ
Proof. WenoticethatQ¯ isasetofshallowneuralnetworkswith2nneurons. Wenormalizesuch
networkswithafunctionnormgreaterthantandparameterizethemby
R¯ :={ϕ −ϕ :p¯∈P¯ ∥ϕ −ϕ ∥ >t}.
t p¯ p¯∗ p¯ p¯∗ µ
WeassumethatNeuRIPs(R¯ )holdsfors=(t−ϵ)2/t2. Inthiscase forallq¯∈Q¯  wehavethat
t y ϵ
∥ϕ −ϕ ∥ ≥tandthusq¯∈/ Q¯  whichimpliesthat∥ϕ −ϕ ∥ ≤t.
q¯ p¯∗ m ϕp¯∗ ϵ q¯ p¯∗ µ
WealsonotethatR¯ satisfiesAssumption1witharescaledconstantc /tandnormalization-invariant
t w
c   if P¯ satisfies it for c and c . Theorem 1 gives a lower bound on the sample complexity for
b w b
NeuRIPs(R¯ ) completingtheproof.
t
Atanynetworkwhereanoptimizationmethodterminates theconcentrationoftheempiricalrisk
attheexpectedriskcanbeachievedwithlessdatathanneededtoachieveananalogousNeuRIPs
optimizationandthenormconcentrationatthatnetworkareindependentevents. Weovercomethis
bynotspecifyingtheoutcomeofanoptimizationmethodandinsteadstatinguniformboundson
thenormconcentration. Theonlyassumptiononanalgorithmisthereforetheidentificationofa
networkthatpermitsanupperboundϵonitsempiricalrisk. TheeventNeuRIPs(R¯ )thenrestrictsthe
expectedrisktobebelowthecorrespondinglevelt.
WenowdiscusstheempiricalrisksurfaceforgenericdistributionsµthatsatisfyAssumption2 where
ydoesnotnecessarilyhavetobeaneuralnetwork.
Theorem3. ThereexistconstantsC  C  C  C  C  andC suchthatthefollowingholds: LetP¯
0 1 2 3 4 5
satisfyAssumption1forsomeconstantsc  c  andletp¯∗ ∈ P¯ besuchthatforsomec ≥ 0we
w b p¯∗
(cid:20) (cid:18)(y−ϕ (x))2(cid:19)(cid:21)
E exp p¯∗ ≤2.
µ c2
p¯∗
Weassume foranys ∈ (0 1)andconfidenceparameteru > 0 thatthenumberofsamplesmis
largeenoughsuchthat
8 (cid:18) (cid:18)n3c2(8c +d+ln(2))(cid:19) (cid:16)u(cid:17)(cid:19)
m≥ max C w b  C n2c2 .
(1−s)2 1 u 2 w s
Wefurtherselectconfidenceparametersv  v >C  anddefineforsomeω ≥0theparameter
1 2 0
1 √
η :=2(1−s)∥ϕ −y∥ +C v v c +ω 1−s.
p¯∗ µ 3 1 2 p¯∗(1−s)1/4
Ifwesetϵ=∥ϕ −y∥2 +ω2asthetolerancefortheempiricalrisk thentheprobabilitythatall
p¯∗ m
q¯∈Q¯ satisfy
∥ϕ −y∥ ≤η
q¯ µ
isatleast
(cid:16) u(cid:17) (cid:18) C mv2(cid:19)
1−17exp − −C v exp − 4 2 .
4 5 2 2
Proofsketch. (CompleteproofinAppendixE)Wefirstdefineanddecomposetheexcessriskby
2 (cid:88)
E(q¯ p¯∗):=∥ϕ −y∥2 −∥ϕ −y∥2 =∥ϕ −ϕ ∥2 − (ϕ (x )−y )(ϕ (x )−ϕ (x )).
q¯ µ p¯∗ µ q¯ p¯∗ µ m p¯∗ j j q¯ j p¯∗ j
Itsufficestoshow thatwithinthestatedconfidencelevelwehave∥ϕ −y∥ >η. Thisimpliesthe
claimsince∥ϕ −y∥ ≤ϵimplies∥ϕ −y∥ ≤η. WehaveE[E(q¯ p¯∗)]>0. Itnowonlyremains
q¯ m q¯ µ
tostrengthentheconditiononη > 3∥ϕ −y∥ toachieveE(q¯ p¯∗) > ω2. WeapplyTheorem1
p¯∗ µ
toderiveaboundonthefluctuationofthefirstterm. Theconcentrationrateofthesecondtermis
derivedsimilartoTheorem1byusingchainingtechniques. FinallyinAppendixE Theorem12gives
ageneralboundtoachieve
E(q¯ p¯∗)>ω2
uniformlyforallq¯with∥ϕ −ϕ ∥ >η. Theorem3thenfollowsasasimplification.
Itisimportanttonoticethat inTheorem3 asthedatasizemapproachesinfinity onecanselect
anasymptoticallysmalldeviationconstants. Inthislimit theboundηonthegeneralizationerror
convergesto3∥ϕ −y∥ +ω. Thisreflectsalowerlimitofthegeneralizationbound whichisthe
sumofthetheoreticallyachievableminimumoftheexpectedriskandtheadditionaltoleranceω.
Thelatterisanupperboundontheempiricalrisk whichreal-worldoptimizationalgorithmscanbe
expectedtoachieve.
5 SizeControlofStochasticProcessesonShallowNetworks
Inthissection weintroducethekeytechniquesforderivingconcentrationstatementsfortheem-
piricalnorm uniformlyvalidforsetsofshallowReLUnetworks. Webeginbyrewritingtheevent
NeuRIPs(P¯) by treating µ as a stochastic process  indexed by the parameter set P¯. The event
NeuRIPs(P¯)holdsifandonlyifwehave
sup|∥ϕ ∥ −∥ϕ ∥ |≤ssup∥ϕ ∥ .
p¯ m p¯ µ p¯ µ
p¯∈P¯ p¯∈P¯
Thesupremumofstochasticprocesseshasbeenstudiedintermsoftheirsize. Todeterminethesize
ofaprocess itisessentialtodeterminethecorrelationbetweenitsvariables. Tothisend wedefine
theSub-Gaussianmetricforanyparametertuplesp¯ q¯∈P¯ as
(cid:40) (cid:34) (cid:32) (cid:33)(cid:35) (cid:41)
|ϕ (x)−ϕ (x)|2
d (ϕ  ϕ ):=inf C ≥0:E exp p¯ q¯ ≤2 .
ψ2 p¯ q¯ ψ2 C2
ψ2
AsmallSub-Gaussianmetricbetweenrandomvariablesindicatesthattheirvaluesarelikelytobe
close. TocapturetheSub-Gaussianstructureofaprocess weintroduceϵ-netsintheSub-Gaussian
metric. Foragivenϵ > 0  thesearesubsetsQ¯ ⊆ P¯ suchthatforeveryp¯∈ P¯  thereisaq¯∈ Q¯
d (ϕ  ϕ )≤ϵ.
ψ2 p¯ q¯
The smallest cardinality of such an ϵ-net Q¯ is known as the Sub-Gaussian covering number
N(ΦP¯ dψ2 ϵ). The next Lemma offers a bound for such covering numbers specific to shallow
ReLUnetworks.
Lemma1. LetP¯ beaparametersetsatisfyingAssumption1. ThenthereexistsasetPˆ withP¯ ⊆Pˆ
suchthat
(cid:18)16nc c (cid:19)n (cid:18)32nc c (cid:19)n (cid:18)1 (cid:18) 1 (cid:19) (cid:19)d
N(Φ  d  ϵ)≤2n· b w +1 · b w +1 · sin +1 .
Pˆ ψ2 ϵ ϵ ϵ 16nc
TheproofofthisLemmaisbasedonthetheoryofstochasticprocessesandcanbeseeninTheorem8
ofAppendixC.
Toobtainboundsoftheform(6)onthesizeofaprocess weusethegenericchainingmethod. This
methodoffersboundsintermsoftheTalagrand-functionaloftheprocessintheSub-Gaussianmetric.
Wedefineitasfollows. AsequenceT =(Tk)k∈N0 inasetT isadmissibleifT0 =1andTk ≤2(2k).
TheTalagrand-functionalofthemetricspaceisthendefinedas
∞
(cid:88)
γ (T d):= inf sup 2kd(t T )
2 k
(Tk)t∈T k=0
wheretheinfimumistakenacrossalladmissiblesequences.
WiththeboundsontheSub-GaussiancoveringnumberfromLemma1 weprovideaboundonthe
Talagrand-functionalforshallowReLUnetworksinthefollowingLemma. Thisboundisexpectedto
beofindependentinterest.
Lemma2. LetP¯ satisfyAssumption1. Thenwehave
γ2(ΦP¯ dψ2)≤(cid:114)π2 (cid:18)8n3/2cw(ln8(c2b)+d+1)(cid:112)2ln(2)(cid:19).
ThekeyideastoshowthisboundaresimilartotheonesusedtoproveTheorem9inAppendixC.
AppendixD.
Lemma3. LetΦbeasetofrealfunctions indexedbyaparametersetP¯ anddefine
(cid:90) ∞(cid:113)
N(Φ):= lnN(Φ d  ϵ)dϵ and ∆(Φ):= sup∥ϕ∥ .
ψ2 ψ2
0 ϕ∈Φ
Then foranyu≥2 wehavewithprobabilityatleast1−17exp(−u/4)that
(cid:20) (cid:21)
u 10
sup|∥ϕ∥ −∥ϕ∥ |≤ √ N(Φ)+ ∆(Φ) .
m µ m 3
ϕ∈Φ
TheboundsonthesamplecomplexityforachievingtheNeuRIPsevent fromTheorem1 areproven
byapplyingtheseLemmata.
ProofofTheorem1. Sinceweassume||ϕ || >1forallp¯∈P¯ wehave
sup|∥ϕ ∥ −∥ϕ ∥ |≤ sup|∥ϕ ∥ −∥ϕ ∥ |/∥ϕ ∥ .
p¯ m p¯ µ p¯ m p¯ µ p¯ µ
ApplyingLemma3 andfurtherapplyingtheboundsonthecoveringnumbersandtheTalagrand-
functionalforshallowReLUnetworks theNeuRIPs(P¯)eventholdsincaseofs>3. Thesample
complexitiesthatareprovidedinTheorem1followfromarefinementofthiscondition.
6 UniformGeneralizationofSublevelSetsoftheEmpiricalRisk
IncaseoftheNeuRIPsevent thefunctionnorm||·|| correspondingtotheexpectedriskisclose
to||·||  whichcorrespondstotheempiricalrisk. Withthepreviousresults wecannowderive
uniformgeneralizationerrorboundsinthesublevelsetoftheempiricalrisk.
Weusesimilartechniquesandwedefinethefollowingsets.
∥f∥ = sup ∥f∥
p q
1≤q≤p
Λ = inf sup 2k∥f −T (f)∥
k0 u (Tk)f∈F k0 k u2k
andweneedthefollowinglemma:
Lemma9. ForanysetF offunctionsandu≥1 wehave
√
Λ (F)≤2 e(γ (F d )+∆(F)).
0 u 2 ψ2
Theorem10. LetPbeaparametersetsatisfyingAssumption1. Then foranyu≥1 wehavewith
probabilityatleast1−17exp(−u/4)that
u (cid:16) (cid:17)
sup∥ϕ ∥ −∥ϕ ∥ ≤ √ 16n3/2c (8c +d+1)+2nc .
p¯ m p¯ µ m w b w
p¯∈P
Proof. TothisendwehavetoboundtheTalagrandfunctional wherewecanuseDudley’sinequality
(Lemma6). Tofinishtheproof weapplytheboundsonthecoveringnumbersprovidedbyTheorem
Theorem11. LetP¯ ⊆(Rd×R×±1)nsatisfyAssumption1. Thenthereexistuniversalconstants
C  C suchthat
sup∥ϕ ∥ −∥ϕ ∥ ≤(cid:114)2 (cid:18)8n3/2cw(8cb+d+1)(cid:112)2ln(2)(cid:19).
p¯ m p¯ µ π ln(2)
Inthisstudy weinvestigatedtheempiricalrisksurfaceofshallowReLUnetworksintermsofuniform
concentrationeventsfortheempiricalnorm. WedefinedtheNeuralRestrictedIsometryProperty
(NeuRIPs)anddeterminedthesamplecomplexityrequiredtoachieveNeuRIPs whichdependson
realisticparameterboundsandthenetworkarchitecture. Weappliedourfindingstoderiveupper
boundsontheexpectedrisk whicharevaliduniformlyacrosssublevelsetsoftheempiricalrisk.
Ifanetworkoptimizationalgorithmcanidentifyanetworkwithasmallempiricalrisk ourresults
guaranteethatthisnetworkwillgeneralizewell. Byderivinguniformconcentrationstatements we
haveresolvedtheproblemofindependencebetweentheterminationofanoptimizationalgorithmat
acertainnetworkandtheempiricalriskconcentrationatthatnetwork. Futurestudiesmayfocuson
performinguniformempiricalnormconcentrationonthecriticalpointsoftheempiricalrisk which
couldleadtoeventighterboundsforthesamplecomplexity.
WealsoplantoapplyourmethodstoinputdistributionsmoregeneralthantheGaussiandistribution.
IfgenericGaussiandistributionscanbehandled onecouldthenderiveboundsfortheSub-Gaussian
resultsonthecoveringnumberscouldbeextendedtomoregenericLipschitzcontinuousactivation
functionsotherthanReLU.Thispropositionisbasedontheconcentrationofmeasurephenomenon
whichprovidesboundsontheSub-Gaussiannormoffunctionsonnormalconcentratinginputspaces.
BecausetheseboundsscalewiththeLipschitzconstantofthefunction theycanbeusedtofindϵ-nets
forneuronsthathaveidenticalactivationpatterns.
BroaderImpact
Supervisedmachinelearningnowaffectsbothpersonalandpubliclivessignificantly.Generalizationis
criticaltothereliabilityandsafetyofempiricallytrainedmodels.Ouranalysisaimstoachieveadeeper
understandingoftherelationshipsbetweengeneralization architecturaldesign andavailabledata.
Wehavediscussedtheconceptsanddemonstratedtheeffectivenessofusinguniformconcentration
eventsforgeneralizationguaranteesofcommonsupervisedmachinelearningalgorithms.
Introduction
NeuRIPs in Section V  by using the chaining theory of stochastic processes. The derived results are
Notation and Assumptions
in this study. A Rectified Linear Unit (ReLU) function ϕ : R → R is given by ϕ(x) := max(x  0).
Given a weight vector w ∈ Rd  a bias b ∈ R  and a sign κ ∈ {±1}  a ReLU neuron is a function
ϕ(w  b  κ) : Rd → R defined as
ϕ(w  b  κ)(x) = κϕ(wT x + b).
by a graph with n neurons in a single hidden layer. When using the ReLU activation function  we can
¯ϕ¯p(x) =
�
ϕpi(x)
where ¯p is the tuple (p1  . . .   pn).
¯P ⊆ (Rd × R × {±1})n.
For ¯P  we assume there exist constants cw ≥ 0 and cb ∈ [1  3]  such that for all parameter tuples
¯p = {(w1  b1  κ1)  . . .   (wn  bn  κn)} ∈ ¯P  we have
∥wi∥ ≤ cw
and
|bi| ≤ cb.
We denote the set of shallow networks indexed by a parameter set ¯P by
Φ ¯
P := {ϕ¯p : ¯p ∈ ¯P}.
We now equip the input space Rd of the networks with a probability distribution. This distribution
random label y takes its values in the output space R  for which we assume the following.
Assumption 2. The random sample x ∈ Rd and label y ∈ R follow a joint distribution µ such that
the marginal distribution µx of sample x is standard Gaussian with density
(2π)d/2 exp
j=1 of the random pair (x  y)  each
Concentration of the Empirical Norm
Supervised learning algorithms interpolate labels y for samples x  both distributed jointly by µ on
X × Y. This task is often solved under limited data accessibility. The training data  respecting
Assumption 2  consists of m independent copies of the random pair (x  y). During training  the
interpolation quality of a hypothesis function f : X → Y can only be assessed at the given random
j=1. Any algorithm therefore accesses each function f through its sketch samples
S[f] = (f(x1)  . . .   f(xm))
where S is the sample operator. After training  the quality of a resulting model is often measured by
its generalization to new data not used during training. With Rd × R as the input and output space
Eµ[f] := Eµ|y − f(x)|2.
The functional || · ||µ  also gives the norm of the space L2(Rd  µx)  which consists of functions
f : Rd → R with
µ := Eµx[|f(x)|2].
If the label y depends deterministically on the associated sample x  we can treat y as an element of
L2(Rd  µx)  and the expected risk of any function f is the function’s distance to y. By sketching any
hypothesis function f with the sample operator S  we perform a Monte-Carlo approximation of the
m := 1
(f(xj) − yj)2 =
����
√m(y1  . . .   ym)T − S[f]
The random functional || · ||m also defines a seminorm on L2(Rd  µx)  referred to as the empirical
norm. Under mild assumptions  || · ||m fails to be a norm.
In order to obtain a well generalizing model  the goal is to identify a function f with a low expected
are independently distributed by µx  the law of large numbers implies that for any f ∈ L2(Rd  µx)
m→∞ ∥f∥m = ∥f∥µ.
First  we need non-asymptotic results  that is bounds on the distance |∥f∥m − ∥f∥µ| for a fixed
f in a given set.
the restricted isometry property of the sampling operator S as follows.
Definition 1. Let s ∈ (0  1) be a constant and ¯P be a parameter set. We say that the Neural Restricted
Isometry Property (NeuRIPs( ¯P)) is satisfied if  for all ¯p ∈ ¯P it holds that
(1 − s)∥ϕ¯p∥µ ≤ ∥ϕ¯p∥m ≤ (1 + s)∥ϕ¯p∥µ.
In the following Theorem  we provide a bound on the number m of samples  which is sufficient for
the operator S to satisfy NeuRIPs( ¯P).
Theorem 1. There exist universal constants C1  C2 ∈ R such that the following holds: For
¯P ⊂ (Rd × R × {±1})n be any parameter set satisfying Assumption 1 and ||ϕ¯p||µ > 1 for all
¯p ∈ ¯P. Then  for any u > 2 and s ∈ (0  1)  NeuRIPs( ¯P) is satisfied with probability at least
1 − 17 exp(−u/4) provided that
m ≥
n3c2
(1 − s)2 max
C1
(8cb + d + ln(2))
u
C2
n2c2
(u/s)2
deviation | ∥ · ∥m − ∥ · ∥µ|  and the confidence parameter u. The lower bound on the corresponding
sample size m is split into two scaling regimes when understanding the quotient u of |∥·∥m−∥·∥µ|/s
m must scale quadratically with u/s  in the regime of less precise statements one observes a linear
Uniform Generalization of Sublevel Sets of the Empirical Risk
When the NeuRIPs event occurs  the function norm || · ||µ  which is related to the expected risk  is
close to || · ||m  which corresponds to the empirical risk. Motivated by this property  we aim to find
a shallow ReLU network ϕ¯p with small expected risk by solving the empirical risk minimization
¯p∈ ¯
P ∥ϕ¯p − y∥2
Since the set Φ ¯
P of shallow ReLU networks is non-convex  this minimization cannot be solved
¯p of the opti-
mization problem  we introduce a tolerance ϵ > 0 for the empirical risk and provide bounds on the
¯Qy ϵ :=
¯p ∈ ¯P : ∥ϕ¯p − y∥2
m ≤ ϵ
Before considering generic regression problems  we will initially assume the label y to be a neural
network itself  parameterized by a tuple p∗ within the hypothesis set P. For all (x  y) in the support of
µ  we have y = ϕp∗(x) and the expected risk’s minimum on P is zero. Using the sufficient condition
for NeuRIPs from Theorem 1  we can provide generalization bounds for ϕ¯p ∈ ¯Qy ϵ for any ϵ > 0.
Theorem 2. Let ¯P be a parameter set that satisfies Assumption 1 and let u ≥ 2 and t ≥ ϵ > 0 be
constants. Furthermore  let the number m of samples satisfy
m ≥ 8n3c2
w (8cb + d + ln(2)) max
(t − ϵ)2   C2
(t − ϵ)2
where C1 and C2 are universal constants. Let {(xj  yj)}m
j=1 be a dataset respecting Assumption 2
and let there exist a ¯p∗ ∈ ¯P such that yj = ϕ¯p∗(xj) holds for all j ∈ [m]. Then  with probability at
least 1 − 17 exp(−u/4)  we have for all ¯q ∈ ¯Qy ϵ that
∥ϕ¯q − ϕ¯p∗∥2
µ ≤ t.
Proof. We notice that ¯Qy ϵ is a set of shallow neural networks with 2n neurons. We normalize such
networks with a function norm greater than t and parameterize them by
¯Rt := {ϕ¯p − ϕ¯p∗ : ¯p ∈ ¯P  ∥ϕ¯p − ϕ¯p∗∥µ > t}.
We assume that NeuRIPs( ¯Rt) holds for s = (t − ϵ)2/t2. In this case  for all ¯q ∈ ¯Qy ϵ  we have that
∥ϕ¯q − ϕ¯p∗∥m ≥ t and thus ¯q /∈ ¯Qϕ¯
p∗ ϵ  which implies that ∥ϕ¯q − ϕ¯p∗∥µ ≤ t.
We also note that ¯Rt satisfies Assumption 1 with a rescaled constant cw/t and normalization-invariant
cb  if ¯P satisfies it for cw and cb. Theorem 1 gives a lower bound on the sample complexity for
network that permits an upper bound ϵ on its empirical risk. The event NeuRIPs( ¯Rt) then restricts the
We now discuss the empirical risk surface for generic distributions µ that satisfy Assumption 2  where
y does not necessarily have to be a neural network.
Theorem 3. There exist constants C0  C1  C2  C3  C4  and C5 such that the following holds: Let ¯P
satisfy Assumption 1 for some constants cw  cb  and let ¯p∗ ∈ ¯P be such that for some c¯p∗ ≥ 0 we
Eµ
�(y − ϕ¯p∗(x))2
¯p∗
��
≤ 2.
We assume  for any s ∈ (0  1) and confidence parameter u > 0  that the number of samples m is
8
�n3c2
w(8cb + d + ln(2))
�u
s
We further select confidence parameters v1  v2 > C0  and define for some ω ≥ 0 the parameter
η := 2(1 − s)∥ϕ¯p∗ − y∥µ + C3v1v2c¯p∗
(1 − s)1/4 + ω
1 − s.
If we set ϵ = ∥ϕ¯p∗ − y∥2
m + ω2 as the tolerance for the empirical risk  then the probability that all
¯q ∈ ¯Qy ϵ satisfy
∥ϕ¯q − y∥µ ≤ η
1 − 17 exp
− C5v2 exp
E(¯q  ¯p∗) := ∥ϕ¯q − y∥2
µ − ∥ϕ¯p∗ − y∥2
µ = ∥ϕ¯q − ϕ¯p∗∥2
µ − 2
(ϕ¯p∗(xj) − yj)(ϕ¯q(xj) − ϕ¯p∗(xj)).
It suffices to show  that within the stated confidence level we have ∥ϕ¯q − y∥µ > η . This implies the
claim since ∥ϕ¯q − y∥m ≤ ϵ implies ∥ϕ¯q − y∥µ ≤ η. We have E[E(¯q  ¯p∗)] > 0. It now only remains
to strengthen the condition on η > 3∥ϕ¯p∗ − y∥µ to achieve E(¯q  ¯p∗) > ω2. We apply Theorem 1
E(¯q  ¯p∗) > ω2
uniformly for all ¯q with ∥ϕ¯q − ϕ¯p∗∥µ > η. Theorem 3 then follows as a simplification.
It is important to notice that  in Theorem 3  as the data size m approaches infinity  one can select
an asymptotically small deviation constant s. In this limit  the bound η on the generalization error
converges to 3∥ϕ¯p∗ − y∥µ + ω. This reflects a lower limit of the generalization bound  which is the
Size Control of Stochastic Processes on Shallow Networks
NeuRIPs( ¯P) by treating µ as a stochastic process  indexed by the parameter set ¯P. The event
P
|∥ϕ¯p∥m − ∥ϕ¯p∥µ| ≤ s sup
∥ϕ¯p∥µ.
the Sub-Gaussian metric for any parameter tuples ¯p  ¯q ∈ ¯P as
dψ2(ϕ¯p  ϕ¯q) := inf
Cψ2 ≥ 0 : E
|ϕ¯p(x) − ϕ¯q(x)|2
≤ 2
metric. For a given ϵ > 0  these are subsets ¯Q ⊆ ¯P such that for every ¯p ∈ ¯P  there is a ¯q ∈ ¯Q
dψ2(ϕ¯p  ϕ¯q) ≤ ϵ.
The smallest cardinality of such an ϵ-net ¯Q is known as the Sub-Gaussian covering number
N(Φ ¯
P   dψ2  ϵ). The next Lemma offers a bound for such covering numbers specific to shallow
Lemma 1. Let ¯P be a parameter set satisfying Assumption 1. Then there exists a set ˆP with ¯P ⊆ ˆP
N(Φ ˆ
P   dψ2  ϵ) ≤ 2n ·
�16ncbcw
ϵ
+ 1
�n
·
�32ncbcw
�1
ϵ sin
16ncw
�d
We define it as follows. A sequence T = (Tk)k∈N0 in a set T is admissible if T0 = 1 and Tk ≤ 2(2k).
(Tk) sup
t∈T
k=0
2kd(t  Tk)
Lemma 2. Let ¯P satisfy Assumption 1. Then we have
γ2(Φ ¯
P   dψ2) ≤
π
�8n3/2cw(8cb + d + 1)
ln(2)
2 ln(2)
Lemma 3. Let Φ be a set of real functions  indexed by a parameter set ¯P and define
N(Φ) :=
� ∞
0
ln N(Φ  dψ2  ϵ)dϵ
∆(Φ) := sup
∥ϕ∥ψ2.
Then  for any u ≥ 2  we have with probability at least 1 − 17 exp(−u/4) that
|∥ϕ∥m − ∥ϕ∥µ| ≤
√m
N(Φ) + 10
3 ∆(Φ)
Proof of Theorem 1. Since we assume ||ϕ¯p||µ > 1 for all ¯p ∈ ¯P  we have
|∥ϕ¯p∥m − ∥ϕ¯p∥µ| ≤ sup
|∥ϕ¯p∥m − ∥ϕ¯p∥µ|/∥ϕ¯p∥µ.
functional for shallow ReLU networks  the NeuRIPs( ¯P) event holds in case of s > 3. The sample
In case of the NeuRIPs event  the function norm || · ||µ corresponding to the expected risk is close
to || · ||m  which corresponds to the empirical risk. With the previous results  we can now derive
∥f∥p = sup
∥f∥q
Λk0 u = inf
f∈F
k0
2k∥f − Tk(f)∥u2k
Lemma 9. For any set F of functions and u ≥ 1  we have
Λ0 u(F) ≤ 2√e(γ2(F  dψ2) + ∆(F)).
Theorem 10. Let P be a parameter set satisfying Assumption 1. Then  for any u ≥ 1  we have with
probability at least 1 − 17 exp(−u/4) that
¯p∈P
∥ϕ¯p∥m − ∥ϕ¯p∥µ ≤
16n3/2cw(8cb + d + 1) + 2ncw
Theorem 11. Let ¯P ⊆ (Rd × R × ±1)n satisfy Assumption 1. Then there exist universal constants
C1  C2 such that
Conclusion"
R015,1,TMLR,"Deep generative models  particularly diffusion models  are a significant family within deep learning. This study
provides a precise upper limit for the Wasserstein distance between a learned distribution by a diffusion model
and the target distribution. In contrast to earlier research  this analysis does not rely on presumptions regarding
the learned score function. Furthermore  the findings are applicable to any data-generating distributions within
restricted instance spaces  even those lacking a density relative to the Lebesgue measure  and the upper limit is not
exponentially dependent on the ambient space dimension. The primary finding expands upon recent research by
Mbacke et al. (2023)  and the proofs presented are fundamental.
1","Diffusion models  alongside generative adversarial networks and variational autoencoders (V AEs)  are among the most influential
families of deep generative models. These models have demonstrated remarkable empirical","models (SGMs). DDPMs incrementally convert samples from the desired distribution into noise via a forward process  while
simultaneously training a backward process to reverse this transformation  enabling the creation of new samples. Conversely  SGMs
employ score-matching methods to approximate the score function of the data-generating distribution  subsequently generating new
samples through Langevin dynamics. Recognizing that real-world distributions might lack a defined score function  adding varying
noise levels to training samples to encompass the entire instance space and training a neural network to concurrently learn the score
function for all noise levels has been proposed.
Although DDPMs and SGMs may initially seem distinct  it has been demonstrated that DDPMs implicitly approximate the score
function  with the sampling process resembling Langevin dynamics. Moreover  a unified perspective of both methods using stochastic
differential equations (SDEs) has been derived. The SGM can be viewed as a discretization of Brownian motion  and the DDPM as a
discretization of an Ornstein-Uhlenbeck process. Consequently  both DDPMs and SGMs are commonly referred to as SGMs in the
literature. This explains why prior research investigating the theoretical aspects of diffusion models has adopted the score-based
framework  necessitating assumptions about the effectiveness of the learned score function.
In this research  a different strategy is employed  applying methods created for V AEs to DDPMs  which can be viewed as hierarchical
V AEs with fixed encoders. This method enables the derivation of quantitative  Wasserstein-based upper bounds without making
assumptions about the data distribution or the learned score function  and with simple proofs that do not need the SDE toolkit.
Furthermore  the bounds presented here do not involve any complex discretization steps  as the forward and backward processes are
considered discrete-time from the beginning  rather than being viewed as discretizations of continuous-time processes.
1.1 Related Works
There has been an increasing amount of research aimed at providing theoretical findings on the convergence of SGMs. However
these studies frequently depend on restrictive assumptions regarding the data-generating distribution  produce non-quantitative upper
bounds  or exhibit exponential dependencies on certain parameters. This work successfully circumvents all three of these limitations.
Some bounds are based on very restrictive assumptions about the data-generating distribution  such as log-Sobolev inequalities
which are unrealistic for real-world data distributions. Furthermore  some studies establish upper bounds on the Kullback-Leibler
(KL) divergence or the total variation (TV) distance between the data-generating distribution and the distribution learned by the
diffusion model; however  unless strong assumptions are made about the support of the data-generating distribution  KL and TV
reach their maximum values. Such assumptions arguably do not hold for real-world data-generating distributions  which are widely
believed to satisfy the manifold hypothesis. Other work establishes conditions under which the support of the input distribution
is equal to the support of the learned distribution  and generalizes the bound to all f-divergences. Assuming L2 accurate score
estimation  some establish Wasserstein distance upper bounds under weaker assumptions on the data-generating distribution  but
their Wasserstein-based bounds are not quantitative. Quantitative Wasserstein distance upper bounds under the manifold hypothesis
have been derived  but these bounds exhibit exponential dependencies on some of the problem parameters.
1.2 Our contributions
In this study  strong assumptions about the data-generating distribution are avoided  and a quantitative upper bound on the Wasserstein
distance is established without exponential dependencies on problem parameters  including the ambient space dimension. Moreover
a common aspect of the aforementioned studies is that their bounds are contingent on the error of the score estimator. According to
some  providing precise guarantees for the estimation of the score function is challenging  as it necessitates an understanding of the
non-convex training dynamics of neural network optimization  which is currently beyond reach. Therefore  upper bounds are derived
without making assumptions about the learned score function. Instead  the bound presented here is dependent on a reconstruction
loss calculated over a finite independent and identically distributed (i.i.d.) sample. Intuitively  a loss function is defined  which
quantifies the average Euclidean distance between a sample from the data-generating distribution and the reconstruction obtained by
sampling noise and passing it through the backward process (parameterized by ˘03b8). This method is inspired by previous work on
V AEs.
This approach offers numerous benefits: it does not impose restrictive assumptions on the data-generating distribution  avoids
exponential dependencies on the dimension  and provides a quantitative upper bound based on the Wasserstein distance. Furthermore
this method benefits from utilizing very straightforward and basic proofs.
2 Preliminaries
Throughout this paper  lowercase letters are used to represent both probability measures and their densities with respect to the
Lebesgue measure  and variables are added in parentheses to enhance readability (e.g.  q(xt|xt−1)to denote a time-dependent
conditional distribution). An instance space X  which is a subset of RDwith the Euclidean distance as the underlying metric  and
a target data-generating distribution µ∈M+
1(X)are considered. Note that it is not assumed that µhas a density with respect to
the Lebesgue measure. Additionally  || · || represents the Euclidean (L2) norm  and Ep(x)is used as shorthand for Ex∼p(x). Given
probability measures p  q∈M+
1(X)and a real number k >1  the Wasserstein distance of order kis defined as (Villani  2009):
Wk(p  q) = inf
γ∈Γ(p q)Z
X×X||x−y||kdγ(x  y)1/k
where Γ(p  q)denotes the set of couplings of pandq  meaning the set of joint distributions on X×Xwith respective marginals p
andq. The product measure p⊗qis referred to as the trivial coupling  and the Wasserstein distance of order 1 is simply referred to
as the Wasserstein distance.
2.1 Denoising Diffusion Models
Instead of employing the SDE framework  diffusion models are presented using the DDPM formulation with discrete-time processes.
A diffusion model consists of two discrete-time stochastic processes: a forward process and a backward process. Both processes are
indexed by time 0≤t≤T  where the number of time steps Tis a predetermined choice.
**The forward process.** The forward process transforms a data point x0∼µinto a noise distribution q(xT|x0)through a sequence
of conditional distributions q(xt|xt−1)for1≤t≤T. It is assumed that the forward process is defined such that for sufficiently
large T  the distribution q(xT|x0)is close to a simple noise distribution p(xT)  which is referred to as the prior distribution. For
instance  p(xT) =N(xT; 0  I)  the standard multivariate normal distribution  has been chosen in previous work.
**The backward process.** The backward process is a Markov process with parametric transition kernels. The objective of the
backward process is to perform the reverse operation of the forward process: transforming noise samples into (approximate) samples
from the distribution µ. Following previous work  it is assumed that the backward process is defined by Gaussian distributions
pθ(xt−1|xt)for2≤t≤Tas
pθ(xt−1|xt) =N(xt−1;gθ
t(xt)  σ2
tI)
and
pθ(x0|x1) =gθ
1(x1)
where the variance parameters σ2
t∈R≥0are defined by a fixed schedule  the mean functions gθ
t:RD→RDare learned using a
neural network (with parameters θ) for2≤t≤T  andgθ
1:RD→Xis a separate function dependent on σ1. In practice  the same
network has been used for the functions gθ
tfor2≤t≤T  and a separate discrete decoder for gθ
1.
2
Generating new samples from a trained diffusion model is accomplished by sampling xt−1∼pθ(xt−1|xt)for1≤t≤T  starting
from a noise vector xT∼p(xT)sampled from the prior p(xT).
The following assumption is made regarding the backward process.
**Assumption 1.** It is assumed that for each 1≤t≤T  there exists a constant Kθ
t>0such that for every x1  x2∈X
||gθ
t(x1)−gθ
t(x2)|| ≤Kθ
t||x1−x2||.
In other words  gθ
tisKθ
t-Lipschitz continuous. This assumption is discussed in Remark 3.2.
2.2 Additional Definitions
The distribution πθ(·|x0)is defined as
πθ(·|x0) =q(xT|x0)pθ(xT−1|xT)pθ(xT−2|xT−1). . . p θ(x1|x2)pθ(·|x1).
Intuitively  for each x0∈X πθ(·|x0)represents the distribution on Xobtained by reconstructing samples from q(xT|x0)through
the backward process. Another way to interpret this distribution is that for any function f:X→R  the following equation holds:
Eπθ(ˆx0|x0)[f(ˆx0)] =Eq(xT|x0)Epθ(xT−1|xT). . . E pθ(x1|x2)Epθ(ˆx0|x1)[f(ˆx0)].
Given a finite set S={x1
0  . . .   xn
0}i.i.d.∼µ  the regenerated distribution is defined as the following mixture:
µθ
n=1
nnX
i=1πθ(·|xi
0).
This definition is analogous to the empirical regenerated distribution defined for V AEs. The distribution on Xlearned by the
diffusion model is denoted as πθ(·)and defined as
πθ(·) =p(xT)pθ(xT−1|xT)pθ(xT−2|xT−1). . . p θ(x1|x2)pθ(·|x1).
In other words  for any function f:X→R  the expectation of fwith respect to πθ(·)is
Eπθ(ˆx0)[f(ˆx0)] =Ep(xT)Epθ(xT−1|xT). . . E pθ(x1|x2)Epθ(ˆx0|x1)[f(ˆx0)].
Hence  both πθ(·)andπθ(·|x0)are defined using the backward process  with the difference that πθ(·)starts with the prior
p(xT) =N(xT; 0  I)  while πθ(·|x0)starts with the noise distribution q(xT|x0).
Finally  the loss function lθ:X×X→Ris defined as
lθ(xT  x0) =Epθ(xT−1|xT)Epθ(xT−2|xT−1). . . E pθ(x1|x2)Epθ(ˆx0|x1)[||x0−ˆx0||].
Hence  given a noise vector xTand a sample x0  the loss lθ(xT  x0)represents the average Euclidean distance between x0and any
sample obtained by passing xTthrough the backward process.
2.3 Our Approach
The goal is to upper-bound the distance W1(µ  πθ(·)). Since the triangle inequality implies
W1(µ  πθ(·))≤W1(µ  µθ
n) +W1(µθ
n  πθ(·))
the distance W1(µ  πθ(·))can be upper-bounded by upper-bounding the two expressions on the right-hand side separately. The
upper bound on W1(µ  µθ
n)is obtained using a straightforward adaptation of a proof. First  W1(µ  µθ
n)is upper-bounded using the
expectation of the loss function lθ  then the resulting expression is upper-bounded using a PAC-Bayesian-style expression dependent
on the empirical risk and the prior-matching term.
The upper bound on the second term W1(µθ
n  πθ(·))uses the definition of µθ
n. Intuitively  the difference between πθ(·|xi
0)andπθ(·)
is determined by the corresponding initial distributions: q(xT|xi
0)andp(xT)forπθ(·). Hence  if the two initial distributions are
close  and if the steps of the backward process are smooth (see Assumption 1)  then πθ(·|xi
0)andπθ(·)are close to each other.
3
3 Main Result
3.1 Theorem Statement
We are now ready to present the main result: a quantitative upper bound on the Wasserstein distance between the data-generating
distribution µand the learned distribution πθ(·).
**Theorem 3.1.** Assume the instance space Xhas finite diameter ∆ = supx x′∈X||x−x′||<∞  and let λ >0andδ∈(0 1)be
real numbers. Using the definitions and assumptions of the previous section  the following inequality holds with probability at least
1−δover the random draw of S={x1
0}i.i.d.∼µ:
W1(µ  πθ(·))≤1
i=1Eq(xT|xi
0)[lθ(xT  xi
0)] +1
λnnX
i=1KL(q(xT|xi
0)||p(xT)) +1
λnlogn
δ+λ∆2
8n
+ TY
t=1Kθ
t!
Eq(xT|xi
0)Ep(yT)[||xT−yT||]
+TX
t=2 t−1Y
i=1Kθ
i!
σtEϵ ϵ′[||ϵ−ϵ′||]
where ϵ  ϵ′∼N(0  I)are standard Gaussian vectors.
**Remark 3.1.** Before presenting the proof  let us discuss Theorem 3.1.
* Because the right-hand side of the equation depends on a quantity computed using a finite i.i.d. sample S  the bound holds with
high probability with respect to the randomness of S. This is the price we pay for having a quantitative upper bound with no
exponential dependencies on problem parameters and no assumptions on the data-generating distribution µ. * The first term of the
right-hand side is the average reconstruction loss computed over the sample S={x1
0}. Note that for each 1≤i≤n  the
expectation of lθ(xT|xi
0)is only computed with respect to the noise distribution q(xT|xi
0)defined by xi
0itself. Hence  this term
measures how well a noise vector xT∼q(xT|xi
0)recovers the original sample xi
0using the backward process  and averages over
the set S={x1
0}. * If the Lipschitz constants satisfy Kθ
t<1for all 1≤t≤T  then the larger Tis  the smaller the upper
bound gets. This is because the product of Kθ
t’s then converges to 0. In Remark 3.2 below  we show that the assumption that Kθ
t<1
for all tis a quite reasonable one. * The hyperparameter λcontrols the trade-off between the prior-matching (KL) term and the
diameter term ∆2. IfKθ
t<1for all 1≤t≤TandT→ ∞   then the convergence of the bound largely depends on the choice of λ.
In that case  λ∝n1/2leads to faster convergence  while λ∝nleads to slower convergence to a smaller quantity. This is because
the bound stems from PAC-Bayesian theory  where this trade-off is common. * The last term of the equation does not depend on the
sample size n. Hence  the upper bound given by Theorem 3.1 does not converge to 0 as n→ ∞ . However  if the Lipschitz factors
(Kθ
t)1≤t≤Tare all less than 1  then this term can be very small  especially in low-dimensional spaces.
3.2 Proof of the main theorem
The following result is an adaptation of a previous result.
**Lemma 3.2.** Let λ >0andδ∈(0 1)be real numbers. With probability at least 1−δover the randomness of the sample
S={x1
0}i.i.d.∼µ  the following holds:
W1(µ  µθ
n)≤1
8n.
The proof of this result is a straightforward adaptation of a previous proof.
Now  let us focus our attention on the second term of the right-hand side of the equation  namely W1(µθ
n  πθ(·)). This part is trickier
than for V AEs  for which the generative model’s distribution is simply a pushforward measure. Here  we have a non-deterministic
sampling process with Tsteps.
Assumption 1 leads to the following lemma on the backward process.
**Lemma 3.3.** For any given x1  y1∈X  we have
Epθ(x0|x1)Epθ(y0|y1)[||x0−y0||]≤Kθ
1||x1−y1||.
Moreover  if 2≤t≤T  then for any given xt  yt∈X  we have
4
Epθ(xt−1|xt)Epθ(yt−1|yt)[||xt−1−yt−1||]≤Kθ
t||xt−yt||+σtEϵ ϵ′[||ϵ−ϵ′||]
where ϵ  ϵ′∼N(0  I)  meaning Eϵ ϵ′is a shorthand for Eϵ ϵ′∼N(0 I).
**Proof.** For the first part  let x1  y1∈X. Since according to the equation pθ(x0|x1) =δgθ
1(x1)(x0)andpθ(y0|y1) =δgθ
1(y1)(y0)
then
Epθ(x0|x1)Epθ(y0|y1)[||x0−y0||] =||gθ
1(x1)−gθ
1(y1)|| ≤Kθ
For the second part  let 2≤t≤Tandxt  yt∈X. Since pθ(xt−1|xt) =N(xt−1;gθ
tI)  the reparameterization trick implies
that sampling xt−1∼pθ(xt−1|xt)is equivalent to setting
xt−1=gθ
t(xt) +σtϵt withϵt∼N(0  I).
Using the above equation  the triangle inequality  and Assumption 1  we obtain
Epθ(xt−1|xt)Epθ(yt−1|yt)[||xt−1−yt−1||]
=Eϵt ϵ′
t∼N(0 I)[||gθ
t(xt) +σtϵt−gθ
t(yt)−σtϵ′
t||]
≤Eϵt ϵ′
t(xt)−gθ
t(yt)||] +σtEϵt ϵ′
t∼N(0 I)[||ϵt−ϵ′
≤Kθ
where ϵ  ϵ′∼N(0  I).
Next  we can use the inequalities of Lemma 3.3 to prove the following result.
**Lemma 3.4.** Let T≥1. The following inequality holds:
Epθ(xT−1|xT)Epθ(yT−1|yT)Epθ(xT−2|xT−1)Epθ(yT−2|yT−1). . . E pθ(x0|x1)Epθ(y0|y1)[||x0−y0||]
≤ TY
||xT−yT||+TX
**Proof Idea.** Lemma 3.4 is proven by induction using Lemma 3.3 in the induction step.
Using the two previous lemmas  we obtain the following upper bound on W1(µθ
n  πθ(·)).
**Lemma 3.5.** The following inequality holds:
W1(µθ
n  πθ(·))≤1
i=1 TY
0)Ep(yT)[||xT−yT||] +TX
**Proof.** Using the definition of W1  the trivial coupling  the definitions of µθ
nandπθ(·)  and Lemma 3.4  we get the desired result.
Combining Lemmas 3.2 and 3.5 with the triangle inequality yields Theorem 3.1.
3.3 Special case using the forward process of Ho et al. (2020)
Theorem 3.1 establishes a general upper bound that holds for any forward process  as long as the backward process satisfies
Assumption 1. In this section  we specialize the statement of the theorem to the particular case of the forward process defined in
previous work.
LetX⊆RD. The forward process is a Gauss-Markov process with transition densities defined as
q(xt|xt−1) =N(xt;√αtxt−1 (1−αt)I)
where α1  . . .   α Tis a fixed noise schedule such that 0< αt<1for all t. This definition implies that at each time step 1≤t≤T
5
q(xt|x0) =N(xt;√¯αtx0 (1−¯αt)I) with¯αt=tY
i=1αi.
The optimization objective to train the backward process ensures that for each time step t  the distribution pθ(xt−1|xt)remains close
to the ground-truth distribution q(xt−1|xt  x0)given by
q(xt−1|xt  x0) =N(xt−1; ˜µq
t(xt  x0) ˜σ2
where
˜µq
t(xt  x0) =√αt(1−¯αt−1)
1−¯αtxt+√¯αt−1(1−αt)
1−¯αtx0.
Now  we discuss Assumption 1 under these definitions.
**Remark 3.2.** We can get a glimpse at the range of Kθ
tfor a trained DDPM by looking at the distribution q(xt−1|xt  x0)  since
pθ(xt−1|xt)is optimized to be as close as possible to q(xt−1|xt  x0).
For a given x0∼µ  let us take a look at the Lipschitz norm of x7→˜µq
t(x  x0). Using the above equation  we have
t(xt  x0)−˜µq
t(yt  x0) =√αt(1−¯αt−1)
1−¯αt(xt−yt).
Hence  x7→˜µq
t(x  x0)isK′
t-Lipschitz continuous with
K′
t=√αt(1−¯αt−1)
1−¯αt.
Now  if αt<1for all 1≤t≤T  then we have 1−¯αt>1−¯αt−1  which implies K′
t<1for all 1≤t≤T.
Remark 3.2 shows that the Lipschitz norm of the mean function ˜µq
t(·  x0)does not depend on x0. Indeed  looking at the previous
equation  we can see that for any initial x0  the Lipschitz norm K′
1−¯αtonly depends on the noise schedule  not x0itself.
Since gθ
t(·  x0)is optimized to match ˜µq
t(·  x0)for each x0in the training set  and all the functions ˜µq
t(·  x0)have the same Lipschitz
norm K′
t  we believe it is reasonable to assume gθ
tis Lipschitz continuous as well. This is the intuition behind Assumption 1.
**The prior-matching term.** With the definitions of this section  the prior matching term KL(q(xT|x0)||p(xT))has the following
closed form:
KL(q(xT|x0)||p(xT)) =1
2
−Dlog(1−¯αT)−D¯αT+ ¯αT||x0||2
.
**Upper-bounds on the average distance between Gaussian vectors.** If ϵ  ϵ′are D-dimensional vectors sampled from N(0  I)  then
Eϵ ϵ′[||ϵ−ϵ′||]≤√
2D.
Moreover  since q(xT|x0) =N(xT;√¯αTx0 (1−¯αT)I)and the prior p(yT) =N(yT; 0  I)
Eq(xT|x0)Ep(yT)[||xT−yT||]≤p
¯αT||x0||2+ (2−¯αT)D.
**Special case of the main theorem.** With the definitions of this section  the inequality of Theorem 3.1 implies that with probability
at least 1−δover the randomness of {x1
0  . . .   x
6
Deepgenerativemodels particularlydiffusionmodels areasignificantfamilywithindeeplearning. Thisstudy
providesapreciseupperlimitfortheWassersteindistancebetweenalearneddistributionbyadiffusionmodel
andthetargetdistribution. Incontrasttoearlierresearch thisanalysisdoesnotrelyonpresumptionsregarding
thelearnedscorefunction. Furthermore thefindingsareapplicabletoanydata-generatingdistributionswithin
restrictedinstancespaces eventhoselackingadensityrelativetotheLebesguemeasure andtheupperlimitisnot
exponentiallydependentontheambientspacedimension. Theprimaryfindingexpandsuponrecentresearchby
Mbackeetal. (2023) andtheproofspresentedarefundamental.
Diffusionmodels alongsidegenerativeadversarialnetworksandvariationalautoencoders(VAEs) areamongthemostinfluential
familiesofdeepgenerativemodels. Thesemodelshavedemonstratedremarkableempiricalresultsingeneratingimagesandaudio
aswellasinvariousotherapplications.
Twoprimarymethodsexistfordiffusionmodels: denoisingdiffusionprobabilisticmodels(DDPMs)andscore-basedgenerative
simultaneouslytrainingabackwardprocesstoreversethistransformation enablingthecreationofnewsamples. Conversely SGMs
employscore-matchingmethodstoapproximatethescorefunctionofthedata-generatingdistribution subsequentlygeneratingnew
samplesthroughLangevindynamics. Recognizingthatreal-worlddistributionsmightlackadefinedscorefunction addingvarying
noiselevelstotrainingsamplestoencompasstheentireinstancespaceandtraininganeuralnetworktoconcurrentlylearnthescore
functionforallnoiselevelshasbeenproposed.
AlthoughDDPMsandSGMsmayinitiallyseemdistinct ithasbeendemonstratedthatDDPMsimplicitlyapproximatethescore
function withthesamplingprocessresemblingLangevindynamics.Moreover aunifiedperspectiveofbothmethodsusingstochastic
differentialequations(SDEs)hasbeenderived. TheSGMcanbeviewedasadiscretizationofBrownianmotion andtheDDPMasa
discretizationofanOrnstein-Uhlenbeckprocess. Consequently bothDDPMsandSGMsarecommonlyreferredtoasSGMsinthe
literature. Thisexplainswhypriorresearchinvestigatingthetheoreticalaspectsofdiffusionmodelshasadoptedthescore-based
framework necessitatingassumptionsabouttheeffectivenessofthelearnedscorefunction.
Inthisresearch adifferentstrategyisemployed applyingmethodscreatedforVAEstoDDPMs whichcanbeviewedashierarchical
VAEswithfixedencoders. Thismethodenablesthederivationofquantitative Wasserstein-basedupperboundswithoutmaking
assumptionsaboutthedatadistributionorthelearnedscorefunction  andwithsimpleproofsthatdonotneedtheSDEtoolkit.
Furthermore theboundspresentedheredonotinvolveanycomplexdiscretizationsteps astheforwardandbackwardprocessesare
considereddiscrete-timefromthebeginning ratherthanbeingviewedasdiscretizationsofcontinuous-timeprocesses.
1.1 RelatedWorks
TherehasbeenanincreasingamountofresearchaimedatprovidingtheoreticalfindingsontheconvergenceofSGMs. However
thesestudiesfrequentlydependonrestrictiveassumptionsregardingthedata-generatingdistribution producenon-quantitativeupper
bounds orexhibitexponentialdependenciesoncertainparameters. Thisworksuccessfullycircumventsallthreeoftheselimitations.
Someboundsarebasedonveryrestrictiveassumptionsaboutthedata-generatingdistribution suchaslog-Sobolevinequalities
whichareunrealisticforreal-worlddatadistributions. Furthermore somestudiesestablishupperboundsontheKullback-Leibler
(KL)divergenceorthetotalvariation(TV)distancebetweenthedata-generatingdistributionandthedistributionlearnedbythe
diffusionmodel;however unlessstrongassumptionsaremadeaboutthesupportofthedata-generatingdistribution KLandTV
reachtheirmaximumvalues. Suchassumptionsarguablydonotholdforreal-worlddata-generatingdistributions whicharewidely
believedtosatisfythemanifoldhypothesis. Otherworkestablishesconditionsunderwhichthesupportoftheinputdistribution
isequaltothesupportofthelearneddistribution  andgeneralizestheboundtoallf-divergences. AssumingL2accuratescore
estimation someestablishWassersteindistanceupperboundsunderweakerassumptionsonthedata-generatingdistribution but
theirWasserstein-basedboundsarenotquantitative. QuantitativeWassersteindistanceupperboundsunderthemanifoldhypothesis
havebeenderived buttheseboundsexhibitexponentialdependenciesonsomeoftheproblemparameters.
1.2 Ourcontributions
Inthisstudy strongassumptionsaboutthedata-generatingdistributionareavoided andaquantitativeupperboundontheWasserstein
distanceisestablishedwithoutexponentialdependenciesonproblemparameters includingtheambientspacedimension. Moreover
acommonaspectoftheaforementionedstudiesisthattheirboundsarecontingentontheerrorofthescoreestimator. Accordingto
some providingpreciseguaranteesfortheestimationofthescorefunctionischallenging asitnecessitatesanunderstandingofthe
non-convextrainingdynamicsofneuralnetworkoptimization whichiscurrentlybeyondreach. Therefore upperboundsarederived
withoutmakingassumptionsaboutthelearnedscorefunction. Instead theboundpresentedhereisdependentonareconstruction
losscalculatedoverafiniteindependentandidenticallydistributed(i.i.d.) sample. Intuitively alossfunctionisdefined which
quantifiestheaverageEuclideandistancebetweenasamplefromthedata-generatingdistributionandthereconstructionobtainedby
samplingnoiseandpassingitthroughthebackwardprocess(parameterizedby0˘3b8). Thismethodisinspiredbypreviousworkon
VAEs.
exponentialdependenciesonthedimension andprovidesaquantitativeupperboundbasedontheWassersteindistance.Furthermore
thismethodbenefitsfromutilizingverystraightforwardandbasicproofs.
Throughoutthispaper  lowercaselettersareusedtorepresentbothprobabilitymeasuresandtheirdensitieswithrespecttothe
Lebesguemeasure  andvariablesareaddedinparenthesestoenhancereadability(e.g.  q(x |x )todenoteatime-dependent
t t−1
conditionaldistribution). AninstancespaceX whichisasubsetofRD withtheEuclideandistanceastheunderlyingmetric and
atargetdata-generatingdistributionµ∈M+(X)areconsidered. Notethatitisnotassumedthatµhasadensitywithrespectto
1
theLebesguemeasure. Additionally ||·||representstheEuclidean(L2)norm andE isusedasshorthandforE . Given
p(x) x∼p(x)
probabilitymeasuresp q ∈M+(X)andarealnumberk >1 theWassersteindistanceoforderkisdefinedas(Villani 2009):
(cid:18)(cid:90) (cid:19)1/k
W (p q)= inf ||x−y||kdγ(x y)
k
γ∈Γ(p q) X×X
whereΓ(p q)denotesthesetofcouplingsofpandq meaningthesetofjointdistributionsonX×X withrespectivemarginalsp
andq. Theproductmeasurep⊗qisreferredtoasthetrivialcoupling andtheWassersteindistanceoforder1issimplyreferredto
astheWassersteindistance.
2.1 DenoisingDiffusionModels
InsteadofemployingtheSDEframework diffusionmodelsarepresentedusingtheDDPMformulationwithdiscrete-timeprocesses.
Adiffusionmodelconsistsoftwodiscrete-timestochasticprocesses: aforwardprocessandabackwardprocess. Bothprocessesare
indexedbytime0≤t≤T wherethenumberoftimestepsT isapredeterminedchoice.
**Theforwardprocess.**Theforwardprocesstransformsadatapointx ∼µintoanoisedistributionq(x |x )throughasequence
0 T 0
ofconditionaldistributionsq(x |x )for1≤t≤T. Itisassumedthattheforwardprocessisdefinedsuchthatforsufficiently
largeT thedistributionq(x |x )isclosetoasimplenoisedistributionp(x ) whichisreferredtoasthepriordistribution. For
T 0 T
instance p(x )=N(x ;0 I) thestandardmultivariatenormaldistribution hasbeenchoseninpreviouswork.
T T
**Thebackwardprocess.**ThebackwardprocessisaMarkovprocesswithparametrictransitionkernels. Theobjectiveofthe
backwardprocessistoperformthereverseoperationoftheforwardprocess: transformingnoisesamplesinto(approximate)samples
fromthedistributionµ. Followingpreviouswork itisassumedthatthebackwardprocessisdefinedbyGaussiandistributions
p (x |x )for2≤t≤T as
θ t−1 t
p (x |x )=N(x ;gθ(x ) σ2I)
θ t−1 t t−1 t t t
p (x |x )=gθ(x )
θ 0 1 1 1
wherethevarianceparametersσ2 ∈R aredefinedbyafixedschedule themeanfunctionsgθ :RD →RD arelearnedusinga
t ≥0 t
neuralnetwork(withparametersθ)for2≤t≤T andgθ :RD →X isaseparatefunctiondependentonσ . Inpractice thesame
1 1
networkhasbeenusedforthefunctionsgθ for2≤t≤T andaseparatediscretedecoderforgθ.
t 1
Generatingnewsamplesfromatraineddiffusionmodelisaccomplishedbysamplingx ∼p (x |x )for1≤t≤T starting
t−1 θ t−1 t
fromanoisevectorx ∼p(x )sampledfromthepriorp(x ).
T T T
Thefollowingassumptionismaderegardingthebackwardprocess.
**Assumption1.**Itisassumedthatforeach1≤t≤T thereexistsaconstantKθ >0suchthatforeveryx  x ∈X
t 1 2
||gθ(x )−gθ(x )||≤Kθ||x −x ||.
t 1 t 2 t 1 2
Inotherwords gθ isKθ-Lipschitzcontinuous. ThisassumptionisdiscussedinRemark3.2.
t t
2.2 AdditionalDefinitions
Thedistributionπ (·|x )isdefinedas
θ 0
π (·|x )=q(x |x )p (x |x )p (x |x )...p (x |x )p (·|x ).
θ 0 T 0 θ T−1 T θ T−2 T−1 θ 1 2 θ 1
Intuitively foreachx ∈X π (·|x )representsthedistributiononX obtainedbyreconstructingsamplesfromq(x |x )through
0 θ 0 T 0
thebackwardprocess. Anotherwaytointerpretthisdistributionisthatforanyfunctionf :X →R thefollowingequationholds:
E [f(xˆ )]=E E ...E E [f(xˆ )].
πθ(xˆ0|x0) 0 q(xT|x0) pθ(xT−1|xT) pθ(x1|x2) pθ(xˆ0|x1) 0
GivenafinitesetS ={x1 ... xn}i.i.d. ∼µ theregenerateddistributionisdefinedasthefollowingmixture:
0 0
n
1 (cid:88)
µθ = π (·|xi).
n n θ 0
i=1
This definition is analogous to the empirical regenerated distribution defined for VAEs. The distribution on X learned by the
diffusionmodelisdenotedasπ (·)anddefinedas
θ
π (·)=p(x )p (x |x )p (x |x )...p (x |x )p (·|x ).
θ T θ T−1 T θ T−2 T−1 θ 1 2 θ 1
Inotherwords foranyfunctionf :X →R theexpectationoff withrespecttoπ (·)is
πθ(xˆ0) 0 p(xT) pθ(xT−1|xT) pθ(x1|x2) pθ(xˆ0|x1) 0
Hence  both π (·) and π (·|x ) are defined using the backward process  with the difference that π (·) starts with the prior
θ θ 0 θ
p(x )=N(x ;0 I) whileπ (·|x )startswiththenoisedistributionq(x |x ).
T T θ 0 T 0
Finally thelossfunctionl :X×X →Risdefinedas
l (x  x )=E E ...E E [||x −xˆ ||].
θ T 0 pθ(xT−1|xT) pθ(xT−2|xT−1) pθ(x1|x2) pθ(xˆ0|x1) 0 0
Hence givenanoisevectorx andasamplex  thelossl (x  x )representstheaverageEuclideandistancebetweenx andany
T 0 θ T 0 0
sampleobtainedbypassingx throughthebackwardprocess.
T
2.3 OurApproach
Thegoalistoupper-boundthedistanceW (µ π (·)). Sincethetriangleinequalityimplies
1 θ
W (µ π (·))≤W (µ µθ)+W (µθ π (·))
1 θ 1 n 1 n θ
thedistanceW (µ π (·))canbeupper-boundedbyupper-boundingthetwoexpressionsontheright-handsideseparately. The
upperboundonW (µ µθ)isobtainedusingastraightforwardadaptationofaproof. First W (µ µθ)isupper-boundedusingthe
1 n 1 n
expectationofthelossfunctionl  thentheresultingexpressionisupper-boundedusingaPAC-Bayesian-styleexpressiondependent
ontheempiricalriskandtheprior-matchingterm.
TheupperboundonthesecondtermW (µθ π (·))usesthedefinitionofµθ. Intuitively thedifferencebetweenπ (·|xi)andπ (·)
1 n θ n θ 0 θ
isdeterminedbythecorrespondinginitialdistributions: q(x |xi)andp(x )forπ (·). Hence ifthetwoinitialdistributionsare
T 0 T θ
close andifthestepsofthebackwardprocessaresmooth(seeAssumption1) thenπ (·|xi)andπ (·)areclosetoeachother.
θ 0 θ
3 MainResult
3.1 TheoremStatement
Wearenowreadytopresentthemainresult: aquantitativeupperboundontheWassersteindistancebetweenthedata-generating
distributionµandthelearneddistributionπ (·).
**Theorem3.1.**AssumetheinstancespaceX hasfinitediameter∆=sup ||x−x′||<∞ andletλ>0andδ ∈(0 1)be
x x′∈X
realnumbers. Usingthedefinitionsandassumptionsoftheprevioussection thefollowinginequalityholdswithprobabilityatleast
1−δovertherandomdrawofS ={x1 ... xn}i.i.d. ∼µ:
1 (cid:88)n 1 (cid:88)n 1 n λ∆2
W (µ π (·))≤ E [l (x  xi)]+ KL(q(x |xi)||p(x ))+ log +
1 θ n q(xT|xi0) θ T 0 λn T 0 T λn δ 8n
i=1 i=1
(cid:32) T (cid:33)
(cid:89)
+ Kθ E E [||x −y ||]
t q(xT|xi0) p(yT) T T
t=1
T (cid:32)t−1 (cid:33)
(cid:88) (cid:89)
+ Kθ σ E [||ϵ−ϵ′||]
i t ϵ ϵ′
t=2 i=1
whereϵ ϵ′ ∼N(0 I)arestandardGaussianvectors.
**Remark3.1.**Beforepresentingtheproof letusdiscussTheorem3.1.
*Becausetheright-handsideoftheequationdependsonaquantitycomputedusingafinitei.i.d. sampleS theboundholdswith
exponentialdependenciesonproblemparametersandnoassumptionsonthedata-generatingdistributionµ. *Thefirsttermofthe
right-handsideistheaveragereconstructionlosscomputedoverthesampleS ={x1 ... xn}. Notethatforeach1≤i≤n the
expectationofl (x |xi)isonlycomputedwithrespecttothenoisedistributionq(x |xi)definedbyxi itself. Hence thisterm
θ T 0 T 0 0
measureshowwellanoisevectorx ∼q(x |xi)recoverstheoriginalsamplexi usingthebackwardprocess andaveragesover
T T 0 0
thesetS ={x1 ... xn}. *IftheLipschitzconstantssatisfyKθ <1forall1≤t≤T thenthelargerT is thesmallertheupper
0 0 t
boundgets. ThisisbecausetheproductofKθ’sthenconvergesto0. InRemark3.2below weshowthattheassumptionthatKθ <1
foralltisaquitereasonableone. *Thehyperparameterλcontrolsthetrade-offbetweentheprior-matching(KL)termandthe
diameterterm∆2. IfKθ <1forall1≤t≤T andT →∞ thentheconvergenceoftheboundlargelydependsonthechoiceofλ.
t
Inthatcase λ∝n1/2leadstofasterconvergence whileλ∝nleadstoslowerconvergencetoasmallerquantity. Thisisbecause
theboundstemsfromPAC-Bayesiantheory wherethistrade-offiscommon. *Thelasttermoftheequationdoesnotdependonthe
samplesizen. Hence theupperboundgivenbyTheorem3.1doesnotconvergeto0asn→∞. However iftheLipschitzfactors
(Kθ) arealllessthan1 thenthistermcanbeverysmall especiallyinlow-dimensionalspaces.
t 1≤t≤T
3.2 Proofofthemaintheorem
Thefollowingresultisanadaptationofapreviousresult.
**Lemma3.2.**Letλ > 0andδ ∈ (0 1)berealnumbers. Withprobabilityatleast1−δ overtherandomnessofthesample
S ={x1 ... xn}i.i.d. ∼µ thefollowingholds:
W (µ µθ)≤ E [l (x  xi)]+ KL(q(x |xi)||p(x ))+ log + .
1 n n q(xT|xi0) θ T 0 λn T 0 T λn δ 8n
Theproofofthisresultisastraightforwardadaptationofapreviousproof.
Now letusfocusourattentiononthesecondtermoftheright-handsideoftheequation namelyW (µθ π (·)). Thispartistrickier
1 n θ
thanforVAEs forwhichthegenerativemodel’sdistributionissimplyapushforwardmeasure. Here wehaveanon-deterministic
samplingprocesswithT steps.
Assumption1leadstothefollowinglemmaonthebackwardprocess.
**Lemma3.3.**Foranygivenx  y ∈X wehave
E E [||x −y ||]≤Kθ||x −y ||.
pθ(x0|x1) pθ(y0|y1) 0 0 1 1 1
Moreover if2≤t≤T thenforanygivenx  y ∈X wehave
E E [||x −y ||]≤Kθ||x −y ||+σ E [||ϵ−ϵ′||]
pθ(xt−1|xt) pθ(yt−1|yt) t−1 t−1 t t t t ϵ ϵ′
whereϵ ϵ′ ∼N(0 I) meaningE isashorthandforE .
ϵ ϵ′ ϵ ϵ′∼N(0 I)
**Proof.**Forthefirstpart letx  y ∈X. Sinceaccordingtotheequationp (x |x )=δ (x )andp (y |y )=δ (y )
1 1 θ 0 1 g1θ(x1) 0 θ 0 1 g1θ(y1) 0
E E [||x −y ||]=||gθ(x )−gθ(y )||≤Kθ||x −y ||.
pθ(x0|x1) pθ(y0|y1) 0 0 1 1 1 1 1 1 1
Forthesecondpart let2≤t≤T andx  y ∈X. Sincep (x |x )=N(x ;gθ(x ) σ2I) thereparameterizationtrickimplies
t t θ t−1 t t−1 t t t
thatsamplingx ∼p (x |x )isequivalenttosetting
x =gθ(x )+σ ϵ   withϵ ∼N(0 I).
t−1 t t t t t
Usingtheaboveequation thetriangleinequality andAssumption1 weobtain
E E [||x −y ||]
pθ(xt−1|xt) pθ(yt−1|yt) t−1 t−1
=E [||gθ(x )+σ ϵ −gθ(y )−σ ϵ′||]
ϵt ϵ′t∼N(0 I) t t t t t t t t
≤E [||gθ(x )−gθ(y )||]+σ E [||ϵ −ϵ′||]
ϵt ϵ′t∼N(0 I) t t t t t ϵt ϵ′t∼N(0 I) t t
≤Kθ||x −y ||+σ E [||ϵ−ϵ′||]
t t t t ϵ ϵ′
whereϵ ϵ′ ∼N(0 I).
Next wecanusetheinequalitiesofLemma3.3toprovethefollowingresult.
**Lemma3.4.**LetT ≥1. Thefollowinginequalityholds:
E E E E ...E E [||x −y ||]
pθ(xT−1|xT) pθ(yT−1|yT) pθ(xT−2|xT−1) pθ(yT−2|yT−1) pθ(x0|x1) pθ(y0|y1) 0 0
(cid:32) T (cid:33) T (cid:32)t−1 (cid:33)
(cid:89) (cid:88) (cid:89)
≤ Kθ ||x −y ||+ Kθ σ E [||ϵ−ϵ′||]
t T T i t ϵ ϵ′
t=1 t=2 i=1
**ProofIdea.**Lemma3.4isprovenbyinductionusingLemma3.3intheinductionstep.
Usingthetwopreviouslemmas weobtainthefollowingupperboundonW (µθ π (·)).
**Lemma3.5.**Thefollowinginequalityholds:
n (cid:32) T (cid:33) T (cid:32)t−1 (cid:33)
1 (cid:88) (cid:89) (cid:88) (cid:89)
W (µθ π (·))≤ Kθ E E [||x −y ||]+ Kθ σ E [||ϵ−ϵ′||]
1 n θ n t q(xT|xi0) p(yT) T T i t ϵ ϵ′
i=1 t=1 t=2 i=1
**Proof.**UsingthedefinitionofW  thetrivialcoupling thedefinitionsofµθ andπ (·) andLemma3.4 wegetthedesiredresult.
CombiningLemmas3.2and3.5withthetriangleinequalityyieldsTheorem3.1.
3.3 SpecialcaseusingtheforwardprocessofHoetal. (2020)
Assumption1. Inthissection wespecializethestatementofthetheoremtotheparticularcaseoftheforwardprocessdefinedin
previouswork.
LetX ⊆RD. TheforwardprocessisaGauss-Markovprocesswithtransitiondensitiesdefinedas
√
q(x |x )=N(x ; α x  (1−α )I)
t t−1 t t t−1 t
whereα  ... α isafixednoiseschedulesuchthat0<α <1forallt. Thisdefinitionimpliesthatateachtimestep1≤t≤T
1 T t
√ (cid:89)t
q(x |x )=N(x ; α¯ x  (1−α¯ )I)  withα¯ = α .
t 0 t t 0 t t i
Theoptimizationobjectivetotrainthebackwardprocessensuresthatforeachtimestept thedistributionp (x |x )remainsclose
totheground-truthdistributionq(x |x  x )givenby
t−1 t 0
q(x |x  x )=N(x ;µ˜q(x  x ) σ˜2I)
t−1 t 0 t−1 t t 0 t
√ √
α (1−α¯ ) α¯ (1−α )
µ˜q(x  x )= t t−1 x + t−1 t x .
t t 0 1−α¯ t 1−α¯ 0
Now wediscussAssumption1underthesedefinitions.
**Remark3.2.**WecangetaglimpseattherangeofKθ foratrainedDDPMbylookingatthedistributionq(x |x  x ) since
t t−1 t 0
p (x |x )isoptimizedtobeascloseaspossibletoq(x |x  x ).
θ t−1 t t−1 t 0
Foragivenx ∼µ letustakealookattheLipschitznormofx(cid:55)→µ˜q(x x ). Usingtheaboveequation wehave
0 t 0
α (1−α¯ )
µ˜q(x  x )−µ˜q(y  x )= t t−1 (x −y ).
t t 0 t t 0 1−α¯ t t
Hence x(cid:55)→µ˜q(x x )isK′-Lipschitzcontinuouswith
t 0 t
K′ = t t−1 .
t 1−α¯
Now ifα <1forall1≤t≤T thenwehave1−α¯ >1−α¯  whichimpliesK′ <1forall1≤t≤T.
t t t−1 t
Remark3.2showsthattheLipschitznormofthemeanfunctionµ˜q(· x )doesnotdependonx . Indeed lookingattheprevious
t √ 0 0
equation wecanseethatforanyinitialx  theLipschitznormK′ = αt(1−α¯t−1) onlydependsonthenoiseschedule notx itself.
0 t 1−α¯t 0
Sincegθ(· x )isoptimizedtomatchµ˜q(· x )foreachx inthetrainingset andallthefunctionsµ˜q(· x )havethesameLipschitz
t 0 t 0 0 t 0
normK′ webelieveitisreasonabletoassumegθ isLipschitzcontinuousaswell. ThisistheintuitionbehindAssumption1.
**Theprior-matchingterm.**Withthedefinitionsofthissection thepriormatchingtermKL(q(x |x )||p(x ))hasthefollowing
closedform:
KL(q(x |x )||p(x ))= 1(cid:2)−Dlog(1−α¯ )−Dα¯ +α¯ ||x ||2(cid:3).
T 0 T 2 T T T 0
**Upper-boundsontheaveragedistancebetweenGaussianvectors.**Ifϵ ϵ′areD-dimensionalvectorssampledfromN(0 I) then
E [||ϵ−ϵ′||]≤ 2D.
ϵ ϵ′
Moreover sinceq(x |x )=N(x ; α¯ x  (1−α¯ )I)andthepriorp(y )=N(y ;0 I)
T 0 T T 0 T T T
(cid:112)
E E [||x −y ||]≤ α¯ ||x ||2+(2−α¯ )D.
q(xT|x0) p(yT) T T T 0 T
**Specialcaseofthemaintheorem.**Withthedefinitionsofthissection theinequalityofTheorem3.1impliesthatwithprobability
atleast1−δovertherandomnessof{x1 ... x
0
Introduction
Diffusion models  alongside generative adversarial networks and variational autoencoders (VAEs)  are among the most influential
In this research  a different strategy is employed  applying methods created for VAEs to DDPMs  which can be viewed as hierarchical
VAEs with fixed encoders. This method enables the derivation of quantitative  Wasserstein-based upper bounds without making
1.1
Related Works
1.2
Our contributions
Preliminaries
Lebesgue measure  and variables are added in parentheses to enhance readability (e.g.  q(xt|xt−1) to denote a time-dependent
conditional distribution). An instance space X  which is a subset of RD with the Euclidean distance as the underlying metric  and
a target data-generating distribution µ ∈ M +
1 (X) are considered. Note that it is not assumed that µ has a density with respect to
the Lebesgue measure. Additionally  || · || represents the Euclidean (L2) norm  and Ep(x) is used as shorthand for Ex∼p(x). Given
probability measures p  q ∈ M +
1 (X) and a real number k > 1  the Wasserstein distance of order k is defined as (Villani  2009):
Wk(p  q) =
inf
γ∈Γ(p q)
��
X×X
||x − y||kdγ(x  y)
�1/k
where Γ(p  q) denotes the set of couplings of p and q  meaning the set of joint distributions on X × X with respective marginals p
and q. The product measure p ⊗ q is referred to as the trivial coupling  and the Wasserstein distance of order 1 is simply referred to
2.1
Denoising Diffusion Models
indexed by time 0 ≤ t ≤ T  where the number of time steps T is a predetermined choice.
**The forward process.** The forward process transforms a data point x0 ∼ µ into a noise distribution q(xT |x0) through a sequence
of conditional distributions q(xt|xt−1) for 1 ≤ t ≤ T. It is assumed that the forward process is defined such that for sufficiently
large T  the distribution q(xT |x0) is close to a simple noise distribution p(xT )  which is referred to as the prior distribution. For
instance  p(xT ) = N(xT ; 0  I)  the standard multivariate normal distribution  has been chosen in previous work.
pθ(xt−1|xt) for 2 ≤ t ≤ T as
pθ(xt−1|xt) = N(xt−1; gθ
t (xt)  σ2
t I)
pθ(x0|x1) = gθ
t ∈ R≥0 are defined by a fixed schedule  the mean functions gθ
t : RD → RD are learned using a
neural network (with parameters θ) for 2 ≤ t ≤ T  and gθ
1 : RD → X is a separate function dependent on σ1. In practice  the same
t for 2 ≤ t ≤ T  and a separate discrete decoder for gθ
Generating new samples from a trained diffusion model is accomplished by sampling xt−1 ∼ pθ(xt−1|xt) for 1 ≤ t ≤ T  starting
from a noise vector xT ∼ p(xT ) sampled from the prior p(xT ).
**Assumption 1.** It is assumed that for each 1 ≤ t ≤ T  there exists a constant Kθ
t > 0 such that for every x1  x2 ∈ X
t (x1) − gθ
t (x2)|| ≤ Kθ
t ||x1 − x2||.
t is Kθ
t -Lipschitz continuous. This assumption is discussed in Remark 3.2.
2.2
Additional Definitions
The distribution πθ(·|x0) is defined as
πθ(·|x0) = q(xT |x0)pθ(xT −1|xT )pθ(xT −2|xT −1) . . . pθ(x1|x2)pθ(·|x1).
Intuitively  for each x0 ∈ X  πθ(·|x0) represents the distribution on X obtained by reconstructing samples from q(xT |x0) through
the backward process. Another way to interpret this distribution is that for any function f : X → R  the following equation holds:
Eπθ(ˆx0|x0)[f(ˆx0)] = Eq(xT |x0)Epθ(xT −1|xT ) . . . Epθ(x1|x2)Epθ(ˆx0|x1)[f(ˆx0)].
Given a finite set S = {x1
0} i.i.d. ∼ µ  the regenerated distribution is defined as the following mixture:
n = 1
�
πθ(·|xi
diffusion model is denoted as πθ(·) and defined as
πθ(·) = p(xT )pθ(xT −1|xT )pθ(xT −2|xT −1) . . . pθ(x1|x2)pθ(·|x1).
In other words  for any function f : X → R  the expectation of f with respect to πθ(·) is
Eπθ(ˆx0)[f(ˆx0)] = Ep(xT )Epθ(xT −1|xT ) . . . Epθ(x1|x2)Epθ(ˆx0|x1)[f(ˆx0)].
Hence  both πθ(·) and πθ(·|x0) are defined using the backward process  with the difference that πθ(·) starts with the prior
p(xT ) = N(xT ; 0  I)  while πθ(·|x0) starts with the noise distribution q(xT |x0).
Finally  the loss function lθ : X × X → R is defined as
lθ(xT   x0) = Epθ(xT −1|xT )Epθ(xT −2|xT −1) . . . Epθ(x1|x2)Epθ(ˆx0|x1)[||x0 − ˆx0||].
Hence  given a noise vector xT and a sample x0  the loss lθ(xT   x0) represents the average Euclidean distance between x0 and any
sample obtained by passing xT through the backward process.
2.3
Our Approach
W1(µ  πθ(·)) ≤ W1(µ  µθ
n) + W1(µθ
the distance W1(µ  πθ(·)) can be upper-bounded by upper-bounding the two expressions on the right-hand side separately. The
n) is obtained using a straightforward adaptation of a proof. First  W1(µ  µθ
n) is upper-bounded using the
n  πθ(·)) uses the definition of µθ
0) and πθ(·)
is determined by the corresponding initial distributions: q(xT |xi
0) and p(xT ) for πθ(·). Hence  if the two initial distributions are
0) and πθ(·) are close to each other.
Main Result
3.1
Theorem Statement
distribution µ and the learned distribution πθ(·).
**Theorem 3.1.** Assume the instance space X has finite diameter ∆ = supx x′∈X ||x − x′|| < ∞  and let λ > 0 and δ ∈ (0  1) be
1 − δ over the random draw of S = {x1
0} i.i.d. ∼ µ:
W1(µ  πθ(·)) ≤ 1
Eq(xT |xi
0)[lθ(xT   xi
0)] + 1
λn
KL(q(xT |xi
0)||p(xT )) + 1
λn log n
δ + λ∆2
+
� T
Kθ
0)Ep(yT )[||xT − yT ||]
t=2
�t−1
i
σtEϵ ϵ′[||ϵ − ϵ′||]
where ϵ  ϵ′ ∼ N(0  I) are standard Gaussian vectors.
right-hand side is the average reconstruction loss computed over the sample S = {x1
0}. Note that for each 1 ≤ i ≤ n  the
expectation of lθ(xT |xi
0) is only computed with respect to the noise distribution q(xT |xi
0) defined by xi
0 itself. Hence  this term
measures how well a noise vector xT ∼ q(xT |xi
0) recovers the original sample xi
0 using the backward process  and averages over
the set S = {x1
t < 1 for all 1 ≤ t ≤ T  then the larger T is  the smaller the upper
t ’s then converges to 0. In Remark 3.2 below  we show that the assumption that Kθ
t < 1
for all t is a quite reasonable one. * The hyperparameter λ controls the trade-off between the prior-matching (KL) term and the
diameter term ∆2. If Kθ
t < 1 for all 1 ≤ t ≤ T and T → ∞  then the convergence of the bound largely depends on the choice of λ.
In that case  λ ∝ n1/2 leads to faster convergence  while λ ∝ n leads to slower convergence to a smaller quantity. This is because
sample size n. Hence  the upper bound given by Theorem 3.1 does not converge to 0 as n → ∞. However  if the Lipschitz factors
t )1≤t≤T are all less than 1  then this term can be very small  especially in low-dimensional spaces.
3.2
Proof of the main theorem
**Lemma 3.2.** Let λ > 0 and δ ∈ (0  1) be real numbers. With probability at least 1 − δ over the randomness of the sample
S = {x1
0} i.i.d. ∼ µ  the following holds:
n) ≤ 1
8n .
than for VAEs  for which the generative model’s distribution is simply a pushforward measure. Here  we have a non-deterministic
sampling process with T steps.
**Lemma 3.3.** For any given x1  y1 ∈ X  we have
Epθ(x0|x1)Epθ(y0|y1)[||x0 − y0||] ≤ Kθ
1||x1 − y1||.
Moreover  if 2 ≤ t ≤ T  then for any given xt  yt ∈ X  we have
Epθ(xt−1|xt)Epθ(yt−1|yt)[||xt−1 − yt−1||] ≤ Kθ
t ||xt − yt|| + σtEϵ ϵ′[||ϵ − ϵ′||]
where ϵ  ϵ′ ∼ N(0  I)  meaning Eϵ ϵ′ is a shorthand for Eϵ ϵ′∼N(0 I).
**Proof.** For the first part  let x1  y1 ∈ X. Since according to the equation pθ(x0|x1) = δgθ
1(x1)(x0) and pθ(y0|y1) = δgθ
Epθ(x0|x1)Epθ(y0|y1)[||x0 − y0||] = ||gθ
1(x1) − gθ
1(y1)|| ≤ Kθ
For the second part  let 2 ≤ t ≤ T and xt  yt ∈ X. Since pθ(xt−1|xt) = N(xt−1; gθ
t I)  the reparameterization trick implies
that sampling xt−1 ∼ pθ(xt−1|xt) is equivalent to setting
xt−1 = gθ
t (xt) + σtϵt  with ϵt ∼ N(0  I).
Epθ(xt−1|xt)Epθ(yt−1|yt)[||xt−1 − yt−1||]
= Eϵt ϵ′
t (xt) + σtϵt − gθ
t (yt) − σtϵ′
≤ Eϵt ϵ′
t (xt) − gθ
t (yt)||] + σtEϵt ϵ′
t∼N(0 I)[||ϵt − ϵ′
≤ Kθ
where ϵ  ϵ′ ∼ N(0  I).
**Lemma 3.4.** Let T ≥ 1. The following inequality holds:
Epθ(xT −1|xT )Epθ(yT −1|yT )Epθ(xT −2|xT −1)Epθ(yT −2|yT −1) . . . Epθ(x0|x1)Epθ(y0|y1)[||x0 − y0||]
≤
||xT − yT || +
n  πθ(·)) ≤ 1
0)Ep(yT )[||xT − yT ||] +
n and πθ(·)  and Lemma 3.4  we get the desired result.
3.3
Special case using the forward process of Ho et al. (2020)
Let X ⊆ RD. The forward process is a Gauss-Markov process with transition densities defined as
q(xt|xt−1) = N(xt; √αtxt−1  (1 − αt)I)
where α1  . . .   αT is a fixed noise schedule such that 0 < αt < 1 for all t. This definition implies that at each time step 1 ≤ t ≤ T
q(xt|x0) = N(xt; √¯αtx0  (1 − ¯αt)I)  with ¯αt =
t�
αi.
The optimization objective to train the backward process ensures that for each time step t  the distribution pθ(xt−1|xt) remains close
to the ground-truth distribution q(xt−1|xt  x0) given by
q(xt−1|xt  x0) = N(xt−1; ˜µq
t(xt  x0)  ˜σ2
t(xt  x0) =
√αt(1 − ¯αt−1)
1 − ¯αt
xt +
√¯αt−1(1 − αt)
x0.
t for a trained DDPM by looking at the distribution q(xt−1|xt  x0)  since
pθ(xt−1|xt) is optimized to be as close as possible to q(xt−1|xt  x0).
For a given x0 ∼ µ  let us take a look at the Lipschitz norm of x �→ ˜µq
t(xt  x0) − ˜µq
t(yt  x0) =
(xt − yt).
Hence  x �→ ˜µq
t(x  x0) is K′
t =
Now  if αt < 1 for all 1 ≤ t ≤ T  then we have 1 − ¯αt > 1 − ¯αt−1  which implies K′
t < 1 for all 1 ≤ t ≤ T.
t(·  x0) does not depend on x0. Indeed  looking at the previous
√αt(1−¯αt−1)
1−¯αt
only depends on the noise schedule  not x0 itself.
t (·  x0) is optimized to match ˜µq
t(·  x0) for each x0 in the training set  and all the functions ˜µq
t(·  x0) have the same Lipschitz
t is Lipschitz continuous as well. This is the intuition behind Assumption 1.
**The prior-matching term.** With the definitions of this section  the prior matching term KL(q(xT |x0)||p(xT )) has the following
KL(q(xT |x0)||p(xT )) = 1
−D log(1 − ¯αT ) − D¯αT + ¯αT ||x0||2�
**Upper-bounds on the average distance between Gaussian vectors.** If ϵ  ϵ′ are D-dimensional vectors sampled from N(0  I)  then
Eϵ ϵ′[||ϵ − ϵ′||] ≤
Moreover  since q(xT |x0) = N(xT ; √¯αT x0  (1 − ¯αT )I) and the prior p(yT ) = N(yT ; 0  I)
Eq(xT |x0)Ep(yT )[||xT − yT ||] ≤
¯αT ||x0||2 + (2 − ¯αT )D.
at least 1 − δ over the randomness of {x1","as well as in various other applications.
Two primary",
R014,1,TMLR,"This research examines a specific category of structured nonconvex-nonconcave min-max problems that demon-
strate a characteristic known as weak Minty solutions. This concept  which has only recently been defined  has
already demonstrated its effectiveness by encompassing various generalizations of monotonicity at the same time.
We establish new convergence findings for an enhanced variant of the optimistic gradient method (OGDA) within
this framework  achieving a convergence rate of 1/k for the most effective iteration  measured by the squared
operator norm  a result that aligns with the extragradient method (EG). Furthermore  we introduce a modified
version of EG that incorporates an adaptive step size  eliminating the need for prior knowledge of the problem’s
specific parameters.
1","The recent advancements in machine learning models  particularly those that can be formulated as min-max optimization problems
have generated significant interest in saddle point problems. Examples of these models include generative adversarial networks
adversarial learning frameworks  adversarial example games  and actor-critic","that generally perform well  the theoretical understanding of scenarios where the objective function is nonconvex in the minimization
component and nonconcave in the maximization component remains limited  with some research even suggesting intractability in
certain cases.
A specific subset of nonconvex-nonconcave min-max problems was analyzed  and it was found that the extragradient method (EG)
exhibited favorable convergence behavior in experimental settings. Surprisingly  these problems did not appear to possess any of
the recognized favorable characteristics  such as monotonicity or Minty solutions. Subsequently  a suitable concept was identified
(see Assumption 1)  which is less restrictive than the presence of a Minty solution (a condition frequently employed in the existing
literature) and also extends the idea of negative comonotonicity. Because of these properties that unify and generalize  the concept of
weak Minty solutions was quickly investigated.
Assumption 1 (Weak Minty solution). For a given operator F:Rd→Rd  there is a point u∗∈Rdand a parameter ρ >0such that:
⟨F(u)  u−u∗⟩ ≥ −ρ
2∥F(u)∥2∀u∈Rd. (1)
Moreover  it has been demonstrated that a modified version of EG is capable of addressing problems with such solutions  achieving
a complexity of O(ϵ−1)for the squared operator norm. This adaptation  referred to as EG+  is based on a bold extrapolation step
followed by a cautious update step. A similar step size approach has been previously examined in the context of a stochastic variant
of EG.
In a similar vein  we explore a variation of the optimistic gradient descent ascent (OGDA)  also known as Forward-Reflected-
Backward (FoRB). We address the following question with an affirmative answer:
Can OGDA achieve convergence guarantees comparable to those of EG when dealing with weak Minty solutions?
Specifically  we demonstrate that a modified version of the OGDA method  defined for a step size a >0and a parameter 0< γ≤1
as follows:
uk= ¯uk−aF(¯uk)
¯uk+1= ¯uk−γaF(uk) ∀k≥0
can achieve the same convergence bounds as EG+ by requiring only a single gradient oracle call in each iteration.
It is worth noting that OGDA is most frequently expressed in a form where γ= 1. However  two recent studies have examined
a more generalized coefficient. While these earlier studies focused on the monotone setting  the true significance of γbecomes
apparent only when dealing with weak Minty solutions. In this context  we find that γmust be greater than 1 to ensure convergence
a phenomenon that is not observed in monotone problems.
When examining a general smooth min-max problem:
min
xmax
yf(x  y)
the operator Fmentioned in Assumption 1 naturally emerges as F(u) := [∇xf(x  y) −∇yf(x  y)]withu= (x  y). However
by examining saddle point problems from the broader viewpoint of variational inequalities (VIs) through the operator F  we can
concurrently address more scenarios  such as certain equilibrium problems.
The parameter ρin the definition of weak Minty solutions (1) is crucial for both the analysis and the experiments. Specifically  it
is essential that the step size exceeds a value proportional to ρ. Simultaneously  as is typical  the step size is limited from above
by the inverse of the Lipschitz constant of F. For instance  since some researchers require the step size to be less than1
4L  their
convergence claim is valid only if ρ <1
4L. This condition was later improved to ρ <1
2Lfor the choice γ= 1and to ρ <1
Lfor
even smaller values of γ. As in the monotone setting  OGDA requires a smaller step size than EG. Nevertheless  through a different
analysis  we are able to match the most general condition on the weak Minty parameter ρ <1
Lfor appropriate γanda.
1.1 Contribution
Our contributions are summarized as follows:
1.We establish a new convergence rate of O(1/k)  measured by the squared operator norm  for a modified version of OGDA
which we call OGDA+. This rate matches that of EG and builds upon the recently introduced concept of weak solutions to
the Minty variational inequality.
2.Even when a stronger condition is imposed  specifically that the operator is also monotone  we enhance the range of feasible
step sizes for OGDA+ and obtain the most favorable result known for the standard method ( γ= 1).
3. We demonstrate a complexity bound of O(ϵ−2)for a stochastic variant of the OGDA+ method.
4.We also introduce an adaptive step size version of EG+. This version achieves the same convergence guarantees without
requiring any knowledge of the Lipschitz constant of the operator F. Consequently  it can potentially take larger steps in
areas with low curvature  enabling convergence where a fixed step size strategy might fail.
1.2 Related literature
We will concentrate on the nonconvex-nonconcave setting  as there is a substantial body of work on convergence rates in terms of a gap
function or distance to a solution for monotone problems  as well as generalizations such as nonconvex-concave  convex-nonconcave
or under the Polyak-Łojasiewicz assumption.
Weak Minty. It was observed that a specific parameterization of the von Neumann ratio game exhibits a novel type of solution
termed 'weak Minty ' without having any of the previously known characteristics like (negative) comonotonicity or Minty solutions.
Convergence in the presence of such solutions was demonstrated for EG  provided that the extrapolation step size is twice as large as
the update step. Subsequently  it was shown that the condition on the weak Minty parameter can be relaxed by further reducing the
length of the update step  and this is done adaptively. To avoid the need for additional hyperparameters  a backtracking line search is
also proposed  which may incur extra gradient computations or require second-order information (in contrast to the adaptive step
size we propose in Algorithm 3). A different approach is taken by focusing on the min-max setting and using multiple ascent steps
per descent step  achieving the same O(1/k)rate as EG.
Minty solutions. Numerous studies have presented various methods for scenarios where the problem at hand has a Minty solution.
It was shown that weakly monotone VIs can be solved by iteratively adding a quadratic proximity term and repeatedly optimizing
the resulting strongly monotone VI using any convergent method. The convergence of the OGDA method was proven  but without a
specific rate. It was noted that the convergence proof for the golden ratio algorithm (GRAAL) is valid without any changes. While
the assumption that a Minty solution exists is a generalization of the monotone setting  it is challenging to find non-monotone
problems that possess such solutions. In our setting  as per Assumption 1  the Minty inequality (MVI) can be violated at any point
by a factor proportional to the squared operator norm.
Negative comonotonicity. Although previously studied under the term 'cohypomonotonicity ' the concept of negative comono-
tonicity has recently been explored. It offers a generalization of monotonicity  but in a direction distinct from the concept of Minty
solutions  and only a limited number of studies have examined methods in this context. An anchored version of EG was studied  and
an improved convergence rate of O(1/k2)(in terms of the squared operator norm) was shown. Similarly  an accelerated version of
the reflected gradient method was investigated. Whether such acceleration is possible in the more general setting of weak Minty
solutions remains an open question (any Stampacchia solution to the VI given by a negatively comonotone operator is a weak Minty
solution). Another intriguing observation was made  where for cohypomonotone problems  a monotonically decreasing gradient
norm was demonstrated when using EG. However  we did not observe this in our experiments  emphasizing the need to differentiate
this class from problems with weak Minty solutions.
2
Interaction dominance. The concept of α-interaction dominance for nonconvex-nonconcave min-max problems was investigated
and it was shown that the proximal-point method converges sublinearly if this condition is met in yand linearly if it is met in both
components. Furthermore  it was demonstrated that if a problem is interaction dominant in both components  it is also negatively
comonotone.
Optimism. The positive effects of introducing the simple modification commonly known as optimism have recently attracted the
attention of the machine learning community. Its name comes from online optimization. The idea dates back even further and has
also been studied in the mathematical programming community.
2 Preliminaries
2.1 Notions of solution
We outline the most frequently used solution concepts in the context of variational inequalities (VIs) and related areas. These
concepts are typically defined with respect to a constraint set C⊆Rd. A Stampacchia solution of the VI given by F:Rd→Rdis a
point u∗such that:
⟨F(u∗)  u−u∗⟩ ≥0∀u∈C. (SVI)
In this work  we only consider the unconstrained case where C=Rd  and the above condition simplifies to F(u∗) = 0 . Closely
related is the following concept: A Minty solution is a point u∗∈Csuch that:
⟨F(u)  u−u∗⟩ ≥0∀u∈C. (MVI)
For a continuous operator F  a Minty solution of the VI is always a Stampacchia solution. The converse is generally not true but
holds  for example  if the operator Fis monotone. Specifically  there are nonmonotone problems with Stampacchia solutions but
without any Minty solutions.
2.2 Notions of monotonicity
This section aims to revisit some fundamental and more contemporary concepts of monotonicity and the relationships between them.
An operator Fis considered monotone if:
⟨F(u)−F(v)  u−v⟩ ≥0.
Such operators naturally arise as the gradients of convex functions  from convex-concave min-max problems  or from equilibrium
problems.
Two frequently studied notions that fall into this category are strongly monotone operators  which satisfy:
⟨F(u)−F(v)  u−v⟩ ≥µ∥u−v∥2
and cocoercive operators  which fulfill:
⟨F(u)−F(v)  u−v⟩ ≥β∥F(u)−F(v)∥2. (2)
Strongly monotone operators emerge as gradients of strongly convex functions or in strongly-convex-strongly-concave min-max
problems. Cocoercive operators appear  for instance  as gradients of smooth convex functions  in which case (2) holds with βequal
to the inverse of the gradient’s Lipschitz constant.
Departing from monotonicity. Both of the aforementioned subclasses of monotonicity can serve as starting points for exploring
the non-monotone domain. Given that general non-monotone operators may display erratic behavior  such as periodic cycles and
spurious attractors  it is reasonable to seek settings that extend the monotone framework while remaining manageable. First and
foremost is the extensively studied setting of ν-weak monotonicity:
⟨F(u)−F(v)  u−v⟩ ≥ − ν∥u−v∥2.
Such operators arise as the gradients of the well-studied class of weakly convex functions  a rather general class of functions as it
includes all functions without upward cusps. In particular  every smooth function with a Lipschitz gradient turns out to fulfill this
property. On the other hand  extending the notion of cocoercivity to allow for negative coefficients  referred to as cohypomonotonicity
has received much less attention and is given by:
⟨F(u)−F(v)  u−v⟩ ≥ − γ∥F(u)−F(v)∥2.
Clearly  if a Stampacchia solution exists for such an operator  then it also fulfills Assumption 1.
Behavior with respect to the solution. While the above properties are standard assumptions in the literature  it is usually sufficient
to require the corresponding condition to hold when one of the arguments is a (Stampacchia) solution. This means that instead of
monotonicity  it is enough to ask for the operator Fto be star-monotone  i.e.
⟨F(u)  u−u∗⟩ ≥0
or star-cocoercive
⟨F(u)  u−u∗⟩ ≥γ∥F(u)∥2.
In this spirit  we can provide a new interpretation to the assumption of the existence of a weak Minty solution as asking for the
operator Fto be negatively star-cocoercive (with respect to at least one solution). Furthermore  we want to point out that while the
above star notions are sometimes required to hold for all solutions u∗  in the following we only require it to hold for a single solution.
3
3 OGDA for problems with weak Minty solutions
The generalized version of OGDA  which we denote with a '+' to emphasize the presence of the additional parameter γ  is given by:
Algorithm 1 OGDA+
Require: Starting point u0=u−1∈Rd  step size a >0and parameter 0< γ < 1.
fork= 0 1  ...do
uk+1=uk−a((1 + γ)F(uk)−F(uk−1))
end for
Theorem 3.1. LetF:Rd→RdbeL-Lipschitz continuous satisfying Assumption 1 with1
L> ρ  and let (uk)k≥0be the iterates
generated by Algorithm 1 with step size asatisfying a > ρ and
aL≤1−γ
1 +γ. (3)
Then  for all k≥0
i=0 ... k−1∥F(ui)∥2≤1
kaγ(a−ρ)∥u0+aF(u0)−u∗∥2.
In particular  as long as ρ <1
L  we can find a γsmall enough such that the above bound holds.
The first observation is that we would like to choose aas large as possible  as this allows us to treat the largest class of problems
withρ < a . To be able to choose a large step size a  we must decrease γ  as evident from (3). However  this degrades the algorithm’s
speed by making the update steps smaller. The same effect can be observed for EG+ and is therefore not surprising. One could
derive an optimal γ(i.e.  minimizing the right-hand side) from Theorem 3.1  but this","ρ. In practice  the strategy of decreasing γuntil convergence is achieved  but not further  yields reasonable results.
Furthermore  we want to point out that the condition ρ <1
Lis precisely the best possible bound for EG+.
3.1 Improved bounds under monotonicity
While the above theorem also holds if the operator Fis monotone  we can modify the proof slightly to obtain a better dependence on
the parameters:
Theorem 3.2. LetF:Rd→Rdbe monotone and L-Lipschitz. If aL=2−γ
2+γ−ϵforϵ >0  then the iterates generated by OGDA+
fulfill
i=0 ... k−1∥F(ui)∥2≤2
ka2γ2ϵ∥u0+aF(u0)−u∗∥2.
In particular  we can choose γ= 1anda <1
2L.
There are different works discussing the convergence of OGDA in terms of the iterates or a gap function with a <1
2L. However  we
want to compare the above bound to more similar results on rates for the best iterate in terms of the operator norm. The same rate as
ours for OGDA is shown  but requires the conservative step size bound a≤1
16L. This was later improved to a≤1
3L. All of these
only deal with the case γ= 1. The only other reference that deals with a generalized (i.e.  not necessarily γ= 1) version of OGDA
is another work  where the resulting step size condition is a≤2−γ
4L  which is strictly worse than ours for any γ. To summarize  not
only do we show for the first time that the step size of a generalization of OGDA can go above1
2L  but we also provide the least
restrictive bound for any value of γ.
3.2 OGDA+ stochastic
In this section  we discuss the setting where  instead of the exact operator F  we only have access to a collection of independent
estimators F(·  ξi)at every iteration. We assume here that the estimator Fis unbiased  i.e.  E[F(uk  ξ)|uk−1] =F(uk)  and has
bounded variance E[∥F(uk  ξ)−F(uk)∥2]≤σ2. We show that we can still guarantee convergence by using batch sizes Bof order
O(ϵ−1).
Algorithm 2 stochastic OGDA+
Require: Starting point u0=u−1∈Rd  step size a >0  parameter 0< γ≤1and batch size B.
Sample i.i.d. (ξi)B
i=1and compute estimator ˜gk=1
BPB
i=1F(uk  ξk
i)
uk+1=uk−a((1 + γ)˜gk−˜gk−1)
4
Theorem 3.3. LetF:Rd→RdbeL-Lipschitz satisfying Assumption 1 with1
L> ρ  and let (uk)k≥0be the sequence of
iterates generated by stochastic OGDA+  with aandγsatisfying ρ < a <1−γ
1+γ1
L. Then  to visit an ϵ-stationary point such that
mini=0 ... k−1E[∥F(ui)∥2]< ϵ  we require
1
kaγ(a−ρ)∥u0+a˜g0−u∗∥2max
1 4σ2
aLϵ
calls to the stochastic oracle ˜F  with large batch sizes of order O(ϵ−1).
In practice  large batch sizes of order O(ϵ−1)are typically not desirable; instead  a small or decreasing step size is preferred. In the
weak Minty setting  this causes additional trouble due to the necessity of large step sizes to guarantee convergence. Unfortunately
the current analysis does not allow for variable γ.
4 EG+ with adaptive step sizes
In this section  we present Algorithm 3  which is able to solve the previously mentioned problems without any knowledge of the
Lipschitz constant L  as it is typically difficult to compute in practice. Additionally  it is well known that rough estimates will lead to
small step sizes and slow convergence behavior. However  in the presence of weak Minty solutions  there is additional interest in
choosing large step sizes. We observed in Theorem 3.1 and related works the fact that a crucial ingredient in the analysis is that the
step size is chosen larger than a multiple of the weak Minty parameter ρto guarantee convergence at all. For these reasons  we want
to outline a method using adaptive step sizes  meaning that no step size needs to be supplied by the user and no line-search is carried
out.
Since the analysis of OGDA+ is already quite involved in the constant step size regime  we choose to equip EG+ with an adaptive
step size which estimates the inverse of the (local) Lipschitz constant  see (4). Due to the fact that the literature on adaptive methods
especially in the context of VIs  is so vast  we do not aim to give a comprehensive review but highlight only a few with especially
interesting properties. In particular  we do not want to touch on methods with a linesearch procedure  which typically result in
multiple gradient computations per iteration.
We use a simple and therefore widely used step size choice that naively estimates the local Lipschitz constant and forces a monotone
decreasing behavior. Such step sizes have been used extensively for monotone VIs and similarly in the context of the mirror-prox
method  which corresponds to EG in the setting of (non-Euclidean) Bregman distances.
A version of EG with a different adaptive step size choice has been investigated  with the unique feature that it is able to achieve the
optimal rates for both smooth and nonsmooth problems without modification. However  these rates are only for monotone VIs and
are in terms of the gap function.
One of the drawbacks of adaptive methods resides in the fact that the step sizes are typically required to be nonincreasing  which
results in poor behavior if a high-curvature area was visited by the iterates before reaching a low-curvature region. To the best of our
knowledge  the only method that is allowed to use nonmonotone step sizes to treat VIs and does not use a possibly costly linesearch
is the golden ratio algorithm. It comes with the additional benefit of not requiring a global bound on the Lipschitz constant of Fat
all. While it is known that this method converges under the stronger assumption of the existence of Minty solutions  a quantitative
convergence result is still open.
Algorithm 3 EG+ with adaptive step size
Require: Starting points u0 ¯u0∈Rd  initial step size a0and parameters τ∈(0 1)and0< γ≤1.
Find the step size:
ak= min
ak−1 τ∥¯uk−¯uk−1∥
∥F(¯uk)−F(¯uk−1)∥
(4)
Compute next iterate:
uk= ¯uk−akF(¯uk)
¯uk+1= ¯uk−akγF(uk).
Clearly  akis monotonically decreasing by construction. Moreover  it is bounded away from zero by the simple observation that
ak≥min{a0  τ/L}>0. The sequence therefore converges to a positive number  which we denote by a∞:= lim kak.
Theorem 4.1. LetF:Rd→RdbeL-Lipschitz that satisfies Assumption 1  where u∗denotes any weak Minty solution  with
a∞>2ρ  and let (uk)k≥0be the iterates generated by Algorithm 3 with γ=1
2andτ∈(0 1). Then  there exists a k0∈Nsuch that
i=k0 ... k∥F(uk)∥2≤1
k−k0L
τ(a∞/2−ρ)∥¯uk0−u∗∥2.
5
Algorithm 3 presented above provides several benefits but also some drawbacks. The main advantage resides in the fact that the
Lipschitz constant of the operator Fdoes not need to be known. Moreover  the step size choice presented in (4) might allow us
to take steps much larger than what would be suggested by a global Lipschitz constant if the iterates never  or only during later
iterations  visit the region of high curvature (large local L). In such cases  these larger step sizes come with the additional advantage
that they allow us to solve a richer class of problems  as we are able to relax the condition ρ <1
4Lin the case of EG+ to ρ < a ∞/2
where a∞= lim kak≥τ/L.
On the other hand  we face the problem that the bounds in Theorem 4.1 only hold after an unknown number of initial iterations when
ak/ak+1≤1
τis finally satisfied. In theory  this might take a long time if the curvature around the solution is much higher than in
the starting area  as this will force the need to decrease the step size very late into the solution process  resulting in the quotient
ak/ak+1being too large. This drawback could be mitigated by choosing τsmaller. However  this will result in poor performance
due to small step sizes. Even for monotone problems where this type of step size has been proposed  this problem could not be
circumvented  and authors instead focused on the convergence of the iterates without any rate.
5 Numerical experiments
In the following  we compare the EG+ method with the two methods we propose: OGDA+ and EG+ with adaptive step size (see
Algorithm 1 and Algorithm 3  respectively). Last but not least  we also include the CurvatureEG+ method  which is a modification
of EG+ that adaptively chooses the ratio of extrapolation and update steps. In addition  a backtracking linesearch is performed with
an initial guess made by second-order information  whose extra cost we ignore in the experiments.
5.1 Von Neumann’s ratio game
We consider von Neumann’s ratio game  which is given by:
x∈∆mmax
y∈∆nV(x  y) =⟨x  Ry⟩
⟨x  Sy⟩  (5)
where R∈Rm×nandS∈Rm×nwith⟨x  Sy⟩>0for all x∈∆m  y∈∆n  with ∆ :={z∈Rd:zi>0 Pd
i=1zi= 1}denoting
the unit simplex. Expression (5) can be interpreted as the value V(x  y)for a stochastic game with a single state and mixed strategies.
We see an illustration of a particularly difficult instance of (5). Interestingly  we still observe good convergence behavior  although
an estimated ρis more than ten times larger than the estimated Lipschitz constant.
5.2 Forsaken
A particularly difficult min-max toy example with a 'Forsaken' solution was proposed and is given by:
x∈Rmax
y∈Rx(y−0.45) + ϕ(x)−ϕ(y)  (6)
where ϕ(z) =1
6z6−2
4z4+1
4z2−1
2z. This problem exhibits a Stampacchia solution at (x∗  y∗)≈(0.08 0.4)  but also two limit
cycles not containing any critical point of the objective function. In addition  it was also observed that the limit cycle closer to
the solution repels possible trajectories of iterates  thus 'shielding' the solution. Later  it was noticed that  restricted to the box
∥(x  y)∥∞<3  the above-mentioned solution is weak Minty with ρ≥2·0.477761   which is much larger than1
2L≈0.08. In line
with these observations  we can see that none of the fixed step size methods with a step size bounded by1
Lconverge. In light of this
observation  a backtracking linesearch was proposed  which potentially allows for larger steps than predicted by the global Lipschitz
constant. Similarly  our proposed adaptive step size version of EG+ (see Algorithm 3) is also able to break through the repelling
limit cycle and converge to the solution. On top of this  it does so at a faster rate and without the need for additional computations in
the backtracking procedure.
5.3 Lower bound example
The following min-max problem was introduced as a lower bound on the dependence between ρandLfor EG+:
y∈Rµxy+ζ
2(x2−y2). (7)
In particular  it was stated that EG+ (with any γ) and constant step size a=1
Lconverges for this problem if and only if (0 0)is a
weak Minty solution with ρ <1−γ
L  where ρandLcan be computed explicitly in the above example and are given by:
L=p
µ2+ζ2and ρ=µ2−ζ2
2µ.
By choosing µ= 3andζ=−1  we get exactly ρ=1
L  therefore predicting divergence of EG+ for any γ  which is exactly what is
empirically observed. Although the general upper bound proved in Theorem 3.1 only states convergence in the case ρ <1
L  we
observe rapid convergence of OGDA+ for this example  showcasing that it can drastically outperform EG+ in some scenarios.
6","Many intriguing questions persist in the domain of min-max problems  particularly when departing from the convex-concave
framework. Very recently  it was demonstrated that the O(1/k)bounds on the squared operator norm for EG and OGDA for the
last iterate (and not just the best one) are valid even in the negatively comonotone setting. Deriving a comparable statement in the
presence of merely weak Minty solutions remains an open question.
In general  our analysis and experiments seem to suggest that there is minimal benefit in employing OGDA+ over EG+ for the
majority of problems  as the reduced iteration cost is counterbalanced by the smaller step size. An exception is presented by problem
(7)  which is not covered by theory  and OGDA+ is the only method capable of converging.
Finally  we note that the previous paradigm in pure minimization of 'smaller step size ensures convergence' but 'larger step size
gets there faster ' where the latter is typically constrained by the reciprocal of the gradient’s Lipschitz constant  does not appear
to hold true for min-max problems anymore. The analysis of various methods in the presence of weak Minty solutions indicates
that convergence can be lost if the step size is excessively small and sometimes needs to be larger than1
L  which one can typically
only hope for in adaptive methods. Our EG+ method with adaptive step size accomplishes this even without the added expense of a
backtracking linesearch.article graphicx
7
Thisresearchexaminesaspecificcategoryofstructurednonconvex-nonconcavemin-maxproblemsthatdemon-
strateacharacteristicknownasweakMintysolutions. Thisconcept whichhasonlyrecentlybeendefined has
alreadydemonstrateditseffectivenessbyencompassingvariousgeneralizationsofmonotonicityatthesametime.
Weestablishnewconvergencefindingsforanenhancedvariantoftheoptimisticgradientmethod(OGDA)within
thisframework achievingaconvergencerateof1/kforthemosteffectiveiteration measuredbythesquared
operatornorm aresultthatalignswiththeextragradientmethod(EG).Furthermore weintroduceamodified
versionofEGthatincorporatesanadaptivestepsize eliminatingtheneedforpriorknowledgeoftheproblem’s
specificparameters.
Therecentadvancementsinmachinelearningmodels particularlythosethatcanbeformulatedasmin-maxoptimizationproblems
havegeneratedsignificantinterestinsaddlepointproblems. Examplesofthesemodelsincludegenerativeadversarialnetworks
adversariallearningframeworks adversarialexamplegames andactor-criticmethods. Whilepracticalmethodshavebeendeveloped
thatgenerallyperformwell thetheoreticalunderstandingofscenarioswheretheobjectivefunctionisnonconvexintheminimization
componentandnonconcaveinthemaximizationcomponentremainslimited withsomeresearchevensuggestingintractabilityin
certaincases.
Aspecificsubsetofnonconvex-nonconcavemin-maxproblemswasanalyzed anditwasfoundthattheextragradientmethod(EG)
exhibitedfavorableconvergencebehaviorinexperimentalsettings. Surprisingly theseproblemsdidnotappeartopossessanyof
therecognizedfavorablecharacteristics suchasmonotonicityorMintysolutions. Subsequently asuitableconceptwasidentified
(seeAssumption1) whichislessrestrictivethanthepresenceofaMintysolution(aconditionfrequentlyemployedintheexisting
literature)andalsoextendstheideaofnegativecomonotonicity. Becauseofthesepropertiesthatunifyandgeneralize theconceptof
weakMintysolutionswasquicklyinvestigated.
Assumption1(WeakMintysolution). ForagivenoperatorF :Rd →Rd thereisapointu∗ ∈Rdandaparameterρ>0suchthat:
ρ
⟨F(u) u−u∗⟩≥− ∥F(u)∥2 ∀u∈Rd. (1)
Moreover ithasbeendemonstratedthatamodifiedversionofEGiscapableofaddressingproblemswithsuchsolutions achieving
acomplexityofO(ϵ−1)forthesquaredoperatornorm. Thisadaptation referredtoasEG+ isbasedonaboldextrapolationstep
followedbyacautiousupdatestep. Asimilarstepsizeapproachhasbeenpreviouslyexaminedinthecontextofastochasticvariant
ofEG.
Backward(FoRB).Weaddressthefollowingquestionwithanaffirmativeanswer:
CanOGDAachieveconvergenceguaranteescomparabletothoseofEGwhendealingwithweakMintysolutions?
Specifically wedemonstratethatamodifiedversionoftheOGDAmethod definedforastepsizea>0andaparameter0<γ ≤1
asfollows:
u =u¯ −aF(u¯ )
k k k
u¯ =u¯ −γaF(u )  ∀k ≥0
k+1 k k
canachievethesameconvergenceboundsasEG+byrequiringonlyasinglegradientoraclecallineachiteration.
ItisworthnotingthatOGDAismostfrequentlyexpressedinaformwhereγ = 1. However tworecentstudieshaveexamined
amoregeneralizedcoefficient. Whiletheseearlierstudiesfocusedonthemonotonesetting thetruesignificanceofγ becomes
apparentonlywhendealingwithweakMintysolutions. Inthiscontext wefindthatγ mustbegreaterthan1toensureconvergence
aphenomenonthatisnotobservedinmonotoneproblems.
Whenexaminingageneralsmoothmin-maxproblem:
minmaxf(x y)
x y
theoperatorF mentionedinAssumption1naturallyemergesasF(u) := [∇ f(x y) −∇ f(x y)]withu = (x y). However
byexaminingsaddlepointproblemsfromthebroaderviewpointofvariationalinequalities(VIs)throughtheoperatorF wecan
concurrentlyaddressmorescenarios suchascertainequilibriumproblems.
TheparameterρinthedefinitionofweakMintysolutions(1)iscrucialforboththeanalysisandtheexperiments. Specifically it
isessentialthatthestepsizeexceedsavalueproportionaltoρ. Simultaneously asistypical thestepsizeislimitedfromabove
bytheinverseoftheLipschitzconstantofF. Forinstance sincesomeresearchersrequirethestepsizetobelessthan 1  their
4L
convergenceclaimisvalidonlyifρ < 1 . Thisconditionwaslaterimprovedtoρ < 1 forthechoiceγ = 1andtoρ < 1 for
4L 2L L
evensmallervaluesofγ. Asinthemonotonesetting OGDArequiresasmallerstepsizethanEG.Nevertheless throughadifferent
analysis weareabletomatchthemostgeneralconditionontheweakMintyparameterρ< 1 forappropriateγ anda.
L
Ourcontributionsaresummarizedasfollows:
1. WeestablishanewconvergencerateofO(1/k) measuredbythesquaredoperatornorm foramodifiedversionofOGDA
whichwecallOGDA+. ThisratematchesthatofEGandbuildsupontherecentlyintroducedconceptofweaksolutionsto
theMintyvariationalinequality.
2. Evenwhenastrongerconditionisimposed specificallythattheoperatorisalsomonotone weenhancetherangeoffeasible
stepsizesforOGDA+andobtainthemostfavorableresultknownforthestandardmethod(γ =1).
3. WedemonstrateacomplexityboundofO(ϵ−2)forastochasticvariantoftheOGDA+method.
4. WealsointroduceanadaptivestepsizeversionofEG+. Thisversionachievesthesameconvergenceguaranteeswithout
requiringanyknowledgeoftheLipschitzconstantoftheoperatorF. Consequently itcanpotentiallytakelargerstepsin
areaswithlowcurvature enablingconvergencewhereafixedstepsizestrategymightfail.
1.2 Relatedliterature
Wewillconcentrateonthenonconvex-nonconcavesetting asthereisasubstantialbodyofworkonconvergenceratesintermsofagap
functionordistancetoasolutionformonotoneproblems aswellasgeneralizationssuchasnonconvex-concave convex-nonconcave
orunderthePolyak-Łojasiewiczassumption.
WeakMinty. ItwasobservedthataspecificparameterizationofthevonNeumannratiogameexhibitsanoveltypeofsolution
termed'weakMinty 'withouthavinganyofthepreviouslyknowncharacteristicslike(negative)comonotonicityorMintysolutions.
ConvergenceinthepresenceofsuchsolutionswasdemonstratedforEG providedthattheextrapolationstepsizeistwiceaslargeas
theupdatestep. Subsequently itwasshownthattheconditionontheweakMintyparametercanberelaxedbyfurtherreducingthe
lengthoftheupdatestep andthisisdoneadaptively. Toavoidtheneedforadditionalhyperparameters abacktrackinglinesearchis
alsoproposed whichmayincurextragradientcomputationsorrequiresecond-orderinformation(incontrasttotheadaptivestep
sizeweproposeinAlgorithm3). Adifferentapproachistakenbyfocusingonthemin-maxsettingandusingmultipleascentsteps
perdescentstep achievingthesameO(1/k)rateasEG.
Mintysolutions. NumerousstudieshavepresentedvariousmethodsforscenarioswheretheproblemathandhasaMintysolution.
ItwasshownthatweaklymonotoneVIscanbesolvedbyiterativelyaddingaquadraticproximitytermandrepeatedlyoptimizing
theresultingstronglymonotoneVIusinganyconvergentmethod. TheconvergenceoftheOGDAmethodwasproven butwithouta
specificrate. Itwasnotedthattheconvergenceproofforthegoldenratioalgorithm(GRAAL)isvalidwithoutanychanges. While
theassumptionthataMintysolutionexistsisageneralizationofthemonotonesetting  itischallengingtofindnon-monotone
problemsthatpossesssuchsolutions. Inoursetting asperAssumption1 theMintyinequality(MVI)canbeviolatedatanypoint
byafactorproportionaltothesquaredoperatornorm.
Negativecomonotonicity. Althoughpreviouslystudiedundertheterm'cohypomonotonicity 'theconceptofnegativecomono-
tonicityhasrecentlybeenexplored. Itoffersageneralizationofmonotonicity butinadirectiondistinctfromtheconceptofMinty
solutions andonlyalimitednumberofstudieshaveexaminedmethodsinthiscontext. AnanchoredversionofEGwasstudied and
animprovedconvergencerateofO(1/k2)(intermsofthesquaredoperatornorm)wasshown. Similarly anacceleratedversionof
thereflectedgradientmethodwasinvestigated. WhethersuchaccelerationispossibleinthemoregeneralsettingofweakMinty
solutionsremainsanopenquestion(anyStampacchiasolutiontotheVIgivenbyanegativelycomonotoneoperatorisaweakMinty
solution). Anotherintriguingobservationwasmade whereforcohypomonotoneproblems amonotonicallydecreasinggradient
normwasdemonstratedwhenusingEG.However wedidnotobservethisinourexperiments emphasizingtheneedtodifferentiate
thisclassfromproblemswithweakMintysolutions.
Interactiondominance. Theconceptofα-interactiondominancefornonconvex-nonconcavemin-maxproblemswasinvestigated
anditwasshownthattheproximal-pointmethodconvergessublinearlyifthisconditionismetinyandlinearlyifitismetinboth
components. Furthermore itwasdemonstratedthatifaproblemisinteractiondominantinbothcomponents itisalsonegatively
Optimism. Thepositiveeffectsofintroducingthesimplemodificationcommonlyknownasoptimismhaverecentlyattractedthe
attentionofthemachinelearningcommunity. Itsnamecomesfromonlineoptimization. Theideadatesbackevenfurtherandhas
alsobeenstudiedinthemathematicalprogrammingcommunity.
2.1 Notionsofsolution
conceptsaretypicallydefinedwithrespecttoaconstraintsetC ⊆Rd. AStampacchiasolutionoftheVIgivenbyF :Rd →Rdisa
pointu∗suchthat:
⟨F(u∗) u−u∗⟩≥0 ∀u∈C. (SVI)
Inthiswork weonlyconsidertheunconstrainedcasewhereC =Rd andtheaboveconditionsimplifiestoF(u∗)=0. Closely
relatedisthefollowingconcept: AMintysolutionisapointu∗ ∈C suchthat:
⟨F(u) u−u∗⟩≥0 ∀u∈C. (MVI)
ForacontinuousoperatorF aMintysolutionoftheVIisalwaysaStampacchiasolution. Theconverseisgenerallynottruebut
holds forexample iftheoperatorF ismonotone. Specifically therearenonmonotoneproblemswithStampacchiasolutionsbut
withoutanyMintysolutions.
2.2 Notionsofmonotonicity
Thissectionaimstorevisitsomefundamentalandmorecontemporaryconceptsofmonotonicityandtherelationshipsbetweenthem.
AnoperatorF isconsideredmonotoneif:
⟨F(u)−F(v) u−v⟩≥0.
Suchoperatorsnaturallyariseasthegradientsofconvexfunctions fromconvex-concavemin-maxproblems orfromequilibrium
Twofrequentlystudiednotionsthatfallintothiscategoryarestronglymonotoneoperators whichsatisfy:
⟨F(u)−F(v) u−v⟩≥µ∥u−v∥2
andcocoerciveoperators whichfulfill:
⟨F(u)−F(v) u−v⟩≥β∥F(u)−F(v)∥2. (2)
Stronglymonotoneoperatorsemergeasgradientsofstronglyconvexfunctionsorinstrongly-convex-strongly-concavemin-max
problems. Cocoerciveoperatorsappear forinstance asgradientsofsmoothconvexfunctions inwhichcase(2)holdswithβ equal
totheinverseofthegradient’sLipschitzconstant.
Departingfrommonotonicity. Bothoftheaforementionedsubclassesofmonotonicitycanserveasstartingpointsforexploring
thenon-monotonedomain. Giventhatgeneralnon-monotoneoperatorsmaydisplayerraticbehavior suchasperiodiccyclesand
spuriousattractors itisreasonabletoseeksettingsthatextendthemonotoneframeworkwhileremainingmanageable. Firstand
foremostistheextensivelystudiedsettingofν-weakmonotonicity:
⟨F(u)−F(v) u−v⟩≥−ν∥u−v∥2.
Suchoperatorsariseasthegradientsofthewell-studiedclassofweaklyconvexfunctions arathergeneralclassoffunctionsasit
includesallfunctionswithoutupwardcusps. Inparticular everysmoothfunctionwithaLipschitzgradientturnsouttofulfillthis
property.Ontheotherhand extendingthenotionofcocoercivitytoallowfornegativecoefficients referredtoascohypomonotonicity
hasreceivedmuchlessattentionandisgivenby:
⟨F(u)−F(v) u−v⟩≥−γ∥F(u)−F(v)∥2.
Clearly ifaStampacchiasolutionexistsforsuchanoperator thenitalsofulfillsAssumption1.
Behaviorwithrespecttothesolution. Whiletheabovepropertiesarestandardassumptionsintheliterature itisusuallysufficient
torequirethecorrespondingconditiontoholdwhenoneoftheargumentsisa(Stampacchia)solution. Thismeansthatinsteadof
monotonicity itisenoughtoaskfortheoperatorF tobestar-monotone i.e.
⟨F(u) u−u∗⟩≥0
orstar-cocoercive
⟨F(u) u−u∗⟩≥γ∥F(u)∥2.
Inthisspirit wecanprovideanewinterpretationtotheassumptionoftheexistenceofaweakMintysolutionasaskingforthe
operatorF tobenegativelystar-cocoercive(withrespecttoatleastonesolution). Furthermore wewanttopointoutthatwhilethe
abovestarnotionsaresometimesrequiredtoholdforallsolutionsu∗ inthefollowingweonlyrequireittoholdforasinglesolution.
3 OGDAforproblemswithweakMintysolutions
ThegeneralizedversionofOGDA whichwedenotewitha'+'toemphasizethepresenceoftheadditionalparameterγ isgivenby:
Algorithm1OGDA+
Require: Startingpointu =u ∈Rd stepsizea>0andparameter0<γ <1.
0 −1
fork =0 1 ...do
u =u −a((1+γ)F(u )−F(u ))
k+1 k k k−1
endfor
Theorem3.1. LetF :Rd →RdbeL-LipschitzcontinuoussatisfyingAssumption1with 1 >ρ andlet(u ) betheiterates
L k k≥0
generatedbyAlgorithm1withstepsizeasatisfyinga>ρand
1−γ
aL≤ . (3)
1+γ
Then forallk ≥0
min ∥F(u )∥2 ≤ ∥u +aF(u )−u∗∥2.
i=0 ... k−1 i kaγ(a−ρ) 0 0
Inparticular aslongasρ< 1 wecanfindaγ smallenoughsuchthattheaboveboundholds.
Thefirstobservationisthatwewouldliketochooseaaslargeaspossible asthisallowsustotreatthelargestclassofproblems
withρ<a. Tobeabletochoosealargestepsizea wemustdecreaseγ asevidentfrom(3). However thisdegradesthealgorithm’s
speedbymakingtheupdatestepssmaller. ThesameeffectcanbeobservedforEG+andisthereforenotsurprising. Onecould
deriveanoptimalγ (i.e. minimizingtheright-handside)fromTheorem3.1 butthisresultsinanon-intuitivecubicdependenceon
ρ. Inpractice thestrategyofdecreasingγ untilconvergenceisachieved butnotfurther yieldsreasonableresults.
Furthermore wewanttopointoutthattheconditionρ< 1 ispreciselythebestpossibleboundforEG+.
3.1 Improvedboundsundermonotonicity
WhiletheabovetheoremalsoholdsiftheoperatorF ismonotone wecanmodifytheproofslightlytoobtainabetterdependenceon
theparameters:
Theorem3.2. LetF :Rd →RdbemonotoneandL-Lipschitz. IfaL= 2−γ −ϵforϵ>0 thentheiteratesgeneratedbyOGDA+
2+γ
i=0 ... k−1 i ka2γ2ϵ 0 0
Inparticular wecanchooseγ =1anda< 1 .
2L
TherearedifferentworksdiscussingtheconvergenceofOGDAintermsoftheiteratesoragapfunctionwitha< 1 . However we
wanttocomparetheaboveboundtomoresimilarresultsonratesforthebestiterateintermsoftheoperatornorm. Thesamerateas
oursforOGDAisshown butrequirestheconservativestepsizebounda≤ 1 . Thiswaslaterimprovedtoa≤ 1 . Allofthese
16L 3L
onlydealwiththecaseγ =1. Theonlyotherreferencethatdealswithageneralized(i.e. notnecessarilyγ =1)versionofOGDA
isanotherwork wheretheresultingstepsizeconditionisa≤ 2−γ whichisstrictlyworsethanoursforanyγ. Tosummarize not
onlydoweshowforthefirsttimethatthestepsizeofageneralizationofOGDAcangoabove 1  butwealsoprovidetheleast
restrictiveboundforanyvalueofγ.
3.2 OGDA+stochastic
Inthissection wediscussthesettingwhere insteadoftheexactoperatorF weonlyhaveaccesstoacollectionofindependent
estimatorsF(· ξ )ateveryiteration. WeassumeherethattheestimatorF isunbiased i.e. E[F(u  ξ)|u ]=F(u ) andhas
i k k−1 k
boundedvarianceE[∥F(u  ξ)−F(u )∥2]≤σ2. WeshowthatwecanstillguaranteeconvergencebyusingbatchsizesBoforder
k k
Algorithm2stochasticOGDA+
Require: Startingpointu =u ∈Rd stepsizea>0 parameter0<γ ≤1andbatchsizeB.
Samplei.i.d. (ξ )B andcomputeestimatorg˜ = 1 (cid:80)B F(u  ξk)
i i=1 k B i=1 k i
u =u −a((1+γ)g˜ −g˜ )
Theorem 3.3. Let F : Rd → Rd be L-Lipschitz satisfying Assumption 1 with 1 > ρ  and let (u ) be the sequence of
iteratesgeneratedbystochasticOGDA+ withaandγ satisfyingρ < a < 1−γ 1. Then tovisitanϵ-stationarypointsuchthat
1+γL
min E[∥F(u )∥2]<ϵ werequire
i=0 ... k−1 i
1 (cid:26) 4σ2(cid:27)
∥u +ag˜ −u∗∥2max 1
kaγ(a−ρ) 0 0 aLϵ
callstothestochasticoracleF˜ withlargebatchsizesoforderO(ϵ−1).
Inpractice largebatchsizesoforderO(ϵ−1)aretypicallynotdesirable;instead asmallordecreasingstepsizeispreferred. Inthe
weakMintysetting thiscausesadditionaltroubleduetothenecessityoflargestepsizestoguaranteeconvergence. Unfortunately
thecurrentanalysisdoesnotallowforvariableγ.
4 EG+withadaptivestepsizes
Inthissection wepresentAlgorithm3 whichisabletosolvethepreviouslymentionedproblemswithoutanyknowledgeofthe
LipschitzconstantL asitistypicallydifficulttocomputeinpractice. Additionally itiswellknownthatroughestimateswillleadto
smallstepsizesandslowconvergencebehavior. However inthepresenceofweakMintysolutions thereisadditionalinterestin
choosinglargestepsizes. WeobservedinTheorem3.1andrelatedworksthefactthatacrucialingredientintheanalysisisthatthe
stepsizeischosenlargerthanamultipleoftheweakMintyparameterρtoguaranteeconvergenceatall. Forthesereasons wewant
tooutlineamethodusingadaptivestepsizes meaningthatnostepsizeneedstobesuppliedbytheuserandnoline-searchiscarried
SincetheanalysisofOGDA+isalreadyquiteinvolvedintheconstantstepsizeregime wechoosetoequipEG+withanadaptive
stepsizewhichestimatestheinverseofthe(local)Lipschitzconstant see(4). Duetothefactthattheliteratureonadaptivemethods
especiallyinthecontextofVIs issovast wedonotaimtogiveacomprehensivereviewbuthighlightonlyafewwithespecially
interestingproperties. Inparticular wedonotwanttotouchonmethodswithalinesearchprocedure whichtypicallyresultin
multiplegradientcomputationsperiteration.
WeuseasimpleandthereforewidelyusedstepsizechoicethatnaivelyestimatesthelocalLipschitzconstantandforcesamonotone
decreasingbehavior. SuchstepsizeshavebeenusedextensivelyformonotoneVIsandsimilarlyinthecontextofthemirror-prox
method whichcorrespondstoEGinthesettingof(non-Euclidean)Bregmandistances.
AversionofEGwithadifferentadaptivestepsizechoicehasbeeninvestigated withtheuniquefeaturethatitisabletoachievethe
optimalratesforbothsmoothandnonsmoothproblemswithoutmodification. However theseratesareonlyformonotoneVIsand
areintermsofthegapfunction.
Oneofthedrawbacksofadaptivemethodsresidesinthefactthatthestepsizesaretypicallyrequiredtobenonincreasing which
resultsinpoorbehaviorifahigh-curvatureareawasvisitedbytheiteratesbeforereachingalow-curvatureregion. Tothebestofour
knowledge theonlymethodthatisallowedtousenonmonotonestepsizestotreatVIsanddoesnotuseapossiblycostlylinesearch
isthegoldenratioalgorithm. ItcomeswiththeadditionalbenefitofnotrequiringaglobalboundontheLipschitzconstantofF at
all. WhileitisknownthatthismethodconvergesunderthestrongerassumptionoftheexistenceofMintysolutions aquantitative
convergenceresultisstillopen.
Algorithm3EG+withadaptivestepsize
Require: Startingpointsu  u¯ ∈Rd initialstepsizea andparametersτ ∈(0 1)and0<γ ≤1.
0 0 0
Findthestepsize:
(cid:26) (cid:27)
τ∥u¯ −u¯ ∥
a =min a   k k−1 (4)
k k−1 ∥F(u¯ )−F(u¯ )∥
k k−1
Computenextiterate:
u =u¯ −a F(u¯ )
k k k k
u¯ =u¯ −a γF(u ).
k+1 k k k
Clearly a ismonotonicallydecreasingbyconstruction. Moreover itisboundedawayfromzerobythesimpleobservationthat
k
a ≥min{a  τ/L}>0. Thesequencethereforeconvergestoapositivenumber whichwedenotebya :=lim a .
k 0 ∞ k k
Theorem4.1. LetF : Rd → Rd beL-LipschitzthatsatisfiesAssumption1 whereu∗ denotesanyweakMintysolution with
a >2ρ andlet(u ) betheiteratesgeneratedbyAlgorithm3withγ = 1 andτ ∈(0 1). Then thereexistsak ∈Nsuchthat
∞ k k≥0 2 0
1 L
min ∥F(u )∥2 ≤ ∥u¯ −u∗∥2.
i=k0 ... k k k−k0τ(a∞/2−ρ) k0
Algorithm3presentedaboveprovidesseveralbenefitsbutalsosomedrawbacks. Themainadvantageresidesinthefactthatthe
LipschitzconstantoftheoperatorF doesnotneedtobeknown. Moreover thestepsizechoicepresentedin(4)mightallowus
totakestepsmuchlargerthanwhatwouldbesuggestedbyaglobalLipschitzconstantiftheiteratesnever oronlyduringlater
iterations visittheregionofhighcurvature(largelocalL). Insuchcases theselargerstepsizescomewiththeadditionaladvantage
thattheyallowustosolvearicherclassofproblems asweareabletorelaxtheconditionρ< 1 inthecaseofEG+toρ<a /2
4L ∞
wherea =lim a ≥τ/L.
∞ k k
Ontheotherhand wefacetheproblemthattheboundsinTheorem4.1onlyholdafteranunknownnumberofinitialiterationswhen
a /a ≤ 1 isfinallysatisfied. Intheory thismighttakealongtimeifthecurvaturearoundthesolutionismuchhigherthanin
k k+1 τ
thestartingarea asthiswillforcetheneedtodecreasethestepsizeverylateintothesolutionprocess resultinginthequotient
a /a beingtoolarge. Thisdrawbackcouldbemitigatedbychoosingτ smaller. However thiswillresultinpoorperformance
k k+1
duetosmallstepsizes. Evenformonotoneproblemswherethistypeofstepsizehasbeenproposed thisproblemcouldnotbe
circumvented andauthorsinsteadfocusedontheconvergenceoftheiterateswithoutanyrate.
5 Numericalexperiments
Inthefollowing wecomparetheEG+methodwiththetwomethodswepropose: OGDA+andEG+withadaptivestepsize(see
Algorithm1andAlgorithm3 respectively). Lastbutnotleast wealsoincludetheCurvatureEG+method whichisamodification
ofEG+thatadaptivelychoosestheratioofextrapolationandupdatesteps. Inaddition abacktrackinglinesearchisperformedwith
aninitialguessmadebysecond-orderinformation whoseextracostweignoreintheexperiments.
5.1 VonNeumann’sratiogame
WeconsidervonNeumann’sratiogame whichisgivenby:
⟨x Ry⟩
min maxV(x y)=   (5)
x∈∆my∈∆n ⟨x Sy⟩
whereR∈Rm×nandS ∈Rm×nwith⟨x Sy⟩>0forallx∈∆  y ∈∆  with∆:={z ∈Rd :z >0 (cid:80)d z =1}denoting
m n i i=1 i
theunitsimplex. Expression(5)canbeinterpretedasthevalueV(x y)forastochasticgamewithasinglestateandmixedstrategies.
Weseeanillustrationofaparticularlydifficultinstanceof(5). Interestingly westillobservegoodconvergencebehavior although
anestimatedρismorethantentimeslargerthantheestimatedLipschitzconstant.
Aparticularlydifficultmin-maxtoyexamplewitha'Forsaken'solutionwasproposedandisgivenby:
minmaxx(y−0.45)+ϕ(x)−ϕ(y)  (6)
x∈R y∈R
whereϕ(z)= 1z6− 2z4+ 1z2− 1z. ThisproblemexhibitsaStampacchiasolutionat(x∗ y∗)≈(0.08 0.4) butalsotwolimit
6 4 4 2
cyclesnotcontaininganycriticalpointoftheobjectivefunction. Inaddition itwasalsoobservedthatthelimitcyclecloserto
thesolutionrepelspossibletrajectoriesofiterates thus'shielding'thesolution. Later itwasnoticedthat restrictedtothebox
∥(x y)∥ <3 theabove-mentionedsolutionisweakMintywithρ≥2·0.477761 whichismuchlargerthan 1 ≈0.08. Inline
∞ 2L
withtheseobservations wecanseethatnoneofthefixedstepsizemethodswithastepsizeboundedby 1 converge. Inlightofthis
observation abacktrackinglinesearchwasproposed whichpotentiallyallowsforlargerstepsthanpredictedbytheglobalLipschitz
constant. Similarly ourproposedadaptivestepsizeversionofEG+(seeAlgorithm3)isalsoabletobreakthroughtherepelling
limitcycleandconvergetothesolution. Ontopofthis itdoessoatafasterrateandwithouttheneedforadditionalcomputationsin
thebacktrackingprocedure.
5.3 Lowerboundexample
Thefollowingmin-maxproblemwasintroducedasalowerboundonthedependencebetweenρandLforEG+:
ζ
minmaxµxy+ (x2−y2). (7)
x∈R y∈R 2
Inparticular itwasstatedthatEG+(withanyγ)andconstantstepsizea= 1 convergesforthisproblemifandonlyif(0 0)isa
weakMintysolutionwithρ< 1−γ whereρandLcanbecomputedexplicitlyintheaboveexampleandaregivenby:
(cid:112) µ2−ζ2
L= µ2+ζ2 and ρ= .
2µ
Bychoosingµ=3andζ =−1 wegetexactlyρ= 1 thereforepredictingdivergenceofEG+foranyγ whichisexactlywhatis
empiricallyobserved. AlthoughthegeneralupperboundprovedinTheorem3.1onlystatesconvergenceinthecaseρ < 1 we
observerapidconvergenceofOGDA+forthisexample showcasingthatitcandrasticallyoutperformEG+insomescenarios.
framework. Veryrecently itwasdemonstratedthattheO(1/k)boundsonthesquaredoperatornormforEGandOGDAforthe
lastiterate(andnotjustthebestone)arevalideveninthenegativelycomonotonesetting. Derivingacomparablestatementinthe
presenceofmerelyweakMintysolutionsremainsanopenquestion.
Ingeneral ouranalysisandexperimentsseemtosuggestthatthereisminimalbenefitinemployingOGDA+overEG+forthe
majorityofproblems asthereducediterationcostiscounterbalancedbythesmallerstepsize. Anexceptionispresentedbyproblem
(7) whichisnotcoveredbytheory andOGDA+istheonlymethodcapableofconverging.
Finally wenotethatthepreviousparadigminpureminimizationof'smallerstepsizeensuresconvergence'but'largerstepsize
getstherefaster 'wherethelatteristypicallyconstrainedbythereciprocalofthegradient’sLipschitzconstant doesnotappear
toholdtrueformin-maxproblemsanymore. TheanalysisofvariousmethodsinthepresenceofweakMintysolutionsindicates
thatconvergencecanbelostifthestepsizeisexcessivelysmallandsometimesneedstobelargerthan 1 whichonecantypically
onlyhopeforinadaptivemethods. OurEG+methodwithadaptivestepsizeaccomplishesthisevenwithouttheaddedexpenseofa
backtrackinglinesearch.articlegraphicx
Introduction
Assumption 1 (Weak Minty solution). For a given operator F : Rd → Rd  there is a point u∗ ∈ Rd and a parameter ρ > 0 such that:
⟨F(u)  u − u∗⟩ ≥ −ρ
2∥F(u)∥2
∀u ∈ Rd.
(1)
a complexity of O(ϵ−1) for the squared operator norm. This adaptation  referred to as EG+  is based on a bold extrapolation step
Specifically  we demonstrate that a modified version of the OGDA method  defined for a step size a > 0 and a parameter 0 < γ ≤ 1
uk = ¯uk − aF(¯uk)
¯uk+1 = ¯uk − γaF(uk)
∀k ≥ 0
It is worth noting that OGDA is most frequently expressed in a form where γ = 1. However  two recent studies have examined
a more generalized coefficient. While these earlier studies focused on the monotone setting  the true significance of γ becomes
apparent only when dealing with weak Minty solutions. In this context  we find that γ must be greater than 1 to ensure convergence
x max
y
f(x  y)
the operator F mentioned in Assumption 1 naturally emerges as F(u) := [∇xf(x  y)  −∇yf(x  y)] with u = (x  y). However
The parameter ρ in the definition of weak Minty solutions (1) is crucial for both the analysis and the experiments. Specifically  it
by the inverse of the Lipschitz constant of F. For instance  since some researchers require the step size to be less than
convergence claim is valid only if ρ <
4L. This condition was later improved to ρ <
2L for the choice γ = 1 and to ρ < 1
L for
analysis  we are able to match the most general condition on the weak Minty parameter ρ < 1
L for appropriate γ and a.
1.1
Contribution
1. We establish a new convergence rate of O(1/k)  measured by the squared operator norm  for a modified version of OGDA
2. Even when a stronger condition is imposed  specifically that the operator is also monotone  we enhance the range of feasible
step sizes for OGDA+ and obtain the most favorable result known for the standard method (γ = 1).
3. We demonstrate a complexity bound of O(ϵ−2) for a stochastic variant of the OGDA+ method.
4. We also introduce an adaptive step size version of EG+. This version achieves the same convergence guarantees without
1.2
Related literature
per descent step  achieving the same O(1/k) rate as EG.
an improved convergence rate of O(1/k2) (in terms of the squared operator norm) was shown. Similarly  an accelerated version of
and it was shown that the proximal-point method converges sublinearly if this condition is met in y and linearly if it is met in both
Preliminaries
2.1
Notions of solution
concepts are typically defined with respect to a constraint set C ⊆ Rd. A Stampacchia solution of the VI given by F : Rd → Rd is a
point u∗ such that:
⟨F(u∗)  u − u∗⟩ ≥ 0
∀u ∈ C.
(SVI)
In this work  we only consider the unconstrained case where C = Rd  and the above condition simplifies to F(u∗) = 0. Closely
related is the following concept: A Minty solution is a point u∗ ∈ C such that:
⟨F(u)  u − u∗⟩ ≥ 0
(MVI)
holds  for example  if the operator F is monotone. Specifically  there are nonmonotone problems with Stampacchia solutions but
2.2
Notions of monotonicity
An operator F is considered monotone if:
⟨F(u) − F(v)  u − v⟩ ≥ 0.
⟨F(u) − F(v)  u − v⟩ ≥ µ∥u − v∥2
⟨F(u) − F(v)  u − v⟩ ≥ β∥F(u) − F(v)∥2.
(2)
problems. Cocoercive operators appear  for instance  as gradients of smooth convex functions  in which case (2) holds with β equal
⟨F(u) − F(v)  u − v⟩ ≥ −ν∥u − v∥2.
⟨F(u) − F(v)  u − v⟩ ≥ −γ∥F(u) − F(v)∥2.
monotonicity  it is enough to ask for the operator F to be star-monotone  i.e.
⟨F(u)  u − u∗⟩ ≥ γ∥F(u)∥2.
operator F to be negatively star-cocoercive (with respect to at least one solution). Furthermore  we want to point out that while the
OGDA for problems with weak Minty solutions
Require: Starting point u0 = u−1 ∈ Rd  step size a > 0 and parameter 0 < γ < 1.
for k = 0  1  ... do
uk+1 = uk − a((1 + γ)F(uk) − F(uk−1))
Theorem 3.1. Let F : Rd → Rd be L-Lipschitz continuous satisfying Assumption 1 with 1
L > ρ  and let (uk)k≥0 be the iterates
generated by Algorithm 1 with step size a satisfying a > ρ and
aL ≤ 1 − γ
1 + γ .
(3)
Then  for all k ≥ 0
i=0 ... k−1 ∥F(ui)∥2 ≤
kaγ(a − ρ)∥u0 + aF(u0) − u∗∥2.
In particular  as long as ρ < 1
L  we can find a γ small enough such that the above bound holds.
The first observation is that we would like to choose a as large as possible  as this allows us to treat the largest class of problems
with ρ < a. To be able to choose a large step size a  we must decrease γ  as evident from (3). However  this degrades the algorithm’s
derive an optimal γ (i.e.  minimizing the right-hand side) from Theorem 3.1  but this results in a non-intuitive cubic dependence on
ρ. In practice  the strategy of decreasing γ until convergence is achieved  but not further  yields reasonable results.
Furthermore  we want to point out that the condition ρ < 1
L is precisely the best possible bound for EG+.
3.1
Improved bounds under monotonicity
While the above theorem also holds if the operator F is monotone  we can modify the proof slightly to obtain a better dependence on
Theorem 3.2. Let F : Rd → Rd be monotone and L-Lipschitz. If aL = 2−γ
2+γ − ϵ for ϵ > 0  then the iterates generated by OGDA+
ka2γ2ϵ∥u0 + aF(u0) − u∗∥2.
In particular  we can choose γ = 1 and a <
There are different works discussing the convergence of OGDA in terms of the iterates or a gap function with a <
ours for OGDA is shown  but requires the conservative step size bound a ≤
16L. This was later improved to a ≤
only deal with the case γ = 1. The only other reference that deals with a generalized (i.e.  not necessarily γ = 1) version of OGDA
is another work  where the resulting step size condition is a ≤ 2−γ
4L   which is strictly worse than ours for any γ. To summarize  not
only do we show for the first time that the step size of a generalization of OGDA can go above
3.2
OGDA+ stochastic
estimators F(·  ξi) at every iteration. We assume here that the estimator F is unbiased  i.e.  E[F(uk  ξ)|uk−1] = F(uk)  and has
bounded variance E[∥F(uk  ξ) − F(uk)∥2] ≤ σ2. We show that we can still guarantee convergence by using batch sizes B of order
Require: Starting point u0 = u−1 ∈ Rd  step size a > 0  parameter 0 < γ ≤ 1 and batch size B.
i=1 and compute estimator ˜gk = 1
B
�B
i=1 F(uk  ξk
i )
uk+1 = uk − a((1 + γ)˜gk − ˜gk−1)
Theorem 3.3. Let F : Rd → Rd be L-Lipschitz satisfying Assumption 1 with 1
L > ρ  and let (uk)k≥0 be the sequence of
iterates generated by stochastic OGDA+  with a and γ satisfying ρ < a < 1−γ
mini=0 ... k−1 E[∥F(ui)∥2] < ϵ  we require
kaγ(a − ρ)∥u0 + a˜g0 − u∗∥2 max
�
1  4σ2
aLϵ
In practice  large batch sizes of order O(ϵ−1) are typically not desirable; instead  a small or decreasing step size is preferred. In the
EG+ with adaptive step sizes
step size is chosen larger than a multiple of the weak Minty parameter ρ to guarantee convergence at all. For these reasons  we want
is the golden ratio algorithm. It comes with the additional benefit of not requiring a global bound on the Lipschitz constant of F at
Require: Starting points u0  ¯u0 ∈ Rd  initial step size a0 and parameters τ ∈ (0  1) and 0 < γ ≤ 1.
ak = min
ak−1
τ∥¯uk − ¯uk−1∥
∥F(¯uk) − F(¯uk−1)∥
uk = ¯uk − akF(¯uk)
¯uk+1 = ¯uk − akγF(uk).
Clearly  ak is monotonically decreasing by construction. Moreover  it is bounded away from zero by the simple observation that
ak ≥ min{a0  τ/L} > 0. The sequence therefore converges to a positive number  which we denote by a∞ := limk ak.
Theorem 4.1. Let F : Rd → Rd be L-Lipschitz that satisfies Assumption 1  where u∗ denotes any weak Minty solution  with
a∞ > 2ρ  and let (uk)k≥0 be the iterates generated by Algorithm 3 with γ = 1
2 and τ ∈ (0  1). Then  there exists a k0 ∈ N such that
i=k0 ... k ∥F(uk)∥2 ≤
k − k0
τ(a∞/2 − ρ)∥¯uk0 − u∗∥2.
Lipschitz constant of the operator F does not need to be known. Moreover  the step size choice presented in (4) might allow us
that they allow us to solve a richer class of problems  as we are able to relax the condition ρ <
4L in the case of EG+ to ρ < a∞/2
where a∞ = limk ak ≥ τ/L.
ak/ak+1 ≤ 1
τ is finally satisfied. In theory  this might take a long time if the curvature around the solution is much higher than in
ak/ak+1 being too large. This drawback could be mitigated by choosing τ smaller. However  this will result in poor performance
Numerical experiments
5.1
Von Neumann’s ratio game
x∈∆m max
y∈∆n V (x  y) = ⟨x  Ry⟩
⟨x  Sy⟩
(5)
where R ∈ Rm×n and S ∈ Rm×n with ⟨x  Sy⟩ > 0 for all x ∈ ∆m  y ∈ ∆n  with ∆ := {z ∈ Rd : zi > 0  �d
i=1 zi = 1} denoting
the unit simplex. Expression (5) can be interpreted as the value V (x  y) for a stochastic game with a single state and mixed strategies.
an estimated ρ is more than ten times larger than the estimated Lipschitz constant.
5.2
Forsaken
x∈R max
y∈R x(y − 0.45) + ϕ(x) − ϕ(y)
(6)
where ϕ(z) = 1
6z6 − 2
4z4 + 1
4z2 − 1
2z. This problem exhibits a Stampacchia solution at (x∗  y∗) ≈ (0.08  0.4)  but also two limit
∥(x  y)∥∞ < 3  the above-mentioned solution is weak Minty with ρ ≥ 2 · 0.477761  which is much larger than
2L ≈ 0.08. In line
with these observations  we can see that none of the fixed step size methods with a step size bounded by 1
L converge. In light of this
5.3
Lower bound example
The following min-max problem was introduced as a lower bound on the dependence between ρ and L for EG+:
y∈R µxy + ζ
2(x2 − y2).
(7)
In particular  it was stated that EG+ (with any γ) and constant step size a = 1
L converges for this problem if and only if (0  0) is a
weak Minty solution with ρ < 1−γ
L   where ρ and L can be computed explicitly in the above example and are given by:
L =
µ2 + ζ2
and
ρ = µ2 − ζ2
.
By choosing µ = 3 and ζ = −1  we get exactly ρ = 1
empirically observed. Although the general upper bound proved in Theorem 3.1 only states convergence in the case ρ < 1
Conclusion
framework. Very recently  it was demonstrated that the O(1/k) bounds on the squared operator norm for EG and OGDA for the
that convergence can be lost if the step size is excessively small and sometimes needs to be larger than 1"
R024,1,TMLR,"We propose Regularized Learning under Label shifts ( RLLS )  a principled and a
practical domain-adaptation algorithm to correct for shifts in the label distribution
between a source and a target domain. We ﬁrst estimate importance weights using
labeled source data and unlabeled target data  and then train a classiﬁer on the
weighted source samples. We derive a generalization bound for the classiﬁer on the
target domain which is independent of the (ambient) data dimensions  and instead
only depends on the complexity of the function class. To the best of our knowledge
this is the ﬁrst generalization bound for the label-shift problem where the labels in
the target domain are not available. Based on this bound  we propose a regularized
estimator for the small-sample regime which accounts for the uncertainty in the
estimated weights. Experiments on the CIFAR-10 and MNIST datasets show that
RLLS improves classiﬁcation accuracy  especially in the low sample and large-shift
regimes  compared to previous","est(targetdistribution)canbesigniﬁcantlyshiftedcomparedtothedistributionofthedataonwhich
themodelwastrained(sourcedistribution). Inmanycases thepubliclyavailablelarge-scaledatasets
withwhichthemodelsaretraineddonotrepresentandreﬂectthestatisticsofaparticulardataset
ofinterest. Thisisforexamplerelevantinmanagedservicesoncloudprovidersusedbyclientsin
differentdomainsandregions ormedicaldiagnostictoolstrainedondatacollectedinasmallnumber
ofhospitalsanddeployedonpreviouslyunobservedpopulationsandtimeframes.
Therearevariouswaystoapproachdistributionshiftsbetween CovariateShift LabelShift
asourcedatadistributionPandatargetdatadistributionQ. If p(x)(cid:54)=q(x) p(y)(cid:54)=q(y)
wedenoteinputvariablesasxandoutputvariablesasy  we p(y|x)=q(y|x) p(x|y)=q(x|y)
considerthetwofollowingsettings: (i)Covariateshift which
assumesthattheconditionaloutputdistributionisinvariant: p(y|x)=q(y|x)betweensourceand
targetdistributions butthesourcedistributionp(x)changes. (ii)Labelshift wheretheconditional
input distribution is invariant: p(x|y) = q(x|y) and p(y) changes from source to target. In the
following weassumethatbothinputandoutputvariablesareobservedinthesourcedistribution
whereasonlyinputvariablesareavailablefromthetargetdistribution.
Whilecovariateshifthasbeenthefocusoftheliteratureondistributionshiftstodate  label-shift
scenariosappearinavarietyofpracticalmachinelearningproblemsandwarrantaseparatediscussion
aswell. Inonesetting  suppliersofmachine-learningmodelssuchascloudprovidershavelarge
resourcesofdiversedatasets(sourceset)totrainthemodels whileduringdeployment theyhaveno
controlovertheproportionoflabelcategories.
Inanothersettingofe.g. medicaldiagnostics thediseasedistributionchangesoverlocationsand
time. Considerthetaskofdiagnosingadiseaseinacountrywithbadinfrastructureandlittledata
diagnosethediseaseinthenewtargetlocationinanefﬁcientway? Howmanylabeledsourceand
unlabeledtargetdatasamplesdoweneedtoobtaingoodperformanceonthetargetdata?
Apartfrombeingrelevantinpractice labelshiftisacomputationallymoretractablescenariothan
covariateshiftwhichcanbemitigated. Thereasonisthattheoutputsytypicallyhaveamuchlower
dimensionthantheinputsx. Labelsareusuallyeithercategoricalvariableswithaﬁnitenumber
ofcategoriesorhavesimplewell-deﬁnedstructures. Despitebeinganintuitivelynaturalscenario
literature. Zhangetal.(2013)proposedakernelmeanmatchingmethodforlabelshiftwhichis
notcomputationallyfeasibleforlarge-scaledata. TheapproachinLiptonetal.(2018)isbasedon
importanceweightsthatareestimatedusingtheconfusionmatrix(alsousedintheproceduresof
Saerensetal.(2002);McLachlan(2004))anddemonstratepromisingperformanceonlarge-scale
data. Usingablack-boxclassiﬁerwhichcanbebiased uncalibratedandinaccurate theyﬁrstestimate
importanceweightsq(y)/p(y)forthesourcesamplesandtrainaclassiﬁerontheweighteddata. In
thefollowingwerefertotheprocedureasblackboxshiftlearning(BBSL)whichtheauthorsproved
tobeeffectiveforlargeenoughsamplesizes.
However therearethreerelevantquestionswhichremainunansweredbytheirwork: Howtoestimate
theimportanceweightsinlowsamplesetting Whatarethegeneralizationguaranteesfortheﬁnal
estimationwhenonlyfewsamplesareavailable? Thispaperaimstoﬁllthegapintermsofboth
theoreticalunderstandingandpracticalmethodsforthelabelshiftsettingandtherebymoveastep
closertowardshavingamorecompleteunderstandingonthegeneraltopicofsupervisedlearningfor
distributionallyshifteddata. Inparticular ourgoalistoﬁndanefﬁcientmethodwhichisapplicable
tolarge-scaledataandtoestablishgeneralizationguarantees.
Ourcontributioninthisworkistrifold. Firstly weproposeanefﬁcientweightestimatorforwhich
wecanobtaingoodstatisticalguaranteeswithoutarequirementontheproblem-dependentminimum
sample complexity as necessary for BBSL. In the BBSL case  the estimation error can become
compensateforthehighestimationerroroftheimportanceweightsinlowtargetsamplesettings.
Itexplicitlycontrolstheinﬂuenceofourweightestimateswhenthetargetsamplesizeislow(in
thefollowingreferredtoasthelowsampleregime). Finally wederiveadimension-independent
generalizationboundfortheﬁnalRegularizedLearningunderLabelShift(RLLS)classiﬁerbasedon
ourweightestimator. Inparticular ourmethodimprovestheweightestimationerrorandexcessrisk
oftheclassiﬁeronreweightedsamplesbyafactorofklog(k) wherekisthenumberofclasses i.e.
thecardinalityofY.
Inordertodemonstratethebeneﬁtoftheproposedmethodforpracticalsituations weempirically
studytheperformanceofRLLSandshowweightestimationaswellaspredictionaccuracycomparison
datasets. Forlargetargetsamplesizesandlargeshifts whenapplyingtheregularizedweightsfully
weachieveanorderofmagnitudesmallerweightestimationerrorthanbaselinemethodsandenjoyat
most20%higheraccuracyandF-1scoreincorrespondingpredictivetasks. Forlowtargetsample
sizes applyingregularizedweightspartiallyalsoyieldsanaccuracyimprovementofatleast10%
overfullyweightedandunweightedmethods.
2 REGULARIZED LEARNING OF LABEL SHIFTS (RLLS)
Formally let us the short hand for the marginal probability mass functions of Y on ﬁnite Y with
respecttoP Qasp q : [k] → [0 1]withp(i) = P(Y = i) andq(i) = Q(Y = i)foralli ∈ [k]
representablebyvectorsinRk whichsumtoone. Inthelabelshiftsetting wedeﬁnetheimportance
weightvectorw ∈Rk betweenthesetwodomainsasw(i)= q(i). Wequantifytheshiftusingthe
p(i)
exponentoftheinﬁniteandsecondorderRenyidivergenceasfollows
k
d∞(q||p):=suppq((ii))   and d(q||p):=EY∼Q(cid:2)w(Y)2(cid:3)=(cid:88)q(i)pq((ii)).
i
GivenahypothesisclassHandalossfunction(cid:96):Y ×Y →[0 1] ouraimistoﬁndthehypothesis
h∈Hwhichminimizes
L(h)=EX Y∼Q[(cid:96)(Y h(X))]=EX Y∼P[w(Y)(cid:96)(Y h(X))]
Intheusualﬁnitesamplesettinghowever Lunknownandweobservesamples{(x  y )}n fromP
j j j=1
instead. Ifwearegiventhevectorofimportanceweightswwecouldthenminimizetheempirical
losswithimportanceweightedsamplesdeﬁnedas
1 (cid:88)
L (h)= w(y )(cid:96)(y  h(x ))
n n j j j
j=1
wherenisthenumberofavailableobservationsdrawnfromPusedtolearntheclassiﬁerh. Aswis
unknowninpractice wehavetoﬁndtheminimizeroftheempiricallosswithestimatedimportance
L (h;w)= w(y )(cid:96)(y  h(x )) (1)
n (cid:98) n (cid:98) j j j
wherewareestimatesofw. GivenasetD ofn samplesfromthesourcedistributionP weﬁrst
(cid:98) p p
divideitintotwosetswhereweuse(1−β)n samplesinsetDweight tocomputetheestimatew
p p (cid:98)
andtheremainingn = βn inthesetDclass toﬁndtheclassiﬁerwhichminimizestheloss(1) i.e.
p p
(cid:98)hw(cid:98) = argminh∈HLn(h;w(cid:98)). In the following  we describe how to estimate the weights w(cid:98) and
provideguaranteesfortheresultingestimator(cid:98)hw(cid:98).
Plug-inweightestimation Thefollowingsimplecorrelationbetweenthelabeldistributionsp q
wasnotedinLiptonetal.(2018): foraﬁxedhypothesish ifforally ∈Y itholdsthatq(y)≥0 =⇒
p(y)≥0 wehave
k k
(cid:88) (cid:88)
q (i):=Q(h(X)=i)= Q(h(X)=i|Y =j)q(j)= P(h(X)=i|Y =j)q(j)
h
j=1 j=1
(cid:88) q(j) (cid:88)
= P(h(X)=i Y =j) = P(h(X)=i Y =j)w
p(j) j
foralli j ∈Y. Thiscanequivalentlybewritteninmatrixvectornotationas
q =C w  (2)
h h
where C is the confusion matrix with [C ] = P(h(X) = i Y = j) and q is the vector
h h i j h
which represents the probability mass function of h(X) under distribution Q. The requirement
q(y)≥0 =⇒ p(y)≥0isareasonableconditionsincewithoutanypriorknowledge thereisno
waytoproperlyreasonaboutaclassinthetargetdomainthatisnotrepresentedinthesourcedomain.
In reality  both q and C can only be estimated by the corresponding ﬁnite sample averages
q(cid:98)h C(cid:98)h. Liptonetal.(2018)simplycomputetheinverseoftheestimatedconfusionmatrixC(cid:98)h in
ordertoestimatetheimportanceweight i.e. w(cid:98) = C(cid:98)h−1q(cid:98)h. WhileCh−1q(cid:98)h isastatisticallyefﬁcient
estimator w(cid:98)withestimatedC(cid:98)h−1canbearbitrarilybadsinceC(cid:98)h−1canbearbitraryclosetoasingular
matrixespeciallyforsmallsamplesizesandsmallminimumsingularvalueoftheconfusionmatrix.
Intuitively whenthereareveryfewsamples theweightestimationwillhavehighvarianceinwhich
caseitmightbebettertoavoidimportanceweightingaltogether. Furthermore evenwhenthesample
complexityinLiptonetal.(2018) unknowninpractice ismet theresultingerrorofthisestimatoris
linearinkwhichisproblematicforlargek.
Wethereforeaimtoaddresstheseshortcomingsbyproposingthefollowingtwo-stepprocedureto
computeimportanceweights. Inthecaseofnoshiftwehavew = 1sothatwedeﬁnetheamount
ofweightshiftasθ = w−1. Givena“decent”blackboxestimatorwhichwedenotebyh  we
0
maketheﬁnalclassiﬁerlesssensitivetotheestimationperformanceofC (i.e. regularizetheweight
estimate)by
1. calculatingthemeasurementerroradjustedθ(cid:98)(describedinSection2.1forh0)and
2. computingtheregularizedweightw(cid:98)=1+λθ(cid:98)whereλdependsonthesamplesize(1−β)np.
By 'decent' we refer to a classiﬁer h which yields a full rank confusion matrix C . A trivial
0 h0
example for a non-”decent” classiﬁer h is one that always outputs a ﬁxed class. As it does not
captureanycharacteristicsofthedata thereisnohopetogainanystatisticalinformationwithoutany
priorinformation.
2.1 ESTIMATORCORRECTINGFORFINITESAMPLEERRORS
BoththeconfusionmatrixC andthelabeldistributionq onthetargetfortheblackboxhypothesis
h0 h0
h0 areunknownandweareinsteadonlygivenaccesstoﬁnitesampleestimatesC(cid:98)h0 q(cid:98)h0. Inwhat
followsallempiricalandpopulationconfusionmatrices aswellaslabeldistributions aredeﬁned
withrespecttothehypothesish=h . Fornotationsimplicity wethusdropthesubscripth inwhat
0 0
follows. Thereparameterizedlinearmodel(2)withrespecttoθthenreads
b:=q−C1=Cθ
withcorrespondingﬁnitesamplequantity(cid:98)b=q(cid:98)−C(cid:98)1. WhenC(cid:98)isnearsingular theestimationofθ
becomesunstable. Furthermore largevaluesinthetrueshiftθresultinlargevariances. Weaddress
thisproblembyaddingaregularizing(cid:96) penaltytermtotheusuallossandthuspushtheamountof
shifttowards0 amethodthathasbeenproposedin(Pires&Szepesvári 2012). Inparticular we
θ(cid:98)=argmin (cid:107)C(cid:98)θ−(cid:98)b(cid:107)2+∆C(cid:107)θ(cid:107)2 (3)
θ
Here ∆C isaparameterwhichwilleventuallybehighprobabilityupperboundsfor(cid:107)C(cid:98)−C(cid:107)2. Let
∆balsodenotethehighprobabilityupperboundsfor(cid:107)(cid:98)b−b(cid:107)2.
Lemma1 Forθ(cid:98)asdeﬁnedinequation(3) wehavewithprobabilityatleast1−δthat1
(cid:107)θ(cid:98)−θ(cid:107)2 ≤(cid:15)θ(np nq (cid:107)θ(cid:107)2 δ)
(cid:32) (cid:115) (cid:115) (cid:115) (cid:33)
1 (cid:0) log(k/δ) log(1/δ) log(1/δ)(cid:1)
(cid:15) (n  n  (cid:107)θ(cid:107)  δ):=O (cid:107)θ(cid:107) + + .
θ p q 2 σ 2 (1−β)n (1−β)n n
min p p q
TheproofofthislemmacanbefoundinAppendixB.1. Acoupleofremarksareinorderatthis
point. First of all  notice that the weight estimation procedure (3) does not require a minimum
samplecomplexitywhichisintheorderofσ−2 toobtaintheguaranteesforBBSL.Thisisdueto
min
thefactthaterrorsinthecovariatesareaccountedfor. Inordertodirectlyseetheimprovements
that in order to obtain their upper bound with a probability of at least 1−δ  it is necessary that
3kn−10+2kn−10 ≤ δ. Asaconsequence theupperboundinTheorem3ofLiptonetal.(2018)
p q
(cid:113) (cid:113)
isbiggerthan 1 (cid:0)(cid:107)θ(cid:107) log(3k/δ) + klog(2k/δ)(cid:1). ThusLemma1improvesupontheprevious
3σmin 2 np nq
upperboundbyafactorofk.
Furthermore asinLiptonetal.(2018) thisresultholdsforanyblackboxestimatorh whichenters
theboundviaσ (C ). Wecandirectlyseehowagoodchoiceofh helpstodecreasetheupper
min h0 0
boundinLemma1. Inparticular ifh isanidealestimator andthesourcesetisbalanced C isthe
unitmatrixwithσ =1/k. Incontrast whenthemodelh isuncertain thesingularvalueσ is
min 0 min
closetozero.
variables itisstandardtouseregularizedtotalleastsquaresapproacheswhichrequiresasingular
valuedecomposition. Finally ourchoiceforthealternativeestimatorinEq.3withnorminsteadof
normsquaredregularizationismotivatedbythecaseswithlargeshiftsθ whereusingthesquared
normmayshrinktheestimateθ(cid:98)toomuchandawayfromthetrueθ.
1Throughoutthepaper Ohidesuniversalconstantfactors.Furthermore weuseO(·+·)forshorttodenote
O(·)+O(·).
Algorithm1RegularizedLearningofLabelShift(RLLS)
1: Input: sourcesetDp Dq θmax estimateofσmin blackboxestimatorh0 modelclassH
2: Determineoptimalsplitratioβ(cid:63)andregularizerλ(cid:63)byminimizingtheRHSofEq.(6)usingan
estimateofσ
3: RandomlypartitionsourcesetDpintoDpclass Dpweightsuchthat|Dpclass|=β(cid:63)np =:n
4: Computeθ(cid:98)usingEq.(3)andw(cid:98):=1+λ(cid:63)θ(cid:98)
5: Minimizetheimportanceweightedempiricallosstoobtaintheweightedestimator
(cid:98)hw(cid:98) =arghm∈iHnLn(h;w(cid:98))  where Ln(h;w(cid:98))= n w(cid:98)(y)(cid:96)(y h(x))
(x y)∈Dclass
p
6: Deploy(cid:98)hw(cid:98) iftheriskisacceptable
2.2 REGULARIZEDESTIMATORANDGENERALIZATIONBOUND
Whenafewsamplesfromthetargetsetareavailableorthelabelshiftismild theestimatedweights
mightbetoouncertaintobeapplied. Wethereforeproposearegularizedestimatordeﬁnedasfollows
w(cid:98)=1+λθ(cid:98). (4)
Notethatw(cid:98)implicitlydependsonλ andβ. Byrewritingw(cid:98) = (1−λ)1+λ(1+θ(cid:98)) weseethat
intuitivelyλcloserto1themorereasonthereistobelievethat1+θ(cid:98)isinfactthetrueweight.
DeﬁnethesetG((cid:96) H) = {g (x y) = w(y)(cid:96)(h(x) y) : h ∈ H}anditsRademachercomplexity
(cid:34) (cid:34) n (cid:35)(cid:35)
Rn(G):=E(Xi Yi)∼P:i∈[n] Eξi:i∈[n]n sup ξigh(Xi h(Yi))
h∈H
i=1
withξ   ∀iastheRademacherrandomvariables(seee.g.Bartlett&Mendelson(2002)). Wecannow
stateageneralizationboundfortheclassiﬁer(cid:98)hw(cid:98) inageneralhypothesisclassH whichistrainedon
sourcedatawiththeestimatedweightsdeﬁnedinequation(4).
Theorem1(Generalizationboundforh(cid:98)w(cid:98)) Given np samples from the source data set and nq
samplesfromthetargetset ahypothesisclassHandlossfunction(cid:96) thefollowinggeneralization
boundholdswithprobabilityatleast1−2δ
L((cid:98)hw(cid:98))−L(h∗)≤(cid:15)G(np δ β)+(1−λ)(cid:107)θ(cid:107)2+λ(cid:15)θ(np nq (cid:107)θ(cid:107)2 δ β). (5)
(cid:40) (cid:115) (cid:114) (cid:41)
log(2/δ) 2d (q||p)log(2/δ) d(q||p)log(2/δ)
(cid:15) (n  δ):=2R (G)+min d (q||p)   ∞ + 2 .
G p n ∞ βn n n
TheproofcanbefoundinAppendixB.4.Additionally wederivetheanalysisalsoforﬁnitehypothesis
classesinAppendixB.6toprovidemoreinsightintotheproofofgeneralhypothesisclasses. The
sizeofR (G)isdeterminedbythestructureofthefunctionclassHandtheloss(cid:96). Forexamplefor
the0/1loss theVCdimensionofHcanbedeployedtoupperboundtheRademachercomplexity.
Thebound(5)inTheorem1holdsforallchoicesofλ. Inordertoexploitthepossibilityofchoosing
λandβtohaveanimprovedaccuracydependingonthesamplesizes weﬁrstlettheuserdeﬁneaset
ofshiftsθagainstwhichwewanttoberobustagainst i.e. allshiftswith(cid:107)θ(cid:107) ≤ θ . Forthese
2 max
shifts weobtainthefollowingupperbound
L((cid:98)hw(cid:98))−L(h∗)≤(cid:15)G(np δ)+(1−λ)θmax+λ(cid:15)θ(np nq θmax δ) (6)
Theboundinequation(6)suggestsusingAlgorithm1asourultimatelabelshiftcorrectionprocedure.
where for step 2 of the algorithm  we choose λ(cid:63) = 1 whenever n ≥ 1 (hereby
q θm2ax(σmin−√1np)2
neglecting the log factors and thus dependencies on k) and 0 else. When using this rule  we
obtain L((cid:98)hw(cid:98))−L(h∗) ≤ (cid:15)G(np δ)+min{θmax (cid:15)θ(np nq θmax δ)} which is smaller than the
unregularizedboundforsmalln  n . Noticethatinpractice wedonotknowσ inadvanceso
q p min
thatinAlgorithm1weneedtouseanestimateofσ  whichcoulde.g. betheminimumeigenvalue
oftheempiricalconfusionmatrixC(cid:98)withanadditionalcomputationalcomplexityofatmostO(k3).
Figure1showshowtheoraclethresholdsvarywithn andσ
q min
whennpiskeptﬁx. Whentheparametersareabovethecurvesfor 105
ﬁxedn  λshouldbechosenas1otherwisethesamplesshould
p 104 σmin
beunweighted i.e. λ = 0. Thisﬁgureillustratesthatwhenthe nq
confusionmatrixhassmallsingularvalues theestimatedweights 103
shouldonlybetrustedforratherhighnq andhighbelievedshifts 102
θ . Althoughtheoverallstatisticalrateoftheexcessriskofthe
max 0.2 0.4 0.6 0.8 1.0
classiﬁerdoesnotchangeasafunctionofthesamplesizes θmax Figure 1: Givθmeanx a σ and
couldbesigniﬁcantlysmallerthan(cid:15) whenσ isverysmalland
θ min θ  λswitchesfrom0to1ata
max
thustheaccuracyinthisregimecouldimprove. Indeedweobserve
particularn . n andkareﬁxed.
q p
thistobethecaseempiricallyinSection3.3.
Inthecaseofslightdeviationfromthelabelshiftsetting weexpecttheAlg.1toperformreasonably.
(cid:104)(cid:12) (cid:12)(cid:105)
Forde(q||p):=E(X Y)∼Q (cid:12)(cid:12)1− pq((XX||YY))(cid:12)(cid:12) asthedeviationformlabelshiftconstraint i.e. zerounder
labelshiftassumption wehave
Theorem2(DriftinLabelshiftassumption) Inthepresenceofd (q||p)deviationfromlabelshift
e
assumption thetrueimportanceweightsω(x y):= q(x y) theRLLSgeneralizesas;
p(x y)
L((cid:98)hw(cid:98) ω)−L(h∗;ω)≤(cid:15)G(np δ)+(1−λ)(cid:107)θ(cid:107)2+λ(cid:15)θ(np nq (cid:107)θ(cid:107)2 δ)+4(1−λ)de(q||p)
withhighprobability. ProofinAppendixB.7.
generatedshiftsontheMNIST(LeCun&Cortes 2010)andCIFAR10 (Krizhevsky&Hinton 2009)
datasets. Weﬁrstrandomlyseparatetheentiredatasetintotwosets(sourceandtargetpool)ofthe
samesize. Thenwesample unlessspeciﬁedotherwise thesamenumberofdatapointsfromeach
pooltoformthesourceandtargetsetrespectively. Wechosetohaveequalsamplesizestoallowfor
faircomparisonsacrossshifts.
Therearevariouskindsofshiftswhichweconsiderinourexperiments. Ingeneralweassumeoneof
thesourceortargetdatasetstohaveuniformdistributionoverthelabels. Withinthenon-uniformset
weconsiderthreetypesofsamplingstrategiesinthemaintext: theTweak-Oneshiftreferstothecase
wherewesetaclasstohaveprobabilityp>0.1 whilethedistributionovertherestoftheclasses
isuniform. The Minority-ClassShiftisamoregeneralversionofTweak-Oneshift whereaﬁxed
numberofclassesmtohaveprobabilityp<0.1 whilethedistributionovertherestoftheclassesis
uniform. FortheDirichletshift wedrawaprobabilityvectorpfromtheDirichletdistributionwith
concentrationparametersettoαforallclasses beforeincludingsamplepointswhichcorrespondto
themultinomiallabelvariableaccordingtop. Resultsforthetweak-oneshiftstrategyasinLipton
etal.(2018)canbefoundinSectionA.0.1.
Afterartiﬁciallyshiftingthelabeldistributioninoneofthesourceandtargetsets wethenfollow
algorithm1 wherewechoosetheblackboxpredictorh tobeatwo-layerfullyconnectedneural
networktrainedon(shifted)sourcedataset. Notethatanyblackboxpredictorcouldbeemployed
here thoughthehighertheaccuracy themorelikelyweightestimationwillbeprecise. Therefore
weusedifferentshiftedsourcedatatoget(corrupted)blackboxpredictoracrossexperiments. Ifnot
noted h istrainedusinguniformdata.
Inordertocomputeω(cid:98) =1+θ(cid:98)inEq.(3) wecallabuilt-insolvertodirectlysolvethelowdimensional
problemminθ(cid:107)C(cid:98)θ−(cid:98)b(cid:107)2+∆C(cid:107)θ(cid:107)2whereweempiricallyobserverthat0.01timesofthetrue∆C
notingthat0.001makesthetheoreticalboundinLemma.1O(1/0.01)timesbigger. Wethustreatit
asahyperparameterthatcanbechosenusingstandardcrossvalidationmethods. Finally wetrain
aclassiﬁeronthesourcesamplesweightedbyω whereweuseatwo-layerfullyconnectedneural
(cid:98)
networkforMNISTandaResNet-18(Heetal. 2016)forCIFAR10.
Wesample20datasetswiththelabeldistributionsforeachshiftparameter. toevaluatetheempirical
mean square estimation error (MSE) and variance of the estimated weights E(cid:107)w−w(cid:107)2 and the
(cid:98) 2
blackboxshiftlearningmethod(BBSL)inLiptonetal.(2018). NoticethatalthoughKMMmethods
(Zhangetal. 2013)wouldbeanotherstandardbaselinetocomparewith itisnotscalabletolarge
samplesizeregimesforn  n aboven=8000asmentionedbyLiptonetal.(2018).
3.1 WEIGHTESTIMATIONANDPREDICTIVEPERFORMANCEFORSOURCESHIFT
InthissetofexperimentsontheCIFAR10dataset weillustrateourweightestimationandprediction
performanceforTweak-OnesourceshiftsandcompareitwithBBSL.Forthissetofexperiments we
setthenumberofdatapointsinbothsourceandtargetsetto10000andsamplefromthetwopools
withoutreplacement.
Figure2illustratestheweightestimationalongsideﬁnalclassiﬁcationperformanceforMinority-Class
sourceshiftofCIFAR10. Wecreatedshiftswithρ>0.5. Weuseaﬁxedblack-boxclassiﬁerthatis
trainedonbiasedsourcedata withtweak-oneρ=0.5. ObservethattheMSEinweightestimationis
relativelylargeandRLLSoutperformsBBSLasthenumberofminorityclassesincreases. Asthe
shiftincreasestheperformanceforallmethodsdeteriorates. Furthermore Figure2(b)illustrates
howtheadvantageofRLLSovertheunweightedclassiﬁerincreasesastheshiftincreases. Across
allshifts theRLLSbasedclassiﬁeryieldshigheraccuracythantheonebasedonBBSL.Resultsfor
MNISTcanbefoundinSectionA.1.
(a) (b)
Figure2: (a)Meansquarederrorinestimatedweightsand(b)accuracyonCIFAR10fortweak-one
shiftedsourceanduniformtargetwithh trainedusingtweak-oneshiftedsourcedata.
3.2 WEIGHTESTIMATIONANDPREDICTIVEPERFORMANCEFORTARGETSHIFT
Inthissection wecomparethepredictiveperformancesbetweenaclassiﬁertrainedonunweighted
sourcedataandtheclassiﬁerstrainedonweightedlossobtainedbytheRLLSandBBSLprocedure
onCIFAR10. ThetargetsetisshiftedusingtheDirichletshiftwithparametersα=[0.01 0.1 1 10].
Thenumberofdatapointsinbothsourceandtargetsetis10000.
Inthecaseoftargetshifts  largershiftsactuallymakethepredictivetaskeasier  suchthatevena
constantmajorityclassvotewouldgivehighaccuracy. Howeveritwouldhavezeroaccuracyon
themethods  wealsocomputethemacro-averagedF-1scorebyaveragingtheper-classquantity
2(precision·recall)/(precision+recall)overallclasses. Foraclassi precisionisthepercentage
ofcorrectpredictionsamongallsamplespredictedtohavelabeli whilerecallistheproportionof
correctlypredictedlabelsoverthenumberofsampleswithtruelabeli. Thismeasuregiveshigher
weighttotheaccuraciesofminorityclasseswhichhavenoeffectonthetotalaccuracy.
Figure3depictstheMSEoftheweightestimation(a) thecorrespondingperformancecomparisonon
accuracy(b)andF-1score(c). Recallthattheaccuracyperformanceforlowshiftsisnotcomparable
withstandardCIFAR10benchmarkresultsbecauseoftheoveralllowersamplesizechosenforthe
comparabilitybetweenshifts. Wecanseethatinthelargetargetshiftcaseforα = 0.01 theF-1
(a) (b) (c)
Figure3: (a)Meansquarederrorinestimatedweights (b)accuracyand(c)F-1scoreonCIFAR10
foruniformsourceandDirichletshiftedtarget. Smallerαcorrespondstobiggershift.
scoreforBBSLandtheunweightedclassiﬁerisratherlowcomparedtoRLLSwhiletheaccuracyis
high. Asmentionedbefore thereasonforthisobservationandwhyinFigure3(b)theaccuracyis
higherwhentheshiftislarger isthatthepredictivetaskactuallybecomeseasierwithhighershift.
3.3 REGULARIZEDWEIGHTSINTHELOWSAMPLEREGIMEFORSOURCESHIFT
Inthefollowing wepresenttheaverageaccuracyofRLLSinFigure4asafunctionofthenumberof
targetsamplesn fordifferentvaluesofλforsmalln . Hereweﬁxthesamplesizeinthesourceset
q q
ton = 1000andinvestigateaMinority-Classsourceshiftwithﬁxedp = 0.01andﬁveminority
AmotivationtouseintermediateλisdiscussedinSection2.2 asλinequation(4)maybechosen
accordingtoθ  σ . Inpractice sinceθ isjustanupperboundonthetrueamountofshift
max min max
(cid:107)θ(cid:107)  insomecasesλshouldinfactideallybe0when 1 ≤n ≤ 1 .
2 θm2ax(σmin−√1nq)2 q (cid:107)θ(cid:107)2(σmin−√1nq)2
Thusfortargetsamplesizesn thatarealittlebitabovethethreshold(dependingonthecertaintyof
q
thebeliefhowclosetoθ thenormoftheshiftisbelievedtobe) itcouldbesensibletousean
intermediatevalueλ∈(0 1).
Figure4: PerformanceonMNISTforMinority-Classshiftedsourceanduniformtargetwithvarious
target sample size and λ using (a) better predictor h trained on tweak-one shifted source with
ρ=0.2 (b)neutralpredictorh withρ=0.5and(c)corruptedpredictorh withρ=0.8.
Figure4suggeststhatunweightedsamples(red)yieldthebestclassiﬁerforveryfewsamplesn
while for 10 ≤ n ≤ 500 an intermediate λ ∈ (0 1) (purple) has the highest accuracy and for
n >1000 theweightestimationiscertainenoughforthefullyweightedclassiﬁer(yellow)tohave
thebestperformance(seealsothecorrespondingdatapointsinFigure2). TheunweightedBBSL
classiﬁerisalsoshownforcompleteness. Wecanconcludethatregularizingtheinﬂuenceofthe
estimatedweightsallowsustoadjusttotheuncertaintyonimportanceweightsandgeneralizewell
forawiderangeoftargetsamplesizes.
Furthermore  the different plots in Figure 4 correspond to black-box predictors h for weight
estimationwhicharetrainedonmoreorlesscorrupteddata i.e. haveabetterorworseconditioned
confusionmatrix. Thefullyweightedmethodswithλ=1achievethebestperformancefasterwitha
bettertrainedblack-boxclassiﬁer(a) whileittakeslongerforittoimprovewithacorruptedone(c).
Furthermore thisreﬂectstherelationbetweeneigenvalueofconfusionmatrixσ andtargetsample
sizen inTheorem1. Inotherwords weneedmoresamplesfromthetargetdatatocompensatea
badpredictorinweightestimation. Sothegeneralizationerrordecreasesfasterwithanincreasing
numberofsamplesforgoodpredictors.
Insummary our RLLS methodoutperforms BBSL inallsettingsforthecommonimagedatasets
MNISTandCIFAR10tovaryingdegrees. Ingeneral signiﬁcantimprovementscomparedtoBBSL
canbeobservedforlargeshiftsandthelowsampleregime. Anoteofcautionisinorder: comparison
betweenthetwomethodsalonemightnotalwaysbemeaningful. Inparticular therearecaseswhen
the estimator trained on unweighted samples outperforms both RLLS and BBSL. Our extensive
experimentsformanydifferentshifts blackboxclassiﬁersandsamplesizesdonotallowforaﬁnal
conclusivestatementabouthowweightingsamplesusingourestimatoraffectspredictiveresultsfor
real-worlddataingeneral asitusuallydoesnotfulﬁllthelabel-shiftassumptions.
4 RELATED WORK
Thecovariateandlabelshiftassumptionsfollownaturallywhenviewingthedatageneratingprocess
asacausaloranti-causalmodel(Schölkopfetal. 2012): Withlabelshift thelabelY causesthe
inputX (thatis X isnotacausalparentofY hence'anti-causal')andthecausalmechanismthat
generatesX fromY isindependentofthedistributionofY. Alonglineofworkhasaddressedthe
reversecausalsettingwhereX causesY andtheconditionaldistributionofY givenX isassumedto
beconstant. Thisassumptionissensiblewhenthereisreasontobelievethatthereisatrueoptimal
mappingfromX toY whichdoesnotchangeifthedistributionofX changes. Mathematicallythis
scenariocorrespondstothecovariateshiftassumption.
Amongthevariousmethodstocorrectforcovariateshift themajorityusestheconceptofimportance
weightsq(x)/p(x)(Zadrozny 2004;Cortesetal. 2010;Cortes&Mohri 2014;Shimodaira 2000)
whichareunknownbutcanbeestimatedforexampleviakernelembeddings(Huangetal. 2007;
Grettonetal. 2009;2012;Zhangetal. 2013;Zarembaetal. 2013)orbylearningabinarydiscrimi-
nativeclassiﬁerbetweensourceandtarget(Lopez-Paz&Oquab 2016;Liuetal. 2017). Aminimax
sourceandtargethasalsobeeninvestigated(Liu&Ziebart 2014;Chenetal. 2016). Sanderson
classconditionalcovariatedistributionswithunknownmixtureweights. Underthepairwisemutual
irreducibility(Scottetal. 2013)assumptionontheclassconditionalcovariatedistributions they
deploytheNeyman-Pearsoncriterion(Blanchardetal. 2010)toestimatetheclassdistributionq(y)
whichalsoinvestigatedinthemaximummeandiscrepancyframework(Iyeretal. 2014).
Commonissuessharedbythesemethodsisthattheyeitherresultinamassivecomputationalburden
forlargesamplesizeproblemsorcannotbedeployedforneuralnetworks. Furthermore importance
weightingmethodssuchas(Shimodaira 2000)estimatethedensity(ratio)beforehand whichisa
difﬁculttaskonitsown whenthedataishigh-dimensional. Theresultinggeneralizationbounds
(q(x)/p(x))2 tobebounded whichmeanstheboundsareextremelylooseinmostcases (Cortes
etal. 2010).
Despitethewideapplicabilityoflabelshift approacheswithglobalguaranteesinhighdimensional
dataregimesremainunder-explored. Thecorrectionoflabelshiftmainlyrequirestoestimatethe
importanceweightsq(y)/p(y)overthelabelswhichtypicallyliveinaverylow-dimensionalspace.
Bayesianandprobabilisticapproachesarestudiedwhenaprioroverthemarginallabeldistributionis
assumed(Storkey 2009;Chan&Ng 2005). Thesemethodsoftenneedtoexplicitlycomputethe
posteriordistributionofyandsufferfromthecurseofdimensionality. RecentadvancesasinLipton
etal.(2018)haveproposedsolutionsapplicablelargescaledata. ThisapproachisrelatedtoBuck
etal.(1966);Forman(2008);Saerensetal.(2002)inthelowdimensionalsettingbutlacksguarantees
fortheexcessrisk.
ExistinggeneralizationboundshavehistoricallybeenmainlydevelopedforthecasewhenP=Q
(seee.g. Vapnik(1999);Bartlett&Mendelson(2002);Kakadeetal.(2009);Wainwright(2019)).
Ben-Davidetal.(2010)providestheoreticalanalysisandgeneralizationguaranteesfordistribution
shiftswhentheH-divergencebetweenjointdistributionsisconsidered whereasCrammeretal.(2008)
Cortesetal.(2010)providesageneralizationboundwhenq(x)/p(x)isknownwhichhoweverdoes
notapplyinpractice. Tothebestofourknowledgeourworkistheﬁrsttogivegeneralizationbounds
forthelabelshiftscenario.
5 DISCUSSION
Inthiswork weestablishtheﬁrstgeneralizationguaranteeforthelabelshiftsettingandproposean
importanceweightingprocedureforwhichnopriorknowledgeofq(y)/p(y)isrequired. Although
RLLSisinspiredbyBBSL itleadstoamorerobustimportanceweightestimatoraswellasgeneral-
izationguaranteesinparticularforthesmallsampleregime whichBBSLdoesnotallowfor. RLLS
isalsoequippedwithasample-size-dependentregularizationtechniqueandfurtherimprovesthe
classiﬁerinbothregimes.
Weconsiderthisworkanecessarystepinthedirectionofsolvingshiftsofthistype althoughthe
labelshiftassumptionitselfmightbetoosimpliﬁedintherealworld. Infuturework weplantoalso
studythesettingwhenitisslightlyviolated. Forinstance xinpracticecannotbesolelyexplained
bythewantedlabely butmayalsodependonattributesz whichmightnotbeobservable. Inthe
diseasepredictiontaskforexample thesymptomsmightnotonlydependonthediseasebutalsoon
thecityandlivingconditionsofitspopulation. Insuchacase thelabelshiftassumptiononlyholds
inaslightlymodiﬁedsense i.e. P(X|Y =y Z =z)=Q(X|Y =y Z =z). IftheattributesZ are
observed thenourframeworkcanreadilybeusedtoperformimportanceweighting.
Furthermore  itisnot clearwhetherthe ﬁnalpredictoris infact“better” ormorerobusttoshifts
justbecauseitachievesabettertargetaccuracythanavanillaunweightedestimator. Infact there
isareasontobelievethatundercertainshiftscenarios  thepredictormightlearntousespurious
correlationstoboostaccuracy. Findingaprocedurewhichcanbothlearnarobustmodelandachieve
highaccuraciesonnewtargetsetsremainstobeanongoingchallenge. Moreover thecurrentchoice
ofregularizationdependsonthenumberofsamplesratherthandata-drivenregularizationwhichis
moredesirable.
wealsohaveanexpertfordiagnosingalimitednumberofpatientsinthetargetlocation. Nowthe
patientsforfurthermedicaldiagnosisortreatment uptosomevaryingcost. Weplantoextendthe
(Beygelzimeretal. 2009)aswellasthecost-sensitivesettingwherewealsoconsiderthecostof
queryinglabels(Krishnamurthyetal. 2017).
Considerarealizableandover-parameterizedsetting wherethereexistsadeterministicmapping
from x to y  and also suppose a perfect interpolation of the source data with a minimum proper
normisdesired. Inthiscase weightingthesamplesintheempiricallossmightnotalterthetrained
classiﬁer(Belkinetal. 2018). Therefore ourresultsmightnotdirectlyhelpthedesignofbetter
classiﬁersinthisparticularregime. However forthegeneraloverparameterizedsettings itremains
anopenproblemofhowtheimportanceweightingcanimprovethegeneralization. Weleavethis
studyforfuturework.
6 ACKNOWLEDGEMENT
K.AzizzadenesheliissupportedinpartbyNSFCareerAwardCCF-1254106. Thisresearchhasbeen
conductedwhentheﬁrstauthorwasavisitingresearcheratCaltech. AnqiLiuissupportedinpart
byDOLCITPostdoctoralFellowshipatCaltechandCaltech’sCenterforAutonomousSystemsand
Technologies. FanYangissupportedbytheInstituteforTheoreticalStudiesETHZurichandtheDr.
MaxRösslerandtheWalterHaefnerFoundation. A.AnandkumarissupportedinpartbyMicrosoft
FacultyFellowship Googlefacultyaward Adobegrant NSFCareerAwardCCF-1254106 and
AFOSRYIPFA9550-15-1-0221.
AnimashreeAnandkumar DanielHsu andShamMKakade. Amethodofmomentsformixture
modelsandhiddenmarkovmodels. InConferenceonLearningTheory pp.33–1 2012.
KamyarAzizzadenesheli AlessandroLazaric andAnimashreeAnandkumar.Reinforcementlearning
ofpomdpsusingspectralmethods. arXivpreprintarXiv:1602.07764 2016.
PeterLBartlettandShaharMendelson. Rademacherandgaussiancomplexities: Riskboundsand
structuralresults. JournalofMachineLearningResearch 3(Nov):463–482 2002.
MikhailBelkin DanielHsu SiyuanMa andSoumikMandal. Reconcilingmodernmachinelearning
andthebias-variancetrade-off. arXivpreprintarXiv:1812.11118 2018.
ShaiBen-David JohnBlitzer KobyCrammer AlexKulesza FernandoPereira andJenniferWortman
Vaughan. Atheoryoflearningfromdifferentdomains. Machinelearning 79(1-2):151–175 2010.
AlinaBeygelzimer SanjoyDasgupta andJohnLangford. Importanceweightedactivelearning. In
Proceedingsofthe26thannualinternationalconferenceonmachinelearning pp.49–56.ACM
GillesBlanchard GyeminLee andClaytonScott. Semi-supervisednoveltydetection. Journalof
MachineLearningResearch 11(Nov):2973–3009 2010.
AABuck JJGart etal. Comparisonofascreeningtestandareferencetestinepidemiologicstudies.
ii.aprobabilisticmodelforthecomparisonofdiagnostictests. AmericanJournalofEpidemiology
83(3):593–602 1966.
YeeSengChanandHweeTouNg. Wordsensedisambiguationwithdistributionestimation. InIJCAI
volume5 pp.1010–5 2005.
XiangliChen MathewMonfort AnqiLiu andBrianDZiebart. Robustcovariateshiftregression. In
ArtiﬁcialIntelligenceandStatistics pp.1270–1279 2016.
algorithmforregression. TheoreticalComputerScience 519:103–126 2014.
CorinnaCortes YishayMansour andMehryarMohri. Learningboundsforimportanceweighting.
InAdvancesinneuralinformationprocessingsystems pp.442–450 2010.
KobyCrammer MichaelKearns andJenniferWortman. Learningfrommultiplesources. Journalof
MachineLearningResearch 9(Aug):1757–1774 2008.
Discovery 17(2):164–206 2008.
DavidAFreedman. Ontailprobabilitiesformartingales. theAnnalsofProbability pp.100–118
ArthurGretton AlexanderJSmola JiayuanHuang MarcelSchmittfull KarstenMBorgwardt and
BernhardSchölkopf. Covariateshiftbykernelmeanmatching. 2009.
ArthurGretton KarstenMBorgwardt MalteJRasch BernhardSchölkopf andAlexanderSmola. A
kerneltwo-sampletest. JournalofMachineLearningResearch 13(Mar):723–773 2012.
recognition. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition
pp.770–778 2016.
DanielHsu ShamMKakade andTongZhang. Aspectralalgorithmforlearninghiddenmarkov
models. JournalofComputerandSystemSciences 78(5):1460–1480 2012.
Correctingsampleselectionbiasbyunlabeleddata. InAdvancesinneuralinformationprocessing
systems pp.601–608 2007.
ArunIyer SakethaNath andSunitaSarawagi. Maximummeandiscrepancyforclassratioestimation:
Convergenceboundsandkernelselection. InInternationalConferenceonMachineLearning pp.
530–538 2014.
ShamMKakade KarthikSridharan andAmbujTewari. Onthecomplexityoflinearprediction: Risk
bounds marginbounds andregularization. InAdvancesinneuralinformationprocessingsystems
pp.793–800 2009.
AkshayKrishnamurthy AlekhAgarwal Tzu-KuoHuang HalDaumeIII andJohnLangford. Active
learningforcost-sensitiveclassiﬁcation. arXivpreprintarXiv:1703.01014 2017.
Technicalreport Citeseer 2009.
YannLeCunandCorinnaCortes. MNISThandwrittendigitdatabase. 2010. URLhttp://yann.
lecun.com/exdb/mnist/.
ZacharyCLipton Yu-XiangWang andAlexSmola. Detectingandcorrectingforlabelshiftwith
blackboxpredictors. arXivpreprintarXiv:1802.03916 2018.
AnqiLiuandBrianZiebart. Robustclassiﬁcationundersampleselectionbias. InAdvancesinneural
informationprocessingsystems pp.37–45 2014.
SongLiu AkikoTakeda TaijiSuzuki andKenjiFukumizu. Trimmeddensityratioestimation. In
AdvancesinNeuralInformationProcessingSystems pp.4518–4528 2017.
arXiv:1610.06545 2016.
GeoffreyMcLachlan. Discriminantanalysisandstatisticalpatternrecognition volume544. John
Wiley&Sons 2004.
BernardoAvilaPiresandCsabaSzepesvári. Statisticallinearestimationwithpenalizedestimators:
anapplicationtoreinforcementlearning. arXivpreprintarXiv:1206.6444 2012.
HarishRamaswamy ClaytonScott andAmbujTewari. Mixtureproportionestimationviakernel
embeddingsofdistributions. InInternationalConferenceonMachineLearning pp.2052–2060
MarcoSaerens PatriceLatinne andChristineDecaestecker. Adjustingtheoutputsofaclassiﬁerto
newaprioriprobabilities: asimpleprocedure. Neuralcomputation 14(1):21–41 2002.
anomalyrejection. InArtiﬁcialIntelligenceandStatistics pp.850–858 2014.
BernhardSchölkopf DominikJanzing JonasPeters EleniSgouritsa KunZhang andJorisMooij.
Oncausalandanticausallearning. arXivpreprintarXiv:1206.6471 2012.
ClaytonScott GillesBlanchard andGregoryHandy. Classiﬁcationwithasymmetriclabelnoise:
Consistencyandmaximaldenoising. InConferenceOnLearningTheory pp.489–511 2013.
HidetoshiShimodaira. Improvingpredictiveinferenceundercovariateshiftbyweightingthelog-
likelihoodfunction. Journalofstatisticalplanningandinference 90(2):227–244 2000.
AmosStorkey. Whentrainingandtestsetsaredifferent: characterizinglearningtransfer. Dataset
shiftinmachinelearning pp.3–28 2009.
JoelATropp. User-friendlytailboundsforsumsofrandommatrices. Foundationsofcomputational
mathematics 12(4):389–434 2012.
VladimirNaumovichVapnik. Anoverviewofstatisticallearningtheory. IEEEtransactionsonneural
networks 10(5):988–999 1999.
M.J.Wainwright. High-dimensionalstatistics: Anon-asymptoticviewpoint. CambridgeUniversity
Press 2019.
YimingYing. Mcdiarmid’sinequalitiesofbernsteinandbennettforms. CityUniversityofHong
Kong 2004.
BiancaZadrozny. Learningandevaluatingclassiﬁersundersampleselectionbias. InProceedingsof
thetwenty-ﬁrstinternationalconferenceonMachinelearning pp.114.ACM 2004.
WojciechZaremba ArthurGretton andMatthewBlaschko. B-test: Anon-parametric lowvariance
kerneltwo-sampletest. InAdvancesinneuralinformationprocessingsystems pp.755–763 2013.
KunZhang BernhardSchölkopf KrikamolMuandet andZhikunWang. Domainadaptationunder
targetandconditionalshift. InInternationalConferenceonMachineLearning pp.819–827 2013.
A MORE EXPERIMENTAL RESULTS
advantageofusingRLLSvs. BBSLaremoreorlesspronounced.
A.0.1 CIFAR10EXPERIMENTSUNDERTWEAK-ONESHIFTANDDIRICHLETSHIFT
HerewecompareweightestimationperformancebetweenRLLSandBBSLfordifferenttypesof
shiftsincludingtheTweak-oneShift forwhichwerandomlychooseoneclass e.g. iandsetp(i)=ρ
whileallotherclassesaredistributedevenly. Figure5depictsthetheweightestimationperformance
ofRLLScomparedtoBBSLforavarietyofvaluesofρandα. Notethatlargershiftscorrespondto
smallerαandlargerρ. Ingeneral oneobservesthatourRLLSestimatorhassmallerMSEandthat
astheshiftincreases theerrorofbothmethodsincreases. Fortweak-oneshiftwecanadditionally
seethatastheshiftincreases RLLSoutperformsBBSLmoreandmoreasbothintermsofbiasand
tweak-oneshiftonsourceanduniformtarget and(b)Dirichletshiftonsourceanduniformtarget. h
istrainedusingthesamesourceshifteddatarespectively.
A.1 MNISTEXPERIMENTSUNDERMINORITY-CLASSSOURCESHIFTSFORDIFFERENT
VALUESOFp
shifts weincludeseveraladditionalsetsofexperimentshereintheappendix. Figure6showsthe
weightestimationerrorandaccuracycomparisonunderaminority-classshiftwithp=0.001. The
trainingandtestingsamplesizeis10000examplesinthiscase. Wecanseethatwhenevertheweight
estimationofRLLSisbetter theaccuracyisalsobetter exceptinthefourclassescasewhenboth
methodsarebadinweightestimation.
Figure6: (a)Meansquarederrorinestimatedweightsand(b)accuracyonMNISTforminority-class
shiftedsourceanduniformtargetwithp=0.001.
Figure7demonstratesanothercaseinminority-classshiftwhenp=0.01. Theblack-boxclassiﬁeris
thesametwo-layersneuralnetworktrainedonabiasedsourcedatasetwithtweak-oneρ=0.5. We
observethatwhenthenumberofminorityclassissmalllike1or2 theweightestimationissimilar
betweentwomethods aswellasintheclassiﬁcationaccuracy. Butwhentheshiftgetlarger the
weightsareworseandtheperformanceinaccuracydecreases gettingevenworsethantheunweighted
Figure7: (a)Meansquarederrorinestimatedweightsand(b)accuracyonMNISTforminority-class
shiftedsourceanduniformtargetwithp=0.01 withh trainedontweak-oneshiftedsourcedata.
Figure8illustratestheweightestimationalongsideﬁnalclassiﬁcationperformanceforMinority-Class
sourceshiftofMNIST.Weuse1000trainingandtestingdata. Wecreatedlargeshiftsofthreeor
moreminorityclasseswithp=0.005. Weuseaﬁxedblack-boxclassiﬁerthatistrainedonbiased
sourcedata withtweak-oneρ=0.5. ObservethattheMSEinweightestimationisrelativelylarge
andRLLSoutperformsBBSLasthenumberofminorityclassesincreases. Astheshiftincreasesthe
performanceforallmethodsdeteriorates. Furthermore Figure8(b)illustrateshowtheadvantage
ofRLLSovertheunweightedclassiﬁerincreasesastheshiftincreases. Acrossallshifts theRLLS
basedclassiﬁeryieldshigheraccuracythantheonebasedonBBSL.
Figure8: (a)Meansquarederrorinestimatedweightsand(b)accuracyonMNISTforminority-class
shiftedsourceanduniformtargetwithp=0.005 withh trainedontweak-oneshiftedsourcedata.
A.2 CIFAR10EXPERIMENTUNDERDIRICHLETSOURCESHIFTS
Figure 9illustrates theweight estimationalongside ﬁnal classiﬁcation performance forDirichlet
sourceshiftofCIFAR10dataset. Weuse10000trainingandtestingdatainthisexperiment following
the way we generate shift on source data. We train h with tweak-one shifted source data with
ρ=0.5. Theresultsshowthatimportanceweightingingeneralisnothelpingtheclassiﬁcationin
thisrelativelylargeshiftcase becausetheweightedmethods includingtrueweightsandestimated
weights aresimilarinaccuracywiththeunweightedmethod.
A.3 MNISTEXPERIMENTUNDERDIRICHLETSHIFTWITHLOWTARGETSAMPLESIZE
WeshowtheperformanceofclassiﬁerwithdifferentregularizationλunderaDirichletshiftwith
α=0.5inFigure10. Thetraininghas5000examplesinthiscase. Wecanseethatinthislowtarget
samplecase λ=1onlytakeoverafterseveralhundredsexample whilesomeλvaluebetween0
and1outperformsitatthebeginning. Similarasinthepaper weusedifferentblack-boxclassiﬁer
thatiscorruptedindifferentlevelstoshowtherelationbetweenthequalityofblack-boxpredictor
andthenecessarytargetsamplesize. Weusebiasedsourcedatawithtweak-oneρ=0 0.2 0.6to
traintheblack-boxclassiﬁer. Weseethatweneedmoretargetsamplesforthefullyweightedversion
λ=1totakeoverforamorecorruptedblack-boxclassiﬁer.
Figure9: (a)Meansquarederrorinestimatedweightsand(b)accuracyonCIFAR10forDirichlet
shiftedsourceanduniformtarget withh trainedontweak-oneshiftedsourcedata.
Figure10: PerformanceonMNISTforDirichletshiftedsourceanduniformtargetwithvarioustarget
samplesizeandλusing(a)betterpredictor (b)neutralpredictorand(c)corruptedpredictor.
B PROOFS
B.1 PROOFOFLEMMA1
FromThm.3.4in(Pires&Szepesvári 2012)weknowthatforθ(cid:98)asdeﬁnedinequation(3) ifwith
probabilityatleast1−δ (cid:107)C(cid:98)−C(cid:107)2 ≤∆C and(cid:107)(cid:98)b−b(cid:107)2 ≤∆bholdsimultaneously then
Υ(θ(cid:98))≤ inf {Υ(θ(cid:48))+2∆C(cid:107)θ(cid:48)(cid:107)α}+2∆b. (7)
θ(cid:48)∈Rk
whereweusetheshorthandΥ(θ(cid:48))=(cid:107)Cθ(cid:48)−b(cid:107) .
Wecangetanupperboundontherighthandsideof(7)istheinﬁmumbysimplychoosingafeasible
θ(cid:48) =θ. Wethenhave(cid:107)Cθ−b(cid:107) =0andhence
inf{Υ(θ(cid:48))+2∆ (cid:107)θ(cid:48)(cid:107) }≤2∆ (cid:107)θ(cid:107)
C 2 C 2
θ(cid:48)
asaconsequence
(cid:16) (cid:17)
Υ(θ(cid:98))=(cid:107)Cθ(cid:98)−b(cid:107)2 =(cid:107)C θ(cid:98)−θ (cid:107)2 ≤2∆C(cid:107)θ(cid:107)2+2∆b
Since(cid:107)C θ(cid:98)−θ (cid:107)2 ≥σmin(C)(cid:107)θ(cid:98)−θ(cid:107)2bydeﬁnitionoftheminimumsingularvalue wethushave
(cid:107)θ(cid:98)−θ(cid:107)2 ≤ σ (2∆C(cid:107)θ(cid:107)2+2∆b)
Letusﬁrstnoticethat
b =q −C1=q −p
h h h h
ThemathematicaldeﬁnitionoftheﬁnitesampleestimatesC(cid:98)h (cid:98)bh(inmatrixandvectorrepresentation)
withrespecttosomehypothesishareasfollows
[C(cid:98)h]ij = (1−β)n Ih(x)=i y=j
(x y)∈Dp
1 (cid:88) 1 (cid:88)
[(cid:98)bh](i)= m Ih(x)=i− (1−β)n Ih(x)=i
(x y)∈Dq (x y)∈Dpweight
where m = |D | and I is the indicator function. C  b can equivalently be expressed with the
q h h
populationoverPforC andoverQforb respectively. Wenowusethefollowingconcentration
LemmastoboundtheestimationerrorsofC(cid:98) (cid:98)bwherewedropthesubscripthforeaseofnotation.
Lemma2(ConcentrationofmeasurementmatrixC(cid:98)) ForﬁnitesampleestimateC(cid:98)wehave
(cid:115)
2log(2k/δ) 2log(2k/δ)
(cid:107)C(cid:98)−C(cid:107)2 ≤ 3(1−β)n + (1−β)n
withprobabilityatleast(1−δ).
Lemma3(Concentrationoflabelmeasurements) Fortheﬁnitesampleestimate(cid:98)bwithrespectto
anyhypothesishitholdsthat
(cid:32) (cid:112) (cid:112) (cid:33)
2 log(1/δ) log(1/δ)
(cid:107)(cid:98)bh−bh(cid:107)2 ≤ (cid:112)log(2) (cid:112)(1−β)np + √nq
withprobabilityatleast1−2δ.
By Lemma. 2 for concentration of C and Lemma. 3 for concentration of b we now have with
probabilityatleast1−δ
(cid:32) (cid:115)
1 2(cid:107)θ(cid:107) log(2k/δ) 18log(4k/δ)
(cid:107)θ(cid:98)−θ(cid:107)2 ≤ σ (12−β)n +(cid:107)θ(cid:107)2 (1−β)n
min p p
(cid:115) (cid:115) (cid:33)
36log(2/δ) 36log(2/δ)
+ + .
n (1−β)n
which consideringthatO(√1 )dominatesO(1) yieldsthestatementoftheLemma1.
n n
B.2 PROOFOFLEMMA2
Weprovethislemmausingthetheorem1.4[MatrixBernstein]andDilationstechniquefromTropp
(2012). We can rewrite Ch = E(x y)∼P(cid:2)eh(x)e(cid:62)y(cid:3) where e(i) is the one-hot-encoding of index i.
Consideraﬁnitesequence{Ψ(i)}ofindependentrandommatriceswithdimensionk. Bydilations
letsconstructanothersequenceofself-adjointrandommatricesof{Ψ(cid:101)(i)}ofdimension2k suchthat
foralli
(cid:20) (cid:21)
0 Ψ(i)
Ψ(cid:101)(i)= Ψ(i)(cid:62) 0
Ψ(i)Ψ(i)(cid:62) 0
Ψ(cid:101)(i)2 = (8)
0 Ψ(i)(cid:62)Ψ(i)
whichresultsin(cid:107)Ψ(cid:101)(i)(cid:107)2 =(cid:107)Ψ(i)(cid:107)2. Thedilationtechniquetranslatestheinitialsequenceofrandom
matricestothesequenceofrandomself-adjointmatriceswherewecanapplytheMatrixBernstein
theoremwhichstatesthat foraﬁnitesequenceofi.i.d. self-adjointmatricesΨ(cid:101)(i) suchthat almost
(cid:104) (cid:105)
surely∀i  E Ψ(cid:101)(i) =0and(cid:107)Ψ(cid:101)(i)(cid:107)≤R thenforallt≥0
1(cid:88)t Rlog(2k/δ) (cid:114)2(cid:37)2log(2k/δ)
(cid:107) Ψ(cid:101)(i)(cid:107)≤ +
t 3t t
withprobabilityatleast1−δwhere(cid:37)2 :=(cid:107)E(cid:104)Ψ(cid:101)2(i)(cid:105)(cid:107)2  ∀iwhichisalso(cid:37)2 =(cid:107)E(cid:2)Ψ2(i)(cid:3)(cid:107)2  ∀i
duetoEq.8. Therefore  thankstothedilationtrickandtheorem1.4[MatrixBernstein]inTropp
(cid:107) Ψ(i)(cid:107)≤ +
withprobabilityatleast1−δ.
Now byplugginginΨ(i)=eh(x(i))e(cid:62)y(i)−C wehaveE Ψ(cid:101)(i) =0. Togetherwith(cid:107)Ψ(cid:101)(i)(cid:107)≤2as
wellas(cid:37)2 =(cid:107)E(cid:2)Ψ2(i)(cid:3)(cid:107) =1andsettingt=n wehave
B.3 PROOFOFLEMMA3
Azizzadeneshelietal.(2016) LemmaF.1inAnandkumaretal.(2012)andProposition19ofHsu
etal.(2012).
Analogoustotheprevioussectionwecanrewritebh =E(x y)∈Q[eh(x)]−E(x y)∈P[eh(x)]wheree(i)
istheone-hot-encodingofindexi. Notethat(droppingthesubscripth)wehave
(cid:107)(cid:98)bh−bh(cid:107)2 ≤(cid:107)q(cid:98)h−qh(cid:107)2+(cid:107)p(cid:98)h−ph(cid:107)2
Wenowboundbothestimatesofprobabilityvectorsseparately.
Consideraﬁxedmultinomialdistributioncharacterizedwithprobabilityvectorofς ∈∆ where
k−1
∆ isak−1dimensionalsimplex. Further considertrealizationofthismultinomialdistribution
{ς(i)}t whereς(i)istheone-hot-encodingofthei’thsample. Considertheempiricalestimate
meanofthisdistributionthroughempiricalaverageofthesamples;ς = 1(cid:80)(i)tς(i) then
(cid:98) t
(cid:114)
1 log(1/δ)
(cid:107)ς −ς(cid:107)≤ √ +
(cid:98) t t
Byplugginginς =qh ς(cid:98)=q(cid:98)hwitht=nq andﬁnally{ς(i)}ni=q1 ={eh(x(i))}(i)nq andequivalently
forp weobtain;
(cid:32) (cid:112) (cid:112) (cid:33) (cid:32) (cid:33)
log(1/δ) log(1/δ) 1 1
(cid:107)(cid:98)bh−bh(cid:107)2 ≤ (cid:112)(1−β)np + √nq + (cid:112)(1−β)np + √nq
withprobabilityatleast1−2δ therefore;
resultinginthestatementintheLemma3.
B.4 PROOFOFTHEOREM1
Wewanttoultimatelybound|L((cid:98)hw(cid:98))−L(h(cid:63))|. Byadditionandsubtractionwehave
L((cid:98)hw(cid:98))−L(h(cid:63))=L((cid:98)hw(cid:98))−Ln((cid:98)hw(cid:98))+Ln((cid:98)hw(cid:98))−Ln((cid:98)hw(cid:98);w(cid:98))
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(b) (a)
+Ln((cid:98)hw(cid:98);w(cid:98))−Ln(h(cid:63);w(cid:98))+Ln(h(cid:63);w(cid:98))−Ln(h(cid:63))+Ln(h(cid:63))−L(h(cid:63)) (9)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
≤0 (a) (b)
wheren=βnpandweusedoptimalityof(cid:98)hw(cid:98). Here(a)istheweightestimationerrorand(b)isthe
ﬁnitesampleerror.
Uniformlawforbounding(b) Webound(b)usingstandardresultsforuniformlawsforuniformly
boundedfunctionswhichholdssince(cid:107)w(cid:107) ≤d (q||p)and(cid:107)(cid:96)(cid:107) ≤1. Since|w(y)(cid:96)(h(x) y)|≤
∞ ∞ ∞
d (q||p)  ∀x y ∈X ×Y bydeployingtheMcDiarmid’sinequalitywethenobtainthat
∞
sup|L (h)−L(h)|≤2R (G((cid:96) H))+d (q||p) δ
n n ∞ n
whereG((cid:96) H)={g (x y)=w(y)(cid:96)(h(x) y):h∈H}andtheRademachercomplexityisdeﬁned
asRn(G):=EX(i) Y(i)∼P:i∈[n](cid:2)Eσi:i∈[n]n1 [suph∈H(cid:80)ni=1σiw(yi)(cid:96)(xi h(yi))](cid:3).
ofthehypothesisclassH(seeforexamplePercyLiangnotesonStatisticalLearningTheoryand
chapter4inWainwright(2019))
Boundingterm(a) Rememberthatk = |Y|isthecardinalityoftheﬁnitedomainofY  orthe
numberofclasses. Letusdeﬁne(cid:96)(cid:101)∈ Rk with(cid:96)(cid:101)j = (cid:80)ni=1Iy(i)=j(cid:96)(y(i) h(x(i))). Noticethatby
deﬁnition(cid:107)(cid:96)(cid:101)(cid:107)1 ≤ nand(cid:107)(cid:96)(cid:101)(cid:107)∞ ≤ nfromwhichitfollowsbyHoelder’sinequalitythat(cid:107)(cid:96)(cid:101)(cid:107)2 ≤ n.
Furthermore weslightlyabusenotationandusewtodenotethek-dimensionalvectorwithw =w(i).
Therefore forallhwehaveviatheCauchySchwarzinequalitythat
|L (h;w)−L (h;w)|=| (w(y(i))−w(y(i)))(cid:96)(h(x(i)) y(i))|
n n (cid:98) n (cid:98)
≤|n (w(j)−w(cid:98)(j))(cid:96)(cid:101)(j)|
≤ n(cid:107)w(cid:98)−w(cid:107)2(cid:107)(cid:96)(cid:101)(cid:107)2 ≤(cid:107)w(cid:98)−w(cid:107)2
≤(cid:107)λθ(cid:98)−θ(cid:107)2 ≤(1−λ)(cid:107)θ(cid:107)2+λ(cid:107)θ(cid:98)−θ(cid:107)2 (10)
ItthenfollowsbyLemma1that
sup|L (h;w)−L (h;w)|≤(1−λ)(cid:107)θ(cid:107)
n n (cid:98) 2
λ (cid:0) log(k/δ) log(1/δ) log(1/δ)(cid:1)
O ((cid:107)θ(cid:107) ) +
σ 2 (1−β)n (1−β)n n
Lemma4(McDiarmid-Doob-Freedman-Rademacher) ForagivenAhypothesisclassH  aset
G((cid:96) H)={g (x y)=w(y)(cid:96)(h(x) y):h∈H} underndatapointsandlossfunction(cid:96)wehave
2d (q||p)log(2/δ) d(q||p)log(2/δ)
sup|L (h)−L(h)|≤2R(G((cid:96) H))+ ∞ + 2
n n n
withprobabilityatleast1−δ
Pluggingbothboundsbackintoequation(9)concludestheproofofthetheorem.
B.5 PROOFOFLEMMA4
Withabitabuseofnotationlet’srestatetheempiricallosswithknownimportanceweightsinsteadon
therandomvariables{(X  Y )}n
i i 1
L (h)= w(Y )(cid:96)(Y  h(X ))
Wefurtherdeﬁneaghostdataset{(X(cid:48) Y(cid:48))}nandthecorrespondingghostloss;
L(cid:48) (h)= w(Y(cid:48))(cid:96)(Y(cid:48) h(X(cid:48)))
Let’sdeﬁnearandomvariableG := sup L (h)−L(h). Thisrandomvariableisthekeyto
n h∈H n
derivethetightgeneralizationboundinLemma4.
Thisrandomvariablehasthefollowingproperties;
E[G ]=E supL (h)−E[L(cid:48) (h)]
Whichwecanrewriteas
(cid:20) (cid:104) (cid:12) (cid:105)(cid:21)
E[G ]=E supE L (h)−L(cid:48) (h)(cid:12){(X  Y )}n
n n n (cid:12) i i 1
andswappingthesupwiththeexpectation
(cid:20) (cid:20) (cid:12) (cid:21)(cid:21)
E[G ]≤E E supL (h)−L(cid:48) (h)(cid:12){(X  Y )}n
Wecanremovetheconditionwithlawofiteratedconditionalexpectationandhaveexpectationon
bothofthedatasets;
E[G ]≤E supL (h)−L(cid:48) (h)
wefurtheropentheexpressionup;
(cid:34) n (cid:35)
E[G ]≤E sup w (cid:96)(h(X ) Y )−w(cid:48)(cid:96)(cid:48)(h(X(cid:48)) Y(cid:48))
n n i i i i i i
InthefollowingweusetheusualsymmetrizingtechniquethroughRademachervariables{ξ }n. Each
i 1
ξ isauniformrandomvariableeither1or−1. Thereforesince(cid:96)(h(X(cid:48)) Y(cid:48))−(cid:96)(cid:48)(h(X(cid:48)) Y(cid:48))isa
symmetricrandomvariablewehave
E[G ]≤E sup ξ [w(Y )(cid:96)(h(X ) Y )−w(Y(cid:48))(cid:96)(cid:48)(h(X(cid:48)) Y(cid:48))]
n n i i i i i i i
wheretheexpectationisalsoovertheRademachervariables. Afterpropagationsup
(cid:34) n n (cid:35)
E[G ]≤E sup ξ w(Y )(cid:96)(h(X ) Y )+ sup −ξ w(Y(cid:48))(cid:96)(cid:48)(h(X(cid:48)) Y(cid:48))
n n i i i i n i i i i
h∈H h∈H
i i
BypropagatingtheexpectationandagainsymmetryintheRademachervariablewehave
E[G ]≤2E sup ξ w(Y )(cid:96)(h(X ) Y ) =2R(G((cid:96) H))
n n i i i i
where the right hand side is two times the Rademacher complexity of class G((cid:96) H). Consider a
sequenceofDoobMartingaleandﬁltration(U  F )deﬁnedonsomeprobabilityspace(Ω F Pr);
j j
(cid:34) 1 (cid:88)n (cid:12) (cid:35)
U :=E sup w(Y )(cid:96)(h(X ) Y )(cid:12){(X  Y )}j
j n i i i (cid:12) i i 1
andthecorrespondingMartingaledifference;
D :=U −U
j j j−1
Inthefollowingweshowthateach|D |isboundedabove.
j
(cid:34) 1 (cid:88)n (cid:12) (cid:35) (cid:34) 1 (cid:88)n (cid:12) (cid:35)
D =E sup w(Y )(cid:96)(h(X ) Y )(cid:12){(X  Y )}j −E sup w(Y )(cid:96)(h(X ) Y )(cid:12){(X  Y )}j−1
j n i i i (cid:12) i i 1 n i i i (cid:12) i i 1
≤maxE sup w(Y )(cid:96)(h(X ) Y )(cid:12){(X  Y )}j−1 x  y
xj yj h∈Hn i i i (cid:12) i i 1 j j
− minE sup w(Y )(cid:96)(h(X ) Y )(cid:12){(X  Y )}j−1 x  y
Let’s deﬁne xmax ymax as the solution to the maximization and xmin ymin the solution to the
j j j j
minimization therefore
D ≤E sup w(Y )(cid:96)(h(X ) Y )(cid:12){(X  Y )}j−1 xmax ymax
j n i i i (cid:12) i i 1 j j
−E sup w(Y )(cid:96)(h(X ) Y )(cid:12){(X  Y )}j−1 xmin ymin
n i i i (cid:12) i i 1 j j
(cid:34) (cid:32)1 (cid:88)n (cid:33)(cid:12) (cid:35)
=E sup w(Y )(cid:96)(h(X ) Y )+w(ymin)(cid:96)(h(xmin) ymin)−w(ymin)(cid:96)(h(xmin) ymin) (cid:12){(x  y )}j−1 xmax ymax
n i i i j j j j j j (cid:12) i i 1 j j
=E sup w(Y )(cid:96)(h(X ) Y )+w(ymax)(cid:96)(h(xmax) ymax)−w(ymin)(cid:96)(h(xmin) ymin) (cid:12){(x  y )}j−1 xmin ymin
1 (cid:12) (cid:12) d (q||p)
≤ sup(cid:12)w(ymax)(cid:96)(h(xmax) ymax)−w(ymin)(cid:96)(h(xmin) ymin)(cid:12)≤ ∞
n (cid:12) j j j j j j (cid:12) n
Thesamewaywecanbound−D . ThereforetheabsolutevalueeachD isboundedby d∞(q||p). In
j j n
thefollowingweboundtheconditionalsecondmoment E(cid:2)D2|F (cid:3);
j j−1
 2 
EE(cid:34)sup n1 (cid:88)n w(Yi)(cid:96)(h(Xi) Yi)(cid:12)(cid:12)(cid:12){(Xi Yi)}j1(cid:35)−E(cid:34)sup n1 (cid:88)n w(Yi)(cid:96)(h(Xi) Yi)(cid:12)(cid:12)(cid:12){(Xi Yi)}j1−1(cid:35) (cid:12)(cid:12)(cid:12){(Xi Yi)}1j−1
 i i  
(a(cid:48)) (b(cid:48))
Let’sconstructaneventC theeventthata(cid:48)isbiggerthanb(cid:48) andalsoC(cid:48) itscompliment. Therefore
fortheE(cid:2)D2|F (cid:3)wehave
(cid:34)(cid:32) (cid:34) 1 (cid:88)n (cid:12) (cid:35)
E E sup w(Y )(cid:96)(h(X ) Y )(cid:12){(X  Y )}j
n i i i (cid:12) i i 1
(cid:34) 1 (cid:88)n (cid:12) (cid:35)(cid:33)2(cid:12) (cid:35)
−E sup w(Y )(cid:96)(h(X ) Y )(cid:12){(X  Y )}j−1 (cid:12){(X  Y )}j−1 C P(C |F )
n i i i (cid:12) i i 1 (cid:12) i i 1 j j j−1
+E E sup w(Y )(cid:96)(h(X ) Y )(cid:12){(X  Y )}j
−E sup w(Y )(cid:96)(h(X ) Y )(cid:12){(X  Y )}j−1 (cid:12){(X  Y )}j−1 C(cid:48) P(C(cid:48)|F )
FortheﬁrsterminEq.11afteragainintroducingghostvariablesX(cid:48) Y(cid:48)wehavethefollowingupper
(cid:34)(cid:32)  1 (cid:88)n 1 (cid:12) 
E Esup n w(Yi)(cid:96)(h(Xi) Yi)+ n supw(Yj)(cid:96)(h(Xj) Yj)(cid:12)(cid:12){(Xi Yi)}j1
i(cid:54)=j
(cid:34)(cid:32)  1 (cid:88)n 1 1 (cid:12) 
≤E Esup n w(Yi)(cid:96)(h(Xi) Yi)+ nw(Yj(cid:48))(cid:96)(h(Xj(cid:48)) Yj(cid:48))+ n supw(Yj)(cid:96)(h(Xj) Yj)(cid:12)(cid:12){(Xi Yi)}j1
(cid:34)(cid:32) (cid:34) 1 (cid:88)n (cid:12) (cid:35) 1
≤E E sup w(Y )(cid:96)(h(X ) Y )(cid:12){(X  Y )}j−1 + supw(Y )(cid:96)(h(X ) Y )
n i i i (cid:12) i i 1 n j j j
(cid:34)(cid:18)1 (cid:19)2(cid:12) (cid:35)
≤E supw(Y )(cid:96)(h(X ) Y ) (cid:12){(X  Y )}j−1 C P(C |F )
n j j j (cid:12) i i 1 j j j−1
(cid:34)(cid:18)1 (cid:19)2(cid:12) (cid:35) d(q||p)
≤E supw(Y )(cid:96)(h(X ) Y ) (cid:12){(X  Y )}j−1 P(C |F )≤ P(C |F )
n j j j (cid:12) i i 1 j j−1 n2 j j−1
SofarwehavethattheﬁrstterminEq.11isboundedby d(q||p). Nowforthesecondtermwehave
thefollowingupperbound;
E E sup w(Y )(cid:96)(h(X ) Y )(cid:12){(X  Y )}j−1
−E sup w(Y )(cid:96)(h(X ) Y )(cid:12){(X  Y )}j (cid:12){(X  Y )}j−1 C(cid:48) P(C(cid:48)|F )
≤E Esup n w(Yi)(cid:96)(h(Xi) Yi)+ n supw(Yj)(cid:96)(h(Xj) Yj)(cid:12)(cid:12){(Xi Yi)}1j−1
 1 (cid:88)n (cid:12) (cid:33)2(cid:12) (cid:35)
−Esup n w(Yi)(cid:96)(h(Xi) Yi)(cid:12)(cid:12){(Xi Yi)}j1−1 (cid:12)(cid:12){(Xi Yi)}j1−1 Cj(cid:48) P(Cj(cid:48)|Fj−1)
(cid:18) (cid:20)1 (cid:12) (cid:21)(cid:19)2
≤ E supw(Y )(cid:96)(h(X ) Y )(cid:12){(X  Y )}j−1
n j j j (cid:12) i i 1
(cid:18) (cid:20)1 (cid:12) (cid:21)(cid:19)2 1
≤ E w(Y )(cid:12){(X  Y )}j−1 = P(C(cid:48)|F )
n j (cid:12) i i 1 n2 j j−1
Therefore sinced(q||p)≥1
E(cid:2)D2|F (cid:3)≤ d(q||p)P(C |F )+ 1 P(C(cid:48)|F )≤ d(q||p)
j j−1 n2 j j−1 n2 j j−1 n2
Fortheﬁrstinequality weusedthefactthatthelossiswithin[0 1]andthesecondoneisfromEq.12.
SincetheﬁrstterminRHSis(b(cid:48)) therefore secondmomentofeachD |F isboundedby 2d(q||p).
j j−1 n
Therefore for the Doob Martingale sequence of D we have |D | ≤ d∞(q||p) as well as
(cid:80)nD2|F ≤ d(q||p). UsingtheFreedman’sinequalityFreedman(1975) wehave
j j j−1 n
(cid:88)n  (cid:32) α2 (cid:33)
P  Dj =Gn−E[Gn]≥α≤exp −2(d(q||p) +αd∞(q||p))
j n n
Moreover ifwemultiplyeachlosswith−1 itresultsinhypothesisclassof−Hwhichhasthesame
RademachercomplexityasH duethesymmetricRademacherrandomvariable. LetG(cid:101)ndenotethe
samequantityasG(cid:101)nbuton−H. Weusethisslackvariableinordertoboundtheabsolutevalueof
G . Therefore
(cid:32) (cid:33)
(cid:15)2
P (G ≥E[G ]+(cid:15))≤exp −
n n 2(d(q||p) +(cid:15)d∞(q||p))
≤exp − ≤δ/2
2(d(q||p) +(cid:15)d∞(q||p))
andthesameboundforG(cid:101)n. Bysolvingitfor(cid:15)andδwehave
|G |≤2R(G((cid:96) H))+ ∞ + 2
Note: Afewdayspriortothecamerareadysubmission werealizedthataquitesimilaranalysisand
statementtoTheorem4isalsostudiedinYing(2004).
B.6 GENERALIZATIONFORFINITEHYPOTHESISCLASSES
Forﬁnitehypothesisclasses onemaybound(b)in(9)usingBernstein’sinequality.
Webound(b)byﬁrstnotingthatw(Y)(cid:96)(Y h(X))satisﬁestheBernsteinconditionssothatBernstein’s
inequalityholds
EP[w(Y)]=1  EP[w(Y)2]=d(q||p)  σP2(w(Y))=d(q||p)−1 (12)
bydeﬁnition. Becauseweassume(cid:96)≤1 wedirectlyhave
EP(cid:2)w(Y)2l2(Y h(X))(cid:3)≤EP(cid:2)w(Y)2(cid:3)=d(q||p) (13)
SincewehaveaboundonthesecondmomentofweightedlosswhileitsﬁrstmomentisL(h)wecan
applyBernstein’sinequalitytoobtainforanyﬁxedhthat
2d (q||p)log(2) 2(d(q||p)−L(h)2)log(2)
|L (h)−L(h)|≤ ∞ δ + δ
n 3n n
Fortheuniformlawforﬁnitehypothesisclassesmaketheunionboundonallthehypotheses;
2d (q||p)log(2|H|) 2(d(q||p))log(2|H|)
sup|L (h)−L(h)|≤ ∞ δ + δ
ThesecondmomentoftheimportanceweightedlossEP(cid:2)ω(Y)2(cid:96)2(h(X) Y)(cid:3) givenah∈H(cid:48)can
beboundedforgeneralα≥0 potentiallyleadingtosmallervaluesthand(q||p):
EP(cid:2)ωY2(cid:96)2(Y h(X))(cid:3)
=(cid:88)k p(i)q2(i)E (cid:2)(cid:96)2(h(X) X)(cid:3)
p2(i) X∼p(X|Y=i)
=(cid:88)q(i)α1 pq((ii))q(i)αα−1EX∼p(X|Y=i)(cid:2)(cid:96)2(h(X) i)(cid:3)
≤(cid:32)(cid:88)k q(i)q(i)α(cid:33)α1 (cid:32)(cid:88)k q(i)E (cid:2)(cid:96)2(h(X) i)(cid:3)α2−α1(cid:33)αα−1
p(i)α X∼p(X|Y=i)
=(cid:32)(cid:88)k q(i)q(i)α(cid:33)α1 (cid:32)(cid:88)k q(i)E (cid:2)l2(h(X) i)(cid:3)E (cid:2)(cid:96)2(h(X) i)(cid:3)αα−+11(cid:33)αα−1
p(i)α X∼p(X|Y=i) X∼p(X|Y=i)
≤(cid:32)(cid:88)k q(i)q(i)α(cid:33)α1 (cid:88)k q(i)E (cid:2)(cid:96)2(h(X) i)(cid:3)1−α1 E (cid:2)(cid:96)2(h(X) i)(cid:3)1+α1
wheretheﬁrstinequalityfollowsfromHölder’sinequality thesecondonefollowsfromJensen’s
inequalityandthefactthatthelossisin[0 1]aswellasthefactthattheexponentiationfunctionis
convexinthisregion. Moreover since1+ 1 ≥1andupperboundforthelosssquare l(· ·)2 ≤1
α
EP(cid:2)w(Y)2(cid:96)2(h(X) Y)(cid:3)≤(cid:32)(cid:88)k q(i)pq((ii))αα(cid:33)α1 (cid:88)k q(i)EX∼P|Y=i(cid:2)(cid:96)2(h(X) i)(cid:3)1−α1
whichgivesboundonthesecondmomentofweightedloss.
B.7 SLIGHTDRIFTFROMTHELABELSHIFT
Driftinlabelshiftassumption: Ifthelabelshiftapproximationisslightlyviolated  weexpect
the generalizing bound to deviate from the statement in the Theorem. 1. Deﬁne d (q||p) :=
E(X Y)∼Q (cid:12)(cid:12)1− pq((XX||YY))(cid:12)(cid:12) as the deviation form label shift constraint which is zero in label shift
Remark1(DriftinLabelshiftassumption) Ifthelabelshiftassumptionslightlyviolates forthe
trueimportanceweightsω(x y):= q(x y) theRLLS withhighprobabilitygeneralizesas;
ConsiderthecasewheretheLabelshiftassumptionisslightlyviolated i.e. foreachcovariateand
label wehavep(x|y) (cid:39) q(x|y) resultingimportanceweightω(x y) := q(x y) foreachcovariate
andlabel. SimilartodecomposinginEq.9 wehave
L((cid:98)hw(cid:98);ω)−L(h(cid:63);ω)=L((cid:98)hw(cid:98);ω)−L((cid:98)hw(cid:98))+L((cid:98)hw(cid:98))−Ln((cid:98)hw(cid:98))+Ln((cid:98)hw(cid:98))−Ln((cid:98)hw(cid:98);w(cid:98))
(c) (b) (a)
+Ln((cid:98)hw(cid:98);w(cid:98))−Ln(h(cid:63);w(cid:98))+Ln(h(cid:63);w(cid:98))−Ln(h(cid:63))+Ln(h(cid:63))−L(h(cid:63))+L(h(cid:63))−L(h(cid:63);ω)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
≤0 (a) (b) (c)
wherethedesiredexcessriskisdeﬁnedwithrespecttoω. ThedifferencesbetweenEq.15andEq.9
areinanewterm(c)aswellasterm(a). Theterm(b)remainsuntouched.
Boundonterm(c) Foranyh thetwocontributingcomponentsin(c) i.e. L(h;ω)andL(h)are
asfollows;
L(h)=E(X Y)∼P[w(Y)(cid:96)(Y h(X))]  andL(h;ω)=E(X Y)∼Q[(cid:96)(Y h(X))]=E(X Y)∼P[ω(X Y)(cid:96)(Y h(X))]
Fortheirdeviationwehave
L(h;ω)−L(h)=E(X Y)∼P[(ω(X Y)−w(Y))(cid:96)(Y h(X))]
(cid:20)(cid:18) (cid:19) (cid:21)
q(X Y) q(Y)
=E(X Y)∼P p(X Y) − p(Y) (cid:96)(Y h(X))
(cid:20)(cid:18) p(X|Y)(cid:19) (cid:21) (cid:20)(cid:12) p(X|Y)(cid:12)(cid:21)
=E(X Y)∼Q 1− q(X|Y) (cid:96)(Y h(X)) ≤de(q||p):=E(X Y)∼Q (cid:12)(cid:12)1− q(X|Y)(cid:12)(cid:12)
WhereinthelastinequalitywedeployCauchySchwarzinequalityaswellaslossisin[0 1]andhold
forh∈H. Itisworthnotingthattheexpectationind (q||p)iswithrespecttoQanddoesnotblow
upifthesupportsofPandQdonotmatch
Boundonterm(a) Foranyh∈H similartothederivationinEq.10wehave
|Ln(h)−Ln(h;w(cid:98))|≤(cid:107)w(cid:98)−w(cid:107)2 ≤(cid:107)λθ(cid:98)−θ(cid:107)2 ≤(1−λ)(cid:107)θ(cid:107)2+λ(cid:107)θ(cid:98)−θ(cid:107)2
Thepreviousweightestimationanalysisdoesnotdirectlyholdforthiscasewherethelabelshift
isslightlyviolated butwithafewmodiﬁcationweprovideanupper-boundontheerror. Givena
classiﬁerh
(cid:88)
q (Y =i)= q(h(X)=i|Y =j)q(j)
h0
= (q(h (X)=i|Y =j)−p(h (X)=i|Y =j))q(j)+ p(h (X)=i|Y =j)q(j)
0 0 0
= (q(h (X)=i|Y =j)−p(h (X)=i|Y =j))q(j)+ p(h (X)=i Y =j)w
0 0 0 j
(cid:20) (cid:18) (cid:19)(cid:21)
p(X|Y) (cid:88)
=E(X Y)∼Q p(h0(X)=i) 1− q(X|Y) + p(h0(X)=i Y =j)wj
wherep(h (X)=i)=q(h (X)=i) resulting;
q =b +Cw
h0 e
wherewedroptheh inbothb andC. BoththeconfusionmatrixC andthelabeldistributionq
0 e h0
onthetargetfortheblackboxhypothesish areunknownandweareinsteadonlygivenaccessto
ﬁnitesampleestimatesC(cid:98)h0 q(cid:98)h0. Similartopreviousanalysiswehave
withcorrespondingﬁnitesamplequantity(cid:98)b = q(cid:98)−C(cid:98)1. Similarlytotheanalysiswhentherewas
noviolationinlabelshiftassumption wehaveΥ(θ(cid:48))=(cid:107)Cθ(cid:48)−b−b (cid:107) andthesolutiontoEq.3
e 2
Υ(θ(cid:98))≤ inf {Υ(θ(cid:48))+2∆C(cid:107)θ(cid:48)(cid:107)α}+2∆b+2(cid:107)be(cid:107)2. (16)
Wecansimplifytheupperboundbysettingθ(cid:48) =θ. Wethenhave
Υ(θ(cid:98))=(cid:107)Cθ(cid:98)−b−be(cid:107)2 =(cid:107)C θ(cid:98)−θ (cid:107)2 ≤2∆C(cid:107)θ(cid:107)2+2∆b+2(cid:107)be(cid:107)2 ≤2∆C(cid:107)θ(cid:107)2+2∆b+2de(q||p)
resultingin
(cid:107)θ(cid:98)−θ(cid:107)2 ≤ σ (2∆C(cid:107)θ(cid:107)2+2∆b+2de(q||p))
kazizzad@uci.edu
Anqi Liu
fan.yang@stat.math.ethz.ch
Animashree Anandkumar
We propose Regularized Learning under Label shifts (RLLS)  a principled and a
INTRODUCTION
the model was trained (source distribution). In many cases  the publicly available large-scale datasets
Covariate Shift
Label Shift
p(x) ̸= q(x)
p(y|x) = q(y|x)
p(y) ̸= q(y)
p(x|y) = q(x|y)
There are various ways to approach distribution shifts between
a source data distribution P and a target data distribution Q. If
we denote input variables as x and output variables as y  we
assumes that the conditional output distribution is invariant: p(y|x) = q(y|x) between source and
target distributions  but the source distribution p(x) changes. (ii) Label shift  where the conditional
covariate shift which can be mitigated. The reason is that the outputs y typically have a much lower
importance weights q(y)/p(y) for the source samples and train a classiﬁer on the weighted data. In
the following we refer to the procedure as black box shift learning (BBSL) which the authors proved
generalization bound for the ﬁnal Regularized Learning under Label Shift (RLLS) classiﬁer based on
of the classiﬁer on reweighted samples by a factor of k log(k)  where k is the number of classes  i.e.
the cardinality of Y.
REGULARIZED LEARNING OF LABEL SHIFTS (RLLS)
respect to P  Q as p  q : [k] → [0  1] with p(i) = P(Y = i)  and q(i) = Q(Y = i) for all i ∈ [k]
+ which sum to one. In the label shift setting  we deﬁne the importance
weight vector w ∈ Rk between these two domains as w(i) = q(i)
d∞(q||p) := sup
and
d(q||p) := EY ∼Q
�
w(Y )2�
=
q(i)q(i)
p(i).
Given a hypothesis class H and a loss function ℓ : Y × Y → [0  1]  our aim is to ﬁnd the hypothesis
h ∈ H which minimizes
L(h) = EX Y ∼Q [ℓ(Y  h(X))] = EX Y ∼P [w(Y )ℓ(Y  h(X))]
In the usual ﬁnite sample setting however  L unknown and we observe samples {(xj  yj)}n
j=1 from P
instead. If we are given the vector of importance weights w we could then minimize the empirical
Ln(h) = 1
w(yj)ℓ(yj  h(xj))
where n is the number of available observations drawn from P used to learn the classiﬁer h. As w is
Ln(h; �w) = 1
�w(yj)ℓ(yj  h(xj))
(1)
where �w are estimates of w. Given a set Dp of np samples from the source distribution P  we ﬁrst
divide it into two sets where we use (1 − β)np samples in set Dweight
to compute the estimate �w
and the remaining n = βnp in the set Dclass
to ﬁnd the classiﬁer which minimizes the loss (1)  i.e.
�h �
w = arg min h∈H Ln(h; �w). In the following  we describe how to estimate the weights �w and
provide guarantees for the resulting estimator �h �
w.
Plug-in weight estimation
The following simple correlation between the label distributions p  q
was noted in Lipton et al. (2018): for a ﬁxed hypothesis h  if for all y ∈ Y it holds that q(y) ≥ 0 =⇒
p(y) ≥ 0  we have
qh(i) := Q(h(X) = i) =
Q(h(X) = i|Y = j)q(j) =
P(h(X) = i|Y = j)q(j)
P(h(X) = i  Y = j)q(j)
p(j) =
P(h(X) = i  Y = j)wj
for all i  j ∈ Y. This can equivalently be written in matrix vector notation as
qh = Chw
(2)
where Ch is the confusion matrix with [Ch]i j = P(h(X) = i  Y = j) and qh is the vector
q(y) ≥ 0 =⇒ p(y) ≥ 0 is a reasonable condition since without any prior knowledge  there is no
In reality  both qh and Ch can only be estimated by the corresponding ﬁnite sample averages
�qh  �Ch. Lipton et al. (2018) simply compute the inverse of the estimated confusion matrix �Ch in
order to estimate the importance weight  i.e. �w = �C−1
h �qh. While C−1
h �qh is a statistically efﬁcient
estimator  �w with estimated �C−1
can be arbitrarily bad since �C−1
can be arbitrary close to a singular
linear in k which is problematic for large k.
compute importance weights. In the case of no shift we have w = 1 so that we deﬁne the amount
of weight shift as θ = w − 1. Given a “decent” black box estimator which we denote by h0  we
make the ﬁnal classiﬁer less sensitive to the estimation performance of C (i.e. regularize the weight
1. calculating the measurement error adjusted �θ (described in Section 2.1 for h0) and
2. computing the regularized weight �w = 1+λ�θ where λ depends on the sample size (1−β)np.
By 'decent' we refer to a classiﬁer h0 which yields a full rank confusion matrix Ch0. A trivial
example for a non-”decent” classiﬁer h0 is one that always outputs a ﬁxed class. As it does not
2.1
ESTIMATOR CORRECTING FOR FINITE SAMPLE ERRORS
Both the confusion matrix Ch0 and the label distribution qh0 on the target for the black box hypothesis
h0 are unknown and we are instead only given access to ﬁnite sample estimates �Ch0  �qh0. In what
with respect to the hypothesis h = h0. For notation simplicity  we thus drop the subscript h0 in what
follows. The reparameterized linear model (2) with respect to θ then reads
b := q − C1 = Cθ
with corresponding ﬁnite sample quantity �b = �q − �C1. When �C is near singular  the estimation of θ
becomes unstable. Furthermore  large values in the true shift θ result in large variances. We address
this problem by adding a regularizing ℓ2 penalty term to the usual loss and thus push the amount of
�θ = arg min
∥ �Cθ − �b∥2 + ∆C∥θ∥2
(3)
Here  ∆C is a parameter which will eventually be high probability upper bounds for ∥ �C − C∥2. Let
∆b also denote the high probability upper bounds for ∥�b − b∥2.
Lemma 1 For �θ as deﬁned in equation (3)  we have with probability at least 1 − δ that1
∥�θ − θ∥2 ≤ ϵθ(np  nq  ∥θ∥2  δ)
ϵθ(np  nq  ∥θ∥2  δ) := O
σmin
∥θ∥2
log(k/δ)
(1 − β)np
log(1/δ)
nq
sample complexity which is in the order of σ−2
min to obtain the guarantees for BBSL. This is due to
that in order to obtain their upper bound with a probability of at least 1 − δ  it is necessary that
3kn−10
+ 2kn−10
≤ δ. As a consequence  the upper bound in Theorem 3 of Lipton et al. (2018)
is bigger than
3σmin
log(3k/δ)
np
k log(2k/δ)
Furthermore  as in Lipton et al. (2018)  this result holds for any black box estimator h0 which enters
the bound via σmin(Ch0). We can directly see how a good choice of h0 helps to decrease the upper
bound in Lemma 1. In particular  if h0 is an ideal estimator  and the source set is balanced  C is the
unit matrix with σmin = 1/k. In contrast  when the model h0 is uncertain  the singular value σmin is
norm squared regularization is motivated by the cases with large shifts θ  where using the squared
norm may shrink the estimate �θ too much and away from the true θ.
1Throughout the paper  O hides universal constant factors. Furthermore  we use O (· + ·) for short to denote
O (·) + O (·).
Algorithm 1 Regularized Learning of Label Shift (RLLS)
1: Input: source set Dp  Dq  θmax  estimate of σmin  black box estimator h0  model class H
2: Determine optimal split ratio β⋆ and regularizer λ⋆ by minimizing the RHS of Eq. (6) using an
estimate of σmin
3: Randomly partition source set Dp into Dclass
Dweight
such that |Dclass
| = β⋆np =: n
4: Compute �θ using Eq. (3) and �w := 1 + λ⋆�θ
5: Minimize the importance weighted empirical loss to obtain the weighted estimator
w = arg min
h∈H Ln(h; �w)
�w(y)ℓ(y  h(x))
6: Deploy �h �
w if the risk is acceptable
2.2
REGULARIZED ESTIMATOR AND GENERALIZATION BOUND
�w = 1 + λ�θ.
(4)
Note that �w implicitly depends on λ  and β. By rewriting �w = (1 − λ)1 + λ(1 + �θ)  we see that
intuitively λ closer to 1 the more reason there is to believe that 1 + �θ is in fact the true weight.
Deﬁne the set G(ℓ  H) = {gh(x  y) = w(y)ℓ(h(x)  y) : h ∈ H} and its Rademacher complexity
Rn(G) := E(Xi Yi)∼P:i∈[n]
Eξi:i∈[n]
ξigh(Xi  h(Yi))
��
with ξi  ∀i as the Rademacher random variables (see e.g. Bartlett & Mendelson (2002)). We can now
state a generalization bound for the classiﬁer �h �
w in a general hypothesis class H  which is trained on
Theorem 1 (Generalization bound for �h �
w) Given np samples from the source data set and nq
samples from the target set  a hypothesis class H and loss function ℓ  the following generalization
bound holds with probability at least 1 − 2δ
L(�h �
w) − L(h∗) ≤ ϵG(np  δ  β) + (1 − λ) ∥θ∥2 + λϵθ(np  nq  ∥θ∥2  δ  β).
(5)
ϵG(np  δ) := 2Rn(G) + min
d∞(q||p)
log(2/δ)
βnp
2d∞(q||p) log(2/δ)
2d(q||p) log(2/δ)
size of Rn(G) is determined by the structure of the function class H and the loss ℓ. For example for
the 0/1 loss  the VC dimension of H can be deployed to upper bound the Rademacher complexity.
The bound (5) in Theorem 1 holds for all choices of λ. In order to exploit the possibility of choosing
λ and β to have an improved accuracy depending on the sample sizes  we ﬁrst let the user deﬁne a set
of shifts θ against which we want to be robust against  i.e. all shifts with ∥θ∥2 ≤ θmax. For these
w) − L(h∗) ≤ ϵG(np  δ) + (1 − λ) θmax + λϵθ(np  nq  θmax  δ)
(6)
The bound in equation (6) suggests using Algorithm 1 as our ultimate label shift correction procedure.
where for step 2 of the algorithm  we choose λ⋆ = 1 whenever nq ≥
θ2
max(σmin−
√np )2 (hereby
obtain L(�h �
w) − L(h∗) ≤ ϵG(np  δ) + min{θmax  ϵθ(np  nq  θmax  δ)} which is smaller than the
unregularized bound for small nq  np. Notice that in practice  we do not know σmin in advance so
that in Algorithm 1 we need to use an estimate of σmin  which could e.g. be the minimum eigenvalue
of the empirical confusion matrix �C with an additional computational complexity of at most O(k3).
0.2
0.4
0.6
0.8
1.0
θmax
Figure 1:
Given a σmin and
θmax  λ switches from 0 to 1 at a
particular nq. np and k are ﬁxed.
Figure 1 shows how the oracle thresholds vary with nq and σmin
when np is kept ﬁx. When the parameters are above the curves for
ﬁxed np  λ should be chosen as 1 otherwise the samples should
be unweighted  i.e. λ = 0. This ﬁgure illustrates that when the
should only be trusted for rather high nq and high believed shifts
θmax. Although the overall statistical rate of the excess risk of the
classiﬁer does not change as a function of the sample sizes  θmax
could be signiﬁcantly smaller than ϵθ when σmin is very small and
For de(q||p) := E(X Y )∼Q
����1 − p(X|Y )
q(X|Y )
���
Theorem 2 (Drift in Label shift assumption) In the presence of de(q||p) deviation from label shift
assumption  the true importance weights ω(x  y) := q(x y)
p(x y)  the RLLS generalizes as;
w  ω) − L(h∗; ω) ≤ ϵG(np  δ) + (1 − λ) ∥θ∥2 + λϵθ(np  nq  ∥θ∥2  δ) + 4 (1 − λ) de(q||p)
EXPERIMENTS
where we set a class to have probability p > 0.1  while the distribution over the rest of the classes
is uniform. The Minority-Class Shift is a more general version of Tweak-One shift  where a ﬁxed
number of classes m to have probability p < 0.1  while the distribution over the rest of the classes is
uniform. For the Dirichlet shift  we draw a probability vector p from the Dirichlet distribution with
concentration parameter set to α for all classes  before including sample points which correspond to
algorithm 1  where we choose the black box predictor h0 to be a two-layer fully connected neural
noted  h0 is trained using uniform data.
In order to compute �ω = 1+�θ in Eq. (3)  we call a built-in solver to directly solve the low dimensional
problem minθ ∥ �Cθ − �b∥2 + ∆C∥θ∥2 where we empirically observer that 0.01 times of the true ∆C
noting that 0.001 makes the theoretical bound in Lemma. 1 O(1/0.01) times bigger. We thus treat it
a classiﬁer on the source samples weighted by �ω  where we use a two-layer fully connected neural
mean square estimation error (MSE) and variance of the estimated weights E∥ �w − w∥2
2 and the
sample size regimes for np  nq above n = 8000 as mentioned by Lipton et al. (2018).
3.1
WEIGHT ESTIMATION AND PREDICTIVE PERFORMANCE FOR SOURCE SHIFT
performance for Tweak-One source shifts and compare it with BBSL. For this set of experiments  we
source shift of CIFAR10. We created shifts with ρ > 0.5. We use a ﬁxed black-box classiﬁer that is
trained on biased source data  with tweak-one ρ = 0.5. Observe that the MSE in weight estimation is
all shifts  the RLLS based classiﬁer yields higher accuracy than the one based on BBSL. Results for
shifted source and uniform target with h0 trained using tweak-one shifted source data.
3.2
WEIGHT ESTIMATION AND PREDICTIVE PERFORMANCE FOR TARGET SHIFT
on CIFAR10. The target set is shifted using the Dirichlet shift with parameters α = [0.01  0.1  1  10].
The number of data points in both source and target set is 10000.
2(precision · recall)/(precision + recall) over all classes. For a class i  precision is the percentage
comparability between shifts. We can see that in the large target shift case for α = 0.01  the F-1
for uniform source and Dirichlet shifted target. Smaller α corresponds to bigger shift.
3.3
REGULARIZED WEIGHTS IN THE LOW SAMPLE REGIME FOR SOURCE SHIFT
target samples nq for different values of λ for small nq. Here we ﬁx the sample size in the source set
to np = 1000 and investigate a Minority-Class source shift with ﬁxed p = 0.01 and ﬁve minority
A motivation to use intermediate λ is discussed in Section 2.2  as λ in equation (4) may be chosen
according to θmax  σmin. In practice  since θmax is just an upper bound on the true amount of shift
∥θ∥2  in some cases λ should in fact ideally be 0 when
θ2max(σmin−
√nq )2 ≤ nq ≤
∥θ∥2(σmin−
√nq )2 .
Thus for target sample sizes nq that are a little bit above the threshold (depending on the certainty of
the belief how close to θmax the norm of the shift is believed to be)  it could be sensible to use an
intermediate value λ ∈ (0  1).
target sample size and λ using (a) better predictor h0 trained on tweak-one shifted source with
ρ = 0.2  (b) neutral predictor h0 with ρ = 0.5 and (c) corrupted predictor h0 with ρ = 0.8.
while for 10 ≤ nq ≤ 500 an intermediate λ ∈ (0  1) (purple) has the highest accuracy and for
nq > 1000  the weight estimation is certain enough for the fully weighted classiﬁer (yellow) to have
Furthermore  the different plots in Figure 4 correspond to black-box predictors h0 for weight
confusion matrix. The fully weighted methods with λ = 1 achieve the best performance faster with a
Furthermore  this reﬂects the relation between eigenvalue of confusion matrix σmin and target sample
size nq in Theorem 1. In other words  we need more samples from the target data to compensate a
RELATED WORK
as a causal or anti-causal model (Schölkopf et al.  2012): With label shift  the label Y causes the
input X (that is  X is not a causal parent of Y   hence 'anti-causal') and the causal mechanism that
generates X from Y is independent of the distribution of Y . A long line of work has addressed the
reverse causal setting where X causes Y and the conditional distribution of Y given X is assumed to
mapping from X to Y which does not change if the distribution of X changes. Mathematically this
weights q(x)/p(x) (Zadrozny  2004; Cortes et al.  2010; Cortes & Mohri  2014; Shimodaira  2000)
(q(x)/p(x))2 to be bounded  which means the bounds are extremely loose in most cases (Cortes
importance weights q(y)/p(y) over the labels which typically live in a very low-dimensional space.
posterior distribution of y and suffer from the curse of dimensionality. Recent advances as in Lipton
Existing generalization bounds have historically been mainly developed for the case when P = Q
Cortes et al. (2010) provides a generalization bound when q(x)/p(x) is known which however does
DISCUSSION
importance weighting procedure for which no prior knowledge of q(y)/p(y) is required. Although
RLLS is inspired by BBSL  it leads to a more robust importance weight estimator as well as general-
study the setting when it is slightly violated. For instance  x in practice cannot be solely explained
by the wanted label y  but may also depend on attributes z which might not be observable. In the
in a slightly modiﬁed sense  i.e. P(X|Y = y  Z = z) = Q(X|Y = y  Z = z). If the attributes Z are
ACKNOWLEDGEMENT
models and hidden markov models. In Conference on Learning Theory  pp. 33–1  2012.
of pomdps using spectral methods. arXiv preprint arXiv:1602.07764  2016.
structural results. Journal of Machine Learning Research  3(Nov):463–482  2002.
and the bias-variance trade-off. arXiv preprint arXiv:1812.11118  2018.
Vaughan. A theory of learning from different domains. Machine learning  79(1-2):151–175  2010.
Proceedings of the 26th annual international conference on machine learning  pp. 49–56. ACM
Machine Learning Research  11(Nov):2973–3009  2010.
ii. a probabilistic model for the comparison of diagnostic tests. American Journal of Epidemiology
Yee Seng Chan and Hwee Tou Ng. Word sense disambiguation with distribution estimation. In IJCAI
Artiﬁcial Intelligence and Statistics  pp. 1270–1279  2016.
algorithm for regression. Theoretical Computer Science  519:103–126  2014.
In Advances in neural information processing systems  pp. 442–450  2010.
Machine Learning Research  9(Aug):1757–1774  2008.
Discovery  17(2):164–206  2008.
David A Freedman. On tail probabilities for martingales. the Annals of Probability  pp. 100–118
kernel two-sample test. Journal of Machine Learning Research  13(Mar):723–773  2012.
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition
models. Journal of Computer and System Sciences  78(5):1460–1480  2012.
systems  pp. 601–608  2007.
Convergence bounds and kernel selection. In International Conference on Machine Learning  pp.
bounds  margin bounds  and regularization. In Advances in neural information processing systems
learning for cost-sensitive classiﬁcation. arXiv preprint arXiv:1703.01014  2017.
black box predictors. arXiv preprint arXiv:1802.03916  2018.
information processing systems  pp. 37–45  2014.
Advances in Neural Information Processing Systems  pp. 4518–4528  2017.
David Lopez-Paz and Maxime Oquab.
Revisiting classiﬁer two-sample tests.
arXiv preprint
arXiv:1610.06545  2016.
Geoffrey McLachlan. Discriminant analysis and statistical pattern recognition  volume 544. John
an application to reinforcement learning. arXiv preprint arXiv:1206.6444  2012.
embeddings of distributions. In International Conference on Machine Learning  pp. 2052–2060
new a priori probabilities: a simple procedure. Neural computation  14(1):21–41  2002.
anomaly rejection. In Artiﬁcial Intelligence and Statistics  pp. 850–858  2014.
On causal and anticausal learning. arXiv preprint arXiv:1206.6471  2012.
Consistency and maximal denoising. In Conference On Learning Theory  pp. 489–511  2013.
likelihood function. Journal of statistical planning and inference  90(2):227–244  2000.
shift in machine learning  pp. 3–28  2009.
mathematics  12(4):389–434  2012.
networks  10(5):988–999  1999.
M. J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint. Cambridge University
Kong  2004.
the twenty-ﬁrst international conference on Machine learning  pp. 114. ACM  2004.
kernel two-sample test. In Advances in neural information processing systems  pp. 755–763  2013.
target and conditional shift. In International Conference on Machine Learning  pp. 819–827  2013.
A
MORE EXPERIMENTAL RESULTS
advantage of using RLLS vs. BBSL are more or less pronounced.
A.0.1
CIFAR10 EXPERIMENTS UNDER TWEAK-ONE SHIFT AND DIRICHLET SHIFT
shifts including the Tweak-one Shift  for which we randomly choose one class  e.g. i and set p(i) = ρ
of RLLS compared to BBSL for a variety of values of ρ and α. Note that larger shifts correspond to
smaller α and larger ρ. In general  one observes that our RLLS estimator has smaller MSE and that
A.1
MNIST EXPERIMENTS UNDER MINORITY-CLASS SOURCE SHIFTS FOR DIFFERENT
Figure 7 demonstrates another case in minority-class shift when p = 0.01. The black-box classiﬁer is
the same two-layers neural network trained on a biased source data set with tweak-one ρ = 0.5. We
shifted source and uniform target with p = 0.01  with h0 trained on tweak-one shifted source data.
more minority classes with p = 0.005. We use a ﬁxed black-box classiﬁer that is trained on biased
source data  with tweak-one ρ = 0.5. Observe that the MSE in weight estimation is relatively large
of RLLS over the unweighted classiﬁer increases as the shift increases. Across all shifts  the RLLS
based classiﬁer yields higher accuracy than the one based on BBSL.
shifted source and uniform target with p = 0.005  with h0 trained on tweak-one shifted source data.
A.2
CIFAR10 EXPERIMENT UNDER DIRICHLET SOURCE SHIFTS
the way we generate shift on source data. We train h0 with tweak-one shifted source data with
ρ = 0.5. The results show that importance weighting in general is not helping the classiﬁcation in
A.3
MNIST EXPERIMENT UNDER DIRICHLET SHIFT WITH LOW TARGET SAMPLE SIZE
We show the performance of classiﬁer with different regularization λ under a Dirichlet shift with
α = 0.5 in Figure 10. The training has 5000 examples in this case. We can see that in this low target
sample case  λ = 1 only take over after several hundreds example  while some λ value between 0
and the necessary target sample size. We use biased source data with tweak-one ρ = 0  0.2  0.6 to
λ = 1 to take over for a more corrupted black-box classiﬁer.
shifted source and uniform target  with h0 trained on tweak-one shifted source data.
sample size and λ using (a) better predictor  (b) neutral predictor and (c) corrupted predictor.
B
PROOFS
B.1
PROOF OF LEMMA 1
From Thm. 3.4 in (Pires & Szepesvári  2012) we know that for �θ as deﬁned in equation (3)  if with
probability at least 1 − δ  ∥ �C − C∥2 ≤ ∆C and ∥�b − b∥2 ≤ ∆b hold simultaneously  then
Υ(�θ) ≤
θ′∈Rk{Υ(θ′) + 2∆C∥θ′∥α} + 2∆b.
(7)
where we use the shorthand Υ(θ′) = ∥Cθ′ − b∥2.
We can get an upper bound on the right hand side of (7) is the inﬁmum by simply choosing a feasible
θ′ = θ. We then have ∥Cθ − b∥2 = 0 and hence
θ′ {Υ(θ′) + 2∆C∥θ′∥2} ≤ 2∆C∥θ∥2
Υ(�θ) = ∥C�θ − b∥2 = ∥C
�θ − θ
∥2 ≤ 2∆C∥θ∥2 + 2∆b
Since ∥C
∥2 ≥ σmin(C)∥�θ − θ∥2 by deﬁnition of the minimum singular value  we thus have
∥�θ − θ∥2 ≤
(2∆C∥θ∥2 + 2∆b)
bh = qh − C1 = qh − ph
The mathematical deﬁnition of the ﬁnite sample estimates �Ch �bh (in matrix and vector representation)
with respect to some hypothesis h are as follows
[ �Ch]ij =
Ih(x)=i y=j
[�bh](i) = 1
m
(x y)∈Dq
Ih(x)=i −
(x y)∈Dweight
Ih(x)=i
where m = |Dq| and I is the indicator function. Ch  bh can equivalently be expressed with the
population over P for Ch and over Q for bh respectively. We now use the following concentration
Lemmas to bound the estimation errors of �C �b where we drop the subscript h for ease of notation.
Lemma 2 (Concentration of measurement matrix �
C) For ﬁnite sample estimate �C we have
∥ �C − C∥2 ≤ 2 log (2k/δ)
3(1 − β)np
2 log (2k/δ)
with probability at least (1 − δ).
Lemma 3 (Concentration of label measurements) For the ﬁnite sample estimate �b with respect to
any hypothesis h it holds that
∥�bh − bh∥2 ≤
log(2)
� �
√nq
with probability at least 1 − 2δ.
probability at least 1 − δ
2∥θ∥2 log (2k/δ)
+ ∥θ∥2
18 log (4k/δ)
36 log (2/δ)
which  considering that O( 1
√n) dominates O( 1
B.2
PROOF OF LEMMA 2
(2012). We can rewrite Ch = E(x y)∼P
eh(x)e⊤
y
where e(i) is the one-hot-encoding of index i.
Consider a ﬁnite sequence {Ψ(i)} of independent random matrices with dimension k. By dilations
lets construct another sequence of self-adjoint random matrices of {�Ψ(i)} of dimension 2k  such that
for all i
�Ψ(i) =
Ψ(i)
Ψ(i)⊤
�Ψ(i)2 =
Ψ(i)Ψ(i)⊤
Ψ(i)⊤Ψ(i)
which results in ∥�Ψ(i)∥2 = ∥Ψ(i)∥2. The dilation technique translates the initial sequence of random
theorem which states that  for a ﬁnite sequence of i.i.d. self-adjoint matrices �Ψ(i)  such that  almost
surely ∀i  E
�Ψ(i)
= 0 and ∥�Ψ(i)∥ ≤ R  then for all t ≥ 0
∥1
�Ψ(i)∥ ≤ R log (2k/δ)
3t
2ϱ2 log (2k/δ)
with probability at least 1 − δ where ϱ2 := ∥E
�Ψ2(i)
∥2  ∀i which is also ϱ2 = ∥E
Ψ2(i)
∥2  ∀i
Ψ(i)∥ ≤ R log (2k/δ)
with probability at least 1 − δ.
Now  by plugging in Ψ(i) = eh(x(i))e⊤
y(i) − C  we have E
= 0. Together with ∥�Ψ(i)∥ ≤ 2 as
well as ϱ2 = ∥E
∥2 = 1 and setting t = n  we have
3(1 − β)n +
(1 − β)n
B.3
PROOF OF LEMMA 3
Analogous to the previous section we can rewrite bh = E(x y)∈Q[eh(x)] − E(x y)∈P[eh(x)] where e(i)
∥�bh − bh∥2 ≤ ∥�qh − qh∥2 + ∥�ph − ph∥2
Consider a ﬁxed multinomial distribution characterized with probability vector of ς ∈ ∆k−1 where
∆k−1 is a k − 1 dimensional simplex. Further  consider t realization of this multinomial distribution
{ς(i)}t
i=1 where ς(i) is the one-hot-encoding of the i’th sample. Consider the empirical estimate
mean of this distribution through empirical average of the samples; �ς = 1
�(i)tς(i)  then
∥�ς − ς∥ ≤ 1
√
t +
log (1/δ)
By plugging in ς = qh  �ς = �qh with t = nq and ﬁnally {ς(i)}nq
i=1 = {eh(x(i))}(i)nq and equivalently
for ph we obtain;
with probability at least 1 − 2δ  therefore;
B.4
PROOF OF THEOREM 1
We want to ultimately bound |L(�h �
w) − L(h⋆)|. By addition and subtraction we have
w) − L(h⋆) = L(�h �
w) − Ln(�h �
w)
+ Ln(�h �
w; �w)
w; �w) − Ln(h⋆; �w)
≤0
+ Ln(h⋆; �w) − Ln(h⋆)
+ Ln(h⋆) − L(h⋆)
(9)
where n = βnp and we used optimality of �h �
w. Here (a) is the weight estimation error and (b) is the
Uniform law for bounding (b)
We bound (b) using standard results for uniform laws for uniformly
bounded functions which holds since ∥w∥∞ ≤ d∞(q||p) and ∥ℓ∥∞ ≤ 1. Since |w(y)ℓ(h(x)  y)| ≤
d∞(q||p)  ∀x  y ∈ X × Y  by deploying the McDiarmid’s inequality we then obtain that
|Ln(h) − L(h)| ≤ 2Rn(G(ℓ  H)) + d∞(q||p)
log 2
δ
where G(ℓ  H) = {gh(x  y) = w(y)ℓ(h(x)  y) : h ∈ H} and the Rademacher complexity is deﬁned
as Rn(G) := EX(i) Y (i)∼P:i∈[n]
Eσi:i∈[n]
n [suph∈H
�n
i=1 σiw(yi)ℓ(xi  h(yi))]
of the hypothesis class H (see for example Percy Liang notes on Statistical Learning Theory and
Bounding term (a)
Remember that k = |Y| is the cardinality of the ﬁnite domain of Y   or the
number of classes. Let us deﬁne �ℓ ∈ Rk with �ℓj = �n
i=1 Iy(i)=jℓ(y(i)  h(x(i))). Notice that by
deﬁnition ∥�ℓ∥1 ≤ n and ∥�ℓ∥∞ ≤ n from which it follows by Hoelder’s inequality that ∥�ℓ∥2 ≤ n.
Furthermore  we slightly abuse notation and use w to denote the k-dimensional vector with wi = w(i).
Therefore  for all h we have via the Cauchy Schwarz inequality that
|Ln(h; w) − Ln(h; �w)| = | 1
(w(y(i)) − �w(y(i)))ℓ(h(x(i))  y(i))|
≤ | 1
(w(j) − �w(j))�ℓ(j)|
≤ 1
n∥ �w − w∥2∥�ℓ∥2 ≤ ∥ �w − w∥2
≤ ∥λ�θ − θ∥2 ≤ (1 − λ)∥θ∥2 + λ∥�θ − θ∥2
(10)
|Ln(h; w) − Ln(h; �w)| ≤ (1 − λ)∥θ∥2
λ
(∥θ∥2)
G(ℓ  H) = {gh(x  y) = w(y)ℓ(h(x)  y) : h ∈ H}  under n data points and loss function ℓ we have
|Ln(h) − L(h)| ≤ 2R(G(ℓ  H)) + 2d∞(q||p) log(2/δ)
with probability at least 1 − δ
B.5
PROOF OF LEMMA 4
the random variables {(Xi  Yi)}n
w(Yj)ℓ(Yj  h(Xj))
We further deﬁne a ghost data set {(X′
i  Y ′
i )}n
1 and the corresponding ghost loss;
L′
n(h) = 1
w(Y ′
j )ℓ(Y ′
j   h(X′
Let’s deﬁne a random variable Gn := suph∈H Ln(h) − L(h). This random variable is the key to
E [Gn] = E
Ln(h) − E [L′
n(h)]
E
Ln(h) − L′
n(h)
���{(Xi  Yi)}n
and swapping the sup with the expectation
E [Gn] ≤ E
wiℓ(h(Xi)  Yi) − w′
iℓ′(h(X′
i)  Y ′
i )
In the following we use the usual symmetrizing technique through Rademacher variables {ξi}n
ξi is a uniform random variable either 1 or −1. Therefore since ℓ(h(X′)  Y ′) − ℓ′(h(X′)  Y ′) is a
ξi [w(Yi)ℓ(h(Xi)  Yi) − w(Y ′
i )ℓ′(h(X′
i )]
ξiw(Yi)ℓ(h(Xi)  Yi) + sup
−ξiw(Y ′
E [Gn] ≤ 2E
ξiw(Yi)ℓ(h(Xi)  Yi)
= 2R(G(ℓ  H))
where the right hand side is two times the Rademacher complexity of class G(ℓ  H). Consider a
sequence of Doob Martingale and ﬁltration (Uj  Fj) deﬁned on some probability space (Ω  F  Pr);
Uj := E
w(Yi)ℓ(h(Xi)  Yi)
���{(Xi  Yi)}j
Dj := Uj − Uj−1
In the following we show that each |Dj| is bounded above.
Dj =E
− E
���{(Xi  Yi)}j−1
≤ max
xj yj E
xj  yj
− min
ymax
as the solution to the maximization and xmin
ymin
the solution to the
Dj ≤ E
xmax
xmin
= E
w(Yi)ℓ(h(Xi)  Yi) + w(ymin
)ℓ(h(xmin
)  ymin
) − w(ymin
)
� ���{(xi  yi)}j−1
w(Yi)ℓ(h(Xi)  Yi) + w(ymax
)ℓ(h(xmax
)  ymax
n sup
���w(ymax
��� ≤ d∞(q||p)
The same way we can bound −Dj. Therefore the absolute value each Dj is bounded by d∞(q||p)
. In
the following we bound the conditional second moment  E
j|Fj−1





(a′)
(b′)





Let’s construct an event Cj the event that a′ is bigger than b′  and also C′
j its compliment. Therefore
for the E
� �2���{(Xi  Yi)}j−1
Cj
P(Cj|Fj−1)
+ E
C′
P(C′
j|Fj−1)
For the ﬁrs term in Eq. 11 after again introducing ghost variables X′  Y ′ we have the following upper
sup
i̸=j
w(Yi)ℓ(h(Xi)  Yi) + 1
w(Yj)ℓ(h(Xj)  Yj)

≤ E
nw(Y ′
j )ℓ(h(X′
j)  Y ′
j ) + 1
+ 1
�� 1
�2 ���{(Xi  Yi)}j−1
P(Cj|Fj−1) ≤ d(q||p)
So far we have that the ﬁrst term in Eq. 11 is bounded by d(q||p)
. Now for the second term we have
�2���{(Xi  Yi)}j−1
≤
� 1
��2
nw(Yj)
= 1
n2 P(C′
Therefore  since d(q||p) ≥ 1
≤ d(q||p)
P(Cj|Fj−1) + 1
j|Fj−1) ≤ d(q||p)
For the ﬁrst inequality  we used the fact that the loss is within [0  1] and the second one is from Eq. 12.
Since the ﬁrst term in RHS is (b′)  therefore  second moment of each Dj|Fj−1 is bounded by 2d(q||p)
Therefore for the Doob Martingale sequence of Dj we have |Dj| ≤
as well as
j D2
j|Fj−1 ≤ d(q||p)
. Using the Freedman’s inequality Freedman (1975)  we have
P
Dj = Gn − E [Gn] ≥ α
 ≤ exp
−
α2
2( d(q||p)
+ α d∞(q||p)
Moreover  if we multiply each loss with −1  it results in hypothesis class of −H which has the same
Rademacher complexity as H  due the symmetric Rademacher random variable. Let �Gn denote the
same quantity as �Gn but on −H. We use this slack variable in order to bound the absolute value of
P (Gn ≥ E [Gn] + ϵ) ≤ exp
ϵ2
+ ϵ d∞(q||p)
≤ exp
≤ δ/2
and the same bound for �Gn. By solving it for ϵ and δ we have
|Gn| ≤ 2R(G(ℓ  H)) + 2d∞(q||p) log(2/δ)
B.6
GENERALIZATION FOR FINITE HYPOTHESIS CLASSES
We bound (b) by ﬁrst noting that w(Y )ℓ(Y  h(X)) satisﬁes the Bernstein conditions so that Bernstein’s
EP[w(Y )] = 1
EP[w(Y )2] = d(q||p)
σ2
P(w(Y )) = d(q||p) − 1
(12)
by deﬁnition. Because we assume ℓ ≤ 1  we directly have
EP
w(Y )2l2(Y  h(X))
≤ EP
= d(q||p)
(13)
Since we have a bound on the second moment of weighted loss while its ﬁrst moment is L(h) we can
apply Bernstein’s inequality to obtain for any ﬁxed h that
|Ln(h) − L(h)| ≤ 2d∞(q||p) log( 2
δ )
3n
2 (d(q||p) − L(h)2) log( 2
|Ln(h) − L(h)| ≤ 2d∞(q||p) log( 2|H|
2 (d(q||p)) log( 2|H|
The second moment of the importance weighted loss EP
ω(Y )2ℓ2(h(X)  Y )
given a h ∈ H′ can
be bounded for general α ≥ 0  potentially leading to smaller values than d(q||p):
ω2
Y ℓ2(Y  h(X))
p(i)q2(i)
p2(i)EX∼p(X|Y =i)
ℓ2(h(X)  X)
α q(i)
p(i)q(i)
α−1
α EX∼p(X|Y =i)
ℓ2(h(X)  i)
� k
q(i)q(i)α
p(i)α
α � k
q(i)EX∼p(X|Y =i)
� 2α
� α−1
l2(h(X)  i)
EX∼p(X|Y =i)
� α+1
�1− 1
�1+ 1
inequality and the fact that the loss is in [0  1] as well as the fact that the exponentiation function is
convex in this region. Moreover  since 1 + 1
α ≥ 1 and upper bound for the loss square  l(·  ·)2 ≤ 1
w(Y )2ℓ2(h(X)  Y )
q(i)EX∼P|Y =i
B.7
SLIGHT DRIFT FROM THE LABEL SHIFT
Drift in label shift assumption:
If the label shift approximation is slightly violated  we expect
the generalizing bound to deviate from the statement in the Theorem. 1. Deﬁne de(q||p) :=
E(X Y )∼Q
true importance weights ω(x  y) := q(x y)
p(x y)  the RLLS  with high probability generalizes as;
label  we have p(x|y) ≃ q(x|y)  resulting importance weight ω(x  y) := q(x y)
p(x y) for each covariate
w; ω) − L(h⋆; ω) = L(�h �
w; ω) − L(�h �
+ L(�h �
+ L(h⋆) − L(h⋆; ω)
where the desired excess risk is deﬁned with respect to ω. The differences between Eq. 15 and Eq. 9
are in a new term (c) as well as term (a). The term (b) remains untouched.
Bound on term (c)
For any h  the two contributing components in (c)  i.e.  L(h; ω) and L(h) are
L(h) = E(X Y )∼P [w(Y )ℓ(Y  h(X))]   and L(h; ω) = E(X Y )∼Q [ℓ(Y  h(X))] = E(X Y )∼P [ω(X  Y )ℓ(Y  h(X))]
L(h; ω) − L(h) = E(X Y )∼P [(ω(X  Y ) − w(Y )) ℓ(Y  h(X))]
= E(X Y )∼P
��q(X  Y )
p(X  Y ) − q(Y )
p(Y )
ℓ(Y  h(X))
= E(X Y )∼Q
1 − p(X|Y )
≤ de(q||p) := E(X Y )∼Q
Where in the last inequality we deploy Cauchy Schwarz inequality as well as loss is in [0  1] and hold
for h ∈ H. It is worth noting that the expectation in de(q||p) is with respect to Q and does not blow
up if the supports of P and Q do not match
Bound on term (a)
For any h ∈ H  similar to the derivation in Eq. 10 we have
|Ln(h) − Ln(h; �w)| ≤ ∥ �w − w∥2 ≤ ∥λ�θ − θ∥2 ≤ (1 − λ)∥θ∥2 + λ∥�θ − θ∥2
classiﬁer h0
qh0(Y = i) =
q(h(X) = i|Y = j)q(j)
(q(h0(X) = i|Y = j) − p(h0(X) = i|Y = j)) q(j) +
p(h0(X) = i|Y = j)q(j)
p(h0(X) = i  Y = j)wj
p(h0(X) = i)
where p(h0(X) = i) = q(h0(X) = i)  resulting;
qh0 = be + Cw
where we drop the h0 in both be and C. Both the confusion matrix C and the label distribution qh0
on the target for the black box hypothesis h0 are unknown and we are instead only given access to
ﬁnite sample estimates �Ch0  �qh0. Similar to previous analysis we have
with corresponding ﬁnite sample quantity �b = �q − �C1. Similarly to the analysis when there was
no violation in label shift assumption  we have Υ(θ′) = ∥Cθ′ − b − be∥2 and the solution to Eq. 3
θ′∈Rk{Υ(θ′) + 2∆C∥θ′∥α} + 2∆b + 2∥be∥2.
(16)
We can simplify the upper bound by setting θ′ = θ. We then have
Υ(�θ) = ∥C�θ − b − be∥2 = ∥C
∥2 ≤ 2∆C∥θ∥2 + 2∆b + 2∥be∥2 ≤ 2∆C∥θ∥2 + 2∆b + 2de(q||p)
(2∆C∥θ∥2 + 2∆b + 2de(q||p))","1 I NTRODUCTION
When machine learning models are employed “in the wild”  the distribution of the data of inter-
est(target distribution) can be signiﬁcantly shifted compared to the distribution of the data on which
the model was trained ( source distribution). In many cases  the publicly available large-scale datasets
with which the models are trained do not represent and reﬂect the statistics of a particular dataset
of interest. This is for example relevant in managed services on cloud providers used by clients in
different domains and regions  or medical diagnostic tools trained on data collected in a small number
of hospitals and deployed on previously unobserved populations and time frames.
Covariate Shift Label Shift
p(x)6=q(x)
p(yjx) =q(yjx)p(y)6=q(y)
p(xjy) =q(xjy)There are various ways to approach distribution shifts between
a source data distribution Pand a target data distribution Q. If
we denote input variables as xand output variables as y  we
consider the two following settings: (i) Covariate shift  which
assumes that the conditional output distribution is invariant: p(yjx) =q(yjx)between source and
target distributions  but the source distribution p(x)changes. (ii) Label shift  where the conditional
input distribution is invariant: p(xjy) =q(xjy)andp(y)changes from source to target. In the
following  we assume that both input and output variables are observed in the source distribution
whereas only input variables are available from the target distribution.
While covariate shift has been the focus of the literature on distribution shifts to date  label-shift
scenarios appear in a variety of practical machine learning problems and warrant a separate","et al. (2018) can be found in Section A.0.1.
After artiﬁcially shifting the label distribution in one of the source and target sets  we then follow
algorithm 1  where we choose the black box predictor h0to be a two-layer fully connected neural
network trained on (shifted) source dataset. Note that any black box predictor could be employed
here  though the higher the accuracy  the more likely weight estimation will be precise. Therefore
we use different shifted source data to get (corrupted) black box predictor across experiments. If not
noted h0is trained using uniform data.
In order to compute b!=1+bin Eq. (3)  we call a built-in solver to directly solve the low dimensional
problem minkbC bbk2+ Ckk2where we empirically observer that 0:01times of the true C
yields in a better estimator on various levels of label shift pre-computed beforehand. It is worth
noting that 0:001makes the theoretical bound in Lemma. 1 O(1=0:01)times bigger. We thus treat it
as a hyperparameter that can be chosen using standard cross validation methods. Finally  we train
6
a classiﬁer on the source samples weighted by b!  where we use a two-layer fully connected neural
network for MNIST and a ResNet-18 (He et al.  2016) for CIFAR10.
We sample 20 datasets with the label distributions for each shift parameter. to evaluate the empirical
mean square estimation error (MSE) and variance of the estimated weights Ekbw wk2
2and the
predictive accuracy on the target set. We use these measures to compare our procedure with the
black box shift learning method (BBSL) in Lipton et al. (2018). Notice that although KMM methods
(Zhang et al.  2013) would be another standard baseline to compare with  it is not scalable to large
sample size regimes for np;nqaboven= 8000 as mentioned by Lipton et al. (2018).
3.1 W EIGHT ESTIMATION AND PREDICTIVE PERFORMANCE FOR SOURCE SHIFT
In this set of experiments on the CIFAR10 dataset  we illustrate our weight estimation and prediction
performance for Tweak-One source shifts and compare it with BBSL . For this set of experiments  we
set the number of data points in both source and target set to 10000 and sample from the two pools
without replacement.
Figure 2 illustrates the weight estimation alongside ﬁnal classiﬁcation performance for Minority-Class
source shift of CIFAR10. We created shifts with >0:5. We use a ﬁxed black-box classiﬁer that is
trained on biased source data  with tweak-one = 0:5. Observe that the MSE in weight estimation is
relatively large and RLLS outperforms BBSL as the number of minority classes increases. As the
shift increases the performance for all methods deteriorates. Furthermore  Figure 2 (b) illustrates
how the advantage of RLLS over the unweighted classiﬁer increases as the shift increases. Across
all shifts  the RLLS based classiﬁer yields higher accuracy than the one based on BBSL . Results for
MNIST can be found in Section A.1.
(a)
(b)
Figure 2: (a) Mean squared error in estimated weights and (b) accuracy on CIFAR10 for tweak-one
shifted source and uniform target with h0trained using tweak-one shifted source data.
3.2 W EIGHT ESTIMATION AND PREDICTIVE PERFORMANCE FOR TARGET SHIFT
In this section  we compare the predictive performances between a classiﬁer trained on unweighted
source data and the classiﬁers trained on weighted loss obtained by the RLLS and BBSL procedure
on CIFAR10. The target set is shifted using the Dirichlet shift with parameters = [0:01;0:1;1;10].
The number of data points in both source and target set is 10000 .
In the case of target shifts  larger shifts actually make the predictive task easier  such that even a
constant majority class vote would give high accuracy. However it would have zero accuracy on
all but one class. Therefore  in order to allow for a more comprehensive performance between
the methods  we also compute the macro-averaged F-1 score by averaging the per-class quantity
2(precisionrecall )=(precision +recall )over all classes. For a class i precision is the percentage
of correct predictions among all samples predicted to have label i  while recall is the proportion of
correctly predicted labels over the number of samples with true label i. This measure gives higher
weight to the accuracies of minority classes which have no effect on the total accuracy.
Figure 3 depicts the MSE of the weight estimation (a)  the corresponding performance comparison on
accuracy (b) and F-1 score (c). Recall that the accuracy performance for low shifts is not comparable
with standard CIFAR10 benchmark results because of the overall lower sample size chosen for the
comparability between shifts. We can see that in the large target shift case for = 0:01  the F-1
7
(c)
Figure 3: (a) Mean squared error in estimated weights  (b) accuracy and (c) F-1 score on CIFAR10
for uniform source and Dirichlet shifted target. Smaller corresponds to bigger shift.
score for BBSL and the unweighted classiﬁer is rather low compared to RLLS while the accuracy is
high. As mentioned before  the reason for this observation and why in Figure 3 (b) the accuracy is
higher when the shift is larger  is that the predictive task actually becomes easier with higher shift.
3.3 R EGULARIZED WEIGHTS IN THE LOW SAMPLE REGIME FOR SOURCE SHIFT
In the following  we present the average accuracy of RLLS in Figure 4 as a function of the number of
target samples nqfor different values of for smallnq. Here we ﬁx the sample size in the source set
tonp= 1000 and investigate a Minority-Class source shift with ﬁxed p= 0:01and ﬁve minority
classes.
A motivation to use intermediate is discussed in Section 2.2  as in equation (4)may be chosen
according to max;min. In practice  since maxis just an upper bound on the true amount of shift
kk2  in some cases should in fact ideally be 0when1
2max(min 1pnq)2nq1
kk2(min 1pnq)2.
Thus for target sample sizes nqthat are a little bit above the threshold (depending on the certainty of
the belief how close to maxthe norm of the shift is believed to be)  it could be sensible to use an
intermediate value 2(0;1).
Figure 4: Performance on MNIST for Minority-Class shifted source and uniform target with various
target sample size and using (a) better predictor h0trained on tweak-one shifted source with
= 0:2  (b) neutral predictor h0with= 0:5and (c) corrupted predictor h0with= 0:8.
Figure 4 suggests that unweighted samples (red) yield the best classiﬁer for very few samples nq
while for 10nq500an intermediate 2(0;1)(purple) has the highest accuracy and for
nq>1000   the weight estimation is certain enough for the fully weighted classiﬁer (yellow) to have
the best performance (see also the corresponding data points in Figure 2). The unweighted BBSL
classiﬁer is also shown for completeness. We can conclude that regularizing the inﬂuence of the
estimated weights allows us to adjust to the uncertainty on importance weights and generalize well
for a wide range of target sample sizes.
Furthermore  the different plots in Figure 4 correspond to black-box predictors h0for weight
estimation which are trained on more or less corrupted data  i.e. have a better or worse conditioned
8
confusion matrix. The fully weighted methods with = 1achieve the best performance faster with a
better trained black-box classiﬁer (a)  while it takes longer for it to improve with a corrupted one (c).
Furthermore  this reﬂects the relation between eigenvalue of confusion matrix minand target sample
sizenqin Theorem 1. In other words  we need more samples from the target data to compensate a
bad predictor in weight estimation. So the generalization error decreases faster with an increasing
number of samples for good predictors.
In summary  our RLLS method outperforms BBSL in all settings for the common image datasets
MNIST and CIFAR10 to varying degrees. In general  signiﬁcant improvements compared to BBSL
can be observed for large shifts and the low sample regime. A note of caution is in order: comparison
between the two methods alone might not always be meaningful. In particular  there are cases when
the estimator trained on unweighted samples outperforms both RLLS and BBSL . Our extensive
experiments for many different shifts  black box classiﬁers and sample sizes do not allow for a ﬁnal
conclusive statement about how weighting samples using our estimator affects predictive results for
real-world data in general  as it usually does not fulﬁll the label-shift assumptions.
4 R ELATED WORK
The covariate and label shift assumptions follow naturally when viewing the data generating process
as a causal or anti-causal model (Schölkopf et al.  2012): With label shift  the label Ycauses the
inputX(that is Xis not a causal parent of Y  hence 'anti-causal') and the causal mechanism that
generatesXfromYis independent of the distribution of Y. A long line of work has addressed the
reverse causal setting where XcausesYand the conditional distribution of YgivenXis assumed to
be constant. This assumption is sensible when there is reason to believe that there is a true optimal
mapping from XtoYwhich does not change if the distribution of Xchanges. Mathematically this
scenario corresponds to the covariate shift assumption.
Among the various methods to correct for covariate shift  the majority uses the concept of importance
weightsq(x)=p(x)(Zadrozny  2004; Cortes et al.  2010; Cortes & Mohri  2014; Shimodaira  2000)
which are unknown but can be estimated for example via kernel embeddings (Huang et al.  2007;
Gretton et al.  2009; 2012; Zhang et al.  2013; Zaremba et al.  2013) or by learning a binary discrimi-
native classiﬁer between source and target (Lopez-Paz & Oquab  2016; Liu et al.  2017). A minimax
approach that aims to be robust to the worst-case shared conditional label distribution between
source and target has also been investigated (Liu & Ziebart  2014; Chen et al.  2016). Sanderson
& Scott (2014); Ramaswamy et al. (2016) formulate the label shift problem as a mixture of the
class conditional covariate distributions with unknown mixture weights. Under the pairwise mutual
irreducibility (Scott et al.  2013) assumption on the class conditional covariate distributions  they
deploy the Neyman-Pearson criterion (Blanchard et al.  2010) to estimate the class distribution q(y)
which also investigated in the maximum mean discrepancy framework (Iyer et al.  2014).
Common issues shared by these methods is that they either result in a massive computational burden
for large sample size problems or cannot be deployed for neural networks. Furthermore  importance
weighting methods such as (Shimodaira  2000) estimate the density (ratio) beforehand  which is a
difﬁcult task on its own when the data is high-dimensional. The resulting generalization bounds
based on importance weighting methods require the second order moments of the density ratio
(q(x)=p(x))2to be bounded  which means the bounds are extremely loose in most cases (Cortes
et al.  2010).
Despite the wide applicability of label shift  approaches with global guarantees in high dimensional
data regimes remain under-explored. The correction of label shift mainly requires to estimate the
importance weights q(y)=p(y)over the labels which typically live in a very low-dimensional space.
Bayesian and probabilistic approaches are studied when a prior over the marginal label distribution is
assumed (Storkey  2009; Chan & Ng  2005). These methods often need to explicitly compute the
posterior distribution of yand suffer from the curse of dimensionality. Recent advances as in Lipton
et al. (2018) have proposed solutions applicable large scale data. This approach is related to Buck
et al. (1966); Forman (2008); Saerens et al. (2002) in the low dimensional setting but lacks guarantees
for the excess risk.
Existing generalization bounds have historically been mainly developed for the case when P=Q
(see e.g. Vapnik (1999); Bartlett & Mendelson (2002); Kakade et al. (2009); Wainwright (2019)).
9
Ben-David et al. (2010) provides theoretical analysis and generalization guarantees for distribution
shifts when the H-divergence between joint distributions is considered  whereas Crammer et al. (2008)
proves generalization bounds for learning from multiple sources. For the covariate shift setting
Cortes et al. (2010) provides a generalization bound when q(x)=p(x)is known which however does
not apply in practice. To the best of our knowledge our work is the ﬁrst to give generalization bounds
for the label shift scenario.
5 D ISCUSSION
In this work  we establish the ﬁrst generalization guarantee for the label shift setting and propose an
importance weighting procedure for which no prior knowledge of q(y)=p(y)is required. Although
RLLS is inspired by BBSL   it leads to a more robust importance weight estimator as well as general-
ization guarantees in particular for the small sample regime  which BBSL does not allow for. RLLS
is also equipped with a sample-size-dependent regularization technique and further improves the
classiﬁer in both regimes.
We consider this work a necessary step in the direction of solving shifts of this type  although the
label shift assumption itself might be too simpliﬁed in the real world. In future work  we plan to also
study the setting when it is slightly violated. For instance  xin practice cannot be solely explained
by the wanted label y  but may also depend on attributes zwhich might not be observable. In the
disease prediction task for example  the symptoms might not only depend on the disease but also on
the city and living conditions of its population. In such a case  the label shift assumption only holds
in a slightly modiﬁed sense  i.e. P(XjY=y;Z=z) =Q(XjY=y;Z=z). If the attributes Zare
observed  then our framework can readily be used to perform importance weighting.
Furthermore  it is not clear whether the ﬁnal predictor is in fact “better” or more robust to shifts
just because it achieves a better target accuracy than a vanilla unweighted estimator. In fact  there
is a reason to believe that under certain shift scenarios  the predictor might learn to use spurious
correlations to boost accuracy. Finding a procedure which can both learn a robust model and achieve
high accuracies on new target sets remains to be an ongoing challenge. Moreover  the current choice
of regularization depends on the number of samples rather than data-driven regularization which is
more desirable.
An important direction towards active learning for the same disease-symptoms scenario is when
we also have an expert for diagnosing a limited number of patients in the target location. Now the
question is which patients would be most 'useful' to diagnose to obtain a high accuracy on the
entire target set? Furthermore  in the case of high risk  we might be able to choose some of the
patients for further medical diagnosis or treatment  up to some varying cost. We plan to extend the
current framework to the active learning setting where we actively query the label of certain x’s
(Beygelzimer et al.  2009) as well as the cost-sensitive setting where we also consider the cost of
querying labels (Krishnamurthy et al.  2017).
Consider a realizable and over-parameterized setting  where there exists a deterministic mapping
fromxtoy  and also suppose a perfect interpolation of the source data with a minimum proper
norm is desired. In this case  weighting the samples in the empirical loss might not alter the trained
classiﬁer (Belkin et al.  2018). Therefore  our results might not directly help the design of better
classiﬁers in this particular regime. However  for the general overparameterized settings  it remains
an open problem of how the importance weighting can improve the generalization. We leave this
study for future work.
6 A CKNOWLEDGEMENT
K. Azizzadenesheli is supported in part by NSF Career Award CCF-1254106. This research has been
conducted when the ﬁrst author was a visiting researcher at Caltech. Anqi Liu is supported in part
by DOLCIT Postdoctoral Fellowship at Caltech and Caltech’s Center for Autonomous Systems and
Technologies. Fan Yang is supported by the Institute for Theoretical Studies ETH Zurich and the Dr.
Max Rössler and the Walter Haefner Foundation. A. Anandkumar is supported in part by Microsoft
Faculty Fellowship  Google faculty award  Adobe grant  NSF Career Award CCF- 1254106  and
AFOSR YIP FA9550-15-1-0221.
10
REFERENCES
Animashree Anandkumar  Daniel Hsu  and Sham M Kakade. A method of moments for mixture
models and hidden markov models. In Conference on Learning Theory   pp. 33–1  2012.
Kamyar Azizzadenesheli  Alessandro Lazaric  and Animashree Anandkumar. Reinforcement learning
of pomdps using spectral methods. arXiv preprint arXiv:1602.07764   2016.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research   3(Nov):463–482  2002.
Mikhail Belkin  Daniel Hsu  Siyuan Ma  and Soumik Mandal. Reconciling modern machine learning
and the bias-variance trade-off. arXiv preprint arXiv:1812.11118   2018.
Shai Ben-David  John Blitzer  Koby Crammer  Alex Kulesza  Fernando Pereira  and Jennifer Wortman
Vaughan. A theory of learning from different domains. Machine learning   79(1-2):151–175  2010.
Alina Beygelzimer  Sanjoy Dasgupta  and John Langford. Importance weighted active learning. In
Proceedings of the 26th annual international conference on machine learning   pp. 49–56. ACM
2009.
Gilles Blanchard  Gyemin Lee  and Clayton Scott. Semi-supervised novelty detection. Journal of
Machine Learning Research   11(Nov):2973–3009  2010.
AA Buck  JJ Gart  et al. Comparison of a screening test and a reference test in epidemiologic studies.
ii. a probabilistic model for the comparison of diagnostic tests. American Journal of Epidemiology
83(3):593–602  1966.
Yee Seng Chan and Hwee Tou Ng. Word sense disambiguation with distribution estimation. In IJCAI
volume 5  pp. 1010–5  2005.
Xiangli Chen  Mathew Monfort  Anqi Liu  and Brian D Ziebart. Robust covariate shift regression. In
Artiﬁcial Intelligence and Statistics   pp. 1270–1279  2016.
Corinna Cortes and Mehryar Mohri. Domain adaptation and sample bias correction theory and
algorithm for regression. Theoretical Computer Science   519:103–126  2014.
Corinna Cortes  Yishay Mansour  and Mehryar Mohri. Learning bounds for importance weighting.
InAdvances in neural information processing systems   pp. 442–450  2010.
Koby Crammer  Michael Kearns  and Jennifer Wortman. Learning from multiple sources. Journal of
Machine Learning Research   9(Aug):1757–1774  2008.
George Forman. Quantifying counts and costs via classiﬁcation. Data Mining and Knowledge
Discovery   17(2):164–206  2008.
David A Freedman. On tail probabilities for martingales. the Annals of Probability   pp. 100–118
1975.
Arthur Gretton  Alexander J Smola  Jiayuan Huang  Marcel Schmittfull  Karsten M Borgwardt  and
Bernhard Schölkopf. Covariate shift by kernel mean matching. 2009.
Arthur Gretton  Karsten M Borgwardt  Malte J Rasch  Bernhard Schölkopf  and Alexander Smola. A
kernel two-sample test. Journal of Machine Learning Research   13(Mar):723–773  2012.
Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition
pp. 770–778  2016.
Daniel Hsu  Sham M Kakade  and Tong Zhang. A spectral algorithm for learning hidden markov
models. Journal of Computer and System Sciences   78(5):1460–1480  2012.
Jiayuan Huang  Arthur Gretton  Karsten M Borgwardt  Bernhard Schölkopf  and Alex J Smola.
Correcting sample selection bias by unlabeled data. In Advances in neural information processing
systems   pp. 601–608  2007.
11
Arun Iyer  Saketha Nath  and Sunita Sarawagi. Maximum mean discrepancy for class ratio estimation:
Convergence bounds and kernel selection. In International Conference on Machine Learning   pp.
530–538  2014.
Sham M Kakade  Karthik Sridharan  and Ambuj Tewari. On the complexity of linear prediction: Risk
bounds  margin bounds  and regularization. In Advances in neural information processing systems
pp. 793–800  2009.
Akshay Krishnamurthy  Alekh Agarwal  Tzu-Kuo Huang  Hal Daume III  and John Langford. Active
learning for cost-sensitive classiﬁcation. arXiv preprint arXiv:1703.01014   2017.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report  Citeseer  2009.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/ .
Zachary C Lipton  Yu-Xiang Wang  and Alex Smola. Detecting and correcting for label shift with
black box predictors. arXiv preprint arXiv:1802.03916   2018.
Anqi Liu and Brian Ziebart. Robust classiﬁcation under sample selection bias. In Advances in neural
information processing systems   pp. 37–45  2014.
Song Liu  Akiko Takeda  Taiji Suzuki  and Kenji Fukumizu. Trimmed density ratio estimation. In
Advances in Neural Information Processing Systems   pp. 4518–4528  2017.
David Lopez-Paz and Maxime Oquab. Revisiting classiﬁer two-sample tests. arXiv preprint
arXiv:1610.06545   2016.
Geoffrey McLachlan. Discriminant analysis and statistical pattern recognition   volume 544. John
Wiley & Sons  2004.
Bernardo Avila Pires and Csaba Szepesvári. Statistical linear estimation with penalized estimators:
an application to reinforcement learning. arXiv preprint arXiv:1206.6444   2012.
Harish Ramaswamy  Clayton Scott  and Ambuj Tewari. Mixture proportion estimation via kernel
embeddings of distributions. In International Conference on Machine Learning   pp. 2052–2060
2016.
Marco Saerens  Patrice Latinne  and Christine Decaestecker. Adjusting the outputs of a classiﬁer to
new a priori probabilities: a simple procedure. Neural computation   14(1):21–41  2002.
Tyler Sanderson and Clayton Scott. Class proportion estimation with application to multiclass
anomaly rejection. In Artiﬁcial Intelligence and Statistics   pp. 850–858  2014.
Bernhard Schölkopf  Dominik Janzing  Jonas Peters  Eleni Sgouritsa  Kun Zhang  and Joris Mooij.
On causal and anticausal learning. arXiv preprint arXiv:1206.6471   2012.
Clayton Scott  Gilles Blanchard  and Gregory Handy. Classiﬁcation with asymmetric label noise:
Consistency and maximal denoising. In Conference On Learning Theory   pp. 489–511  2013.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihood function. Journal of statistical planning and inference   90(2):227–244  2000.
Amos Storkey. When training and test sets are different: characterizing learning transfer. Dataset
shift in machine learning   pp. 3–28  2009.
Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational
mathematics   12(4):389–434  2012.
Vladimir Naumovich Vapnik. An overview of statistical learning theory. IEEE transactions on neural
networks   10(5):988–999  1999.
M. J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint . Cambridge University
Press  2019.
12
Yiming Ying. Mcdiarmid’s inequalities of bernstein and bennett forms. City University of Hong
Kong   2004.
Bianca Zadrozny. Learning and evaluating classiﬁers under sample selection bias. In Proceedings of
the twenty-ﬁrst international conference on Machine learning   pp. 114. ACM  2004.
Wojciech Zaremba  Arthur Gretton  and Matthew Blaschko. B-test: A non-parametric  low variance
kernel two-sample test. In Advances in neural information processing systems   pp. 755–763  2013.
Kun Zhang  Bernhard Schölkopf  Krikamol Muandet  and Zhikun Wang. Domain adaptation under
target and conditional shift. In International Conference on Machine Learning   pp. 819–827  2013.
13
A M ORE EXPERIMENTAL RESULTS
This section contains more experiments that provide more insights about in which settings the
advantage of using RLLS vs.BBSL are more or less pronounced.
A.0.1 CIFAR10 E XPERIMENTS UNDER TWEAK -ONE SHIFT AND DIRICHLET SHIFT
Here we compare weight estimation performance between RLLS and BBSL for different types of
shifts including the Tweak-one Shift   for which we randomly choose one class  e.g. iand setp(i) =
while all other classes are distributed evenly. Figure 5 depicts the the weight estimation performance
ofRLLS compared to BBSL for a variety of values of and. Note that larger shifts correspond to
smallerand larger. In general  one observes that our RLLS estimator has smaller MSE and that
as the shift increases  the error of both methods increases. For tweak-one shift we can additionally
see that as the shift increases  RLLS outperforms BBSL more and more as both in terms of bias and
variance.
Figure 5: Comparing MSE of estimated weights using BBSL and RLLS on CIFAR10 with (a)
tweak-one shift on source and uniform target  and (b) Dirichlet shift on source and uniform target. h0
is trained using the same source shifted data respectively.
A.1 MNIST E XPERIMENTS UNDER MINORITY -CLASS SOURCE SHIFTS FOR DIFFERENT
VALUES OF p
In order to show weight estimation and classiﬁcation performance under different level of label
shifts  we include several additional sets of experiments here in the appendix. Figure 6 shows the
weight estimation error and accuracy comparison under a minority-class shift with p = 0.001. The
training and testing sample size is 10000 examples in this case. We can see that whenever the weight
estimation of RLLS is better  the accuracy is also better  except in the four classes case when both
methods are bad in weight estimation.
Figure 6: (a) Mean squared error in estimated weights and (b) accuracy on MNIST for minority-class
shifted source and uniform target with p = 0.001.
Figure 7 demonstrates another case in minority-class shift when p= 0:01. The black-box classiﬁer is
the same two-layers neural network trained on a biased source data set with tweak-one = 0:5. We
observe that when the number of minority class is small like 1 or 2  the weight estimation is similar
14
between two methods  as well as in the classiﬁcation accuracy. But when the shift get larger  the
weights are worse and the performance in accuracy decreases  getting even worse than the unweighted
classiﬁer.
Figure 7: (a) Mean squared error in estimated weights and (b) accuracy on MNIST for minority-class
shifted source and uniform target with p = 0.01  with h0trained on tweak-one shifted source data.
Figure 8 illustrates the weight estimation alongside ﬁnal classiﬁcation performance for Minority-Class
source shift of MNIST. We use 1000 training and testing data. We created large shifts of three or
more minority classes with p= 0:005. We use a ﬁxed black-box classiﬁer that is trained on biased
source data  with tweak-one = 0:5. Observe that the MSE in weight estimation is relatively large
and RLLS outperforms BBSL as the number of minority classes increases. As the shift increases the
performance for all methods deteriorates. Furthermore  Figure 8 (b) illustrates how the advantage
ofRLLS over the unweighted classiﬁer increases as the shift increases. Across all shifts  the RLLS
based classiﬁer yields higher accuracy than the one based on BBSL .
Figure 8: (a) Mean squared error in estimated weights and (b) accuracy on MNIST for minority-class
shifted source and uniform target with p = 0.005  with h0trained on tweak-one shifted source data.
A.2 CIFAR10 E XPERIMENT UNDER DIRICHLET SOURCE SHIFTS
Figure 9 illustrates the weight estimation alongside ﬁnal classiﬁcation performance for Dirichlet
source shift of CIFAR10 dataset. We use 10000 training and testing data in this experiment  following
the way we generate shift on source data. We train h0with tweak-one shifted source data with
= 0:5. The results show that importance weighting in general is not helping the classiﬁcation in
this relatively large shift case  because the weighted methods  including true weights and estimated
weights  are similar in accuracy with the unweighted method.
A.3 MNIST E XPERIMENT UNDER DIRICHLET SHIFT WITH LOW TARGET SAMPLE SIZE
We show the performance of classiﬁer with different regularization under a Dirichlet shift with
= 0:5in Figure 10. The training has 5000 examples in this case. We can see that in this low target
sample case  = 1only take over after several hundreds example  while some value between 0
and 1 outperforms it at the beginning. Similar as in the paper  we use different black-box classiﬁer
that is corrupted in different levels to show the relation between the quality of black-box predictor
and the necessary target sample size. We use biased source data with tweak-one = 0;0:2;0:6to
train the black-box classiﬁer. We see that we need more target samples for the fully weighted version
= 1to take over for a more corrupted black-box classiﬁer.
15
Figure 9: (a) Mean squared error in estimated weights and (b) accuracy on CIFAR10 for Dirichlet
shifted source and uniform target  with h0trained on tweak-one shifted source data.
Figure 10: Performance on MNIST for Dirichlet shifted source and uniform target with various target
sample size and using (a) better predictor  (b) neutral predictor and (c) corrupted predictor.
B P ROOFS
B.1 P ROOF OF LEMMA 1
From Thm. 3.4 in (Pires & Szepesvári  2012) we know that for bas deﬁned in equation (3)  if with
probability at least 1  kbC Ck2Candkbb bk2bhold simultaneously  then
(b)inf
02Rkf(0) + 2Ck0kg+ 2b: (7)
where we use the shorthand (0) =kC0 bk2.
We can get an upper bound on the right hand side of (7)is the inﬁmum by simply choosing a feasible
0=. We then havekC bk2= 0and hence
inf
0f(0) + 2Ck0k2g2Ckk2
as a consequence
(b) =kCb bk2=kC
b 
k22Ckk2+ 2b
SincekC
k2min(C)kb k2by deﬁnition of the minimum singular value  we thus have
kb k21
min(2Ckk2+ 2b)
Let us ﬁrst notice that
bh=qh C1=qh ph
16
The mathematical deﬁnition of the ﬁnite sample estimates bCh;bbh(in matrix and vector representation)
with respect to some hypothesis hare as follows
[bCh]ij=1
(1 )npX
(x;y)2DpIh(x)=i;y=j
[bbh](i) =1
mX
(x;y)2DqIh(x)=i 1
(x;y)2Dweight
pIh(x)=i
wherem=jDqjandIis the indicator function. Ch;bhcan equivalently be expressed with the
population over PforChand over Qforbhrespectively. We now use the following concentration
Lemmas to bound the estimation errors of bC;bbwhere we drop the subscript hfor ease of notation.
Lemma 2 (Concentration of measurement matrix bC)For ﬁnite sample estimate bCwe have
kbC Ck22 log (2k=)
3(1 )np+s
2 log (2k=)
(1 )np
with probability at least (1 ).
Lemma 3 (Concentration of label measurements) For the ﬁnite sample estimate bbwith respect to
any hypothesis hit holds that
kbbh bhk22p
log(2) p
log(1=)p
(1 )np+p
pnq!
with probability at least 1 2.
By Lemma. 2 for concentration of Cand Lemma. 3 for concentration of bwe now have with
probability at least 1 
min
2kk2log (2k=)
(1 )np+kk2s
18 log (4k=)
+s
36 log (2=)
nq+s
(1 )np!
which  considering that O(1pn)dominatesO(1
n)  yields the statement of the Lemma 1.
B.2 P ROOF OF LEMMA 2
We prove this lemma using the theorem 1.4[Matrix Bernstein] and Dilations technique from Tropp
(2012). We can rewrite Ch=E(x;y)P
eh(x)e>
y
wheree(i)is the one-hot-encoding of index i.
Consider a ﬁnite sequence f	(i)gof independent random matrices with dimension k. By dilations
lets construct another sequence of self-adjoint random matrices of fe	(i)gof dimension 2k  such that
for alli
e	(i) =0 	(i)
(i)>0
therefore
e	(i)2=
(i)	(i)>0
0 	(i)>	(i)
(8)
which results inke	(i)k2=k	(i)k2. The dilation technique translates the initial sequence of random
matrices to the sequence of random self-adjoint matrices where we can apply the Matrix Bernstein
theorem which states that  for a ﬁnite sequence of i.i.d. self-adjoint matrices e	(i)  such that  almost
surely8i;Eh
e	(i)i
= 0andke	(i)kR  then for all t0
k1
ttX
i=1e	(i)kRlog (2k=)
3t+r
2%2log (2k=)
t
17
with probability at least 1 where%2:=kEh
e	2(i)i
k2;8iwhich is also %2=kE
2(i)
k2;8i
due to Eq. 8. Therefore  thanks to the dilation trick and theorem 1.4[Matrix Bernstein] in Tropp
(2012)
i=1	(i)kRlog (2k=)
with probability at least 1 .
Now  by plugging in 	(i) =eh(x(i))e>
y(i) C  we have Eh
= 0. Together withke	(i)k2as
well as%2=kE
k2= 1and settingt=n  we have
3(1 )n+s
(1 )n
B.3 P ROOF OF LEMMA 3
The proof of this lemma is mainly based on a special case of and appreared at proposition 6 in
Azizzadenesheli et al. (2016)  Lemma F.1 in Anandkumar et al. (2012) and Proposition 19 of Hsu
et al. (2012).
Analogous to the previous section we can rewrite bh=E(x;y)2Q[eh(x)] E(x;y)2P[eh(x)]wheree(i)
is the one-hot-encoding of index i. Note that (dropping the subscript h) we have
kbbh bhk2kbqh qhk2+kbph phk2
We now bound both estimates of probability vectors separately.
Consider a ﬁxed multinomial distribution characterized with probability vector of &2k 1where
k 1is ak 1dimensional simplex. Further  consider trealization of this multinomial distribution
f&(i)gt
i=1where&(i)is the one-hot-encoding of the i’th sample. Consider the empirical estimate
mean of this distribution through empirical average of the samples; b&=1
tP(i)t&(i)  then
kb& &k1p
t+r
log (1=)
By plugging in &=qh b&=bqhwitht=nqand ﬁnallyf&(i)gnq
i=1=feh(x(i))g(i)nqand equivalently
forphwe obtain;
kbbh bhk2 p
+
1p
(1 )np+1pnq!
with probability at least 1 2  therefore;
resulting in the statement in the Lemma 3.
B.4 P ROOF OF THEOREM 1
We want to ultimately bound jL(bhbw) L(h?)j. By addition and subtraction we have
L(bhbw) L(h?) =L(bhbw) Ln(bhbw)|{z}
(b)+Ln(bhbw) Ln(bhbw;bw)|{z}
+Ln(bhbw;bw) Ln(h?;bw)|{z}
0+Ln(h?;bw) Ln(h?)|{z}
(a)+Ln(h?) L(h?)|{z}
(b)(9)
wheren=npand we used optimality of bhbw. Here (a) is the weight estimation error and (b) is the
ﬁnite sample error.
18
Uniform law for bounding (b) We bound (b) using standard results for uniform laws for uniformly
bounded functions which holds since kwk1d1(qjjp)andk`k11. Sincejw(y)`(h(x);y)j
d1(qjjp);8x;y2XY   by deploying the McDiarmid’s inequality we then obtain that
h2HjLn(h) L(h)j2Rn(G(`;H)) +d1(qjjp)s
log2

n
whereG(`;H) =fgh(x;y) =w(y)`(h(x);y) :h2Hg and the Rademacher complexity is deﬁned
asRn(G) :=EX(i);Y(i)P:i2[n]
Ei:i2[n]1
n[suph2HPn
i=1iw(yi)`(xi;h(yi))]
.
of the hypothesis class H(see for example Percy Liang notes on Statistical Learning Theory and
chapter 4 in Wainwright (2019))
Bounding term (a) Remember that k=jYjis the cardinality of the ﬁnite domain of Y  or the
number of classes. Let us deﬁne e`2Rkwithe`j=Pn
i=1Iy(i)=j`(y(i);h(x(i))). Notice that by
deﬁnitionke`k1nandke`k1nfrom which it follows by Hoelder’s inequality that ke`k2n.
Furthermore  we slightly abuse notation and use wto denote the k-dimensional vector with wi=w(i).
Therefore  for all hwe have via the Cauchy Schwarz inequality that
jLn(h;w) Ln(h;bw)j=j1
i=1(w(y(i)) bw(y(i)))`(h(x(i));y(i))j
j1
nkX
j=1(w(j) bw(j))e`(j)j
1
nkbw wk2ke`k2kbw wk2
kb k2(1 )kk2+kb k2 (10)
It then follows by Lemma 1 that
h2HjLn(h;w) Ln(h;bw)j(1 )kk2
O

(kk2)s
(1 )nps
Lemma 4 (McDiarmid-Doob-Freedman-Rademacher) For a given A hypothesis class H  a set
G(`;H) =fgh(x;y) =w(y)`(h(x);y) :h2Hg   underndata points and loss function `we have
h2HjLn(h) L(h)j2R(G(`;H)) +2d1(qjjp) log(2=)
with probability at least 1 
Plugging both bounds back into equation (9) concludes the proof of the theorem.
B.5 P ROOF OF LEMMA 4
With a bit abuse of notation let’s restate the empirical loss with known importance weights instead on
the random variables f(Xi;Yi)gn
j=1w(Yj)`(Yj;h(Xj))
We further deﬁne a ghost data set f(X0
i;Y0
i)gn
1and the corresponding ghost loss;
L0
n(h) =1
j=1w(Y0
j)`(Y0
j;h(X0
j))
19
Let’s deﬁne a random variable Gn:= suph2HLn(h) L(h). This random variable is the key to
derive the tight generalization bound in Lemma 4.
This random variable has the following properties;
E[Gn] =E
h2HLn(h) E[L0
n(h)]
Which we can rewrite as
h2HEh
Ln(h) L0
n(h)f(Xi;Yi)gn
1i
and swapping the supwith the expectation
E[Gn]E
E
h2HLn(h) L0
1
We can remove the condition with law of iterated conditional expectation and have expectation on
both of the data sets;
n(h)
we further open the expression up;
E[Gn]E'
h2H1
iwi`(h(Xi);Yi) w0
i`0(h(X0
i);Y0
i)#
In the following we use the usual symmetrizing technique through Rademacher variables fign
1. Each
iis a uniform random variable either 1or 1. Therefore since `(h(X0);Y0) `0(h(X0);Y0)is a
symmetric random variable we have
ii[w(Yi)`(h(Xi);Yi) w(Y0
i)`0(h(X0
i)]#
where the expectation is also over the Rademacher variables. After propagation sup
iiw(Yi)`(h(Xi);Yi) + sup
i iw(Y0
By propagating the expectation and again symmetry in the Rademacher variable we have
E[Gn]2E'
iiw(Yi)`(h(Xi);Yi)#
= 2R(G(`;H))
where the right hand side is two times the Rademacher complexity of class G(`;H). Consider a
sequence of Doob Martingale and ﬁltration (Uj;Fj)deﬁned on some probability space (
;F;Pr);
Uj:=E'
iw(Yi)`(h(Xi);Yi)f(Xi;Yi)gj
1#
and the corresponding Martingale difference;
Dj:=Uj Uj 1
In the following we show that each jDjjis bounded above.
Dj=E'
 E'
iw(Yi)`(h(Xi);Yi)f(Xi;Yi)gj 1
max
xj;yjE'
1;xj;yj#
 min
20
Let’s deﬁne xmax
j;ymax
j as the solution to the maximization and xmin
j;ymin
jthe solution to the
minimization  therefore
DjE'
1;xmax
j#
1;xmin
=E'
h2H
iw(Yi)`(h(Xi);Yi) +w(ymin
j)`(h(xmin
j);ymin
j) w(ymin
j)!f(xi;yi)gj 1
iw(Yi)`(h(Xi);Yi) +w(ymax
j)`(h(xmax
j);ymax
nsup
h2Hw(ymax
j)d1(qjjp)
The same way we can bound  Dj. Therefore the absolute value each Djis bounded byd1(qjjp)
n. In
the following we bound the conditional second moment  E
D2
jjFj 1
;
E2
666640
BBBB@E'
| {z }
(a0) E'
(b0)1
CCCCA2
f(Xi;Yi)gj 1
77775
Let’s construct an event Cjthe event that a0is bigger than b0  and alsoC0
jits compliment. Therefore
for the E
we have
E'
1#!2f(Xi;Yi)gj 1
1;Cj#
P(CjjFj 1)
+E'
1;C0
P(C0
jjFj 1)
(11)
21
For the ﬁrs term in Eq. 11 after again introducing ghost variables X0;Y0we have the following upper
bound
4sup
i6=jw(Yi)`(h(Xi);Yi) +1
h2Hw(Yj)`(h(Xj);Yj)f(Xi;Yi)gj
E'
nw(Y0
j)`(h(X0
j);Y0
j) +1
+1
h2Hw(Yj)`(h(Xj);Yj)
E'1
h2Hw(Yj)`(h(Xj);Yj)2f(Xi;Yi)gj 1
P(CjjFj 1)d(qjjp)
n2P(CjjFj 1)
So far we have that the ﬁrst term in Eq. 11 is bounded byd(qjjp)
n2. Now for the second term we have
the following upper bound;
h2Hw(Yj)`(h(Xj);Yj)f(Xi;Yi)gj 1
 E2
i6=jw(Yi)`(h(Xi);Yi)f(Xi;Yi)gj 1
5!2f(Xi;Yi)gj 1

E1
12
nw(Yj)f(Xi;Yi)gj 1
=1
n2P(C0
Therefore  since d(qjjp)1
E
d(qjjp)
n2P(CjjFj 1) +1
jjFj 1)d(qjjp)
n2
For the ﬁrst inequality  we used the fact that the loss is within [0;1]and the second one is from Eq. 12.
Since the ﬁrst term in RHS is (b0)  therefore  second moment of each DjjFj 1is bounded by2d(qjjp)
n.
22
Therefore for the Doob Martingale sequence of Djwe havejDjj d1(qjjp)
nas well asPn
jD2
jjFj 1d(qjjp)
n. Using the Freedman’s inequality Freedman (1975)  we have
P0
@nX
jDj=Gn E[Gn]1
Aexp
 2
2(d(qjjp)
n+d1(qjjp)
n)!
Moreover  if we multiply each loss with  1  it results in hypothesis class of  H which has the same
Rademacher complexity as H  due the symmetric Rademacher random variable. Let eGndenote the
same quantity as eGnbut on H. We use this slack variable in order to bound the absolute value of
Gn. Therefore
P(GnE[Gn] +)exp
 2
n+d1(qjjp)
exp
=2
and the same bound for eGn. By solving it for andwe have
jGnj2R(G(`;H)) +2d1(qjjp) log(2=)
Note: A few days prior to the camera ready submission  we realized that a quite similar analysis and
statement to Theorem 4 is also studied in Ying (2004).
B.6 G ENERALIZATION FOR FINITE HYPOTHESIS CLASSES
For ﬁnite hypothesis classes  one may bound (b) in (9) using Bernstein’s inequality.
We bound (b) by ﬁrst noting that w(Y)`(Y;h(X))satisﬁes the Bernstein conditions so that Bernstein’s
inequality holds
EP[w(Y)] = 1;EP[w(Y)2] =d(qjjp); 2
P(w(Y)) =d(qjjp) 1 (12)
by deﬁnition. Because we assume `1  we directly have
EP
w(Y)2l2(Y;h(X))
EP
=d(qjjp) (13)
Since we have a bound on the second moment of weighted loss while its ﬁrst moment is L(h)we can
apply Bernstein’s inequality to obtain for any ﬁxed hthat
jLn(h) L(h)j2d1(qjjp) log(2
)
3n+s
2 (d(qjjp) L(h)2) log(2
For the uniform law for ﬁnite hypothesis classes make the union bound on all the hypotheses;
h2HjLn(h) L(h)j2d1(qjjp) log(2jHj
2 (d(qjjp)) log(2jHj
The second moment of the importance weighted loss EP
!(Y)2`2(h(X);Y)
given ah2H0can
be bounded for general 0  potentially leading to smaller values than d(qjjp):
23
!2
Y`2(Y;h(X))
ip(i)q2(i)
p2(i)EXp(XjY=i)
`2(h(X);X)
iq(i)1
q(i)
p(i)q(i) 1
EXp(XjY=i)
`2(h(X);i)
 kX
p(i)!1
kX
iq(i)EXp(XjY=i)
`2(h(X);i)2
 1! 1
= kX
l2(h(X);i)
`2(h(X);i)+1
`2(h(X);i)1 1
`2(h(X);i)1+1
(14)
where the ﬁrst inequality follows from Hölder’s inequality  the second one follows from Jensen’s
inequality and the fact that the loss is in [0;1]as well as the fact that the exponentiation function is
convex in this region. Moreover  since 1 +1
1and upper bound for the loss square  l(;)21
then;
w(Y)2`2(h(X);Y)
iq(i)EXPjY=i
which gives bound on the second moment of weighted loss.
B.7 S LIGHT DRIFT FROM THE LABEL SHIFT
Drift in label shift assumption: If the label shift approximation is slightly violated  we expect
the generalizing bound to deviate from the statement in the Theorem. 1. Deﬁne de(qjjp) :=
E(X;Y)Qh1 p(XjY)
as the deviation form label shift constraint which is zero in label shift
setting.
Remark 1 (Drift in Label shift assumption) If the label shift assumption slightly violates  for the
true importance weights !(x;y) :=q(x;y)
p(x;y)  the RLLS   with high probability generalizes as;
Consider the case where the Label shift assumption is slightly violated  i.e.  for each covariate and
label  we have p(xjy)'q(xjy)  resulting importance weight !(x;y) :=q(x;y)
p(x;y)for each covariate
and label. Similar to decomposing in Eq. 9  we have
L(bhbw;!) L(h?;!) =L(bhbw;!) L(bhbw)|{z}
(c)+L(bhbw) Ln(bhbw)|{z}
(b)+L(h?) L(h?;!)|{z}
(15)
where the desired excess risk is deﬁned with respect to !. The differences between Eq. 15 and Eq. 9
are in a new term (c)as well as term (a). The term (b)remains untouched.
24
Bound on term (c)For anyh  the two contributing components in (c)  i.e. L(h;!)andL(h)are
as follows;
L(h) =E(X;Y)P[w(Y)`(Y;h(X))];andL(h;!) =E(X;Y)Q[`(Y;h(X))] =E(X;Y)P[!(X;Y )`(Y;h(X))]
For their deviation we have
L(h;!) L(h) =E(X;Y)P[(!(X;Y ) w(Y))`(Y;h(X))]
=E(X;Y)Pq(X;Y )
p(X;Y ) q(Y)
p(Y)
`(Y;h(X))
=E(X;Y)Q
1 p(XjY)
q(XjY)
de(qjjp) :=E(X;Y)Q1 p(XjY)
q(XjY)
Where in the last inequality we deploy Cauchy Schwarz inequality as well as loss is in [0;1]and hold
forh2H. It is worth noting that the expectation in de(qjjp)is with respect to Qand does not blow
up if the supports of PandQdo not match
Bound on term (a)For anyh2H  similar to the derivation in Eq. 10 we have
jLn(h) Ln(h;bw)jkbw wk2kb k2(1 )kk2+kb k2
The previous weight estimation analysis does not directly hold for this case where the label shift
is slightly violated  but with a few modiﬁcation we provide an upper-bound on the error. Given a
classiﬁerh0
qh0(Y=i) =X
jq(h(X) =ijY=j)q(j)
=X
j(q(h0(X) =ijY=j) p(h0(X) =ijY=j))q(j) +X
jp(h0(X) =ijY=j)q(j)
jp(h0(X) =i;Y=j)wj
=E(X;Y)Q
p(h0(X) =i)
q(XjY)
+X
wherep(h0(X) =i) =q(h0(X) =i)  resulting;
qh0=be+Cw
where we drop the h0in bothbeandC. Both the confusion matrix Cand the label distribution qh0
on the target for the black box hypothesis h0are unknown and we are instead only given access to
ﬁnite sample estimates bCh0;bqh0. Similar to previous analysis we have
with corresponding ﬁnite sample quantity bb=bq bC1. Similarly to the analysis when there was
no violation in label shift assumption  we have (0) =kC0 b bek2and the solution to Eq. 3
satisﬁes;
02Rkf(0) + 2Ck0kg+ 2b+ 2kbek2: (16)
We can simplify the upper bound by setting 0=. We then have
(b) =kCb b bek2=kC
k22Ckk2+ 2b+ 2kbek22Ckk2+ 2b+ 2de(qjjp)
resulting in
min(2Ckk2+ 2b+ 2de(qjjp))
25
PublishedasaconferencepaperatICLR2019
KamyarAzizzadenesheli AnqiLiu
UniversityofCalifornia Irvine CaliforniaInstituteofTechnology
kazizzad@uci.edu anqiliu@caltech.edu
FannyYang AnimashreeAnandkumar
InstituteofTheoreticalStudies ETHZürich CaliforniaInstituteofTechnology
fan.yang@stat.math.ethz.ch anima@caltech.edu
WeproposeRegularizedLearningunderLabelshifts(RLLS) aprincipledanda
practicaldomain-adaptationalgorithmtocorrectforshiftsinthelabeldistribution
betweenasourceandatargetdomain. Weﬁrstestimateimportanceweightsusing
weightedsourcesamples. Wederiveageneralizationboundfortheclassiﬁeronthe
targetdomainwhichisindependentofthe(ambient)datadimensions andinstead
onlydependsonthecomplexityofthefunctionclass. Tothebestofourknowledge
thisistheﬁrstgeneralizationboundforthelabel-shiftproblemwherethelabelsin
thetargetdomainarenotavailable. Basedonthisbound weproposearegularized
estimatorforthesmall-sampleregimewhichaccountsfortheuncertaintyinthe
estimatedweights. ExperimentsontheCIFAR-10andMNISTdatasetsshowthat
RLLSimprovesclassiﬁcationaccuracy especiallyinthelowsampleandlarge-shift
regimes comparedtopreviousmethods.
1","as well. In one setting  suppliers of machine-learning models such as cloud providers have large
resources of diverse data sets (source set) to train the models  while during deployment  they have no
control over the proportion of label categories.
In another setting of e.g. medical diagnostics  the disease distribution changes over locations and
time. Consider the task of diagnosing a disease in a country with bad infrastructure and little data
1
based on reported symptoms. Can we use data from a different location with data abundance to
diagnose the disease in the new target location in an efﬁcient way? How many labeled source and
unlabeled target data samples do we need to obtain good performance on the target data?
Apart from being relevant in practice  label shift is a computationally more tractable scenario than
covariate shift which can be mitigated. The reason is that the outputs ytypically have a much lower
dimension than the inputs x. Labels are usually either categorical variables with a ﬁnite number
of categories or have simple well-deﬁned structures. Despite being an intuitively natural scenario
in many real-world application  even this simpliﬁed model has only been scarcely studied in the
literature. Zhang et al. (2013) proposed a kernel mean matching method for label shift which is
not computationally feasible for large-scale data. The approach in Lipton et al. (2018) is based on
importance weights that are estimated using the confusion matrix (also used in the procedures of
Saerens et al. (2002); McLachlan (2004)) and demonstrate promising performance on large-scale
data. Using a black-box classiﬁer which can be biased  uncalibrated and inaccurate  they ﬁrst estimate
importance weights q(y)=p(y)for the source samples and train a classiﬁer on the weighted data. In
the following we refer to the procedure as black box shift learning (BBSL ) which the authors proved
to be effective for large enough sample sizes.
However  there are three relevant questions which remain unanswered by their work: How to estimate
the importance weights in low sample setting  What are the generalization guarantees for the ﬁnal
predictor which uses the weighted samples? How do we deal with the uncertainty of the weight
estimation when only few samples are available? This paper aims to ﬁll the gap in terms of both
theoretical understanding and practical methods for the label shift setting and thereby move a step
closer towards having a more complete understanding on the general topic of supervised learning for
distributionally shifted data. In particular  our goal is to ﬁnd an efﬁcient method which is applicable
to large-scale data and to establish generalization guarantees.
Our contribution in this work is trifold. Firstly  we propose an efﬁcient weight estimator for which
we can obtain good statistical guarantees without a requirement on the problem-dependent minimum
sample complexity as necessary for BBSL . In the BBSL case  the estimation error can become
arbitrarily large for small sample sizes. Secondly  we propose a novel regularization method to
compensate for the high estimation error of the importance weights in low target sample settings.
It explicitly controls the inﬂuence of our weight estimates when the target sample size is low (in
the following referred to as the low sample regime). Finally  we derive a dimension-independent
generalization bound for the ﬁnal Regularized Learning under Label Shift ( RLLS ) classiﬁer based on
our weight estimator. In particular  our method improves the weight estimation error and excess risk
of the classiﬁer on reweighted samples by a factor of klog(k)  wherekis the number of classes  i.e.
the cardinality ofY.
In order to demonstrate the beneﬁt of the proposed method for practical situations  we empirically
study the performance of RLLS and show weight estimation as well as prediction accuracy comparison
for a variety of shifts  sample sizes and regularization parameters on the CIFAR-10 and MNIST
datasets. For large target sample sizes and large shifts  when applying the regularized weights fully
we achieve an order of magnitude smaller weight estimation error than baseline methods and enjoy at
most 20% higher accuracy and F-1 score in corresponding predictive tasks. For low target sample
sizes  applying regularized weights partially also yields an accuracy improvement of at least 10%
over fully weighted and unweighted methods.
2 R EGULARIZED LEARNING OF LABEL SHIFTS (RLLS )
Formally let us the short hand for the marginal probability mass functions of Yon ﬁniteYwith
respect to P;Qasp;q: [k]![0;1]withp(i) =P(Y=i)  andq(i) =Q(Y=i)for alli2[k]
representable by vectors in Rk
+which sum to one. In the label shift setting  we deﬁne the importance
weight vector w2Rkbetween these two domains as w(i) =q(i)
p(i). We quantify the shift using the
exponent of the inﬁnite and second order Renyi divergence as follows
d1(qjjp) := sup
iq(i)
p(i);andd(qjjp) :=EYQ
w(Y)2
=kX
iq(i)q(i)
p(i):
2
Given a hypothesis class Hand a loss function `:YY! [0;1]  our aim is to ﬁnd the hypothesis
h2H which minimizes
L(h) =EX;YQ[`(Y;h(X))] =EX;YP[w(Y)`(Y;h(X))]
In the usual ﬁnite sample setting however  Lunknown and we observe samples f(xj;yj)gn
j=1fromP
instead. If we are given the vector of importance weights wwe could then minimize the empirical
loss with importance weighted samples deﬁned as
Ln(h) =1
nnX
j=1w(yj)`(yj;h(xj))
wherenis the number of available observations drawn from Pused to learn the classiﬁer h. Aswis
unknown in practice  we have to ﬁnd the minimizer of the empirical loss with estimated importance
weights
Ln(h;bw) =1
j=1bw(yj)`(yj;h(xj)) (1)
wherebware estimates of w. Given a set Dpofnpsamples from the source distribution P  we ﬁrst
divide it into two sets where we use (1 )npsamples in set Dweight
p to compute the estimate bw
and the remaining n=npin the setDclass
pto ﬁnd the classiﬁer which minimizes the loss (1)  i.e.
bhbw= arg minh2HLn(h;bw). In the following  we describe how to estimate the weights bwand
provide guarantees for the resulting estimator bhbw.
Plug-in weight estimation The following simple correlation between the label distributions p;q
was noted in Lipton et al. (2018): for a ﬁxed hypothesis h  if for ally2Y it holds thatq(y)0 =)
p(y)0  we have
qh(i) :=Q(h(X) =i) =kX
j=1Q(h(X) =ijY=j)q(j) =kX
j=1P(h(X) =ijY=j)q(j)
j=1P(h(X) =i;Y=j)q(j)
p(j)=kX
j=1P(h(X) =i;Y=j)wj
for alli;j2Y. This can equivalently be written in matrix vector notation as
qh=Chw; (2)
whereChis the confusion matrix with [Ch]i;j=P(h(X) =i;Y =j)andqhis the vector
which represents the probability mass function of h(X)under distribution Q. The requirement
q(y)0 =)p(y)0is a reasonable condition since without any prior knowledge  there is no
way to properly reason about a class in the target domain that is not represented in the source domain.
In reality  both qhandChcan only be estimated by the corresponding ﬁnite sample averages
bqh;bCh. Lipton et al. (2018) simply compute the inverse of the estimated confusion matrix bChin
order to estimate the importance weight  i.e. bw=bC 1
hbqh. WhileC 1
hbqhis a statistically efﬁcient
estimator bwwith estimated bC 1
hcan be arbitrarily bad since bC 1
hcan be arbitrary close to a singular
matrix especially for small sample sizes and small minimum singular value of the confusion matrix.
Intuitively  when there are very few samples  the weight estimation will have high variance in which
case it might be better to avoid importance weighting altogether. Furthermore  even when the sample
complexity in Lipton et al. (2018)  unknown in practice  is met  the resulting error of this estimator is
linear inkwhich is problematic for large k.
We therefore aim to address these shortcomings by proposing the following two-step procedure to
compute importance weights. In the case of no shift we have w=1so that we deﬁne the amount
of weight shift as =w 1. Given a “decent” black box estimator which we denote by h0  we
make the ﬁnal classiﬁer less sensitive to the estimation performance of C(i.e. regularize the weight
estimate) by
3
1. calculating the measurement error adjusted b(described in Section 2.1 for h0) and
2.computing the regularized weight bw=1+bwheredepends on the sample size (1 )np.
By 'decent' we refer to a classiﬁer h0which yields a full rank confusion matrix Ch0. A trivial
example for a non-”decent” classiﬁer h0is one that always outputs a ﬁxed class. As it does not
capture any characteristics of the data  there is no hope to gain any statistical information without any
prior information.
2.1 E STIMATOR CORRECTING FOR FINITE SAMPLE ERRORS
Both the confusion matrix Ch0and the label distribution qh0on the target for the black box hypothesis
h0are unknown and we are instead only given access to ﬁnite sample estimates bCh0;bqh0. In what
follows all empirical and population confusion matrices  as well as label distributions  are deﬁned
with respect to the hypothesis h=h0. For notation simplicity  we thus drop the subscript h0in what
follows. The reparameterized linear model (2) with respect to then reads
b:=q C1=C
with corresponding ﬁnite sample quantity bb=bq bC1. WhenbCis near singular  the estimation of 
becomes unstable. Furthermore  large values in the true shift result in large variances. We address
this problem by adding a regularizing `2penalty term to the usual loss and thus push the amount of
shift towards 0  a method that has been proposed in (Pires & Szepesvári  2012). In particular  we
compute
b= arg min
kbC bbk2+ Ckk2 (3)
Here  Cis a parameter which will eventually be high probability upper bounds for kbC Ck2. Let
balso denote the high probability upper bounds for kbb bk2.
Lemma 1 Forbas deﬁned in equation (3)  we have with probability at least 1 that1
kb k2(np;nq;kk2;)
where
(np;nq;kk2;) :=O
min 
kk2s
log(k=)
(1 )np+s
log(1=)
nq!
:
The proof of this lemma can be found in Appendix B.1. A couple of remarks are in order at this
point. First of all  notice that the weight estimation procedure (3)does not require a minimum
sample complexity which is in the order of  2
minto obtain the guarantees for BBSL . This is due to
the fact that errors in the covariates are accounted for. In order to directly see the improvements
in the upper bound of Lemma 1 compared to Theorem 3 in Lipton et al. (2018)  ﬁrst observe
that in order to obtain their upper bound with a probability of at least 1   it is necessary that
3kn 10
p+ 2kn 10
q. As a consequence  the upper bound in Theorem 3 of Lipton et al. (2018)
is bigger than1
3min 
kk2q
log(3k=)
np+q
klog(2k=)
nq
. Thus Lemma 1 improves upon the previous
upper bound by a factor of k.
Furthermore  as in Lipton et al. (2018)  this result holds for anyblack box estimator h0which enters
the bound via min(Ch0). We can directly see how a good choice of h0helps to decrease the upper
bound in Lemma 1. In particular  if h0is an ideal estimator  and the source set is balanced  Cis the
unit matrix with min= 1=k. In contrast  when the model h0is uncertain  the singular value minis
close to zero.
Moreover  for least square problems with Gaussian measurement errors in both input and target
variables  it is standard to use regularized total least squares approaches which requires a singular
value decomposition. Finally  our choice for the alternative estimator in Eq. 3 with norm instead of
norm squared regularization is motivated by the cases with large shifts   where using the squared
norm may shrink the estimate btoo much and away from the true .
1Throughout the paper  Ohides universal constant factors. Furthermore  we use O(+)for short to denote
O() +O().
4
Algorithm 1 Regularized Learning of Label Shift ( RLLS )
1:Input: source set Dp Dq max  estimate of min  black box estimator h0  model classH
2:Determine optimal split ratio ?and regularizer ?by minimizing the RHS of Eq. (6)using an
estimate ofmin
3:Randomly partition source set DpintoDclass
p;Dweight
p such thatjDclass
pj=?np=:n
4:Computebusing Eq. (3) and bw:= 1 +?b
5:Minimize the importance weighted empirical loss to obtain the weighted estimator
bhbw= arg min
h2HLn(h;bw); whereLn(h;bw) =1
nX
(x;y)2Dclasspbw(y)`(y;h(x))
6:Deploybhbwif the risk is acceptable
2.2 R EGULARIZED ESTIMATOR AND GENERALIZATION BOUND
When a few samples from the target set are available or the label shift is mild  the estimated weights
might be too uncertain to be applied. We therefore propose a regularized estimator deﬁned as follows
bw=1+b: (4)
Note thatbwimplicitly depends on   and. By rewriting bw= (1 )1+(1+b)  we see that
intuitivelycloser to 1the more reason there is to believe that 1+bis in fact the true weight.
Deﬁne the setG(`;H) =fgh(x;y) =w(y)`(h(x);y) :h2Hg and its Rademacher complexity
measure
Rn(G) :=E(Xi;Yi)P:i2[n]'
Ei:i2[n]1
n'
sup
h2HnX
i=1igh(Xi;h(Yi))##
withi;8ias the Rademacher random variables (see e.g. Bartlett & Mendelson (2002)). We can now
state a generalization bound for the classiﬁer bhbwin a general hypothesis class H  which is trained on
source data with the estimated weights deﬁned in equation (4).
Theorem 1 (Generalization bound for bhbw)Givennpsamples from the source data set and nq
samples from the target set  a hypothesis class Hand loss function `  the following generalization
bound holds with probability at least 1 2
L(bhbw) L(h)G(np;;) + (1 )kk2+(np;nq;kk2;;): (5)
G(np;) := 2Rn(G) + min(
d1(qjjp)s
log(2=)
np;2d1(qjjp) log(2=)
n+r
2d(qjjp) log(2=)
n)
The proof can be found in Appendix B.4. Additionally  we derive the analysis also for ﬁnite hypothesis
classes in Appendix B.6 to provide more insight into the proof of general hypothesis classes. The
size ofRn(G)is determined by the structure of the function class Hand the loss `. For example for
the0=1loss  the VC dimension of Hcan be deployed to upper bound the Rademacher complexity.
The bound (5)in Theorem 1 holds for all choices of . In order to exploit the possibility of choosing
andto have an improved accuracy depending on the sample sizes  we ﬁrst let the user deﬁne a set
of shiftsagainst which we want to be robust against  i.e. all shifts with kk2max. For these
shifts  we obtain the following upper bound
L(bhbw) L(h)G(np;) + (1 )max+(np;nq;max;) (6)
The bound in equation (6)suggests using Algorithm 1 as our ultimate label shift correction procedure.
where for step 2 of the algorithm  we choose ?= 1 whenevernq1
2max(min 1pnp)2(hereby
neglecting the log factors and thus dependencies on k) and 0else. When using this rule  we
5
obtainL(bhbw) L(h)G(np;) + minfmax;(np;nq;max;)gwhich is smaller than the
unregularized bound for small nq;np. Notice that in practice  we do not know minin advance so
that in Algorithm 1 we need to use an estimate of min  which could e.g. be the minimum eigenvalue
of the empirical confusion matrix bCwith an additional computational complexity of at most O(k3).
0.2 0.4 0.6 0.8 1.0
θmax102103104105nqσmin
Figure 1: Given a min and
max switches from 0to1at a
particularnq.npandkare ﬁxed.Figure 1 shows how the oracle thresholds vary with nqandmin
whennpis kept ﬁx. When the parameters are above the curves for
ﬁxednp should be chosen as 1otherwise the samples should
be unweighted  i.e. = 0. This ﬁgure illustrates that when the
confusion matrix has small singular values  the estimated weights
should only be trusted for rather high nqand high believed shifts
max. Although the overall statistical rate of the excess risk of the
classiﬁer does not change as a function of the sample sizes  max
could be signiﬁcantly smaller than whenminis very small and
thus the accuracy in this regime could improve. Indeed we observe
this to be the case empirically in Section 3.3.
In the case of slight deviation from the label shift setting  we expect the Alg. 1 to perform reasonably.
Forde(qjjp) :=E(X;Y)Qh1 p(XjY)
q(XjY)i
as the deviation form label shift constraint  i.e.  zero under
label shift assumption  we have
Theorem 2 (Drift in Label shift assumption) In the presence of de(qjjp)deviation from label shift
assumption  the true importance weights !(x;y) :=q(x;y)
p(x;y)  the RLLS generalizes as;
L(bhbw;!) L(h;!)G(np;) + (1 )kk2+(np;nq;kk2;) + 4 (1 )de(qjjp)
with high probability. Proof in Appendix B.7.
3 EXPERIMENTS
In this section we illustrate the theoretical analysis by running RLLS on a variety of artiﬁcially
generated shifts on the MNIST (LeCun & Cortes  2010) and CIFAR10 (Krizhevsky & Hinton  2009)
datasets. We ﬁrst randomly separate the entire dataset into two sets (source and target pool) of the
same size. Then we sample  unless speciﬁed otherwise  the same number of data points from each
pool to form the source and target set respectively. We chose to have equal sample sizes to allow for
fair comparisons across shifts.
There are various kinds of shifts which we consider in our experiments. In general we assume one of
the source or target datasets to have uniform distribution over the labels. Within the non-uniform set
we consider three types of sampling strategies in the main text: the Tweak-One shift refers to the case
where we set a class to have probability p>0:1  while the distribution over the rest of the classes
is uniform. The Minority-Class Shift is a more general version of Tweak-One shift   where a ﬁxed
number of classes mto have probability p<0:1  while the distribution over the rest of the classes is
uniform. For the Dirichlet shift   we draw a probability vector pfrom the Dirichlet distribution with
concentration parameter set to for all classes  before including sample points which correspond to
the multinomial label variable according to p."
R025,1,TMLR,"Few-shot classiﬁcation aims to learn a classiﬁer to recognize unseen classes during
training with limited labeled examples. While signiﬁcant progress has been made
the growing complexity of network designs  meta-learning algorithms  and differ-
ences in implementation details make a fair comparison difﬁcult. In this paper
we present 1) a consistent comparative analysis of several representative few-shot
classiﬁcation algorithms  with","asimageclassiﬁcation. Thestrongperformance however heavilyreliesontraininganetworkwith
abundantlabeledinstanceswithdiversevisualvariations(e.g. thousandsofexamplesforeachnew
classevenwithpre-trainingonlarge-scaledatasetwithbaseclasses). Thehumanannotationcostas
wellasthescarcityofdatainsomeclasses(e.g. rarespecies)signiﬁcantlylimittheapplicabilityof
currentvisionsystemstolearnnewvisualconceptsefﬁciently.Incontrast thehumanvisualsystems
canrecognizenewclasseswithextremelyfewlabeledexamples. Itisthusofgreatinteresttolearn
togeneralizetonewclasseswithalimitedamountoflabeledexamplesforeachnovelclass.
siﬁcation  has attracted considerable attention Vinyals et al. (2016); Snell et al. (2017); Finn et al.
(2017);Finnetal.(2017)  metriclearningmethodsVinyalsetal.(2016);Snelletal.(2017);Sung
etal.(2018) andhallucinationbasedmethodsAntoniouetal.(2018);Hariharan&Girshick(2017);
Wangetal.(2018). AnotherlineofworkGidaris&Komodakis(2018);Qietal.(2018)alsodemon-
stratespromisingresultsbydirectlypredictingtheweightsoftheclassiﬁersfornovelclasses.
Limitations. Whilemanyfew-shotclassiﬁcationalgorithmshavereportedimprovedperformance
overthestate-of-the-art therearetwomainchallengesthatpreventusfrommakingafaircompari-
sonandmeasuringtheactualprogress. First thediscrepancyoftheimplementationdetailsamong
multiplefew-shotlearningalgorithmsobscurestherelativeperformancegain. Theperformanceof
baselineapproachescanalsobesigniﬁcantlyunder-estimated(e.g. trainingwithoutdataaugmenta-
tion). Second whilethecurrentevaluationfocusesonrecognizingnovelclasswithlimitedtraining
examples thesenovelclassesaresampledfromthesamedataset. Thelackofdomainshiftbetween
thebaseandnovelclassesmakestheevaluationscenariosunrealistic.
Ourwork. Inthispaper wepresentadetailedempiricalstudytoshednewlightonthefew-shot
deepbackboneshrinkstheperformancegapbetweendifferentmethodsinthesettingoflimiteddo-
methodissurprisinglycompetitivetocurrentstate-of-artmeta-learningalgorithms. Third weintro-
duceapracticalevaluationsettingwherethereexistsdomainshiftbetweenbaseandnovelclasses
(e.g. samplingbaseclassesfromgenericobjectcategoriesandnovelclassesfromﬁne-grainedcat-
futureprogressintheﬁeld.1
Ourcontributions.
comparison.Ourempiricalevaluationresultsrevealthattheuseofashallowbackbonecommonly
betweendifferentmethodswhendomaindifferencesarelimited.
2. Weshowthatabaselinemethodwithadistance-basedclassiﬁersurprisinglyachievescompetitive
3. Weinvestigateapracticalevaluationsettingwherebaseandnovelclassesaresampledfromdif-
ferentdomains. Weshowthatcurrentfew-shotclassiﬁcationalgorithmsfailtoaddresssuchdo-
mainshiftsandareinferioreventothebaselinemethod highlightingtheimportanceoflearning
toadapttodomaindifferencesinfew-shotlearning.
2 RELATED WORK
devotedtoovercomethedataefﬁciencyissue. Inthefollowing wediscussrepresentativefew-shot
learningalgorithmsorganizedintothreemaincategories:initializationbased metriclearningbased
andhallucinationbasedmethods.
Oneapproachaimstolearngoodmodelinitialization(i.e. theparametersofanetwork)sothatthe
classiﬁersfornovelclassescanbelearnedwithalimitednumberoflabeledexamplesandasmall
(2019). Anotherlineofworkfocusesonlearninganoptimizer. ExamplesincludetheLSTM-based
theweight-updatemechanismwithanexternalmemoryMunkhdalai&Yu(2017). Whiletheseini-
tializationbasedmethodsarecapableofachievingrapidadaptionwithalimitednumberoftraining
domainshiftsbetweenbaseandnovelclasses.
Distancemetriclearningbasedmethods addressthefew-shotclassiﬁcationproblemby“learn-
ingtocompare”. Theintuitionisthatifamodelcandeterminethesimilarityoftwoimages itcan
classifyanunseeninputimagewiththelabeledinstancesKochetal.(2015).Tolearnasophisticated
comparisonmodels meta-learningbasedmethodsmaketheirpredictionconditionedondistanceor
cosinesimilarityVinyalsetal.(2016)  Euclideandistancetoclass-meanrepresentationSnelletal.
(2017) CNN-basedrelationmoduleSungetal.(2018) ridgeregressionBertinettoetal.(2019) and
graphneuralnetworkGarcia&Bruna(2018). Inthispaper wecomparetheperformanceofthree
distancemetriclearningmethods. Ourresultsshowthatasimplebaselinemethodwithadistance-
competitiveperformancewithrespecttoothersophisticatedalgorithms.
Komodakis(2018)learnsaweightgeneratortopredictthenovelclassclassiﬁerusinganattention-
focus however istoshowthatsimplyreducingintra-classvariationinabaselinemethodusingthe
baseclassdataleadstocompetitiveperformance.
Hallucinationbasedmethods directlydealwithdatadeﬁciencyby“learningtoaugment”. This
hallucinatenewnovelclassdatafordataaugmentation. Onetypeofgeneratoraimsattransferring
baseclassdatatonovelclasses Hariharan&Girshick(2017) oruseGANmodelsAntoniouetal.
(2018)totransferthestyle. Anothertypeofgeneratorsdoesnotexplicitlyspecifywhattotransfer
leaveitforfuturework.
Domainadaptation techniquesaimtoreducethedomainshiftsbetweensourceandtargetdomain
Panetal.(2010);Ganin&Lempitsky(2015) aswellasnoveltasksinadifferentdomainHsuetal.
(2018). Similartodomainadaptation wealsoinvestigatetheimpactofdomaindifferenceonfew-
shotclassiﬁcationalgorithmsinSection4.5. Incontrasttomostdomainadaptationproblemswhere
methodinDong&Xing(2018)addressestheone-shotnovelcategorydomainadaptationproblem
whereinthetestingstageboththedomainand thecategorytoclassifyarechanged. Similarly our
domainshift. Toputtheseproblemsettingsincontext weprovidedadetailedcomparisonofsetting
differenceintheappendixA1.
3 OVERVIEW OF FEW-SHOT CLASSIFICATION ALGORITHMS
Inthissection  weﬁrstoutlinethedetailsofthebaselinemodel(Section3.1)anditsvariant(Sec-
ourexperiments. GivenabundantbaseclasslabeleddataX andasmallamountofnovelclassla-
b
beleddataX   thegoaloffew-shotclassiﬁcationalgorithmsistotrainclassiﬁersfornovelclasses
n
(unseenduringtraining)withfewlabeledexamples.
3.1 BASELINE
ﬁne-tuning. Figure1illustratestheoverallprocedure.
Trainingstage. Wetrainafeatureextractor f (parametrizedbythenetworkparametersθ)and
θ
theclassiﬁerC(·|W )(parametrizedbytheweightmatrixW ∈Rd×c)fromscratchbyminimizing
b b
a standard cross-entropy classiﬁcation loss L using the training examples in the base classes
pred
x ∈X . Here  we denote the dimension of the encoded feature as d and the number of output
i b
classes as c. The classiﬁer C(.|W ) consists of a linear layer W(cid:62)f (x) followed by a softmax
b b θ i
functionσ.
Training stage Fine-tuning stage
Base class data Novel class data Fixed
(Many) Feature  (Few)  Feature
Classifier Classifier
extractor extractor
Classifier
Baseline Baseline++
Llianyeearr   Softmax𝜎 dCisotsain…ncee  … Softmax𝜎
Figure1: BaselineandBaseline++few-shotclassiﬁcationmethods. Boththebaselineand
baseline++methodtrainafeatureextractor f andclassiﬁerC(.|W )withbaseclassdatainthe
θ b
trainingstageIntheﬁne-tuningstage weﬁxthenetworkparametersθ inthefeatureextractor f
andtrainanewclassiﬁerC(.|W )withthegivenlabeledexamplesinnovelclasses. The
baseline++methoddiffersfromthebaselinemodelintheuseofcosinedistancesbetweentheinput
featureandtheweightvectorforeachclassthataimstoreduceintra-classvariations.
Fine-tuningstage. Toadaptthemodeltorecognizenovelclassesintheﬁne-tuningstage weﬁx
the pre-trained network parameter θ in our feature extractor f and train a new classiﬁerC(.|W )
θ n
(parametrizedbytheweightmatrixW )byminimizingL usingthefewlabeledofexamples(i.e.
n pred
thesupportset)inthenovelclassesX .
3.2 BASELINE++
Baseline++ whichexplicitlyreducesintra-classvariationamongfeaturesduringtraining. Theim-
ingHuetal.(2015)andfew-shotclassiﬁcationmethodsGidaris&Komodakis(2018).
classiﬁerdesign. AsshowninFigure1 westillhaveaweightmatrixW ∈Rd×coftheclassiﬁerin
thetrainingstageandaW intheﬁne-tuningstageinBaseline++. Theclassiﬁerdesign however is
differentfromthelinearclassiﬁerusedintheBaseline. TaketheweightmatrixW asanexample.
WecanwritetheweightmatrixW as[w  w  ...w ] whereeachclasshasad-dimensionalweight
b 1 2 c
vector. In the training stage  for an input feature f (x) where x ∈X   we compute its cosine
θ i i b
similarity to each weight vector [w  ··· w ] and obtain the similarity scores [s  s  ··· s ] for
1 c i 1 i 2 i c
all classes  where s = f (x)(cid:62)w /(cid:107)f (x)(cid:107)(cid:107)w (cid:107). We can then obtain the prediction probability
i j θ i j θ i j
vectorsrepresentingeachclass. Consequently trainingthemodelwiththisdistance-basedclassiﬁer
explicitly reduce intra-class variations. Intuitively  the learned weight vectors [w  ··· w ] can be
1 c
interpretedasprototypes(similartoSnelletal.(2017);Vinyalsetal.(2016))foreachclassandthe
classiﬁcationisbasedonthedistanceoftheinputfeaturetotheselearnedprototypes. Thesoftmax
functionpreventsthelearnedweightvectorscollapsingtozeros.
WeclarifythatthenetworkdesigninBaseline++isnotourcontribution. Theconceptofdistance-
revisitedinthefew-shotclassiﬁcationsettingGidaris&Komodakis(2018);Qietal.(2018).
3.3 META-LEARNINGALGORITHMS
Herewedescribetheformulationsofmeta-learningmethodsusedinourstudy. Weconsiderthree
Meta-training stage Meta-testing stage
Sampled 𝑁 classes
𝑌'
Base class data  Novel support set
(Many) Support set (Novel class data     )
Base query set conditioned model
Support set conditioned model
MatchingNet ProtoNet RelationNet MAML
Cmleaassn  𝜇 Cmleaassn  𝜇 Line
a
r
eFxetraatcutroer dCisotsainncee  Eduicsltiadnecaen RMeoladtuiolen Lin Gradient
e
Figure2: Meta-learningfew-shotclassiﬁcationalgorithms. Themeta-learningclassiﬁerM(·|S)
isconditionedonthesupportsetS. (Top)Inthemeta-trainstage thesupportsetS andthequery
setQ areﬁrstsampledfromrandomN classes andthentraintheparametersinM(.|S )to
minimizetheN-waypredictionlossL . Inthemeta-testingstage theadaptedclassiﬁer
N−way
M(.|S )canpredictnovelclasseswiththesupportsetinthenovelclassesS . (Bottom)Thedesign
n n
ofM(·|S)indifferentmeta-learningalgorithms.
(2017) andRelationNetSungetal.(2018))andoneinitializationbasedmethod(MAMLFinnetal.
becauseitmakesthetrainingprocedureexplicitlylearntolearnfromagivensmallsupportset.
AsshowninFigure2 meta-learningalgorithmsconsistofameta-trainingandameta-testingstage.
In the meta-training stage  the algorithm ﬁrst randomly select N classes  and sample small base
support set S and a base query set Q from data samples within these classes. The objective is
to train a classiﬁcation model M that minimizes N-way prediction loss L of the samples in
the query set Q . Here  the classiﬁer M is conditioned on provided support set S . By making
predictionconditionedonthegivensupportset ameta-learningmethodcanlearnhowtolearnfrom
limitedlabeleddatathroughtrainingfromacollectionoftasks(episodes). Inthemeta-testingstage
allnovelclassdataX areconsideredasthesupportsetfornovelclassesS  andtheclassiﬁcation
modelMcanbeadaptedtopredictnovelclasseswiththenewsupportsetS .
Differentmeta-learningmethodsdifferintheirstrategiestomakepredictionconditionedonsupport
set(seeFigure2). ForbothMatchingNetVinyalsetal.(2016)andProtoNetSnelletal.(2017) the
predictionoftheexamplesinaquerysetQisbasedoncomparingthedistancebetweenthequery
featureandthesupportfeaturefromeachclass. MatchingNetcomparescosinedistancebetweenthe
queryfeatureandeachsupportfeature andcomputesaveragecosinedistanceforeachclass while
features. RelationNetSungetal.(2018)sharesasimilaridea butitreplacesdistancewithalearn-
ablerelationmodule. TheMAMLmethodFinnetal.(2017)isaninitializationbasedmeta-learning
algorithm  whereeachsupportsetisusedtoadapttheinitialmodelparametersusingfewgradient
updates. Asdifferentsupportsetshavedifferentgradientupdates theadaptedmodelisconditioned
themeta-trainingstage thelossofthequerysetisusedtoupdatetheinitialmodel nottheadapted
4 EXPERIMENTAL RESULTS
4.1 EXPERIMENTALSETUP
Datasetsandscenarios. Weaddressthefew-shotclassiﬁcationproblemunderthreescenarios: 1)
genericobjectrecognition 2)ﬁne-grainedimageclassiﬁcation and3)cross-domainadaptation.
ImageNetdatasetDengetal.(2009)andcontains600imagesforeachclass. Thedatasetwasﬁrst
Forﬁne-grainedclassiﬁcation weuseCUB-200-2011datasetWahetal.(2011)(referredtoasthe
validation and50novelclasses.
For the cross-domain scenario (mini-ImageNet →CUB)  we use mini-ImageNet as our base class
andthe50validationand50novelclassfromCUB.Evaluatingthecross-domainscenarioallowsus
tounderstandtheeffectsofdomainshiftstoexistingfew-shotclassiﬁcationapproaches.
train400epochswithabatchsizeof16. Inthemeta-trainingstageformeta-learningmethods we
to select the training episodes with the best accuracy.2 In each episode  we sample N classes to
form N-way classiﬁcation (N is 5 in both meta-training and meta-testing stages unless otherwise
mentioned). Foreachclass wepickklabeledinstancesasoursupportsetand16instancesforthe
querysetforak-shottask.
Intheﬁne-tuningormeta-testingstageforallmethods weaveragetheresultsover600experiments.
pickkinstancesforthesupportsetand16forthequeryset.ForBaselineandBaseline++ weusethe
entiresupportsettotrainanewclassiﬁerfor100iterationswithabatchsizeof4. Formeta-learning
methods weobtaintheclassiﬁcationmodelconditionedonthesupportsetasinSection3.3.
All methods are trained from scratch and use the Adam optimizer with initial learning rate 10−3.
Weapplystandarddataaugmentationincludingrandomcrop left-rightﬂip andcolorjitterinboth
foreachmethod. ForBaseline++ wemultiplythecosinesimilaritybyaconstantscalar2toadjust
originalvaluerange[-1 1]tobemoreappropriateforsubsequentsoftmaxlayer. ForMatchingNet
weuseanFCEclassiﬁcationlayerwithoutﬁne-tuninginallexperimentsandalsomultiplycosine
4.2 EVALUATIONUSINGTHESTANDARDSETTING
four-layerconvolutionbackbone(Conv-4)withaninputsizeof84x84asinSnelletal.(2017)and
perform5-wayclassiﬁcationforonlynovelclassesduringtheﬁne-tuningormeta-testingstage.
Tovalidatethecorrectnessofourimplementation weﬁrstcompareourresultstothereportednum-
tionedinSection4.1;however theofﬁcialreportedresultsfromProtoNetuses30-wayforoneshot
and20-wayforﬁveshotinthemeta-trainingstageinspiteofusing5-wayinthemeta-testingstage.
Wereportthisresultforcompleteness.
fallmorethan2%behindreportedperformance. Theseminordifferencescanbeattributedtoour
2Forexample theexactepisodesforexperimentsonthemini-ImageNetinthe5-shotsettingwithafour-
layerConvNetare:ProtoNet:24 600;MatchingNet:35 300;RelationNet:37 100;MAML:36 700.
3Reportedresultsarefrom Ravi&Larochelle(2017)
Table1: Validatingourre-implementation. Wevalidateourfew-shotclassiﬁcation
implementationonthemini-ImageNetdatasetusingaConv-4backbone. Wereportthemeanof
600randomlygeneratedtestepisodesaswellasthe95%conﬁdenceintervals. Ourreproduced
resultstoallfew-shotmethodsdonotfallbehindbymorethan2%tothereportedresultsinthe
literature. Weattributetheslightdiscrepancytodifferentrandomseedsandminorimplementation
differencesineachmethod. “Baseline∗”denotestheresultswithoutapplyingdataaugmentation
duringtraining. ProtoNet#indicatesperforming30-wayclassiﬁcationin1-shotand20-wayin
5-shotduringthemeta-trainingstage.
Baseline - 42.11±0.71 - 62.53±0.69
Baseline∗3 41.08±0.70 36.35±0.64 51.04±0.65 54.50±0.66
MatchingNet3Vinyalsetal.(2016) 43.56±0.84 48.14±0.78 55.31±0.73 63.48±0.66
ProtoNet - 44.42±0.84 - 64.24±0.72
ProtoNet#Snelletal.(2017) 49.42±0.78 47.74±0.84 68.20±0.66 66.68±0.68
MAMLFinnetal.(2017) 48.07±1.75 46.47±0.82 63.15±0.91 62.71±0.71
RelationNetSungetal.(2018) 50.44±0.82 49.31±0.85 65.32±0.70 66.60±0.69
Table2: Few-shotclassiﬁcationresultsforboththemini-ImageNetandCUBdatasets. The
Baseline++consistentlyimprovestheBaselinemodelbyalargemarginandiscompetitivewiththe
state-of-the-artmeta-learningmethods. Allexperimentsarefrom5-wayclassiﬁcationwitha
Conv-4backboneanddataaugmentation.
CUB mini-ImageNet
Baseline 47.12±0.74 64.16±0.71 42.11±0.71 62.53±0.69
Baseline++ 60.53±0.83 79.34±0.61 48.24±0.75 66.43±0.63
MatchingNetVinyalsetal.(2016) 61.16±0.89 72.86±0.70 48.14±0.78 63.48±0.66
ProtoNetSnelletal.(2017) 51.31±0.91 70.77±0.69 44.42±0.84 64.24±0.72
MAMLFinnetal.(2017) 55.92±0.95 72.09±0.76 46.47±0.82 62.71±0.71
RelationNetSungetal.(2018) 62.45±0.98 76.11±0.69 49.31±0.85 66.60±0.69
modiﬁcationsofsomeimplementationdetailstoensureafaircomparisonamongallmethods such
asusingthesameoptimizerforallmethods.
Moreover ourimplementationofexistingworkalsoimprovestheperformanceofsomeofthemeth-
ods. Forexample ourresultsshowthattheBaselineapproachunder5-shotsettingcanbeimproved
byalargemarginsincepreviousimplementationsoftheBaselinedonotincludedataaugmentation
intheirtrainingstage therebyleadstoover-ﬁtting. WhileourBaseline∗ isnotasgoodasreported
in 1-shot  our Baseline with augmentationstill improves on it  and could be evenhigher if our re-
producedBaseline∗matchesthereportedstatistics. Ineithercase theperformanceoftheBaseline
methodisseverelyunderestimated. WealsoimprovetheresultsofMatchingNetbyadjustingthe
inputscoretothesoftmaxlayertoamoreappropriaterangeasstatedinSection4.1. Ontheother
hand whileProtoNet# isnotasgoodasProtoNet asmentionedintheoriginalpaperamorechal-
5-wayclassiﬁcationsettinginsubsequentexperimentstohaveafaircomparisontoothermethods.
ThisissuecanberesolvedbyusingadeeperbackboneasshowninSection4.3.
Aftervalidatingourre-implementation wenowreporttheaccuracyinTable2. Besidesadditionally
ﬁndthatBaseline++improvestheBaselinebyalargemarginandbecomescompetitiveevenwhen
variationisanimportantfactorinthecurrentfew-shotclassiﬁcationproblemsetting.
Baseline Baseline++ MatchingNet ProtoNet MAML RelationNet
1-shot 5-shot 1-shot 5-shot
75% 90%  55%  80%
75%
65% 80%  50%
70%
55% 70%  45%
65%
45% 60%  40%  60%
Conv-4 Conv-6 ResNet-10 ResNet-18 ResNet-34 Conv-4 Conv-6 ResNet-10 ResNet-18 ResNet-34 Conv-4 Conv-6 ResNet-10 ResNet-18 ResNet-34 Conv-4 Conv-6 ResNet-10 ResNet-18 ResNet-34
Figure3: Few-shotclassiﬁcationaccuracyvs. backbonedepth. IntheCUBdataset gapsamong
differentmethodsdiminishasthebackbonegetsdeeper. Inmini-ImageNet5-shot some
meta-learningmethodsareevenbeatenbyBaselinewithadeeperbackbone. (Pleasereferto
FigureA3andTableA5forlargerﬁgureanddetailedstatistics.)
However  notethatourcurrentsettingonlyusesa4-layerbackbone  whileadeeperbackbonecan
backbonedepthinthenextsection.
4.3 EFFECTOFINCREASINGTHENETWORKDEPTH
StartingfromConv-4 wegraduallyincreasethefeaturebackbonetoConv-6 ResNet-10 18and34
whereConv-6havetwoadditionalconvolutionblockswithoutpoolingafterConv-4. ResNet-18and
34arethesameasdescribedinHeetal.(2016)withaninputsizeof224×224 whileResNet-10is
asimpliﬁedversionofResNet-18whereonlyoneresidualbuildingblockisusedineachlayer. The
statisticsofthisexperimentwouldalsobehelpfultootherworkstomakeafaircomparisonunder
differentfeaturebackbones.
ResultsoftheCUBdatasetshowsaclearertendencyinFigure3. Asthebackbonegetsdeeper the
rapidlyasthebackbonegetsdeeper. Whileusingaconsistent5-wayclassiﬁcationasdiscussedin
Section4.2degradestheaccuracyofProtoNetwithConv-4 itworkswellwithadeeperbackbone.
methodswouldbereducediftheirintra-classvariationareallreducedbyadeeperbackbone.
However theresultofmini-ImageNetinFigure3ismuchmorecomplicated. Inthe5-shotsetting
bothBaselineandBaseline++achievegoodperformancewithadeeperbackbone  butsomemeta-
assumethatthedatasetisalsoimportantinfew-shotclassiﬁcation.OnedifferencebetweenCUBand
mini-ImageNetistheirdomaindifferenceinbaseandnovelclassessinceclassesinmini-ImageNet
havealargerdivergencethanCUBinaword-nethierarchyMiller(1995). Tobetterunderstandthe
classiﬁcationresults.
4.4 EFFECTOFDOMAINDIFFERENCESBETWEENBASEANDNOVELCLASSES
cross-domain scenario: mini-ImageNet →CUB as mentioned in Section 4.1. We believe that
duetoincreasedavailability)butcollectingimagesfromﬁne-grainedclassesmightbemoredifﬁcult.
WeconducttheexperimentswithaResNet-18featurebackbone. AsshowninTable3 theBaseline
outperformsallmeta-learningmethodsunderthisscenario. Whilemeta-learningmethodslearnto
learnfromthesupportsetduringthemeta-trainingstage theyarenotabletoadapttonovelclasses
thataretoodifferentsinceallofthebasesupportsetsarewithinthesamedataset. Asimilarconcept
newclassiﬁerbasedonthefewgivennovelclassdata  whichallowsittoquicklyadapttoanovel
90%
mini-ImageNet→CUB 80%
Baseline 65.57±0.70 70%
Baseline++ 62.04±0.76 60%
MatchingNet 53.07±0.74 50%
ProtoNet 62.02±0.70 40%
MAML 51.34±0.72 CUB miniImageNet miniImageNet -> CUB
RelationNet 57.71±0.73 Small Large
Domain Difference
Table3: 5-shotaccuracyunderthe Figure4: 5-shotaccuracyindifferentscenarios
cross-domainscenariowithaResNet-18 withaResNet-18backbone. TheBaseline
backbone. Baselineoutperformsallother modelperformsrelativewellwithlargerdomain
methodsunderthisscenario. differences.
CUB mini-ImageNet CUB→mini-ImageNet
Figure5: Meta-learningmethodswithfurtheradaptationsteps. Furtheradaptationimproves
MatchingNetandMAML buthaslessimprovementtoRelationNet andcouldinsteadharm
ProtoNetunderthescenarioswithlittledomaindifferences.Allstatisticsarefor5-shotaccuracy
withResNet-18backbone. Notethatdifferentmethodsusedifferentfurtheradaptationstrategies.
alsoperformsbetterthantheBaseline++method possiblybecauseadditionallyreducingintra-class
becomesrelativelyhigherasthedomaindifferencegetslarger. Thatis  asthedomaindifference
growslarger theadaptationbasedonafewnovelclassinstancesbecomesmoreimportant.
4.5 EFFECTOFFURTHERADAPTATION
featuresandtrainanewsoftmaxclassiﬁer. WeapplythissimpleadaptationschemetoMatchingNet
ratherthanthefeaturevectors sowearenotabletoreplaceitwithasoftmax. Asanalternative we
relationmodulefor100epochs.
ImageNet →CUB scenario. The results demonstrate that lack of adaptation is the reason they fall
behindtheBaseline. However  changingthesettinginthemeta-testingstagecanleadtoinconsis-
tencywiththemeta-trainingstage. TheProtoNetresultshowsthatperformancecandegradeinsce-
narioswithlessdomaindifference. Thus webelievethatlearninghowtoadaptinthemeta-training
real-worldapplications weconsiderthatlearningtolearnadaptationinthemeta-trainingstage
wouldbeanimportantdirectionforfuturemeta-learningresearchinfew-shotclassiﬁcation.
5","main differences  2) a modiﬁed baseline method that surprisingly achieves com-
petitive performance when compared with the state-of-the-art on both the mini-
ImageNet and the CUB datasets  and 3) a new experimental setting for evaluating
the cross-domain generalization ability for few-shot classiﬁcation algorithms. Our
results reveal that reducing intra-class variation is an important factor when the
feature backbone is shallow  but not as critical when using deeper backbones. In
a realistic cross-domain evaluation setting  we show that a baseline method with
a standard ﬁne-tuning practice compares favorably against other state-of-the-art
few-shot learning algorithms.
1 I NTRODUCTION
Deep learning models have achieved state-of-the-art performance on visual recognition tasks such
as image classiﬁcation. The strong performance  however  heavily relies on training a network with
abundant labeled instances with diverse visual variations (e.g.  thousands of examples for each new
class even with pre-training on large-scale dataset with base classes). The human annotation cost as
well as the scarcity of data in some classes (e.g.  rare species) signiﬁcantly limit the applicability of
current vision systems to learn new visual concepts efﬁciently. In contrast  the human visual systems
can recognize new classes with extremely few labeled examples. It is thus of great interest to learn
to generalize to new classes with a limited amount of labeled examples for each novel class.
The problem of learning to generalize to unseen classes during training  known as few-shot clas-
siﬁcation   has attracted considerable attention Vinyals et al. (2016); Snell et al. (2017); Finn et al.
(2017); Ravi & Larochelle (2017); Sung et al. (2018); Garcia & Bruna (2018); Qi et al. (2018).
One promising direction to few-shot classiﬁcation is the meta-learning paradigm where transfer-
able knowledge is extracted and propagated from a collection of tasks to prevent overﬁtting and
improve generalization. Examples include model initialization based methods Ravi & Larochelle
(2017); Finn et al. (2017)  metric learning methods Vinyals et al. (2016); Snell et al. (2017); Sung
et al. (2018)  and hallucination based methods Antoniou et al. (2018); Hariharan & Girshick (2017);
Wang et al. (2018). Another line of work Gidaris & Komodakis (2018); Qi et al. (2018) also demon-
strates promising results by directly predicting the weights of the classiﬁers for novel classes.
Limitations. While many few-shot classiﬁcation algorithms have reported improved performance
over the state-of-the-art  there are two main challenges that prevent us from making a fair compari-
son and measuring the actual progress. First  the discrepancy of the implementation details among
multiple few-shot learning algorithms obscures the relative performance gain. The performance of
1
baseline approaches can also be signiﬁcantly under-estimated (e.g.  training without data augmenta-
tion). Second  while the current evaluation focuses on recognizing novel class with limited training
examples  these novel classes are sampled from the same dataset. The lack of domain shift between
the base and novel classes makes the evaluation scenarios unrealistic.
Our work. In this paper  we present a detailed empirical study to shed new light on the few-shot
classiﬁcation problem. First  we conduct consistent comparative experiments to compare several
representative few-shot classiﬁcation methods on common ground. Our results show that using a
deep backbone shrinks the performance gap between different methods in the setting of limited do-
main differences between base and novel classes. Second  by replacing the linear classiﬁer with
a distance-based classiﬁer as used in Gidaris & Komodakis (2018); Qi et al. (2018)  the baseline
method is surprisingly competitive to current state-of-art meta-learning algorithms. Third  we intro-
duce a practical evaluation setting where there exists domain shift between base and novel classes
(e.g.  sampling base classes from generic object categories and novel classes from ﬁne-grained cat-
egories). Our results show that sophisticated few-shot learning algorithms do not provide perfor-
mance improvement over the baseline under this setting. Through making the source code and
model implementations with a consistent evaluation setting publicly available  we hope to foster
future progress in the ﬁeld.1
Our contributions.
1. We provide a uniﬁed testbed for several different few-shot classiﬁcation algorithms for a fair
comparison. Our empirical evaluation results reveal that the use of a shallow backbone commonly
used in existing work leads to favorable results for methods that explicitly reduce intra-class
variation. Increasing the model capacity of the feature backbone reduces the performance gap
between different methods when domain differences are limited.
2. We show that a baseline method with a distance-based classiﬁer surprisingly achieves competitive
performance with the state-of-the-art meta-learning methods on both mini-ImageNet and CUB
datasets.
3. We investigate a practical evaluation setting where base and novel classes are sampled from dif-
ferent domains. We show that current few-shot classiﬁcation algorithms fail to address such do-
main shifts and are inferior even to the baseline method  highlighting the importance of learning
to adapt to domain differences in few-shot learning.
2 R ELATED WORK
Given abundant training examples for the base classes  few-shot learning algorithms aim to learn
to recognizing novel classes with a limited amount of labeled examples. Much efforts have been
devoted to overcome the data efﬁciency issue. In the following  we discuss representative few-shot
learning algorithms organized into three main categories: initialization based  metric learning based
and hallucination based methods.
Initialization based methods tackle the few-shot learning problem by “learning to ﬁne-tune”.
One approach aims to learn good model initialization (i.e.  the parameters of a network) so that the
classiﬁers for novel classes can be learned with a limited number of labeled examples and a small
number of gradient update steps Finn et al. (2017; 2018); Nichol & Schulman (2018); Rusu et al.
(2019). Another line of work focuses on learning an optimizer . Examples include the LSTM-based
meta-learner for replacing the stochastic gradient decent optimizer Ravi & Larochelle (2017) and
the weight-update mechanism with an external memory Munkhdalai & Yu (2017). While these ini-
tialization based methods are capable of achieving rapid adaption with a limited number of training
examples for novel classes  our experiments show that these methods have difﬁculty in handling
domain shifts between base and novel classes.
Distance metric learning based methods address the few-shot classiﬁcation problem by “learn-
ing to compare”. The intuition is that if a model can determine the similarity of two images  it can
classify an unseen input image with the labeled instances Koch et al. (2015). To learn a sophisticated
comparison models  meta-learning based methods make their prediction conditioned on distance or
1https://github.com/wyharveychen/CloserLookFewShot
2
metric to few labeled instances during the training process. Examples of distance metrics include
cosine similarity Vinyals et al. (2016)  Euclidean distance to class-mean representation Snell et al.
(2017)  CNN-based relation module Sung et al. (2018)  ridge regression Bertinetto et al. (2019)  and
graph neural network Garcia & Bruna (2018). In this paper  we compare the performance of three
distance metric learning methods. Our results show that a simple baseline method with a distance-
based classiﬁer (without training over a collection of tasks/episodes as in meta-learning) achieves
competitive performance with respect to other sophisticated algorithms.
Besides meta-learning methods  both Gidaris & Komodakis (2018) and Qi et al. (2018) develop
a similar method to our Baseline++ (described later in Section 3.2). The method in Gidaris &
Komodakis (2018) learns a weight generator to predict the novel class classiﬁer using an attention-
based mechanism (cosine similarity)  and the Qi et al. (2018) directly use novel class features as
their weights. Our Baseline++ can be viewed as a simpliﬁed architecture of these methods. Our
focus  however  is to show that simply reducing intra-class variation in a baseline method using the
base class data leads to competitive performance.
Hallucination based methods directly deal with data deﬁciency by “learning to augment”. This
class of methods learns a generator from data in the base classes and use the learned generator to
hallucinate new novel class data for data augmentation. One type of generator aims at transferring
appearance variations exhibited in the base classes. These generators either transfer variance in
base class data to novel classes Hariharan & Girshick (2017)  or use GAN models Antoniou et al.
(2018) to transfer the style. Another type of generators does not explicitly specify what to transfer
but directly integrate the generator into a meta-learning algorithm for improving the classiﬁcation
accuracy Wang et al. (2018). Since hallucination based methods often work with other few-shot
methods together (e.g. use hallucination based and metric learning based methods together) and
lead to complicated comparison  we do not include these methods in our comparative study and
leave it for future work.
Domain adaptation techniques aim to reduce the domain shifts between source and target domain
Pan et al. (2010); Ganin & Lempitsky (2015)  as well as novel tasks in a different domain Hsu et al.
(2018). Similar to domain adaptation  we also investigate the impact of domain difference on few-
shot classiﬁcation algorithms in Section 4.5. In contrast to most domain adaptation problems where
a large amount of data is available in the target domain (either labeled or unlabeled)  our problem
setting differs because we only have very few examples in the new domain. Very recently  the
method in Dong & Xing (2018) addresses the one-shot novel category domain adaptation problem
where in the testing stage both the domain andthe category to classify are changed. Similarly  our
work highlights the limitations of existing few-shot classiﬁcation algorithms problem in handling
domain shift. To put these problem settings in context  we provided a detailed comparison of setting
difference in the appendix A1.
3 O VERVIEW OF FEW-SHOT CLASSIFICATION ALGORITHMS
In this section  we ﬁrst outline the details of the baseline model (Section 3.1) and its variant (Sec-
tion 3.2)  followed by describing representative meta-learning algorithms (Section 3.3) studied in
our experiments. Given abundant base class labeled data Xband a small amount of novel class la-
beled data Xn  the goal of few-shot classiﬁcation algorithms is to train classiﬁers for novel classes
(unseen during training) with few labeled examples.
3.1 B ASELINE
Our baseline model follows the standard transfer learning procedure of network pre-training and
ﬁne-tuning. Figure 1 illustrates the overall procedure.
Training stage. We train a feature extractor fq(parametrized by the network parameters q) and
the classiﬁer C(jWb)(parametrized by the weight matrix Wb2Rdc) from scratch by minimizing
a standard cross-entropy classiﬁcation loss Lpredusing the training examples in the base classes
xi2Xb. Here  we denote the dimension of the encoded feature as dand the number of output
classes as c. The classiﬁer C(:jWb)consists of a linear layer W>
bfq(xi)followed by a softmax
function s.
3
Baseline++BaselineTraining stageClassifier
Feature extractorNovel class data(Few) Fine-tuning stage
FixedFeature extractorBase class data(Many)
Linearlayer  Softmax𝜎
Softmax𝜎Cosine distanceClassifierClassifier
……
Figure 1: Baseline and Baseline++ few-shot classiﬁcation methods. Both the baseline and
baseline++ method train a feature extractor fqand classiﬁer C(:jWb)with base class data in the
training stage In the ﬁne-tuning stage  we ﬁx the network parameters qin the feature extractor fq
and train a new classiﬁer C(:jWn)with the given labeled examples in novel classes. The
baseline++ method differs from the baseline model in the use of cosine distances between the input
feature and the weight vector for each class that aims to reduce intra-class variations.
Fine-tuning stage. To adapt the model to recognize novel classes in the ﬁne-tuning stage  we ﬁx
the pre-trained network parameter qin our feature extractor fqand train a new classiﬁer C(:jWn)
(parametrized by the weight matrix Wn) by minimizing Lpredusing the few labeled of examples (i.e.
the support set) in the novel classes Xn.
3.2 B ASELINE ++
In addition to the baseline model  we also implement a variant of the baseline model  denoted as
Baseline++  which explicitly reduces intra-class variation among features during training. The im-
portance of reducing intra-class variations of features has been highlighted in deep metric learn-
ing Hu et al. (2015) and few-shot classiﬁcation methods Gidaris & Komodakis (2018).
The training procedure of Baseline++ is the same as the original Baseline model except for the
classiﬁer design. As shown in Figure 1  we still have a weight matrix Wb2Rdcof the classiﬁer in
the training stage and a Wnin the ﬁne-tuning stage in Baseline++. The classiﬁer design  however  is
different from the linear classiﬁer used in the Baseline. Take the weight matrix Wbas an example.
We can write the weight matrix Wbas[w1;w2;:::wc]  where each class has a d-dimensional weight
vector. In the training stage  for an input feature fq(xi)where xi2Xb  we compute its cosine
similarity to each weight vector [w1;;wc]and obtain the similarity scores [si;1;si;2;;si;c]for
all classes  where si;j=fq(xi)>wj=kfq(xi)kkwjk. We can then obtain the prediction probability
for each class by normalizing these similarity scores with a softmax function. Here  the classiﬁer
makes a prediction based on the cosine distance between the input feature and the learned weight
vectors representing each class. Consequently  training the model with this distance-based classiﬁer
explicitly reduce intra-class variations. Intuitively  the learned weight vectors [w1;;wc]can be
interpreted as prototypes (similar to Snell et al. (2017); Vinyals et al. (2016)) for each class and the
classiﬁcation is based on the distance of the input feature to these learned prototypes. The softmax
function prevents the learned weight vectors collapsing to zeros.
We clarify that the network design in Baseline++ is notour contribution. The concept of distance-
based classiﬁcation has been extensively studied in Mensink et al. (2012) and recently has been
revisited in the few-shot classiﬁcation setting Gidaris & Komodakis (2018); Qi et al. (2018).
3.3 M ETA-LEARNING ALGORITHMS
Here we describe the formulations of meta-learning methods used in our study. We consider three
distance metric learning based methods (MatchingNet Vinyals et al. (2016)  ProtoNet Snell et al.
4
Meta-training stageMeta-testing stage
Support setconditioned modelNovel support set    (Novel class data     )Base query set
Base support set
𝑌'Sampled 𝑁classesSupport set conditioned modelFeature extractorMatchingNetCosine distanceRelationNetRelationModule𝜇ProtoNetEuclideandistance𝜇MAMLGradientLinearLinearBase class data (Many)
Class meanClass mean
Figure 2: Meta-learning few-shot classiﬁcation algorithms. The meta-learning classiﬁer M(jS)
is conditioned on the support set S. (Top) In the meta-train stage  the support set Sband the query
setQbare ﬁrst sampled from random Nclasses  and then train the parameters in M(:jSb)to
minimize the N-way prediction loss LN way. In the meta-testing stage  the adapted classiﬁer
M(:jSn)can predict novel classes with the support set in the novel classes Sn. (Bottom ) The design
ofM(jS)in different meta-learning algorithms.
(2017)  and RelationNet Sung et al. (2018)) and one initialization based method (MAML Finn et al.
(2017)). While meta-learning is not a clearly deﬁned  Vinyals et al. (2016) considers a few-shot
classiﬁcation method as meta-learning if the prediction is conditioned on a small support set S
because it makes the training procedure explicitly learn to learn from a given small support set.
As shown in Figure 2  meta-learning algorithms consist of a meta-training and a meta-testing stage.
In the meta-training stage  the algorithm ﬁrst randomly select Nclasses  and sample small base
support set Sband a base query set Qbfrom data samples within these classes. The objective is
to train a classiﬁcation model Mthat minimizes N-way prediction loss LN wayof the samples in
the query set Qb. Here  the classiﬁer Mis conditioned on provided support set Sb. By making
prediction conditioned on the given support set  a meta-learning method can learn how to learn from
limited labeled data through training from a collection of tasks (episodes). In the meta-testing stage
all novel class data Xnare considered as the support set for novel classes Sn  and the classiﬁcation
model Mcan be adapted to predict novel classes with the new support set Sn.
Different meta-learning methods differ in their strategies to make prediction conditioned on support
set (see Figure 2). For both MatchingNet Vinyals et al. (2016) and ProtoNet Snell et al. (2017)  the
prediction of the examples in a query set Qis based on comparing the distance between the query
feature and the support feature from each class. MatchingNet compares cosine distance between the
query feature and each support feature  and computes average cosine distance for each class  while
ProtoNet compares the Euclidean distance between query features and the class mean of support
features. RelationNet Sung et al. (2018) shares a similar idea  but it replaces distance with a learn-
able relation module. The MAML method Finn et al. (2017) is an initialization based meta-learning
algorithm  where each support set is used to adapt the initial model parameters using few gradient
updates. As different support sets have different gradient updates  the adapted model is conditioned
on the support set. Note that when the query set instances are predicted by the adapted model in
the meta-training stage  the loss of the query set is used to update the initial model  not the adapted
model.
4 E XPERIMENTAL RESULTS
4.1 E XPERIMENTAL SETUP
Datasets and scenarios. We address the few-shot classiﬁcation problem under three scenarios: 1)
generic object recognition  2) ﬁne-grained image classiﬁcation  and 3) cross-domain adaptation.
5
For object recognition  we use the mini-ImageNet dataset commonly used in evaluating few-shot
classiﬁcation algorithms. The mini-ImageNet dataset consists of a subset of 100 classes from the
ImageNet dataset Deng et al. (2009) and contains 600 images for each class. The dataset was ﬁrst
proposed by Vinyals et al. (2016)  but recent works use the follow-up setting provided by Ravi &
Larochelle (2017)  which is composed of randomly selected 64 base  16 validation  and 20 novel
classes.
For ﬁne-grained classiﬁcation  we use CUB-200-2011 dataset Wah et al. (2011) (referred to as the
CUB hereafter). The CUB dataset contains 200 classes and 11 788 images in total. Following
the evaluation protocol of Hilliard et al. (2018)  we randomly split the dataset into 100 base  50
validation  and 50 novel classes.
For the cross-domain scenario ( mini-ImageNet!CUB)  we use mini-ImageNet as our base class
and the 50 validation and 50 novel class from CUB. Evaluating the cross-domain scenario allows us
to understand the effects of domain shifts to existing few-shot classiﬁcation approaches.
Implementation details. In the training stage for the Baseline and the Baseline++ methods  we
train 400 epochs with a batch size of 16. In the meta-training stage for meta-learning methods  we
train 60 000 episodes for 1-shot and 40 000 episodes for 5-shot tasks. We use the validation set
to select the training episodes with the best accuracy.2In each episode  we sample Nclasses to
form N-way classiﬁcation ( Nis 5 in both meta-training and meta-testing stages unless otherwise
mentioned). For each class  we pick klabeled instances as our support set and 16 instances for the
query set for a k-shot task.
In the ﬁne-tuning or meta-testing stage for all methods  we average the results over 600 experiments.
In each experiment  we randomly sample 5 classes from novel classes  and in each class  we also
pick kinstances for the support set and 16 for the query set. For Baseline and Baseline++  we use the
entire support set to train a new classiﬁer for 100 iterations with a batch size of 4. For meta-learning
methods  we obtain the classiﬁcation model conditioned on the support set as in Section 3.3.
All methods are trained from scratch and use the Adam optimizer with initial learning rate 10 3.
We apply standard data augmentation including random crop  left-right ﬂip  and color jitter in both
the training or meta-training stage. Some implementation details have been adjusted individually
for each method. For Baseline++  we multiply the cosine similarity by a constant scalar 2 to adjust
original value range [-1 1] to be more appropriate for subsequent softmax layer. For MatchingNet
we use an FCE classiﬁcation layer without ﬁne-tuning in all experiments and also multiply cosine
similarity by a constant scalar. For RelationNet  we replace the L2 norm with a softmax layer
to expedite training. For MAML  we use a ﬁrst-order approximation in the gradient for memory
efﬁciency. The approximation has been shown in the original paper and in our appendix to have
nearly identical performance as the full version. We choose the ﬁrst-order approximation for its
efﬁciency.
4.2 E VALUATION USING THE STANDARD SETTING
We now conduct experiments on the most common setting in few-shot classiﬁcation  1-shot and
5-shot classiﬁcation  i.e.  1 or 5 labeled instances are available from each novel class. We use a
four-layer convolution backbone (Conv-4) with an input size of 84x84 as in Snell et al. (2017) and
perform 5-way classiﬁcation for only novel classes during the ﬁne-tuning or meta-testing stage.
To validate the correctness of our implementation  we ﬁrst compare our results to the reported num-
bers for the mini-ImageNet dataset in Table 1. Note that we have a ProtoNet#  as we use 5-way
classiﬁcation in the meta-training and meta-testing stages for all meta-learning methods as men-
tioned in Section 4.1; however  the ofﬁcial reported results from ProtoNet uses 30-way for one shot
and 20-way for ﬁve shot in the meta-training stage in spite of using 5-way in the meta-testing stage.
We report this result for completeness.
From Table 1  we can observe that all of our re-implementation for meta-learning methods do not
fall more than 2% behind reported performance. These minor differences can be attributed to our
2For example  the exact episodes for experiments on the mini-ImageNet in the 5-shot setting with a four-
layer ConvNet are: ProtoNet: 24 600; MatchingNet: 35 300; RelationNet: 37 100; MAML: 36 700.
3Reported results are from Ravi & Larochelle (2017)
6
Table 1: Validating our re-implementation. We validate our few-shot classiﬁcation
implementation on the mini-ImageNet dataset using a Conv-4 backbone. We report the mean of
600 randomly generated test episodes as well as the 95% conﬁdence intervals. Our reproduced
results to all few-shot methods do not fall behind by more than 2% to the reported results in the
literature. We attribute the slight discrepancy to different random seeds and minor implementation
differences in each method. “Baseline” denotes the results without applying data augmentation
during training. ProtoNet#indicates performing 30-way classiﬁcation in 1-shot and 20-way in
5-shot during the meta-training stage.
1-shot 5-shot
Method Reported Ours Reported Ours
Baseline - 42.110.71 - 62.53 0.69
Baseline341.080.70 36.350.64 51.040.65 54.500.66
MatchingNet3Vinyals et al. (2016) 43.56 0.84 48.140.78 55.310.73 63.480.66
ProtoNet - 44.420.84 - 64.24 0.72
ProtoNet#Snell et al. (2017) 49.42 0.78 47.740.84 68.200.66 66.680.68
MAML Finn et al. (2017) 48.07 1.75 46.470.82 63.150.91 62.710.71
RelationNet Sung et al. (2018) 50.44 0.82 49.310.85 65.320.70 66.600.69
Table 2: Few-shot classiﬁcation results for both the mini -ImageNet and CUB datasets. The
Baseline++ consistently improves the Baseline model by a large margin and is competitive with the
state-of-the-art meta-learning methods. All experiments are from 5-way classiﬁcation with a
Conv-4 backbone and data augmentation.
CUB mini -ImageNet
Method 1-shot 5-shot 1-shot 5-shot
Baseline 47.120.74 64.160.71 42.110.71 62.530.69
Baseline++ 60.530.83 79.340.61 48.240.75 66.430.63
MatchingNet Vinyals et al. (2016) 61.16 0.89 72.860.70 48.140.78 63.480.66
ProtoNet Snell et al. (2017) 51.31 0.91 70.770.69 44.420.84 64.240.72
MAML Finn et al. (2017) 55.92 0.95 72.090.76 46.470.82 62.710.71
RelationNet Sung et al. (2018) 62.45 0.98 76.110.69 49.310.85 66.600.69
modiﬁcations of some implementation details to ensure a fair comparison among all methods  such
as using the same optimizer for all methods.
Moreover  our implementation of existing work also improves the performance of some of the meth-
ods. For example  our results show that the Baseline approach under 5-shot setting can be improved
by a large margin since previous implementations of the Baseline do not include data augmentation
in their training stage  thereby leads to over-ﬁtting. While our Baselineis not as good as reported
in 1-shot  our Baseline with augmentation still improves on it  and could be even higher if our re-
produced Baselinematches the reported statistics. In either case  the performance of the Baseline
method is severely underestimated . We also improve the results of MatchingNet by adjusting the
input score to the softmax layer to a more appropriate range as stated in Section 4.1. On the other
hand  while ProtoNet#is not as good as ProtoNet  as mentioned in the original paper a more chal-
lenging setting in the meta-training stage leads to better accuracy. We choose to use a consistent
5-way classiﬁcation setting in subsequent experiments to have a fair comparison to other methods.
This issue can be resolved by using a deeper backbone as shown in Section 4.3.
After validating our re-implementation  we now report the accuracy in Table 2. Besides additionally
reporting results on the CUB dataset  we also compare Baseline++ to other methods. Here  we
ﬁnd that Baseline++ improves the Baseline by a large margin and becomes competitive even when
compared with other meta-learning methods. The results demonstrate that reducing intra-class
variation is an important factor in the current few-shot classiﬁcation problem setting.
7
45%55%65%75%Conv-4Conv-6ResNet-10ResNet-18ResNet-34
60% 70% 80% 90% Conv-4Conv-6ResNet-10ResNet-18ResNet-34
40% 45% 50% 55% Conv-4Conv-6ResNet-10ResNet-18ResNet-34
60% 65% 70% 75% 80% Conv-4Conv-6ResNet-10ResNet-18ResNet-34
BaselineBaseline++MatchingNetProtoNetMAMLRelationNet
CUB
1-shot 5-shotmini-ImageNet
Figure 3: Few-shot classiﬁcation accuracy vs. backbone depth . In the CUB dataset  gaps among
different methods diminish as the backbone gets deeper. In mini-ImageNet 5-shot  some
meta-learning methods are even beaten by Baseline with a deeper backbone. (Please refer to
Figure A3 and Table A5 for larger ﬁgure and detailed statistics.)
However  note that our current setting only uses a 4-layer backbone  while a deeper backbone can
inherently reduce intra-class variation. Thus  we conduct experiments to investigate the effects of
backbone depth in the next section.
4.3 E FFECT OF INCREASING THE NETWORK DEPTH
In this section  we change the depth of the feature backbone to reduce intra-class variation for all
methods. See appendix for statistics on how network depth correlates with intra-class variation.
Starting from Conv-4  we gradually increase the feature backbone to Conv-6  ResNet-10  18 and 34
where Conv-6 have two additional convolution blocks without pooling after Conv-4. ResNet-18 and
34 are the same as described in He et al. (2016) with an input size of 224 224  while ResNet-10 is
a simpliﬁed version of ResNet-18 where only one residual building block is used in each layer. The
statistics of this experiment would also be helpful to other works to make a fair comparison under
different feature backbones.
Results of the CUB dataset shows a clearer tendency in Figure 3. As the backbone gets deeper  the
gap among different methods drastically reduces. Another observation is how ProtoNet improves
rapidly as the backbone gets deeper. While using a consistent 5-way classiﬁcation as discussed in
Section 4.2 degrades the accuracy of ProtoNet with Conv-4  it works well with a deeper backbone.
Thus  the two observations above demonstrate that in the CUB dataset  the gap among existing
methods would be reduced if their intra-class variation are all reduced by a deeper backbone.
However  the result of mini-ImageNet in Figure 3 is much more complicated. In the 5-shot setting
both Baseline and Baseline++ achieve good performance with a deeper backbone  but some meta-
learning methods become worse relative to them. Thus  other than intra-class variation  we can
assume that the dataset is also important in few-shot classiﬁcation. One difference between CUB and
mini-ImageNet is their domain difference in base and novel classes since classes in mini-ImageNet
have a larger divergence than CUB in a word-net hierarchy Miller (1995). To better understand the
effect  below we discuss how domain differences between base and novel classes impact few-shot
classiﬁcation results.
4.4 E FFECT OF DOMAIN DIFFERENCES BETWEEN BASE AND NOVEL CLASSES
To further dig into the issue of domain difference  we design scenarios that provide such domain
shifts. Besides the ﬁne-grained classiﬁcation and object recognition scenarios  we propose a new
cross-domain scenario :mini-ImageNet!CUB as mentioned in Section 4.1. We believe that
this is practical scenario since collecting images from a general class may be relatively easy (e.g.
due to increased availability) but collecting images from ﬁne-grained classes might be more difﬁcult.
We conduct the experiments with a ResNet-18 feature backbone. As shown in Table 3  the Baseline
outperforms all meta-learning methods under this scenario. While meta-learning methods learn to
learn from the support set during the meta-training stage  they are not able to adapt to novel classes
that are too different since all of the base support sets are within the same dataset. A similar concept
is also mentioned in Vinyals et al. (2016). In contrast  the Baseline simply replaces and trains a
new classiﬁer based on the few given novel class data  which allows it to quickly adapt to a novel
8
mini -ImageNet!CUB
Baseline 65.57 0.70
Baseline++ 62.040.76
MatchingNet 53.070.74
ProtoNet 62.020.70
MAML 51.340.72
RelationNet 57.710.73
Table 3: 5-shot accuracy under the
cross-domain scenario with a ResNet-18
backbone. Baseline outperforms all other
methods under this scenario.
40% 50% 60% 70% 80% 90%
CUBminiImageNetminiImageNet -> CUB BaselineBaseline++MatchingNetProtoNetMAMLRelationNet
Domain DifferenceLargeSmallFigure 4: 5-shot accuracy in different scenarios
with a ResNet-18 backbone. The Baseline
model performs relative well with larger domain
differences.
CUB mini-ImageNet CUB !mini-ImageNet
Figure 5: Meta-learning methods with further adaptation steps. Further adaptation improves
MatchingNet and MAML  but has less improvement to RelationNet  and could instead harm
ProtoNet under the scenarios with little domain differences.All statistics are for 5-shot accuracy
with ResNet-18 backbone. Note that different methods use different further adaptation strategies.
class and is less affected by domain shift between the source and target domains. The Baseline
also performs better than the Baseline++ method  possibly because additionally reducing intra-class
variation compromises adaptability. In Figure 4  we can further observe how Baseline accuracy
becomes relatively higher as the domain difference gets larger. That is  as the domain difference
grows larger  the adaptation based on a few novel class instances becomes more important.
4.5 E FFECT OF FURTHER ADAPTATION
To further adapt meta-learning methods as in the Baseline method  an intuitive way is to ﬁx the
features and train a new softmax classiﬁer. We apply this simple adaptation scheme to MatchingNet
and ProtoNet. For MAML  it is not feasible to ﬁx the feature as it is an initialization method. In
contrast  since it updates the model with the support set for only a few iterations  we can adapt
further by updating for as many iterations as is required to train a new classiﬁcation layer  which
is 100 updates as mentioned in Section 4.1. For RelationNet  the features are convolution maps
rather than the feature vectors  so we are not able to replace it with a softmax. As an alternative  we
randomly split the few training data in novel class into 3 support and 2 query data to ﬁnetune the
relation module for 100 epochs.
The results of further adaptation are shown in Figure 5; we can observe that the performance of
MatchingNet and MAML improves signiﬁcantly after further adaptation  particularly in the mini-
ImageNet!CUB scenario. The results demonstrate that lack of adaptation is the reason they fall
behind the Baseline. However  changing the setting in the meta-testing stage can lead to inconsis-
tency with the meta-training stage. The ProtoNet result shows that performance can degrade in sce-
9
narios with less domain difference. Thus  we believe that learning how to adapt in the meta-training
stage is important future direction. In summary  as domain differences are likely to exist in many
real-world applications  we consider that learning to learn adaptation in the meta-training stage
would be an important direction for future meta-learning research in few-shot classiﬁcation.
5 C ONCLUSIONS
In this paper  we have investigated the limits of the standard evaluation setting for few-shot classi-
ﬁcation. Through comparing methods on a common ground  our results show that the Baseline++
model is competitive to state of art under standard conditions  and the Baseline model achieves
competitive performance with recent state-of-the-art meta-learning algorithms on both CUB and
mini-ImageNet benchmark datasets when using a deeper feature backbone. Surprisingly  the Base-
line compares favorably against all the evaluated meta-learning algorithms under a realistic scenario
where there exists domain shift between the base and novel classes. By making our source code
publicly available  we believe that community can beneﬁt from the consistent comparative experi-
ments and move forward to tackle the challenge of potential domain shifts in the context of few-shot
learning.
REFERENCES
Antreas Antoniou  Amos Storkey  and Harrison Edwards. Data augmentation generative adversarial
networks. In Proceedings of the International Conference on Learning Representations Work-
shops (ICLR Workshops)   2018. 1  3
Luca Bertinetto  Jo ˜ao F Henriques  Philip HS Torr  and Andrea Vedaldi. Meta-learning with dif-
ferentiable closed-form solvers. In Proceedings of the International Conference on Learning
Representations (ICLR)   2019. 3
Gregory Cohen  Saeed Afshar  Jonathan Tapson  and Andr ´e van Schaik. Emnist: an extension of
mnist to handwritten letters. arXiv preprint arXiv:1702.05373   2017. 13
David L Davies and Donald W Bouldin. A cluster separation measure. IEEE Transactions on
Pattern Analysis and Machine Intelligence   1979. 14
Jia Deng  Wei Dong  Richard Socher  Li-Jia Li  Kai Li  and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)   2009. 6
Nanqing Dong and Eric P Xing. Domain adaption in one-shot learning. In Joint European Con-
ference on Machine Learning and Knowledge Discovery in Databases. Springer   2018. 3  12
13
Chelsea Finn  Pieter Abbeel  and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the International Conference on Machine Learning (ICML)
2017. 1  2  5  7  12
Chelsea Finn  Kelvin Xu  and Sergey Levine. Probabilistic model-agnostic meta-learning. In Ad-
vances in Neural Information Processing Systems (NIPS)   2018. 2
Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In
Proceedings of the International Conference on Machine Learning (ICML)   2015. 3
Victor Garcia and Joan Bruna. Few-shot learning with graph neural networks. In Proceedings of the
International Conference on Learning Representations (ICLR)   2018. 1  3
Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)   2018.
1  2  3  4
Bharath Hariharan and Ross Girshick. Low-shot visual recognition by shrinking and hallucinating
features. In Proceedings of the IEEE International Conference on Computer Vision (ICCV)   2017.
1  3
10
Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)   2016. 8
Nathan Hilliard  Lawrence Phillips  Scott Howland  Art ¨em Yankov  Courtney D Corley  and
Nathan O Hodas. Few-shot learning with metric-agnostic conditional embeddings. arXiv preprint
arXiv:1802.04376   2018. 6
Yen-Chang Hsu  Zhaoyang Lv  and Zsolt Kira. Learning to cluster in order to transfer across do-
mains and tasks. 2018. 3  12
Junlin Hu  Jiwen Lu  and Yap-Peng Tan. Deep transfer metric learning. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR)   2015. 4
Gregory Koch  Richard Zemel  and Ruslan Salakhutdinov. Siamese neural networks for one-shot
image recognition. In Proceedings of the International Conference on Machine Learning Work-
shops (ICML Workshops)   2015. 2
Brenden Lake  Ruslan Salakhutdinov  Jason Gross  and Joshua Tenenbaum. One shot learning of
simple visual concepts. In Cogsci   2011. 13
Thomas Mensink  Jakob Verbeek  Florent Perronnin  and Gabriela Csurka. Metric learning for large
scale image classiﬁcation: Generalizing to new classes at near-zero cost. In Proceedings of the
European Conference on Computer Vision (ECCV) . Springer  2012. 4
George A Miller. Wordnet: a lexical database for english. Communications of the ACM   1995. 8
Saeid Motiian  Quinn Jones  Seyed Iranmanesh  and Gianfranco Doretto. Few-shot adversarial
domain adaptation. In Advances in Neural Information Processing Systems (NIPS)   2017. 12
Tsendsuren Munkhdalai and Hong Yu. Meta networks. In Proceedings of the International Confer-
ence on Machine Learning (ICML)   2017. 2
Alex Nichol and John Schulman. Reptile: a scalable metalearning algorithm. arXiv preprint
arXiv:1803.02999   2018. 2
Sinno Jialin Pan  Qiang Yang  et al. A survey on transfer learning. IEEE Transactions on Knowledge
and Data Engineering (TKDE)   2010. 3
Hang Qi  Matthew Brown  and David G Lowe. Low-shot learning with imprinted weights. In
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In Proceedings
of the International Conference on Learning Representations (ICLR)   2017. 1  2  6  12  13
Andrei A Rusu  Dushyant Rao  Jakub Sygnowski  Oriol Vinyals  Razvan Pascanu  Simon Osindero
and Raia Hadsell. Meta-learning with latent embedding optimization. In Proceedings of the
International Conference on Learning Representations (ICLR)   2019. 2
Jake Snell  Kevin Swersky  and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in Neural Information Processing Systems (NIPS)   2017. 1  3  4  5  6  7  12  13  16
Flood Sung  Yongxin Yang  Li Zhang  Tao Xiang  Philip HS Torr  and Timothy M Hospedales.
Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR)   2018. 1  3  5  7
Oriol Vinyals  Charles Blundell  Tim Lillicrap  Daan Wierstra  et al. Matching networks for one
shot learning. In Advances in Neural Information Processing Systems (NIPS)   2016. 1  3  4  5  6
7  8  12  13
Catherine Wah  Steve Branson  Peter Welinder  Pietro Perona  and Serge Belongie. The caltech-ucsd
birds-200-2011 dataset. 2011. 6
Yu-Xiong Wang  Ross Girshick  Martial Hebert  and Bharath Hariharan. Low-shot learning from
imaginary data. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR)   2018. 1  3  12
11
APPENDIX
A1 R ELATIONSHIP BETWEEN DOMAIN ADAPTATION AND FEW -SHOT CLASSIFICATION
As mentioned in Section 2  here we discuss the relationship between domain adaptation andfew-
shot classiﬁcation to clarify different experimental settings. As shown in Table A1  in general
domain adaptation aims at adapting source dataset knowledge to the same class in target dataset.
On the other hand  the goal of few-shot classiﬁcation is to learn from base classes to classify novel
classes in the same dataset.
Several recent work tackle the problem at the intersection of the two ﬁelds of study. For example
cross-task domain adaptation Hsu et al. (2018) also discuss novel classes in the target dataset. In
contrast  while Motiian et al. (2017) has “few-shot” in the title  their evaluation setting focuses on
classifying the same class in the target dataset.
If base and novel classes are both drawn from the same dataset  minor domain shift exists between
the base and novel classes  as we demonstrated in Section 4.4. To highlight the impact of domain
shift  we further propose the mini-ImageNet!CUB setting. The domain shift in few-shot classiﬁ-
cation is also discussed in Dong & Xing (2018).
Table A1: Relationship between domain adaptation and few-shot classiﬁcation. The two
ﬁeld-of-studies have overlapping in the development. Notation ”*” indicates minor domain shifts
exist between base and novel classes.
Domain shift Source to target dataset Base to novel class
Domain adaptation
Motiian et al. (2017)V V -
Cross-task domain adaptation
Hsu et al. (2018)V V V
Few-shot classiﬁcation
Ours (CUB  mini-ImageNet )* - V
Cross-domain few-shot
Ours ( mini-ImageNet!CUB)
Dong & Xing (2018)V V V
A2 T ERMINOLOGY DIFFERENCE
Different meta-learning works use different terminology in their works. We highlight their differ-
ences in appendix Table A2 to clarify the inconsistency.
Table A2: Different terminology used in other works. Notation ”-” indicates the term is the same
as in this paper.
Our termsMatchingNet
Vinyals et al.ProtoNet
Snell et al.MAML
Finn et al.Meta-learn LSTM
Ravi & LarochelleImaginary
Wang et al.
meta-training stage training training - - -
meta-testing stage test test - - -
base class training set training set task meta-training set -
novel class test set test set new task meta-testing set -
support set - - sample training dataset training data
query set batch - test time sample test dataset test data
12
A3 A DDITIONAL RESULTS ON OMNIGLOT AND OMNIGLOT!EMNIST
For completeness  here we also show the results under two additional scenarios in 4) character
recognition 5) cross-domain character recognition.
For character recognition  we use the Omniglot dataset Lake et al. (2011) commonly used in eval-
uating few-shot classiﬁcation algorithms. Omniglot contains 1 623 characters from 50 languages
and we follow the evaluation protocol of Vinyals et al. (2016) to ﬁrst augment the classes by rota-
tions in 90  180  270 degrees  resulting in 6492 classes. We then follow Snell et al. (2017) to split
these classes into 4112 base  688 validation  and 1692 novel classes. Unlike Snell et al. (2017)  our
validation classes are only used to monitor the performance during meta-training.
For cross-domain character recognition (Omniglot !EMNIST)  we follow the setting of Dong &
Xing (2018) to use Omniglot without Latin characters and without rotation augmentation as base
classes  so there are 1597 base classes. On the other hand  EMNIST dataset Cohen et al. (2017)
contains 10-digits and upper and lower case alphabets in English  so there are 62 classes in total. We
split these classes into 31 validation and 31 novel classes  and invert the white-on-black characters
to black-on-white as in Omniglot.
We use a Conv-4 backbone with input size 28x28 for both settings. As Omniglot characters are
black-and-white  center-aligned and rotation sensitive  we do not use data augmentation in this ex-
periment. To reduce the risk of over-ﬁtting  we use the validation set to select the epoch or episode
with the best accuracy for all methods  including baseline and baseline++.4
As shown in Table A3  in both Omniglot and Omniglot !EMNIST settings  meta-learning methods
outperform baseline and baseline++ in 1-shot. However  all methods reach comparable performance
in the 5-shot classiﬁcation setting. We attribute this to the lack of data augmentation for the baseline
and baseline++ methods as they tend to over-ﬁt base classes. When sufﬁcient examples in novel
classes are available  the negative impact of over-ﬁtting is reduced.
Table A3: Few-shot classiﬁcation results for both the Omniglot andOmniglot!EMNIST .All
experiments are from 5-way classiﬁcation with a Conv-4 backbone and without data augmentation.
Omniglot Omniglot !EMNIST
Baseline 94.890.45 99.120.13 63.940.87 86.000.59
Baseline++ 95.410.39 99.380.10 64.740.82 87.310.58
MatchingNet 97.780.30 99.370.11 72.710.79 87.600.56
ProtoNet 98.010.30 99.150.12 70.430.80 87.040.55
MAML 98.570.19 99.530.08 72.040.83 88.240.56
RelationNet 97.220.33 99.300.10 75.550.87 88.940.54
A4 B ASELINE WITH 1-NN CLASSIFIER
Some prior work (Vinyals et al. (2016)) apply a Baseline with 1-NN classiﬁer in the test stage. We
include our result as in Table A4. The result shows that using 1-NN classiﬁer has better performance
than that of using the softmax classiﬁer in 1-shot setting  but softmax classiﬁer performs better in
5-shot setting. We note that the number here are not directly comparable to results in Vinyals et al.
(2016) because we use a different mini-ImageNet as in Ravi & Larochelle (2017).
Table A4: Baseline with softmax and 1-NN classiﬁer in test stage. We note that we use cosine
distance in 1-NN.
softmax 1-NN softmax 1-NN
Baseline 42.110.71 44.180.69 62.530.69 56.680.67
Baseline++ 48.240.75 49.570.73 66.430.63 61.930.65
4The exact epoch of baseline and baseline++ on Omniglot and Omniglot !EMNIST is 5 epochs
A5 MAML AND MAML WITH FIRST -ORDER APPROXIMATION
As discussed in Section 4.1  we use ﬁrst-order approximation MAML to improve memory efﬁciency
in all of our experiments. To demonstrate this design choice does not affect the accuracy  we compare
their validation accuracy trends on Omniglot with 5-shot as in Figure A1. We observe that while the
full version MAML converge faster  both versions reach similar accuracy in the end.
This phenomena is consistent with the difference of ﬁrst-order (e.g. gradient descent) and second-
order methods (e.g. Newton) in convex optimization problems. Second-order methods converge
faster at the cost of memory  but they both converge to similar objective value.
Figure A1: Validation accuracy trends of MAML and MAML with ﬁrst order approximation.
Both versions converge to the same validation accuracy. The experimental results are on Omniglot
with 5-shot with a Conv-4 backbone.
A6 I NTRA -CLASS VARIATION AND BACKBONE DEPTH
As mentioned in Section 4.3  here we demonstrate decreased intra-class variation as the network
depth gets deeper as in Figure A2. We use the Davies-Bouldin index Davies & Bouldin (1979) to
measure intra-class variation. The Davies-Bouldin index is a metric to evaluate the tightness in a
cluster (or class  in our case). Our results show that both intra-class variation in the base and novel
class feature decrease using deeper backbones.
2345678
Conv-4Conv-6Resnet-10Resnet-18Resnet-34
34567
BaselineBaseline++MatchingNetProtoNet
Base class feature Novel class featureDavies-Bouldin index
(Intra-class variation)
Figure A2: Intra-class variation decreases as backbone gets deeper. Here we use
Davies-Bouldin index to represent intra-class variation  which is a metric to evaluate the tightness
in a cluster (or class  in our case). The statistics are Davies-Bouldin index for all base and novel
class feature (extracted by feature extractor learned after training or meta-training stage) for CUB
dataset under different backbone.
14
A7 D ETAILED STATISTICS IN EFFECTS OF INCREASING BACKBONE DEPTH
Here we show a high-resolution version of Figure 3 in Figure A3 and show detailed statistics in
Table A5 for easier comparison.
45% 55% 65% 75%
40% 45% 50% 55%
60% 70% 80% 90%
60% 65% 70% 75% 80%
CUB mini-ImageNet1-shot 5-shot
Figure A3: Few-shot classiﬁcation accuracy vs. backbone depth . In the CUB dataset  gaps
among different methods diminish as the backbone gets deeper. In mini-ImageNet 5-shot  some
meta-learning methods are even beaten by Baseline with a deeper backbone.
Table A5: Detailed statistics in Figure 3. We put exact value here for reference.
Conv-4 Conv-6 Resnet-10 Resnet-18 Resnet-34
1-shotBaseline 47.120.74 55.770.86 63.340.91 65.510.87 67.960.89
Baseline++ 60.530.83 66.000.89 69.550.89 67.020.90 68.000.83
MatchingNet 61.160.89 67.160.97 71.290.90 72.360.90 71.440.96
ProtoNet 51.310.91 66.070.97 70.130.94 71.880.91 72.030.91
MAML 55.920.95 65.910.97 71.290.95 69.961.01 67.281.08
RelationNet 62.450.98 63.110.94 68.650.91 67.591.02 66.200.99
5-shotBaseline 64.160.71 73.070.71 81.270.57 82.850.55 84.270.53
Baseline++ 79.340.61 82.020.55 85.170.50 83.580.54 84.500.51
MatchingNet 72.860.70 77.080.66 83.590.58 83.640.60 83.780.56
ProtoNet 70.770.69 78.140.67 84.760.52 87.420.48 85.980.53
MAML 72.090.76 76.310.74 80.330.70 82.700.65 83.470.59
RelationNet 76.110.69 77.810.66 81.120.63 82.750.58 82.300.58
mini-ImageNet
1-shotBaseline 42.110.71 45.820.74 52.370.79 51.750.80 49.820.73
Baseline++ 48.240.75 48.290.72 53.970.79 51.870.77 52.650.83
MatchingNet 48.140.78 50.470.86 54.490.81 52.910.88 53.200.78
ProtoNet 44.420.84 50.370.83 51.980.84 54.160.82 53.900.83
MAML 46.470.82 50.960.92 54.690.89 49.610.92 51.460.90
RelationNet 49.310.85 51.840.88 52.190.83 52.480.86 51.740.83
5-shotBaseline 62.530.69 66.420.67 74.690.64 74.270.63 73.450.65
Baseline++ 66.430.63 68.090.69 75.900.61 75.680.63 76.160.63
MatchingNet 63.480.66 63.190.70 68.820.65 68.880.69 68.320.66
ProtoNet 64.240.72 67.330.67 72.640.64 73.680.65 74.650.64
MAML 62.710.71 66.090.71 66.620.83 65.720.77 65.900.79
RelationNet 66.600.69 64.550.70 70.200.66 69.830.68 69.610.67
15
A8 M ORE-WAY IN META -TESTING STAGE
We experiment with a practical setting that handles different testing scenarios. Speciﬁcally  we
conduct the experiments of 5-way meta-training and N-way meta-testing (where N = 5  10  20) to
examine the effect of testing scenarios that are different from training.
As in Table A6  we compare the methods Baseline  Baseline++  MatchingNet  ProtoNet  and Re-
lationNet. Note that we are unable to apply the MAML method as MAML learns the initialization
for the classiﬁer and can thus only be updated to classify the same number of classes. Our results
show that for classiﬁcation with a larger N-way in the meta-testing stage  the proposed Baseline++
compares favorably against other methods in both shallow or deeper backbone settings.
We attribute the results to two reasons. First  to perform well in a larger N-way classiﬁcation setting
one needs to further reduce the intra-class variation to avoid misclassiﬁcation. Thus  Baseline++ has
better performance than Baseline in both backbone settings. Second  as meta-learning algorithms
were trained to perform 5-way classiﬁcation in the meta-training stage  the performance of these
algorithms may drop signiﬁcantly when increasing the N-way in the meta-testing stage because the
tasks of 10-way or 20-way classiﬁcation are harder than that of 5-way one.
One may address this issue by performing a larger N-way classiﬁcation in the meta-training stage
(as suggested in Snell et al. (2017)). However  it may encounter the issue of memory constraint.
For example  to perform a 20-way classiﬁcation with 5 support images and 15 query images in each
class  we need to ﬁt a batch size of 400 (20 x (5 + 15)) that must ﬁt into the GPUs. Without special
hardware parallelization  the large batch size may prevent us from training models with deeper
backbones such as ResNet.
Table A6: 5-way meta-training and N-way meta-testing experiment. The experimental results
are on mini-ImageNet with 5-shot. We could see Baseline++ compares favorably against other
methods in both shallow or deeper backbone settings.
Conv-4 ResNet-18
N-way test 5-way 10-way 20-way 5-way 10-way 20-way
Baseline 62.530.69 46.440.41 32.270.24 74.270.63 55.000.46 42.030.25
Baseline++ 66.430.63 52.260.40 38.030.24 75.680.63 63.400.44 50.850.25
MatchingNet 63.480.66 47.610.44 33.970.24 68.880.69 52.270.46 36.780.25
ProtoNet 64.240.68 48.770.45 34.580.23 73.680.65 59.220.44 44.960.26
RelationNet 66.60 0.69 47.770.43 33.720.22 69.830.68 53.880.48 39.170.25
16
PublishedasaconferencepaperatICLR2019
A CLOSER LOOK AT FEW-SHOT CLASSIFICATION
Wei-YuChen Yen-ChengLiu&ZsoltKira
CarnegieMellonUniversity GeorgiaTech
weiyuc@andrew.cmu.edu {ycliu zkira}@gatech.edu
Yu-ChiangFrankWang Jia-BinHuang
NationalTaiwanUniversity VirginiaTech
ycwang@ntu.edu.tw jbhuang@vt.edu
Few-shotclassiﬁcationaimstolearnaclassiﬁertorecognizeunseenclassesduring
trainingwithlimitedlabeledexamples.Whilesigniﬁcantprogresshasbeenmade
thegrowingcomplexityofnetworkdesigns meta-learningalgorithms anddiffer-
wepresent1)aconsistentcomparativeanalysisofseveralrepresentativefew-shot
classiﬁcationalgorithms withresultsshowingthatdeeperbackbonessigniﬁcantly
reducetheperformancedifferencesamongmethodsondatasetswithlimiteddo-
ImageNetandtheCUBdatasets and3)anewexperimentalsettingforevaluating
thecross-domaingeneralizationabilityforfew-shotclassiﬁcationalgorithms.Our
featurebackboneisshallow butnotascriticalwhenusingdeeperbackbones. In
arealisticcross-domainevaluationsetting  weshowthatabaselinemethodwith
few-shotlearningalgorithms.",reduce the performance differences among,"Inthispaper wehaveinvestigatedthelimitsofthestandardevaluationsettingforfew-shotclassi-
ﬁcation. Throughcomparingmethodsonacommonground  ourresultsshowthattheBaseline++
mini-ImageNetbenchmarkdatasetswhenusingadeeperfeaturebackbone. Surprisingly theBase-
linecomparesfavorablyagainstalltheevaluatedmeta-learningalgorithmsunderarealisticscenario
publiclyavailable  webelievethatcommunitycanbeneﬁtfromtheconsistentcomparativeexperi-
mentsandmoveforwardtotacklethechallengeofpotentialdomainshiftsinthecontextoffew-shot
AntreasAntoniou AmosStorkey andHarrisonEdwards. Dataaugmentationgenerativeadversarial
shops(ICLRWorkshops) 2018. 1 3
Luca Bertinetto  Joa˜o F Henriques  Philip HS Torr  and Andrea Vedaldi. Meta-learning with dif-
Representations(ICLR) 2019. 3
Gregory Cohen  Saeed Afshar  Jonathan Tapson  and Andre´ van Schaik. Emnist: an extension of
mnisttohandwrittenletters. arXivpreprintarXiv:1702.05373 2017. 13
PatternAnalysisandMachineIntelligence 1979. 14
PatternRecognition(CVPR) 2009. 6
ference on Machine Learning and Knowledge Discovery in Databases. Springer  2018. 3  12
ChelseaFinn PieterAbbeel andSergeyLevine. Model-agnosticmeta-learningforfastadaptation
ofdeepnetworks. InProceedingsoftheInternationalConferenceonMachineLearning(ICML)
2017. 1 2 5 7 12
vancesinNeuralInformationProcessingSystems(NIPS) 2018. 2
ProceedingsoftheInternationalConferenceonMachineLearning(ICML) 2015. 3
VictorGarciaandJoanBruna. Few-shotlearningwithgraphneuralnetworks. InProceedingsofthe
InternationalConferenceonLearningRepresentations(ICLR) 2018. 1 3
ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition(CVPR) 2018.
1 2 3 4
BharathHariharanandRossGirshick. Low-shotvisualrecognitionbyshrinkingandhallucinating
features.InProceedingsoftheIEEEInternationalConferenceonComputerVision(ICCV) 2017.
1 3
KaimingHe XiangyuZhang ShaoqingRen andJianSun. Deepresiduallearningforimagerecog-
(CVPR) 2016. 8
Nathan Hilliard  Lawrence Phillips  Scott Howland  Arte¨m Yankov  Courtney D Corley  and
NathanOHodas.Few-shotlearningwithmetric-agnosticconditionalembeddings.arXivpreprint
arXiv:1802.04376 2018. 6
mainsandtasks. 2018. 3 12
JunlinHu JiwenLu andYap-PengTan. Deeptransfermetriclearning. InProceedingsoftheIEEE
ConferenceonComputerVisionandPatternRecognition(CVPR) 2015. 4
imagerecognition. InProceedingsoftheInternationalConferenceonMachineLearningWork-
shops(ICMLWorkshops) 2015. 2
simplevisualconcepts. InCogsci 2011. 13
ThomasMensink JakobVerbeek FlorentPerronnin andGabrielaCsurka. Metriclearningforlarge
EuropeanConferenceonComputerVision(ECCV).Springer 2012. 4
GeorgeAMiller. Wordnet: alexicaldatabaseforenglish. CommunicationsoftheACM 1995. 8
domainadaptation. InAdvancesinNeuralInformationProcessingSystems(NIPS) 2017. 12
TsendsurenMunkhdalaiandHongYu. Metanetworks. InProceedingsoftheInternationalConfer-
enceonMachineLearning(ICML) 2017. 2
arXiv:1803.02999 2018. 2
SinnoJialinPan QiangYang etal.Asurveyontransferlearning.IEEETransactionsonKnowledge
andDataEngineering(TKDE) 2010. 3
SachinRaviandHugoLarochelle. Optimizationasamodelforfew-shotlearning. InProceedings
oftheInternationalConferenceonLearningRepresentations(ICLR) 2017. 1 2 6 12 13
AndreiARusu DushyantRao JakubSygnowski OriolVinyals RazvanPascanu SimonOsindero
InternationalConferenceonLearningRepresentations(ICLR) 2019. 2
AdvancesinNeuralInformationProcessingSystems(NIPS) 2017. 1 3 4 5 6 7 12 13 16
Learningtocompare: Relationnetworkforfew-shotlearning. InProceedingsoftheIEEECon-
ferenceonComputerVisionandPatternRecognition(CVPR) 2018. 1 3 5 7
shotlearning. InAdvancesinNeuralInformationProcessingSystems(NIPS) 2016. 1 3 4 5 6
7 8 12 13
CatherineWah SteveBranson PeterWelinder PietroPerona andSergeBelongie.Thecaltech-ucsd
birds-200-2011dataset. 2011. 6
imaginarydata. InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecog-
nition(CVPR) 2018. 1 3 12
A1 RELATIONSHIPBETWEENDOMAINADAPTATIONANDFEW-SHOTCLASSIFICATION
As mentioned in Section 2  here we discuss the relationship between domain adaptation and few-
Ontheotherhand thegoaloffew-shotclassiﬁcationistolearnfrombaseclassestoclassifynovel
classesinthesamedataset.
Severalrecentworktackletheproblemattheintersectionofthetwoﬁeldsofstudy. Forexample
contrast  whileMotiianetal.(2017)has“few-shot”inthetitle  theirevaluationsettingfocuseson
classifyingthesameclassinthetargetdataset.
Ifbaseandnovelclassesarebothdrawnfromthesamedataset minordomainshiftexistsbetween
the baseand novelclasses  aswe demonstrated inSection 4.4. Tohighlight theimpact of domain
shift wefurtherproposethemini-ImageNet→CUBsetting. Thedomainshiftinfew-shotclassiﬁ-
cationisalsodiscussedinDong&Xing(2018).
TableA1: Relationshipbetweendomainadaptationandfew-shotclassiﬁcation. Thetwo
ﬁeld-of-studieshaveoverlappinginthedevelopment. Notation”*”indicatesminordomainshifts
existbetweenbaseandnovelclasses.
Domainshift Sourcetotargetdataset Basetonovelclass
Domainadaptation
V V -
Motiianetal.(2017)
Cross-taskdomainadaptation
V V V
Hsuetal.(2018)
Few-shotclassiﬁcation
* - V
Ours(CUB mini-ImageNet)
Cross-domainfew-shot
Ours(mini-ImageNet→CUB) V V V
Dong&Xing(2018)
A2 TERMINOLOGYDIFFERENCE
encesinappendixTableA2toclarifytheinconsistency.
TableA2: Differentterminologyusedinotherworks. Notation”-”indicatesthetermisthesame
asinthispaper.
MatchingNet ProtoNet MAML Meta-learnLSTM Imaginary
Ourterms
Vinyalsetal. Snelletal. Finnetal. Ravi&Larochelle Wangetal.
meta-trainingstage training training - - -
meta-testingstage test test - - -
baseclass trainingset trainingset task meta-trainingset -
novelclass testset testset newtask meta-testingset -
supportset - - sample trainingdataset trainingdata
queryset batch - testtimesample testdataset testdata
A3 ADDITIONALRESULTSONOMNIGLOTANDOMNIGLOT→EMNIST
recognition5)cross-domaincharacterrecognition.
Forcharacterrecognition  weusetheOmniglotdatasetLakeetal.(2011)commonlyusedineval-
andwefollowtheevaluationprotocolofVinyalsetal.(2016)toﬁrstaugmenttheclassesbyrota-
tionsin90 180 270degrees resultingin6492classes. WethenfollowSnelletal.(2017)tosplit
theseclassesinto4112base 688validation and1692novelclasses. UnlikeSnelletal.(2017) our
validationclassesareonlyusedtomonitortheperformanceduringmeta-training.
For cross-domain character recognition (Omniglot→EMNIST)  we follow the setting of Dong &
contains10-digitsandupperandlowercasealphabetsinEnglish sothereare62classesintotal.We
splittheseclassesinto31validationand31novelclasses andinvertthewhite-on-blackcharacters
toblack-on-whiteasinOmniglot.
black-and-white center-alignedandrotationsensitive wedonotusedataaugmentationinthisex-
periment. Toreducetheriskofover-ﬁtting weusethevalidationsettoselecttheepochorepisode
withthebestaccuracyforallmethods includingbaselineandbaseline++.4
AsshowninTableA3 inbothOmniglotandOmniglot→EMNISTsettings meta-learningmethods
outperformbaselineandbaseline++in1-shot.However allmethodsreachcomparableperformance
inthe5-shotclassiﬁcationsetting. Weattributethistothelackofdataaugmentationforthebaseline
classesareavailable thenegativeimpactofover-ﬁttingisreduced.
TableA3: Few-shotclassiﬁcationresultsforboththeOmniglotandOmniglot→EMNIST.All
experimentsarefrom5-wayclassiﬁcationwithaConv-4backboneandwithoutdataaugmentation.
Omniglot Omniglot→EMNIST
Baseline 94.89±0.45 99.12±0.13 63.94±0.87 86.00±0.59
Baseline++ 95.41±0.39 99.38±0.10 64.74±0.82 87.31±0.58
MatchingNet 97.78±0.30 99.37±0.11 72.71±0.79 87.60±0.56
ProtoNet 98.01±0.30 99.15±0.12 70.43±0.80 87.04±0.55
MAML 98.57±0.19 99.53±0.08 72.04±0.83 88.24±0.56
RelationNet 97.22±0.33 99.30±0.10 75.55±0.87 88.94±0.54
A4 BASELINEWITH1-NNCLASSIFIER
Somepriorwork(Vinyalsetal.(2016))applyaBaselinewith1-NNclassiﬁerintheteststage. We
includeourresultasinTableA4.Theresultshowsthatusing1-NNclassiﬁerhasbetterperformance
5-shotsetting. WenotethatthenumberherearenotdirectlycomparabletoresultsinVinyalsetal.
(2016)becauseweuseadifferentmini-ImageNetasinRavi&Larochelle(2017).
TableA4: Baselinewithsoftmaxand1-NNclassiﬁerinteststage. Wenotethatweusecosine
distancein1-NN.
Baseline 42.11±0.71 44.18±0.69 62.53±0.69 56.68±0.67
Baseline++ 48.24±0.75 49.57±0.73 66.43±0.63 61.93±0.65
4Theexactepochofbaselineandbaseline++onOmniglotandOmniglot→EMNISTis5epochs
A5 MAMLANDMAMLWITHFIRST-ORDERAPPROXIMATION
AsdiscussedinSection4.1 weuseﬁrst-orderapproximationMAMLtoimprovememoryefﬁciency
inallofourexperiments.Todemonstratethisdesignchoicedoesnotaffecttheaccuracy wecompare
theirvalidationaccuracytrendsonOmniglotwith5-shotasinFigureA1. Weobservethatwhilethe
fullversionMAMLconvergefaster bothversionsreachsimilaraccuracyintheend.
Thisphenomenaisconsistentwiththedifferenceofﬁrst-order(e.g. gradientdescent)andsecond-
fasteratthecostofmemory buttheybothconvergetosimilarobjectivevalue.
FigureA1: ValidationaccuracytrendsofMAMLandMAMLwithﬁrstorderapproximation.
Bothversionsconvergetothesamevalidationaccuracy. TheexperimentalresultsareonOmniglot
with5-shotwithaConv-4backbone.
A6 INTRA-CLASSVARIATIONANDBACKBONEDEPTH
depthgetsdeeperasinFigureA2. WeusetheDavies-BouldinindexDavies&Bouldin(1979)to
cluster(orclass inourcase). Ourresultsshowthatbothintra-classvariationinthebaseandnovel
classfeaturedecreaseusingdeeperbackbones.
Baseline Baseline++ ProtoNet MatchingNet
Baseclassfeature Novelclassfeature
8 7
x)
en
do 7
inati 6
ouldinssvari 56 5
Ba
avies-ntra-cl 4 4
D(I 3
2 3
Conv-4 Conv-6 Resnet-10 Resnet-18 Resnet-34 Conv-4 Conv-6 Resnet-10 Resnet-18 Resnet-34
FigureA2: Intra-classvariationdecreasesasbackbonegetsdeeper. Hereweuse
Davies-Bouldinindextorepresentintra-classvariation whichisametrictoevaluatethetightness
inacluster(orclass inourcase). ThestatisticsareDavies-Bouldinindexforallbaseandnovel
classfeature(extractedbyfeatureextractorlearnedaftertrainingormeta-trainingstage)forCUB
datasetunderdifferentbackbone.
A7 DETAILEDSTATISTICSINEFFECTSOFINCREASINGBACKBONEDEPTH
TableA5foreasiercomparison.
75%  55%
ot65%  50%
h
s
-
155%  45%
45%  40%
90%  80%
ot80%
h 70%
570%
60%  60%
FigureA3: Few-shotclassiﬁcationaccuracyvs. backbonedepth. IntheCUBdataset gaps
amongdifferentmethodsdiminishasthebackbonegetsdeeper. Inmini-ImageNet5-shot some
meta-learningmethodsareevenbeatenbyBaselinewithadeeperbackbone.
TableA5: DetailedstatisticsinFigure3. Weputexactvaluehereforreference.
Baseline 47.12±0.74 55.77±0.86 63.34±0.91 65.51±0.87 67.96±0.89
Baseline++ 60.53±0.83 66.00±0.89 69.55±0.89 67.02±0.90 68.00±0.83
CUB MatchingNet 61.16±0.89 67.16±0.97 71.29±0.90 72.36±0.90 71.44±0.96
1-shot ProtoNet 51.31±0.91 66.07±0.97 70.13±0.94 71.88±0.91 72.03±0.91
MAML 55.92±0.95 65.91±0.97 71.29±0.95 69.96±1.01 67.28±1.08
RelationNet 62.45±0.98 63.11±0.94 68.65±0.91 67.59±1.02 66.20±0.99
Baseline 64.16±0.71 73.07±0.71 81.27±0.57 82.85±0.55 84.27±0.53
Baseline++ 79.34±0.61 82.02±0.55 85.17±0.50 83.58±0.54 84.50±0.51
CUB MatchingNet 72.86±0.70 77.08±0.66 83.59±0.58 83.64±0.60 83.78±0.56
5-shot ProtoNet 70.77±0.69 78.14±0.67 84.76±0.52 87.42±0.48 85.98±0.53
MAML 72.09±0.76 76.31±0.74 80.33±0.70 82.70±0.65 83.47±0.59
RelationNet 76.11±0.69 77.81±0.66 81.12±0.63 82.75±0.58 82.30±0.58
Baseline 42.11±0.71 45.82±0.74 52.37±0.79 51.75±0.80 49.82±0.73
Baseline++ 48.24±0.75 48.29±0.72 53.97±0.79 51.87±0.77 52.65±0.83
mini-ImageNet MatchingNet 48.14±0.78 50.47±0.86 54.49±0.81 52.91±0.88 53.20±0.78
1-shot ProtoNet 44.42±0.84 50.37±0.83 51.98±0.84 54.16±0.82 53.90±0.83
MAML 46.47±0.82 50.96±0.92 54.69±0.89 49.61±0.92 51.46±0.90
RelationNet 49.31±0.85 51.84±0.88 52.19±0.83 52.48±0.86 51.74±0.83
Baseline 62.53±0.69 66.42±0.67 74.69±0.64 74.27±0.63 73.45±0.65
Baseline++ 66.43±0.63 68.09±0.69 75.90±0.61 75.68±0.63 76.16±0.63
mini-ImageNet MatchingNet 63.48±0.66 63.19±0.70 68.82±0.65 68.88±0.69 68.32±0.66
5-shot ProtoNet 64.24±0.72 67.33±0.67 72.64±0.64 73.68±0.65 74.65±0.64
MAML 62.71±0.71 66.09±0.71 66.62±0.83 65.72±0.77 65.90±0.79
RelationNet 66.60±0.69 64.55±0.70 70.20±0.66 69.83±0.68 69.61±0.67
A8 MORE-WAYINMETA-TESTINGSTAGE
conducttheexperimentsof5-waymeta-trainingandN-waymeta-testing(whereN=5  10  20)to
examinetheeffectoftestingscenariosthataredifferentfromtraining.
lationNet. NotethatweareunabletoapplytheMAMLmethodasMAMLlearnstheinitialization
fortheclassiﬁerandcanthusonlybeupdatedtoclassifythesamenumberofclasses. Ourresults
showthatforclassiﬁcationwithalargerN-wayinthemeta-testingstage theproposedBaseline++
comparesfavorablyagainstothermethodsinbothshallowordeeperbackbonesettings.
Weattributetheresultstotworeasons.First toperformwellinalargerN-wayclassiﬁcationsetting
oneneedstofurtherreducetheintra-classvariationtoavoidmisclassiﬁcation.Thus Baseline++has
algorithmsmaydropsigniﬁcantlywhenincreasingtheN-wayinthemeta-testingstagebecausethe
tasksof10-wayor20-wayclassiﬁcationareharderthanthatof5-wayone.
Forexample toperforma20-wayclassiﬁcationwith5supportimagesand15queryimagesineach
class weneedtoﬁtabatchsizeof400(20x(5+15))thatmustﬁtintotheGPUs. Withoutspecial
backbonessuchasResNet.
TableA6: 5-waymeta-trainingandN-waymeta-testingexperiment. Theexperimentalresults
areonmini-ImageNetwith5-shot. WecouldseeBaseline++comparesfavorablyagainstother
methodsinbothshallowordeeperbackbonesettings.
N-waytest 5-way 10-way 20-way 5-way 10-way 20-way
Baseline 62.53±0.69 46.44±0.41 32.27±0.24 74.27±0.63 55.00±0.46 42.03±0.25
Baseline++ 66.43±0.63 52.26±0.40 38.03±0.24 75.68±0.63 63.40±0.44 50.85±0.25
MatchingNet 63.48±0.66 47.61±0.44 33.97±0.24 68.88±0.69 52.27±0.46 36.78±0.25
ProtoNet 64.24±0.68 48.77±0.45 34.58±0.23 73.68±0.65 59.22±0.44 44.96±0.26
RelationNet 66.60±0.69 47.77±0.43 33.72±0.22 69.83±0.68 53.88±0.48 39.17±0.25
weiyuc@andrew.cmu.edu
Yen-Cheng Liu & Zsolt Kira
{ycliu zkira}@gatech.edu
ycwang@ntu.edu.tw
Jia-Bin Huang
INTRODUCTION
Limitations.
While many few-shot classiﬁcation algorithms have reported improved performance
Our work.
In this paper  we present a detailed empirical study to shed new light on the few-shot
RELATED WORK
Initialization based methods
tackle the few-shot learning problem by “learning to ﬁne-tune”.
(2019). Another line of work focuses on learning an optimizer. Examples include the LSTM-based
Distance metric learning based methods
address the few-shot classiﬁcation problem by “learn-
Hallucination based methods
directly deal with data deﬁciency by “learning to augment”. This
techniques aim to reduce the domain shifts between source and target domain
where in the testing stage both the domain and the category to classify are changed. Similarly  our
OVERVIEW OF FEW-SHOT CLASSIFICATION ALGORITHMS
our experiments. Given abundant base class labeled data Xb and a small amount of novel class la-
3.1
BASELINE
Training stage.
We train a feature extractor fθ (parametrized by the network parameters θ) and
the classiﬁer C(·|Wb) (parametrized by the weight matrix Wb ∈ Rd×c) from scratch by minimizing
a standard cross-entropy classiﬁcation loss Lpred using the training examples in the base classes
xi ∈ Xb. Here  we denote the dimension of the encoded feature as d and the number of output
classes as c. The classiﬁer C(.|Wb) consists of a linear layer W⊤
b fθ(xi) followed by a softmax
function σ.
Baseline++
Baseline
Training stage
Feature
extractor
Novel class data
(Few)
Fine-tuning stage
Fixed
Base class data
(Many)
Linear
layer
Softmax
𝜎
Cosine
distance
…
baseline++ method train a feature extractor fθ and classiﬁer C(.|Wb) with base class data in the
training stage In the ﬁne-tuning stage  we ﬁx the network parameters θ in the feature extractor fθ
and train a new classiﬁer C(.|Wn) with the given labeled examples in novel classes. The
Fine-tuning stage.
To adapt the model to recognize novel classes in the ﬁne-tuning stage  we ﬁx
the pre-trained network parameter θ in our feature extractor fθ and train a new classiﬁer C(.|Wn)
(parametrized by the weight matrix Wn) by minimizing Lpred using the few labeled of examples (i.e.
3.2
BASELINE++
classiﬁer design. As shown in Figure 1  we still have a weight matrix Wb ∈ Rd×c of the classiﬁer in
the training stage and a Wn in the ﬁne-tuning stage in Baseline++. The classiﬁer design  however  is
different from the linear classiﬁer used in the Baseline. Take the weight matrix Wb as an example.
We can write the weight matrix Wb as [w1 w2 ...wc]  where each class has a d-dimensional weight
vector. In the training stage  for an input feature fθ(xi) where xi ∈ Xb  we compute its cosine
similarity to each weight vector [w1 ···  wc] and obtain the similarity scores [si 1 si 2 ···  si c] for
all classes  where si  j = fθ(xi)⊤w j/∥ fθ(xi)∥∥w j∥. We can then obtain the prediction probability
explicitly reduce intra-class variations. Intuitively  the learned weight vectors [w1 ···  wc] can be
We clarify that the network design in Baseline++ is not our contribution. The concept of distance-
3.3
META-LEARNING ALGORITHMS
Meta-training stage
Meta-testing stage
Support set
conditioned model
Novel support set
(Novel class data     )
Base query set
MatchingNet
RelationNet
Relation
Module
𝜇
ProtoNet
Euclidean
MAML
Gradient
Class
mean
Figure 2: Meta-learning few-shot classiﬁcation algorithms. The meta-learning classiﬁer M(·|S)
is conditioned on the support set S. (Top) In the meta-train stage  the support set Sb and the query
set Qb are ﬁrst sampled from random N classes  and then train the parameters in M(.|Sb) to
minimize the N-way prediction loss LN−way. In the meta-testing stage  the adapted classiﬁer
M(.|Sn) can predict novel classes with the support set in the novel classes Sn. (Bottom) The design
of M(·|S) in different meta-learning algorithms.
support set Sb and a base query set Qb from data samples within these classes. The objective is
to train a classiﬁcation model M that minimizes N-way prediction loss LN−way of the samples in
the query set Qb. Here  the classiﬁer M is conditioned on provided support set Sb. By making
all novel class data Xn are considered as the support set for novel classes Sn  and the classiﬁcation
model M can be adapted to predict novel classes with the new support set Sn.
prediction of the examples in a query set Q is based on comparing the distance between the query
EXPERIMENTAL RESULTS
4.1
EXPERIMENTAL SETUP
Datasets and scenarios.
We address the few-shot classiﬁcation problem under three scenarios: 1)
Implementation details.
In the training stage for the Baseline and the Baseline++ methods  we
mentioned). For each class  we pick k labeled instances as our support set and 16 instances for the
pick k instances for the support set and 16 for the query set. For Baseline and Baseline++  we use the
4.2
EVALUATION USING THE STANDARD SETTING
fall more than 2% behind reported performance.
These minor differences can be attributed to our
differences in each method. “Baseline∗” denotes the results without applying data augmentation
during training. ProtoNet# indicates performing 30-way classiﬁcation in 1-shot and 20-way in
1-shot
5-shot
Method
Reported
Ours
42.11 ± 0.71
62.53 ±0.69
Baseline∗3
41.08 ± 0.70
36.35 ± 0.64
51.04 ± 0.65
54.50 ±0.66
MatchingNet3 Vinyals et al. (2016)
43.56 ± 0.84
48.14 ± 0.78
55.31 ±0.73
63.48 ±0.66
44.42 ± 0.84
64.24 ±0.72
ProtoNet# Snell et al. (2017)
49.42 ± 0.78
47.74 ± 0.84
68.20 ±0.66
66.68 ±0.68
MAML Finn et al. (2017)
48.07 ± 1.75
46.47 ± 0.82
63.15 ±0.91
62.71 ±0.71
RelationNet Sung et al. (2018)
50.44 ± 0.82
49.31 ± 0.85
65.32 ±0.70
66.60 ±0.69
Table 2: Few-shot classiﬁcation results for both the mini-ImageNet and CUB datasets. The
47.12 ± 0.74
64.16 ± 0.71
60.53 ± 0.83
79.34 ± 0.61
48.24 ± 0.75
66.43 ±0.63
MatchingNet Vinyals et al. (2016)
61.16 ± 0.89
72.86 ± 0.70
ProtoNet Snell et al. (2017)
51.31 ± 0.91
70.77 ± 0.69
55.92 ± 0.95
72.09 ± 0.76
62.45 ± 0.98
76.11 ± 0.69
in their training stage  thereby leads to over-ﬁtting. While our Baseline∗ is not as good as reported
produced Baseline∗ matches the reported statistics. In either case  the performance of the Baseline
method is severely underestimated. We also improve the results of MatchingNet by adjusting the
hand  while ProtoNet# is not as good as ProtoNet  as mentioned in the original paper a more chal-
45%
55%
Conv-4
Conv-6
ResNet-10
ResNet-18
ResNet-34
60%
80%
40%
50%
Figure 3: Few-shot classiﬁcation accuracy vs. backbone depth. In the CUB dataset  gaps among
4.3
EFFECT OF INCREASING THE NETWORK DEPTH
34 are the same as described in He et al. (2016) with an input size of 224×224  while ResNet-10 is
4.4
EFFECT OF DOMAIN DIFFERENCES BETWEEN BASE AND NOVEL CLASSES
cross-domain scenario: mini-ImageNet →CUB as mentioned in
Section 4.1. We believe that
mini-ImageNet →CUB
65.57±0.70
62.04±0.76
53.07±0.74
62.02±0.70
51.34±0.72
57.71±0.73
miniImageNet
miniImageNet -> CUB
Large
Small
Figure 4: 5-shot accuracy in different scenarios
CUB→mini-ImageNet
4.5
EFFECT OF FURTHER ADAPTATION
CONCLUSIONS
shops (ICLR Workshops)  2018. 1  3
Luca Bertinetto  Jo˜ao F Henriques  Philip HS Torr  and Andrea Vedaldi. Meta-learning with dif-
Representations (ICLR)  2019. 3
Gregory Cohen  Saeed Afshar  Jonathan Tapson  and Andr´e van Schaik. Emnist: an extension of
mnist to handwritten letters. arXiv preprint arXiv:1702.05373  2017. 13
Pattern Analysis and Machine Intelligence  1979. 14
Pattern Recognition (CVPR)  2009. 6
of deep networks. In Proceedings of the International Conference on Machine Learning (ICML)
vances in Neural Information Processing Systems (NIPS)  2018. 2
Proceedings of the International Conference on Machine Learning (ICML)  2015. 3
International Conference on Learning Representations (ICLR)  2018. 1  3
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2018.
features. In Proceedings of the IEEE International Conference on Computer Vision (ICCV)  2017.
(CVPR)  2016. 8
Nathan Hilliard  Lawrence Phillips  Scott Howland  Art¨em Yankov  Courtney D Corley  and
arXiv:1802.04376  2018. 6
Conference on Computer Vision and Pattern Recognition (CVPR)  2015. 4
shops (ICML Workshops)  2015. 2
simple visual concepts. In Cogsci  2011. 13
European Conference on Computer Vision (ECCV). Springer  2012. 4
George A Miller. Wordnet: a lexical database for english. Communications of the ACM  1995. 8
Saeid Motiian  Quinn Jones  Seyed Iranmanesh  and Gianfranco Doretto.
Few-shot adversarial
domain adaptation. In Advances in Neural Information Processing Systems (NIPS)  2017. 12
ence on Machine Learning (ICML)  2017. 2
Alex Nichol and John Schulman.
Reptile: a scalable metalearning algorithm.
arXiv preprint
arXiv:1803.02999  2018. 2
and Data Engineering (TKDE)  2010. 3
of the International Conference on Learning Representations (ICLR)  2017. 1  2  6  12  13
International Conference on Learning Representations (ICLR)  2019. 2
Advances in Neural Information Processing Systems (NIPS)  2017. 1  3  4  5  6  7  12  13  16
ference on Computer Vision and Pattern Recognition (CVPR)  2018. 1  3  5  7
shot learning. In Advances in Neural Information Processing Systems (NIPS)  2016. 1  3  4  5  6
nition (CVPR)  2018. 1  3  12
A1
RELATIONSHIP BETWEEN DOMAIN ADAPTATION AND FEW-SHOT CLASSIFICATION
shift  we further propose the mini-ImageNet →CUB setting. The domain shift in few-shot classiﬁ-
Domain shift
Source to target dataset
Base to novel class
Motiian et al. (2017)
V
Hsu et al. (2018)
Ours (CUB  mini-ImageNet )
*
Ours (mini-ImageNet →CUB)
Dong & Xing (2018)
A2
TERMINOLOGY DIFFERENCE
Our terms
Vinyals et al.
Snell et al.
Finn et al.
Meta-learn LSTM
Ravi & Larochelle
Imaginary
meta-training stage
training
meta-testing stage
test
base class
training set
task
meta-training set
novel class
test set
new task
meta-testing set
support set
sample
training dataset
training data
query set
batch
test time sample
test dataset
test data
A3
ADDITIONAL RESULTS ON OMNIGLOT AND OMNIGLOT→EMNIST
As shown in Table A3  in both Omniglot and Omniglot→EMNIST settings  meta-learning methods
Table A3: Few-shot classiﬁcation results for both the Omniglot and Omniglot→EMNIST. All
Omniglot
Omniglot→EMNIST
94.89 ± 0.45
99.12 ± 0.13
63.94 ± 0.87
86.00 ± 0.59
95.41 ± 0.39
99.38 ± 0.10
64.74 ± 0.82
87.31 ± 0.58
97.78 ± 0.30
99.37 ± 0.11
72.71 ± 0.79
87.60 ± 0.56
98.01 ± 0.30
99.15 ± 0.12
70.43 ± 0.80
87.04 ± 0.55
98.57 ± 0.19
99.53 ± 0.08
72.04 ± 0.83
88.24 ± 0.56
97.22 ± 0.33
99.30 ± 0.10
75.55 ± 0.87
88.94 ± 0.54
A4
BASELINE WITH 1-NN CLASSIFIER
softmax
1-NN
42.11±0.71
44.18±0.69
62.53±0.69
56.68±0.67
48.24±0.75
49.57±0.73
66.43±0.63
61.93±0.65
4The exact epoch of baseline and baseline++ on Omniglot and Omniglot→EMNIST is 5 epochs
A5
MAML AND MAML WITH FIRST-ORDER APPROXIMATION
A6
INTRA-CLASS VARIATION AND BACKBONE DEPTH
Resnet-10
Resnet-18
Resnet-34
Base class feature
Novel class feature
Davies-Bouldin index
A7
DETAILED STATISTICS IN EFFECTS OF INCREASING BACKBONE DEPTH
Figure A3: Few-shot classiﬁcation accuracy vs. backbone depth. In the CUB dataset  gaps
47.12±0.74
55.77±0.86
63.34±0.91
65.51±0.87
67.96±0.89
60.53±0.83
66.00±0.89
69.55±0.89
67.02±0.90
68.00±0.83
61.16±0.89
67.16±0.97
71.29±0.90
72.36±0.90
71.44±0.96
51.31±0.91
66.07±0.97
70.13±0.94
71.88±0.91
72.03±0.91
55.92±0.95
65.91±0.97
71.29±0.95
69.96±1.01
67.28±1.08
62.45±0.98
63.11±0.94
68.65±0.91
67.59±1.02
66.20±0.99
64.16±0.71
73.07±0.71
81.27±0.57
82.85±0.55
84.27±0.53
79.34±0.61
82.02±0.55
85.17±0.50
83.58±0.54
84.50±0.51
72.86±0.70
77.08±0.66
83.59±0.58
83.64±0.60
83.78±0.56
70.77±0.69
78.14±0.67
84.76±0.52
87.42±0.48
85.98±0.53
72.09±0.76
76.31±0.74
80.33±0.70
82.70±0.65
83.47±0.59
76.11±0.69
77.81±0.66
81.12±0.63
82.75±0.58
82.30±0.58
45.82±0.74
52.37±0.79
51.75±0.80
49.82±0.73
48.29±0.72
53.97±0.79
51.87±0.77
52.65±0.83
48.14±0.78
50.47±0.86
54.49±0.81
52.91±0.88
53.20±0.78
44.42±0.84
50.37±0.83
51.98±0.84
54.16±0.82
53.90±0.83
46.47±0.82
50.96±0.92
54.69±0.89
49.61±0.92
51.46±0.90
49.31±0.85
51.84±0.88
52.19±0.83
52.48±0.86
51.74±0.83
66.42±0.67
74.69±0.64
74.27±0.63
73.45±0.65
68.09±0.69
75.90±0.61
75.68±0.63
76.16±0.63
63.48±0.66
63.19±0.70
68.82±0.65
68.88±0.69
68.32±0.66
64.24±0.72
67.33±0.67
72.64±0.64
73.68±0.65
74.65±0.64
62.71±0.71
66.09±0.71
66.62±0.83
65.72±0.77
65.90±0.79
66.60±0.69
64.55±0.70
70.20±0.66
69.83±0.68
69.61±0.67
A8
MORE-WAY IN META-TESTING STAGE
N-way test
5-way
10-way
20-way
46.44±0.41
32.27±0.24
55.00±0.46
42.03±0.25
52.26±0.40
38.03±0.24
63.40±0.44
50.85±0.25
47.61±0.44
33.97±0.24
52.27±0.46
36.78±0.25
64.24±0.68
48.77±0.45
34.58±0.23
59.22±0.44
44.96±0.26
47.77±0.43
33.72±0.22
53.88±0.48
39.17±0.25"
